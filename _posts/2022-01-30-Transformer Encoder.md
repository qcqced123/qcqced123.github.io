---
title:  "Transformer Encoder"
excerpt: "Self-Attention을 비롯한 Transformer Encoder에 대한 설명"
date: 2022-01-30T21:10:00
categories:
  - Blog
last_modified_at: 2022-01-30T21:10:00-00:00
---

# 📝 Transformer: Encoder Block

---

## 📄 1.1 Introduce To Transformer

트랜스포머는 Google에서 2017년에 발표한 논문 ["Attention Is All You Need"](https://arxiv.org/abs/1706.03762?context=cs)"에 소개되는

**"자연어 처리용"** 신경망 모델입니다. 

기존 모델들이 가지고 있던 **"장기기억 의존성 문제"**를 해결해 오늘날 자연어 처리 모델의 베이스라인으로 선택되고 있습니다. (Google BERT, GPT-3 등)

자연어 처리 분야의 ResNet 정도라고 생각하시면 될 것 같습니다. :)

(ResNet은 **“Vanishing & Exploding Gradient”** 현상을 해결해 현대 컴퓨터 비전 분야의 발전을 이끌었습니다)

트랜스포머는 N개의 인코더 블록으로 구성된 **인코더 레이어**와 N개의 디코더 블록으로 구성된 **디코더 레이어**로 구성됩니다.

## **📄 1.2 Understand Transformer’s Encoder**

![                                     *출처: Attention Is All You Need*](/assets/images/transformer_encoder/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2022-09-20_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_9.04.14.png)


                                     *출처: Attention Is All You Need*

- **[Encoder]**
    
    그림의 왼쪽 부분은 트랜스포머의 인코더 블록 1개를 표현한 모습입니다.
    
    인코더 블록 N개를 쌓은 Layer를 **“Encoder”**라고 정의합니다.
    
    한편, N번째 인코더 블록의 결과값은 Inputs의 최종 표현 결과가 됩니다.
    
    인코더 블록의 구성요소를 살펴보면 1개의 **“멀티 헤드 어텐션 블록”,** 1개의 **“위치 임베딩 블록”,** 1개의 **“피드 포워드 신경망 블록”** 그리고 블록 사이와 마지막에 **“잔차 & 정규화 블록”**을 두고 있습니다.
    
    그렇다면 각 구성요소들은 인코더 블록에서 어떤 역할을 가지고 있고, 나아가 인코더 블록의 작동원리는 무엇인지 알아봅시다.
    
    - **[Self-Attention]**
        
        멀티 헤드 어텐션의 역할을 이해하기 위해서는 작동 원리인 **“Self-Attention”**에 대한 이해가 선행되어야 합니다.
        
        ![       *출처: Attention Is All You Need*](/assets/images/transformer_encoder/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2022-09-20_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_10.12.05.png)
        
               *출처: Attention Is All You Need*
        
        셀프 어텐션은 입력값 내부의 단어끼리 대응 시키고, **“단어 임베딩”**과 **“행렬의 내적”**을 활용하여 대응 관계의 유사도를 표현한 **“어텐션 행렬”**을 도출하는 과정을 의미합니다.
        
        ![                                 단어의 대응관계 표현                                                                            출처: [http://jalammar.github.io/illustrated-transformer/](http://jalammar.github.io/illustrated-transformer/)](/assets/images/transformer_encoder/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2022-09-20_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_9.04.14.png)
        
                                         단어의 대응관계 표현                                                                            출처: [http://jalammar.github.io/illustrated-transformer/](http://jalammar.github.io/illustrated-transformer/)
        
        셀프 어텐션을 수행하는 과정은 다음과 같이 6단계로 나뉩니다.
        
        - **Step 1. 입력값 ⇒ 입력 행렬 변환**
            
            입력된 문장을 벡터로 변환하여 입력 행렬로 변환합니다.
            
            **(입력 행렬 ⇒ “단어의 분산 표현” 혹은 “단어 임베딩”)**
            
            이 때, 입력 행렬의 차원은 **[입력값의 길이 x 임베딩 차원]** 으로 정의됩니다.
            
            논문에서는 임베딩 차원을 512로 설정하고 있습니다.
            
        - **Step 2. 쿼리, 키, 벨류 행렬 만들기**
            
            입력값의 특정 단어와 나머지 모든 단어 사이의 관계성(유사도)를 행렬에 담아내는 것은 컴퓨터에게 좋은 표현을 학습시키는 것에 도움이 됩니다. 이를 위해 셀프 어텐션은 입력 행렬 X와 임의로 초기화된 가중치 행렬 $W^Q,\ W^K,\ W^V$과의 곱을 통해  쿼리, 키, 벨류 행렬을 만들고 내적 연산을 수행하고 있습니다.
            
            $Q,\ K,\ V$ 행렬 역시 **[입력값의 길이 x 임베딩 차원]**로 차원이 정의, 행렬값은 초기에 임의로 설정된 가중치 행렬의 값이 신경망의 학습을 통해 갱신되며 함께 최적화되는 방식입니다. 
            
            $Q,\ K,\ V$**의 임베딩 차원은 사용자가 정의한 입력 행렬의 임베딩 차원과 멀티 헤드 어텐션의 개수에 따라서 상이합니다 .**
            
            **논문에서는 입력 행렬의 임베딩 차원을 512로 멀티 헤드 어텐션 개수를 8로 설정했기 때문에** 
            
            $Q,\ K,\ V$ **행렬의 차원은 64로 정의됩니다.**
            
            (입력 행렬 X에 대해서 단일 어텐션 행렬을 사용할 때와 여러 개를 사용할 때의 차원을 맞춰주기 위해서 입니다. 뒤에서 더 자세히 다루겠지만, 굳이 여러개의 어텐션 행렬을 사용하는 이유는 **“앙상블 학습”을 사용하는 이유**와 유사합니다)
            
        - **Step 3. $Q$ 행렬과 $K^T$ 행렬의 전치에 대한 내적(스칼라 곱)**
            
            두 행렬의 내적을 통해 **“유사도 행렬”**을 도출합니다. 
            
            전치를 수행하는 이유는 키 행렬의 차원을 뒤집어야 내적의 연산조건을 맞출 수 있기 때문입니다.
            
            한편, 도출된 유사도 행렬의 차원은 **[입력값의 길이 x 입력값의 길이]**가 됩니다.
            
        - **Step 4. 유사도 행렬의 스케일 조정**
            
            안정적인 신경망 학습을 위해서 유사도 행렬에 $K$ 행렬의 임베딩 차원의 제곱근 값을 나눠줍니다.
            
            고차원의 임베딩에 대한 내적 연산을 수행했기 때문에 유사도 행렬은 매우 큰 단위의 값을 가질 가능성이 높습니다. 
            
            $$
            Similarity = \frac{Q*K^T}{\sqrt{
            {\overset{}
              {d_{k}^{}}
            }}}
            $$
            
            이는 역전파 과정에서 기울기의 불안정성을 야기하기 때문에 학습에 악영향을 미칠 수 있습니다.  
            
            따라서 셀프 어텐션은 학습의 안정성을 확보하기 위해 유사도 행렬 값에 대한 스케일 조정을 수행하고 있습니다.
            
        - **Step 5. 정규화**
            
            Step 4에서 스케일 조정을 했지만, 여전히 유사도 행렬은 정규화 된 값이 아닙니다.
            
            셀프 어텐션은 신경망의 **“학습 속도 향상과 성능 개선”**을 위해서 유사도 행렬의 정규화를 수행하고 있습니다.
            
            $$
            Score\ Matrics=Softmax(\frac{Q*K^T}{\sqrt{
            {\overset{}
              {d_{k}^{}}
            }}})
            $$
            
            이렇게 도출된 결과를 **“스코어 행렬”**이라고 정의하고 있습니다.
            
        - **Step 6. 스코어 행렬 ⇒ 어텐션 행렬 변환**
            
            이제 드디어 행렬 삼총사 중에서 하나 남은 **“벨류 행렬”**을 사용할 차례가 되었습니다.
            
            벨류 행렬의 값에 스코어 행렬을 가중치로 활용하는 **“가중합”** 방식을 통해 **“어텐션 행렬”**을 도출합니다.
            
            구체적인 연산 과정은 아래 수식을 참고해주세요.
            
            **(스코어의 임베딩 차원 길이는 입력값의 길이 N, 벨류의 임베딩 차원은 논문에 따라 64입니다.**
            
            **두 행렬 모두 행 차원의 길이는 입력값의 길이 N에 해당, 다만 표현의 한계로 3까지만 표시했습니다.)**
            
            $$
            Score = \begin{vmatrix}
              0.90 & 0.07 & 0.03 & ..... \\
              0.025 & 0.95 & 0.025 & ..... \\
              0.21 & 0.03 & 0.76 & .....
            \end{vmatrix}\ ,\ V=\begin{vmatrix}
              67.85 & 90 & 91 & ..... \\
              62 & 40 & 50 & ..... \\
              37 & 41 & 20 & .....
            \end{vmatrix}
            $$
            
            $$
            Z = \begin{vmatrix}
              {\overset{}{z_{1}^{}}}\\
              {\overset{}{z_{2}^{}}}\\
              {\overset{}{z_{3}^{}}}\\
              .\\
              .\\
              .\\
              {\overset{}{z_{n}^{}}}\\
            \end{vmatrix}
            $$
            
            ${\overset{}{z_{1}^{}}} = {\overset{}{Score_{11}^{}}}({\overset{}{V_{11}^{}}}\ + \ {\overset{}{V_{12}^{}}}\ .....) \ + \ {\overset{}{Score_{12}^{}}}({\overset{}{V_{21}^{}}}\ + \ {\overset{}{V_{22}^{}}}\ .....)\ + \ .......$
            
            ${\overset{}{z_{2}^{}}} = {\overset{}{Score_{21}^{}}}({\overset{}{V_{11}^{}}}\ + \ {\overset{}{V_{12}^{}}}\ .....) \ + \ {\overset{}{Score_{22}^{}}}({\overset{}{V_{21}^{}}}\ + \ {\overset{}{V_{22}^{}}}\ .....)\ + \ .......$
            
            ……
            
            ${\overset{}{z_{n}^{}}} = {\overset{}{Score_{n1}^{}}}({\overset{}{V_{11}^{}}}\ + \ {\overset{}{V_{12}^{}}}\ .....) \ + \ {\overset{}{Score_{n2}^{}}}({\overset{}{V_{21}^{}}}\ + \ {\overset{}{V_{22}^{}}}\ .....)\ + \ .......$
            
            이제 어텐션 행렬의 의미를 살펴봅시다. 
            
            어텐션 행렬 내부 인덱스는 입력값의 단어 순서에 정확히 대응됩니다.
            
            예를 들어 입력값으로 “I am good”이라는 문장을 전달했다면,
            
            ${\overset{}{z_{1}^{}}}$⇒  i,  ${\overset{}{z_{2}^{}}}$ ⇒ am, ${\overset{}{z_{3}^{}}}$ ⇒ good
            
            이런식으로 입력값에 어텐션 행렬의 값을 맵핑하고 있고 그래서 단어 “i”와 가장 큰 유사도를 보이는 단어는 ${\overset{}{Score_{11}^{}}}$에 해당되는 단어 “i” 자기 자신이라고 할 수 있습니다.
            
            **(${\overset{}{z_{1}^{}}}$의 값 중에서 ${\overset{}{Score_{11}^{}}}$과 벨류 행렬의 가중합이 가장 큰 비중을 차지하고 있기 때문)**
            
            따라서 셀프 어텐션에서 **“어텐션 행렬”**은 전체 입력값의 맥락에서 특정 단어와 나머지 단어의 사이의 상대적인 유사도를 표현하는 역할을 하고 있습니다. 
            
            그래서 미니 배치 단위에서 윈도우 사이즈에 해당되는 단어와 유사도를 도출하는 기존 Word2Vec 방식보다 훨씬 풍부한 표현을 **“단어의 분산 표현”**에 담을 수 있습니다.
            
            **(셀프 어텐션은 입력값을 한꺼번에 처리하여 단어의 분산 표현을 도출하기 때문)**
            
    - **[Multi-Head Attention]**
        
        ![스크린샷 2022-09-21 오전 9.59.08.png](/assets/images/transformer_encoder/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2022-09-21_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%258C%25E1%2585%25A5%25E1%2586%25AB_9.59.08.png)
        
        ![                                          *출처: [http://jalammar.github.io/illustrated-transformer/](http://jalammar.github.io/illustrated-transformer/)*](/assets/images/transformer_encoder/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2022-09-21_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%258C%25E1%2585%25A5%25E1%2586%25AB_9.59.35.png)
        
                                                  *출처: [http://jalammar.github.io/illustrated-transformer/](http://jalammar.github.io/illustrated-transformer/)*
        
        Multi-Head Attention 블록은 Self-Attention을 여러 Attention-Head에서 독립적으로 수행하여 여러 개의 어텐션 행렬을 만들고 임베딩 차원 방향으로 이어 붙인 뒤, 임의로 초기화 된 가중치 행렬 $W^0$과의 내적 결과인 행렬 $Z$ 를 반환하는 역할을 하고 있습니다.
        
        가중치 행렬 $W^0$ 역시 신경망의 학습 과정에서 가중치의 갱신이 이뤄지며, 이어 붙인 어텐션 행렬과의 내적을 수행하는 이유는 행렬의 차원을 **“단일 어텐션 헤드”**를 사용할 때 반환되는 **“단일 어텐션 행렬의 차원”**과 동일하게 맞추기 위해서 입니다. 
        
        이어 붙인 어텐션 행렬의 차원은 **[입력값의 길이 x 어텐션 헤드의 개수]**가 되고, 내적 수행을 위해 가중치 행렬 $W^0$의 차원은 **[어텐션 헤드의 개수 x 단일 어텐션 행렬의 임베딩 차원]**이 됩니다.
        
    - **[[Positional Encoding]](https://wikidocs.net/162099)**
        
        Transformer Model에서 가장 중요한 블록을 선택한다면 저는 단연 **“Positional Encoding”**을 뽑을 것 같습니다.
        
        **“Self-Attention”**은 입력값을 병렬 처리하는 과정에서 시계열 처리를 하지 않기 때문에 단어의 순서 정보를 어텐션 행렬에 표현하지 못하는 단점을 가지고 있습니다. 
        
        자연어 처리에서 단어의 순서는 매우 중요한 정보입니다. 같은 토큰으로 구성된 문장이라도 순서에 따라서 그 의미는 완전히 달라질 수 있기 때문입니다. 
        
        예를 들어 “Samsung love Apple”과 “Apple love Samsung”라는 문장을 살펴봅시다.
        
        삼성과 애플이라는 명사의 순서만 바꿨을 뿐인데 행위의 주체와 대상이 뒤바뀌게 되어 명백히 다른 의미를 갖는 문장이 되어버립니다.
        
        이러한 문제를 극복하기 위해서 사용하는 것이 바로 **“위치 인코딩”** 입니다.
        
        **‘과연 실제로 셀프 어텐션은 단어의 순서 정보를 어텐션 행렬에 반영하지 못할까??’** 
        
        직관적으로 시계열과 관련된 아무런 처리가 이루어지지 않기 때문에 당연히 순서 정보를 반영하지 못하겠다고 생각이 들었지만… 눈으로 직접 결과를 확인해본 것이 아니기 때문에 무언가 찜찜했습니다. 
        
        이런 의문을 해결하기 위해 알아야 할 개념이 바로 **“Equivariant”**, **“Permutation Equivariant”**입니다
        
        - **[Equivariant]**
            
            **“Equivariant”**는 $y=f(x)$에서 $Input \ x$값이 변하면 $Output\ y$값 역시 변하는 것을 의미하는 용어입니다. 
            
            반대 의미로 사용되는 용어로는 **“Invariant”**가 있습니다. 이 용어는 $Input \ x$값이 변해도 $Output \ y$는 항상 동일함을 의미합니다
            
        - **[Permutation Equivariant]**
            
            **“Permutation Equivariant”**는 $Input \ x$값의 입력 Order 변화에 따라 $Output \ y$값의 출력 순서 역시 동일하게 바뀌는 성질을 의미합니다.
            
            반대 의미로 사용되는 용어로는 **“Permutation Invariant”**가 있습니다. 이 용어는 $Input\ x$값의 입력 순서 변화가 있어도 $Output \ y$값의 출력 순서가 변하지 않는 성질을 의미합니다.
            
        
        자연어 처리 맥락에서 위에 두 용어를 살펴봅시다. 자연어를 처리할 때 “단어의 등장 순서”가 달라지면 전체 문장의 의미가 달라질 수 있기 때문에 자연어 처리를 수행하는 모델은 반드시 **“Equivariant”**하고 **“Permutation Equivariant”**한 특성을 모두 가지고 있어야 합니다. 전체 문장의 의미가 달라진다는 것은 같은 단어 구성을 가졌지만 등장 순서가 다른 두 문장을 모델의 입력으로 전달할 때 반환되는 출력값이 변화한 순서를 반영하여 서로 다른 값을 가져야 하기 때문입니다. 
        
        **‘서로 다른 값”이라는 말은 정확히 무슨 의미일까’**
        
        처음에는 행렬을 구성하는 요소의 값은 동일하지만 등장 하는 순서만 변화 하여도 **“서로 다른 값”**이라고 생각했지만, 여러 블로그와 논문을 참고해보니, 행렬을 구성하는 요소의 값 자체도 변화해야 **“서로 다른 값”**으로 정의하는 것 같습니다. 
        
        **(서로 다른 값에 대해서 위와 같이 정의해야 셀프 어텐션에서 “사인파 위치 인코딩”을 사용하는 이유를 정확히 설명할 수 있다고 생각하여 저렇게 기술하였습니다. 어디까지나 제 생각이기 때문에 정확히 아시는 분 계시면 댓글로 피드백 부탁드립니다.)**
        
        다음은 같은 단어 구성을 가졌지만 등장 순서가 다른 두 문장에 셀프 어텐션을 적용하여 “어텐션 행렬”을 도출한 결과입니다.(동일한 단어 3개로 구성, 첫 번째 단어와 두 번째 단어의 순서롤 바끔)
        
        [Original Sentence’s Attention Mat]
        
        ![스크린샷 2022-09-22 오전 10.04.39.png](/assets/images/transformer_encoder/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2022-09-21_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_3.45.00.png)
        
        [Change Order Sentence’s Attention Mat]
        
        ![스크린샷 2022-09-22 오전 10.05.03.png](/assets/images/transformer_encoder/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2022-09-22_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%258C%25E1%2585%25A5%25E1%2586%25AB_10.04.39.png)
        
        두 결과를 자세히 살펴보면, 입력에서 첫 번째 단어와 두 번째 단어의 순서를 바꾼 것에 따라 “어텐션 행렬”의 출력값 역시 첫 번째 행과 두 번째 행의 값의 순서만 바뀐 모습을 볼 수 있습니다.
        
        다시 말해, 셀프 어텐션은 **“Invariant” & “Permutation Equivariant”**한 특성을 가지고 있고 실제로 단어 순서 정보를 반영하지 못한다고 볼 수 있습니다. 
        
        따라서 트랜스포머에 **“위치 인코딩”**을 사용하여 시계열 정보를 처리하는 것은 자명합니다.
        
        **다시 “위치 인코딩”을 살펴보겠습니다.**
        
        위치 인코딩 방식에는 여러가지가 있지만 논문에서 소개한 **“Sinusoid Positional Encoding”**을 중심으로 설명하겠습니다.
        
        **“Sinusoid Positional Encoding”**은 삼각함수를 활용하는 위치 인코딩의 한 종류입니다.
        
        그렇다면 왜 트랜스포머의 저자들은 **“사인파 인코딩 방식”**을 선택했을까요?
        
        ~~(논문의 위치 인코딩에 대한 설명이 매우 불친절하여 🤬 여러 블로그를 참고하여 작성했습니다)~~
        
        [블로그에 따르면 위치 인코딩 방식을 선정할 때 반드시 고려해야 할 규칙이 2가지 있다고 합니다.](https://kazemnejad.com/blog/transformer_architecture_positional_encoding/)
        
        - [**1) 시퀀스에 상관없이 동일한 인덱스 값을 갖는 토큰은 같은 위치 인코딩 값을 가져야 합니다.](https://www.blossominkyung.com/deeplearning/transfomer-positional-encoding)**
            
            ![출처: [https://www.blossominkyung.com/deeplearning/transfomer-positional-encoding](https://www.blossominkyung.com/deeplearning/transfomer-positional-encoding)](/assets/images/transformer_encoder/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2022-09-22_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%258C%25E1%2585%25A5%25E1%2586%25AB_10.05.03.png)
            
            출처: [https://www.blossominkyung.com/deeplearning/transfomer-positional-encoding](https://www.blossominkyung.com/deeplearning/transfomer-positional-encoding)
            
        - [**2) 위치 인코딩의 실제 값은 임베딩 벡터의 요소 값에 비해 너무 크지 않아야 합니다.**](https://www.blossominkyung.com/deeplearning/transfomer-positional-encoding)
            
            ![출처: [https://www.blossominkyung.com/deeplearning/transfomer-positional-encoding](https://www.blossominkyung.com/deeplearning/transfomer-positional-encoding)](/assets/images/transformer_encoder/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2022-09-25_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_8.35.22.png)
            
            출처: [https://www.blossominkyung.com/deeplearning/transfomer-positional-encoding](https://www.blossominkyung.com/deeplearning/transfomer-positional-encoding)
            
            위치 인코딩 값을 정수 단위의 배열 인덱스로 설정하는 상황을 가정하겠습니다.
            입력 문장의 시퀀스가 200이라고 했을 때, 해당 문장의 처음과 끝의 위치 인코딩 값의 차이는 200이 됩니다. 이것을 그대로 입력값의 단어 벡터값에 전달하게 되면 **“단어 벡터값 + 위치 인코딩 값”**에서 위치 인코딩 값이 차지하는 비율이 너무 커지게 되어 단어 벡터값의 의미가 (단어의 유사성 & 기타 복잡한 패턴) 희석될 수 있습니다. 또한 **“단어 벡터값 + 위치 인코딩 값”**이 너무 큰 단위가 되면서 역전파 도중 **“Vanishing & Exploding Gradient”** 문제를 야기할 가능성이 발생합니다.
            
        
        **왜 사인 & 코사인 함수는 트랜스포머의 위치 인코딩 방식에 적합할까요??**
        
        사인함수는 [-1, 1] 범위에서 결과값을 출력하기 때문에 위치 임베딩 값이 너무 커지지 않아 역전파 도중 발생하는 **“Vanishing & Exploding Gradient”** 문제를 예방할 수 있습니다. 또한 단어 임베딩 값을 희석시키지 않아 유의미한 학습 결과를 도출할 수 있습니다. 
        
        하지만 주기성을 갖는 사인 & 코사인 함수가 어떻게 1번 조건을 충족시킬 수 있을지 의문이 듭니다. 주기가 반복되면서 서로 다른 위치에 있는 토큰들이 같은 위치 임베딩 값을 갖는 경우가 발생하기 때문입니다.
        
        **사인파 위치 인코딩은 1번 조건을 어떻게 만족시킬까??**
        
        논문의 저자들은 1번 조건을 만족시키기 위해 다음과 같은 조건을 설정합니다.
        
        **[“입력값 임베딩 차원 길이” == “위치 인코딩 임베딩 차원 길이”](https://www.youtube.com/watch?v=-jze8IC-hI0&ab_channel=MachineLearningwithPytorch)**
        
        $$
        PE(pos, 2i) = sin(pos/\overset{}
          {10000_{}^{2i/dmodel}}) \\
        PE(pos, 2i+1) = cos(pos/\overset{}
          {10000_{}^{2i/dmodel}})
        $$
        
        1) $pos$ ⇒ 토큰 위치, 인덱스 
        
        2) $i$ ⇒ 위치 인코딩의 임베딩 차원 인덱스
        
        3) $dmodel$ ⇒ 입력값 & 위치 인코딩의 임베딩 차원의 길이 (논문 ⇒ 512) 
        
        다음 수식은 논문에서 제시하는 트랜스포머의 위치 인코딩 수식입니다. 수식을 참고하면서 논문의 저자들이 설정한 조건이 무슨 의미인지 알아봅시다. 
        
        하나의 토큰은 **2개의 삼각함수**를 갖게 됩니다. 또한 토큰의 인덱스를 활용해 삼각함수의 주기성을 조절하여 서로 다른 위치에 있는 토큰들이 결정적이며 적당히 차이나는, 그러면서 정규화하기에 용이한 값을 도출하도록 설정했습니다. 그 결과 전체 위치 인코딩 행렬은 토큰의 개수만큼 서로 다른 위치 인코딩 값(삼각함수 쌍)을 갖게 됩니다.
        
        ![                         출처: [https://gaussian37.github.io/dl-concept-positional_encoding/](https://gaussian37.github.io/dl-concept-positional_encoding/)](/assets/images/transformer_encoder/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2022-09-25_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_9.41.09.png)
        
                                 출처: [https://gaussian37.github.io/dl-concept-positional_encoding/](https://gaussian37.github.io/dl-concept-positional_encoding/)
        
        따라서 $Input X$에 대한 위치 인코딩 행렬은 **“토큰 길이 x 2”** 만큼의 서로 다른 삼각함수를 갖게 됩니다. 
        
        아래 그림은 50개의 토큰과 256의 임베딩 차원 길이를 갖는 입력값 행렬에 대한  트랜스포머의 위치 인코딩 방식을 시각화한 모습입니다.
        
        ![                                         *출처: [https://wikidocs.net/162099](https://wikidocs.net/162099)*](/assets/images/transformer_encoder/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2022-10-09_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_4.54.53.png)
        
                                                 *출처: [https://wikidocs.net/162099](https://wikidocs.net/162099)*
        
        개별 토큰(Y축)이 서로 다른 색상 주기를 가지고 파동을 왼쪽에서 오른쪽으로 내보내고 있습니다. **또한 마치 파도가 뻗어나가듯한 모습의 파장의 주기가 토큰의 인덱스가 커질수록 짧아지고 있습니다.** 토큰의 인덱스 값이 커지면서 삼각함수의 주기성이 짧아지는 성질을 시각적으로 잘 표현하고 있다고 볼 수 있겠습니다. 
        
        아래 예시 그래프를 함께 살펴보시면 더욱 이해가 쉬우실 듯 합니다.
        
        **1) Green Color Graph  $= sin(1/\overset{}
          {10000_{}^{2i/dmodel}})$**
        
        **2) Orange Color Graph $= sin(50/\overset{}
          {10000_{}^{2i/dmodel}})$**
        
        ![스크린샷 2022-10-09 오후 4.54.53.png](/assets/images/transformer_encoder/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2022-10-09_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_5.13.48.png)
        
        이렇게 도출된 위치 인코딩 행렬(입력값 행렬의 차원과 동일)을 **“행렬의 합”** 연산으로 입력값 행렬에 전달하게 됩니다. **“행렬의 합”**은 **‘피연산 대상의 정보를 모두 반영하여 결과를 구성하겠다’** 정도로 생각하면 될 것 같습니다.
        
        **~~[(기하적 원리를 설명한 포스트를 레딧에서 찾았긴 했지만, 제가 정확히 이해하기엔 너무 어려워 일단 위와 같이 간단히 이해하고 넘어가기로 했습니다. 혹시 필요하신 분들을 위해 링크 걸었습니다. 이해하시고 저도 좀 알려주세요…ㅠㅠ)](https://www.reddit.com/r/MachineLearning/comments/cttefo/comment/exs7d08/)~~**
        
    - **[Add & Norm]**
        - **1) Add Block**
            
            ResNet의 Skip-Connection Block을 활용한 **“Residual Structure”**를 의미합니다.$Input\  X$와 Multi-Head Attention의 결과값을 연결하여 “Layer Normalization”에 전달하는 역할을 하고 있습니다. 
            
            [**(“Residual Structure”에 대한 자세한 설명은 생략하겠습니다. 링크를 참고하세요!)**](https://lswook.tistory.com/105)
            
        - **2) Layer Normalization Block**
            
            학습의 안정성(기울기의 안정성)과 속도 향상을 위해 인코더의 서브 레이어 블록들(멀티 헤드 어텐션, 피드포워드) 다음에 배치하는 계층입니다.
            
            ![Untitled](/assets/images/transformer_encoder/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2022-10-09_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_5.13.48.png)
            
            **“Batch Normalization” vs “Layer Normalization”**
            
            **“Batch Normalization”**은 이전 Layer에게 전달 받은 Input을 “Mini-Batch” 단위로 묶고 동일한 인덱스를 갖는 모든 Feature에 대한 평균과 표준편차를 구하여 정규화를 진행합니다. 
            
            예를 들어 이전 Layer에서 입력 행렬로 다음과 같은 사이즈의 행렬([4 x 3 x 256 x 256])을 전달했다고 가정해보겠습니다. 그렇다면 배치 정규화 계층은 4개의 Data Instance에 대한 **“3 x 256 x 256 개”**의 서로 다른 평균과 표준편차를 구하여 정규화를 하게 됩니다. 
            
            한편, 미니 배치 단위로 묶어서 정규화를 진행하기 때문에 배치 정규화에는 다음과 같은 문제 상황이 발생합니다. 
            
            - Case **1. 정규화가 불가능한 상황**
                
                극단적으로 미니 배치 사이즈가 1인 상황을 가정해보겠습니다. 
                
                미니 배치 단위로 통계값을 구하는 배치 정규화 알고리즘은 입력값이 1개만 존재하는 대상에 대해서 평균과 표준편차를 구할 수 없습니다. 따라서 정규화가 불가능합니다. 현실에서 미니 배치 사이즈를 1로 설정하는 경우가 그렇게 많지 않겠지만, 현대 신경망 모델의 사이즈가 점차 거대해지는 것을 감안하면 미니 배치 사이즈를 1로 설정하여 학습을 진행하는 경우도 빈번하게 발생할 수 있습니다.
                
                **~~(실제 Kaggle Code Competition에서 현대 딥러닝 모델을 사용하는 경우 컴퓨팅 리소스의 한계로 미니 배치 사이즈를 1로 설정할 때도 있습니다. 저의 경험입니다 ㅎㅎ)~~**
                
            - **Case 2. 입력값의 시퀀스 길이가 일정하지 않는 경우**
                
                Image를 다루는 Convolution의 경우 대부분 입력값의 형태가 고정되어 있습니다. 따라서 배치 정규화처럼 미니 배치 단위로 데이터를 묶어서 정규화를 진행해도 입력값의 길이가 모두 동일하기 때문에 안정적인 정규화 결과를 도출하는 것이 가능합니다.
                
                **~~[(애초에 원 논문을 살펴보면 Image Classification Task의 학습 효율 개선을 목적으로 고안된 알고리즘이라는 것을 알 수 있습니다.)](https://arxiv.org/pdf/1502.03167.pdf)~~**
                
                한편 시계열, 자연어와 같은 시퀀스를 갖는 데이터의 경우 입력값의 형태가 고정되어 있지 않는 경우가 빈번합니다. 입력값의 길이가 다르기 때문에 미니 배치 단위로 묶어서 정규화를 하면 **“Case 1”**과 같은 상황이 발생하여 정규화 결과가 불안정하고 이는 결국 학습의 안정성을 떨어뜨리고 성능을 저해하는 요소로 작용하게 됩니다.
                
                이러한 문제를 개선하는 것을 목표로 고안된 알고리즘이 바로 **”Layer Normalization”**입니다. 
                
            
            ![스크린샷 2022-10-09 오후 5.53.01.png](/assets/images/transformer_encoder/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2022-11-11_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%258C%25E1%2585%25A5%25E1%2586%25AB_12.21.50.png)
            
            **“Layer Normalization”**은 이전 Layer에게 전달 받은 Input을 “Data Instance” 단위로 묶어 한 개의 입력값 단위가 가지는 모든 Feature에 대한 평균과 표준편차를 구하여 정규화를 진행합니다. 
            
            이번에도 역시 이전 Layer에서 ([4 x 3 x 256 x 256]) 사이즈의 행렬을 전달받았다고 가정하겠습니다. 그렇다면 레이어 정규화 계층은 “3 x 256 x 256개”의 서로 다른 Feature에 대한 **“4개”**의 서로 다른 평균과 표준편차를 구하여 정규화를 하게 됩니다. 
            
            위 내용들을 정리하자면 두 방법론 모두 정규화 알고리즘을 사용하여 **Internal Covariate Shift(내부 공변량 이동)**를 해결하고자 했지만, 정규화 하는 대상과 범위가 다르다는 것입니다. 따라서 각자의 방법을 사용하기에 적합한 Data & Task가 달라지게 됩니다.
            
            **~~[(사실 두 방법론 모두 내부 공변량 문제를 해결한 것은 아니라는 것이 2018년에 밝혀졌습니다. 이에 대한 내용은 다른 포스팅에서 자세히 다루겠습니다.)](https://arxiv.org/pdf/1805.11604.pdf)~~**
            
    - **[Position-Wise Feed Forward Network]**
        
        ![                             *출처: 고려대학교 강필성 교수님 “Text Analytics”*](/assets/images/transformer_encoder/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2022-11-11_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%258C%25E1%2585%25A5%25E1%2586%25AB_12.22.51.png)
        
                                     *출처: 고려대학교 강필성 교수님 “Text Analytics”*
        
        Fully Connected Neural Network 구조를 가진 인코더의 서브 레이어의 한 종류를 의미합니다. 인코더의 Feed Forward Network를 수식으로 표현하면 다음과 같습니다.
        
        $$
        FFN(x) = max(0,\ x{\overset{}{W_{1}^{}}} + {\overset{}{b_{1}^{}}}){\overset{}{W_{2}^{}}} + {\overset{}{b_{2}^{}}}
        $$
        
        괄호 안의 식부터 해석해보자면, 입력값 $x$를 FCN Layer에 입력하여 얻은 결과를 다시 “Relu Function”에 전달하여 결과를 얻습니다. 활성 함수를 통해 얻은 출력을 다시 한 번 FCN Layer에 전달하여 “피드 포워드 네트워크”의 최종 출력값을 얻게 됩니다.
        
        ![스크린샷 2022-11-11 오전 12.22.51.png](/assets/images/transformer_encoder/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2022-11-11_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%258C%25E1%2585%25A5%25E1%2586%25AB_12.40.16.png)
        
        한편 인코더의 “피드 포워드 네트워크”는 다음과 같은 2가지 성질을 모두 만족해야 합니다.
        
        - **1) The Linear Transformations are the same across different position**
            
            동일한 인코더 구조 내부의 FCN 구조와 같은 서브 레이어 선상에 존재하는  파라미터 값은 동일해야 합니다. 위 그림을 예시로 구체적인 설명을 하겠습니다. 동일한 인코더 블록에서  ${\overset{}{Z_{1}^{}}},\ {\overset{}{Z_{2}^{}}}$를 입력값으로 전달받는 FFN Layer의 Structure는 토큰의 포지션(${\overset{}{Z_{1}^{}}},\ {\overset{}{Z_{2}^{}}}$)에 관계없이 모두 동일해야 한다는 것을 의미며 같은 선상에 존재하는 ${\overset{}{W_{1}^{}}},\ {\overset{}{b_{1}^{}}}$값 역시 토큰의 포지션에 관계없이 모두 동일해야 합니다. 
            
        - **2) They use different parameters from Layer to Layer**
            
            한편 인코더 블록의 파라미터 값과 네트워크 구조는 개별 블록마다 서로 상이할 수 있습니다.
            
        
        FFN Layer의 구체적인 작동 방식을 살펴보겠습니다. 
        
        ![스크린샷 2022-11-11 오전 12.40.16.png](/assets/images/transformer_encoder/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2022-11-11_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%258C%25E1%2585%25A5%25E1%2586%25AB_12.49.03.png)
        
        위에 수식을 해석하면서 살펴 보았듯, 인코더의 FFN Layer는 이중 선형 레이어를 가지고 있습니다.
        
        바로 위에 보이는 그림은 첫번째 선형 레이어에서 일어나는 일을 시각화한 모습입니다. CNN에서 사이즈가 1인 Filter를 사용하는 것처럼 크기가 1인 2048개의 Filter를 사용해 개별 필터에서 얻은 결과값들을 어텐션 행렬의 행 개수만큼 반환하고 나머지 필터의 사용 결과와 함께 이어붙여 총 [2, 2048] 의 행렬이 탄생하게 됩니다.
        
        ![스크린샷 2022-11-11 오전 12.49.03.png](/assets/images/transformer_encoder/Untitled.png)
        
        다음은 두번째 선형 레이어 입니다. 첫번째 레이어에서 전달해준 행렬을 다시 FFN Layer 입력값 차원과 동일하게 맞춰주기 위해 512개의 Filter를 사용해 개별 필터에서 얻은 결과값들을 이어붙여 총 [2, 512]의 행렬이 다시 탄생하게 됩니다.
        

지금까지 Transformers의 Encoder Block과 Sub-Layer Block들의 세부 동작 원리에 대해서 살펴보았습니다. 추가로 기억하고 계시면 좋을 것 같은 내용은 Encoder가 기본적으로 문장 전체를 반영하여 토큰을 벡터로 표현하고 있기 때문에 “Bi-Directional Representation”한 성격을 지녔다는 것입니다. 인코더의 쌍방향적 성질은 BERT 계열 모델이 고안되어 다양한 자연어 처리 Task에서 널리 활용되고 있는 이유가 되었습니다. 뒤에서 자세히 설명 드리겠지만, 디코더는 모델 구조의 특성상 순방향적인 학습 절차를 갖고 있습니다. 디코더의 순방향적 특징은 BERT와 함께 현대 언어 모델의 양대 산맥으로 불리는 GPT 계열의 모델이 고안된 이유가 되었습니다. 이렇듯 인코더와 디코더가 서로 다른 학습 절차를 갖고 있다는 사실은 오늘날 자연어 처리 연구의 흐름을 파악할 수 있는 중요한 성질이 되기 때문에 꼭 기억하시면 좋을 것 같습니다. 

Transformers Introduce에서 설명드렸듯, 이제 Decoder에 대해서 알아볼 차례입니다. 인코더에 대한 내용이 너무 길어져, 디코더 내용은 다른 포스팅에 올리도록 하겠습니다.