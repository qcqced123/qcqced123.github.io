---
title: "ğŸ”¥Â Pytorch Tensor Indexing ìì£¼ ì‚¬ìš©í•˜ëŠ” ë©”ì„œë“œ ëª¨ìŒì§‘"
excerpt: "íŒŒì´í† ì¹˜ì—ì„œ ìì£¼ ì‚¬ìš©í•˜ëŠ” í…ì„œ ì¸ë±ì‹± ê´€ë ¨ ë©”ì„œë“œ ëª¨ìŒ"
permalink: "/framework-library/"
toc: true  # option for table of content
toc_sticky: true  # option for table of content
categories:
  - Framework & Library
tags:
  - Pytorch
  - Tensor
  - Linear Algebra
  
last_modified_at: 2023-08-04T12:00:00-05:00
---
íŒŒì´í† ì¹˜ì—ì„œ í•„ìê°€ ìì£¼ ì‚¬ìš©í•˜ëŠ” í…ì„œ ì¸ë±ì‹± ê´€ë ¨ ë©”ì„œë“œì˜ ì‚¬ìš©ë²• ë° ì‚¬ìš© ì˜ˆì‹œë¥¼ í•œë°©ì— ì •ë¦¬í•œ í¬ìŠ¤íŠ¸ë‹¤. ë©”ì„œë“œ í•˜ë‚˜ë‹¹ í•˜ë‚˜ì˜ í¬ìŠ¤íŠ¸ë¡œ ë§Œë“¤ê¸°ì—ëŠ” ë„ˆë¬´ ê¸¸ì´ê°€ ì§§ë‹¤ ìƒê°í•´ í•œ í˜ì´ì§€ì— ëª¨ë‘ ë„£ê²Œ ë˜ì—ˆë‹¤. ì§€ì†ì ìœ¼ë¡œ ì—…ë°ì´íŠ¸ ë  ì˜ˆì •ì´ë‹¤. ë˜í•œ í…ì„œ ì¸ë±ì‹± ë§ê³ ë„ ë‹¤ë¥¸ ì£¼ì œë¡œë„ ê´€ë ¨ ë©”ì„œë“œë¥¼ ì •ë¦¬í•´ ì˜¬ë¦´ ì˜ˆì •ì´ë‹ˆ ë§ì€ ê´€ì‹¬ ë¶€íƒë“œë¦°ë‹¤.

### `ğŸ”Â torch.argmax`

ì…ë ¥ í…ì„œì—ì„œ ê°€ì¥ í° ê°’ì„ ê°–ê³  ìˆëŠ” ì›ì†Œì˜ ì¸ë±ìŠ¤ë¥¼ ë°˜í™˜í•œë‹¤. ìµœëŒ€ê°’ì„ ì°¾ì„ ì°¨ì›ì„ ì§€ì •í•´ì¤„ ìˆ˜ ìˆë‹¤. ì•„ë˜ ì˜ˆì‹œ ì½”ë“œë¥¼ í™•ì¸í•´ë³´ì.

```python
# torch.argmax params
torch.argmax(tensor, dim=None, keepdim=False)

# torch.argmax example 1
test = torch.tensor([1,29,2,45,22,3])
torch.argmax(test)
torch.argmax(test2, keepdim=True)

<Result>
tensor(3)

# torch.argmax example 2
test = torch.tensor([[4, 2, 3],
                     [4, 5, 6]])

torch.argmax(test, dim=0, keepdim=True)
<Result>
tensor([[0, 1, 1]])

# torch.argmax example 3
test = torch.tensor([[4, 2, 3],
                     [4, 5, 6]])

torch.argmax(test, dim=1, keepdim=True)
tensor([[0],
        [2]])
```

`dim` ë§¤ê°œë³€ìˆ˜ì— ì›í•˜ëŠ” ì°¨ì›ì„ ì…ë ¥í•˜ë©´ í•´ë‹¹ ì°¨ì› ë·°ì—ì„œ ê°€ì¥ í° ì›ì†Œë¥¼ ì°¾ì•„ ì¸ë±ìŠ¤ ê°’ì„ ë°˜í™˜í•´ì¤„ ê²ƒì´ë‹¤. ì´ ë•Œ `keepdim=True` ë¡œ ì„¤ì •í•œë‹¤ë©´ ì…ë ¥ ì°¨ì›ì—ì„œ ê°€ì¥ í° ì›ì†Œì˜ ì¸ë±ìŠ¤ë¥¼ ë°˜í™˜í•˜ë˜ ì›ë³¸ í…ì„œì˜ ì°¨ì›ê³¼ ë™ì¼í•œ í˜•íƒœë¡œ ì¶œë ¥í•´ì¤€ë‹¤. `example 2` ì˜ ê²½ìš° `dim=0` ë¼ì„œ í–‰ì´ ëˆ„ì ëœ ë°©í–¥ìœ¼ë¡œ í…ì„œë¥¼ ë°”ë¼ë´ì•¼ í•œë‹¤. í–‰ì´ ëˆ„ì ëœ ë°©í–¥ìœ¼ë¡œ í…ì„œë¥¼ ë³´ê²Œ ë˜ë©´ `tensor([[0, 1, 1]])`ì´ ëœë‹¤.

### `ğŸ“šÂ torch.stack`

```python
"""
torch.stack
Args:
	tensors(sequence of Tensors): í…ì„œê°€ ë‹´ê¸´ íŒŒì´ì¬ ì‹œí€€ìŠ¤ ê°ì²´
	dim(int): ì¶”ê°€í•  ì°¨ì› ë°©í–¥ì„ ì„¸íŒ…, ê¸°ë³¸ê°’ì€ 0
"""
torch.stack(tensors, dim=0)
```

ë§¤ê°œë³€ìˆ˜ë¡œ ì£¼ì–´ì§„ íŒŒì´ì¬ ì‹œí€€ìŠ¤ ê°ì²´(ë¦¬ìŠ¤íŠ¸, íŠœí”Œ)ë¥¼ ì‚¬ìš©ìê°€ ì§€ì •í•œ ìƒˆë¡œìš´ ì°¨ì›ì— ìŒ“ëŠ” ê¸°ëŠ¥ì„ í•œë‹¤. ë§¤ê°œë³€ìˆ˜ `tensors` ëŠ” í…ì„œê°€ ë‹´ê¸´ íŒŒì´ì¬ì˜ ì‹œí€€ìŠ¤ ê°ì²´ë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ëŠ”ë‹¤. `dim` ì€ ì‚¬ìš©ìê°€ í…ì„œ ì ì¬ë¥¼ í•˜ê³  ì‹¶ì€ ìƒˆë¡œìš´ ì°¨ì›ì„ ì§€ì •í•´ì£¼ë©´ ëœë‹¤. ê¸°ë³¸ê°’ì€ 0ì°¨ì›ìœ¼ë¡œ ì§€ì • ë˜ì–´ìˆìœ¼ë©°, í…ì„œì˜ ë§¨ ì•ì°¨ì›ì´ ìƒˆë¡­ê²Œ ìƒê¸°ê²Œ ëœë‹¤. `torch.stack` ì€ ê¸°ê³„í•™ìŠµ, íŠ¹íˆ ë”¥ëŸ¬ë‹ì—ì„œ ì •ë§ ìì£¼ ì‚¬ìš©ë˜ê¸° ë•Œë¬¸ì— ì‚¬ìš©ë²• ë° ì‚¬ìš©ìƒí™©ì„ ìµí˜€ë‘ë©´ ë„ì›€ì´ ëœë‹¤. ì˜ˆì‹œë¥¼ í†µí•´ í•´ë‹¹ ë©”ì„œë“œë¥¼ ì–´ë–¤ ìƒí™©ì—ì„œ ì–´ë–»ê²Œ ì‚¬ìš©í•˜ëŠ”ì§€ ì•Œì•„ë³´ì.

```python
""" torch.stack example """

class Projector(nn.Module):
    """
    Making projection matrix(Q, K, V) for each attention head
    When you call this class, it returns projection matrix of each attention head
    For example, if you call this class with 8 heads, it returns 8 set of projection matrices (Q, K, V)
    Args:
        num_heads: number of heads in MHA, default 8
        dim_head: dimension of each attention head, default 64
    """
    def __init__(self, num_heads: int = 8, dim_head: int = 64) -> None:
        super(Projector, self).__init__()
        self.dim_model = num_heads * dim_head
        self.num_heads = num_heads
        self.dim_head = dim_head

    def __call__(self):
        fc_q = nn.Linear(self.dim_model, self.dim_head)
        fc_k = nn.Linear(self.dim_model, self.dim_head)
        fc_v = nn.Linear(self.dim_model, self.dim_head)
        return fc_q, fc_k, fc_v

num_heads = 8
dim_head = 64
projector = Projector(num_heads, dim_head)  # init instance
projector_list = [list(projector()) for _ in range(num_heads)]  # call instance
x = torch.rand(10, 512, 512) # x.shape: [Batch_Size, Sequence_Length, Dim_model]
Q, K, V = [], [], []

for i in range(num_heads):
    Q.append(projector_list[i][0](x)) # [10, 512, 64]
    K.append(projector_list[i][1](x)) # [10, 512, 64]
	  V.append(projector_list[i][2](x)) # [10, 512, 64]
 
Q = torch.stack(Q, dim=1) # Q.shape: [10, 8, 512, 64]
K = torch.stack(K, dim=1) # K.shape: [10, 8, 512, 64]
V = torch.stack(V, dim=1) # V.shape: [10, 8, 512, 64]
```

ìœ„ ì½”ë“œëŠ” `Transformer` ì˜ `Multi-Head Attention` êµ¬í˜„ì²´ ì¼ë¶€ë¥¼ ë°œì·Œí•´ì˜¨ ê²ƒì´ë‹¤. `Multi-Head Attention` ì€ ê°œë³„ ì–´í…ì…˜ í•´ë“œë³„ë¡œ í–‰ë ¬ $Q, K, V$ë¥¼ ê°€ì ¸ì•¼ í•œë‹¤. ë”°ë¼ì„œ ì…ë ¥ ì„ë² ë”©ì„ ê°œë³„ ì–´í…ì…˜ í—¤ë“œì— `Linear Combination` í•´ì¤˜ì•¼ í•˜ëŠ”ë° í—¤ë“œ ê°œìˆ˜ê°€ 8ê°œë‚˜ ë˜ê¸° ë•Œë¬¸ì— ê°œë³„ì ìœ¼ë¡œ `Projection Matrix` ë¥¼ ì„ ì–¸í•´ì£¼ëŠ” ê²ƒì€ ë§¤ìš° ë¹„íš¨ìœ¨ì ì´ë‹¤. ë”°ë¼ì„œ ê°ì²´  `Projector` ì— í–‰ë ¬ $Q, K, V$ì— ëŒ€í•œ `Projection Matrix` ë¥¼ ì •ì˜í•´ì¤¬ë‹¤. ì´í›„ í—¤ë“œ ê°œìˆ˜ë§Œí¼ ê°ì²´  `Projector` ë¥¼ í˜¸ì¶œí•´ ë¦¬ìŠ¤íŠ¸ì— í•´ë“œë³„ `Projection Matrix` ë¥¼ ë‹´ì•„ì¤€ë‹¤. ê·¸ ë‹¤ìŒ `torch.stack`ì„ ì‚¬ìš©í•´ `Attention Head` ë°©í–¥ì˜ ì°¨ì›ìœ¼ë¡œ ë¦¬ìŠ¤íŠ¸ ë‚´ë¶€ í…ì„œë“¤ì„ ìŒ“ì•„ì£¼ë©´ ëœë‹¤.

### `ğŸ”¢Â torch.arange`

ì‚¬ìš©ìê°€ ì§€ì •í•œ ì‹œì‘ì ë¶€í„° ëì ê¹Œì§€ ì¼ì •í•œ ê°„ê²©ìœ¼ë¡œ í…ì„œë¥¼ ë‚˜ì—´í•œë‹¤. Pythonì˜ ë‚´ì¥ ë©”ì„œë“œ `range`ì™€ ë™ì¼í•œ ì—­í• ì„ í•˜ëŠ”ë°, ëŒ€ì‹  í…ì„œ ê·¸ ê²°ê³¼ë¥¼ í…ì„œ êµ¬ì¡°ì²´ë¡œ ë°˜í™˜í•œë‹¤ê³  ìƒê°í•˜ë©´ ë˜ê² ë‹¤.

```python
# torch.arange usage
torch.arange(start=0, end, step=1)

>>> torch.arange(5)
tensor([ 0,  1,  2,  3,  4])

>>> torch.arange(1, 4)
tensor([ 1,  2,  3])

>>> torch.arange(1, 2.5, 0.5)
tensor([ 1.0000,  1.5000,  2.0000])
```

`step` ë§¤ê°œë³€ìˆ˜ë¡œ ì›ì†Œê°„ ê°„ê²© ì¡°ì •ì„ í•  ìˆ˜ ìˆëŠ”ë°, ê¸°ë³¸ì€ 1ë¡œ ì§€ì • ë˜ì–´ ìˆìœ¼ë‹ˆ ì°¸ê³ í•˜ì. í•„ìì˜ ê²½ìš°ì—ëŠ” `nn.Embedding`ì˜ ì…ë ¥ í…ì„œë¥¼ ë§Œë“¤ ë•Œ ê°€ì¥ ë§ì´ ì‚¬ìš©í–ˆë‹¤. `nn.Embedding` ì˜ ê²½ìš° Inputìœ¼ë¡œ `IntTensor`, `LongTensor`ë¥¼ ë°›ê²Œ ë˜ì–´ ìˆìœ¼ë‹ˆ ì•Œì•„ë‘ì. 

### `ğŸ”Â torch.repeat`

ì…ë ¥ê°’ìœ¼ë¡œ ì£¼ì–´ì§„ í…ì„œë¥¼ ì‚¬ìš©ìê°€ ì§€ì •í•œ ë°˜ë³µ íšŸìˆ˜ë§Œí¼ íŠ¹ì • ì°¨ì› ë°©í–¥ìœ¼ë¡œ ëŠ˜ë¦°ë‹¤. ì˜ˆë¥¼ ë“¤ë©´ `[1,2,3] * 3`ì˜ ê²°ê³¼ëŠ” `[1, 2, 3, 1, 2, 3, 1, 2, 3]` ì¸ë°, ì´ê²ƒì„ ì‚¬ìš©ìê°€ ì§€ì •í•œ ë°˜ë³µ íšŸìˆ˜ë§Œí¼ íŠ¹ì • ì°¨ì›ìœ¼ë¡œ ìˆ˜í–‰í•˜ê² ë‹¤ëŠ” ê²ƒì´ë‹¤. ì•„ë˜ ì‚¬ìš© ì˜ˆì œë¥¼ í™•ì¸í•´ë³´ì.

```python
# torch.repeat example

>>> x = torch.tensor([1, 2, 3])
>>> x.repeat(4, 2)
tensor([[ 1,  2,  3,  1,  2,  3],
        [ 1,  2,  3,  1,  2,  3],
        [ 1,  2,  3,  1,  2,  3],
        [ 1,  2,  3,  1,  2,  3]])

>>> x.repeat(4, 2, 1)
tensor([[[1, 2, 3],
         [1, 2, 3]],

        [[1, 2, 3],
         [1, 2, 3]],

        [[1, 2, 3],
         [1, 2, 3]],

        [[1, 2, 3],
         [1, 2, 3]]])

>>> x.repeat(4, 2, 1).size
torch.Size([4, 2, 3])

>>> x.repeat(4, 2, 2)
tensor([[[1, 2, 3, 1, 2, 3],
         [1, 2, 3, 1, 2, 3]],

        [[1, 2, 3, 1, 2, 3],
         [1, 2, 3, 1, 2, 3]],

        [[1, 2, 3, 1, 2, 3],
         [1, 2, 3, 1, 2, 3]],

        [[1, 2, 3, 1, 2, 3],
         [1, 2, 3, 1, 2, 3]]])
```

 $t$ë¥¼ ì–´ë–¤ í…ì„œ êµ¬ì¡°ì²´ $x$ì˜ ìµœëŒ€ ì°¨ì›ì´ë¼ê³  í–ˆì„ , $x_t$ë¥¼ ê°€ì¥ ì™¼ìª½ì— ë„£ê³  ê°€ì¥ ë‚®ì€ ì°¨ì›ì¸ 0ì°¨ì›ì— ëŒ€í•œ ë°˜ë³µ íšŸìˆ˜ë¥¼ ì˜¤ë¥¸ìª½ ëì— ëŒ€ì…í•´ì„œ ì‚¬ìš©í•˜ë©´ ëœë‹¤. (`torch.repeat(`$x_t, x_{t-1}, ... x_2, x_1, x_0$`))`. 

```python
# torch.arange & torch.repeate usage example

>>> pos_x = torch.arange(self.num_patches + 1).repeat(inputs.shape[0]).to(inputs)
>>> pos_x.shape
torch.tensor([16, 1025])
```

í•„ìì˜ ê²½ìš°, `position embedding`ì˜ ì…ë ¥ì„ ë§Œë“¤ê³  ì‹¶ì„ ë•Œ `torch.arange` ì™€ ì—°ê³„í•´ ìì£¼ ì‚¬ìš© í–ˆë˜ ê²ƒ ê°™ë‹¤. ìœ„ ì½”ë“œë¥¼ ì°¸ê³ í•˜ì.

### `ğŸ”¬Â torch.clamp`

ì…ë ¥ í…ì„œì˜ ì›ì†Œê°’ì„ ì‚¬ìš©ìê°€ ì§€ì •í•œ ìµœëŒ€â€¢ìµœì†Œê°’ ë²”ìœ„ ì´ë‚´ë¡œ ì œí•œí•˜ëŠ” ë©”ì„œë“œë‹¤.

```python
# torch.clamp params

>>> torch.clamp(input, min=None, max=None, *, out=None) â†’ Tensor

# torch.clamp usage example

>>> a = torch.randn(4)
>>> a
tensor([-1.7120,  0.1734, -0.0478, -0.0922])

>>> torch.clamp(a, min=-0.5, max=0.5)
tensor([-0.5000,  0.1734, -0.0478, -0.0922])
```

ì…ë ¥ëœ í…ì„œì˜ ì›ì†Œë¥¼ ì§€ì • ìµœëŒ€â€¢ìµœì†Œ ì„¤ì •ê°’ê³¼ í•˜ë‚˜ í•˜ë‚˜ ëŒ€ì¡°í•´ì„œ í…ì„œ ë‚´ë¶€ì˜ ëª¨ë“  ì›ì†Œê°€ ì§€ì • ë²”ìœ„ ì•ˆì— ë“¤ë„ë¡ ë§Œë“¤ì–´ì¤€ë‹¤. `torch.clamp` ì—­ì‹œ ë‹¤ì–‘í•œ ìƒí™©ì—ì„œ ì‚¬ìš©ë˜ëŠ”ë°, í•„ìì˜ ê²½ìš° ëª¨ë¸ ë ˆì´ì–´ ì¤‘ê°„ì— ì œê³±ê·¼ì´ë‚˜ ì§€ìˆ˜, ë¶„ìˆ˜ í˜¹ì€ ê°ë„ ê´€ë ¨ ì—°ì‚°ì´ ë“¤ì–´ê°€ `Backward Pass`ì—ì„œ `NaN`ì´ ë°œìƒí•  ìˆ˜ ìˆëŠ” ê²½ìš°ì— ì•ˆì „ì¥ì¹˜ë¡œ ë§ì´ ì‚¬ìš©í•˜ê³  ìˆë‹¤. ([ìì„¸íˆ ì•Œê³  ì‹¶ë‹¤ë©´ í´ë¦­](https://qcqced123.github.io/framework-library/backward-nan/))

### `ğŸ‘©â€ğŸ‘©â€ğŸ‘§â€ğŸ‘¦Â torch.gather`

í…ì„œ ê°ì²´ ë‚´ë¶€ì—ì„œ ì›í•˜ëŠ” ì¸ë±ìŠ¤ì— ìœ„ì¹˜í•œ ì›ì†Œë§Œ ì¶”ì¶œí•˜ê³  ì‹¶ì„ ë•Œ ì‚¬ìš©í•˜ë©´ ë§¤ìš° ìœ ìš©í•œ ë©”ì„œë“œë‹¤. í…ì„œ ì—­ì‹œ `iterable` ê°ì²´ë¼ì„œ `loop` ë¥¼ ì‚¬ìš©í•´ ì ‘ê·¼í•˜ëŠ” ê²ƒì´ ì§ê´€ì ìœ¼ë¡œ ë³´ì¼ ìˆ˜ ìˆìœ¼ë‚˜, í†µìƒì ìœ¼ë¡œ í…ì„œë¥¼ ì‚¬ìš©í•˜ëŠ” ìƒí™©ì´ë¼ë©´ ê°ì²´ì˜ ì°¨ì›ì´ ì–´ë§ˆë¬´ì‹œ í•˜ê¸° ë•Œë¬¸ì— ë£¨í”„ë¡œ ì ‘ê·¼í•´ ê´€ë¦¬í•˜ëŠ” ê²ƒì€ ë§¤ìš° ë¹„íš¨ìœ¨ì ì´ë‹¤. ë£¨í”„ë¥¼ í†µí•´ ì ‘ê·¼í•˜ë©´ íŒŒì´ì¬ì˜ ë‚´ì¥ ë¦¬ìŠ¤íŠ¸ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒê³¼ ë³„ë°˜ ë‹¤ë¥¼ê²Œ ì—†ì–´ì§€ê¸° ë•Œë¬¸ì—, í…ì„œë¥¼ ì‚¬ìš©í•˜ëŠ” ë©”ë¦¬íŠ¸ê°€ ì‚¬ë¼ì§„ë‹¤. ë¹„êµì  í¬ì§€ ì•Šì€ 2~3ì°¨ì›ì˜ í…ì„œ ì •ë„ë¼ë©´ ì‚¬ìš©í•´ë„ í¬ê²Œ ë¬¸ì œëŠ” ì—†ì„ê±°ë¼ ìƒê°í•˜ì§€ë§Œ ê·¸ë˜ë„ ì½”ë“œì˜ ì¼ê´€ì„±ì„ ìœ„í•´ `torch.gather` ì‚¬ìš©ì„ ê¶Œì¥í•œë‹¤. ì´ì œ `torch.gather`ì˜ ì‚¬ìš©ë²•ì— ëŒ€í•´ ì•Œì•„ë³´ì.

```python
# torch.gather params

>>> torch.gather(input, dim, index, *, sparse_grad=False, out=None)
```

`dim`ê³¼ `index`ì— ì£¼ëª©í•´ë³´ì. ë¨¼ì € `dim`ì€ ì‚¬ìš©ìê°€ ì¸ë±ì‹±ì„ ì ìš©í•˜ê³  ì‹¶ì€ ì°¨ì›ì„ ì§€ì •í•´ì£¼ëŠ” ì—­í• ì„ í•œë‹¤. `index` ë§¤ê°œë³€ìˆ˜ë¡œ ì „ë‹¬í•˜ëŠ” í…ì„œ ì•ˆì—ëŠ” ì›ì†Œì˜ `â€˜ì¸ë±ìŠ¤â€™`ë¥¼ ì˜ë¯¸í•˜ëŠ” ìˆ«ìë“¤ì´ ë§ˆêµ¬ì¡ì´ë¡œ ë‹´ê²¨ìˆëŠ”ë°, í•´ë‹¹ ì¸ë±ìŠ¤ê°€ ëŒ€ìƒ í…ì„œì˜ ì–´ëŠ ì°¨ì›ì„ ê°€ë¦¬í‚¬ ê²ƒì¸ì§€ë¥¼ ì»´í“¨í„°ì—ê²Œ ì•Œë ¤ì¤€ë‹¤ê³  ìƒê°í•˜ë©´ ëœë‹¤. `index` ëŠ” ì•ì—ì„œ ì„¤ëª…í–ˆë“¯ì´ ì›ì†Œì˜ `â€˜ì¸ë±ìŠ¤â€™`ë¥¼ ì˜ë¯¸í•˜ëŠ” ìˆ«ìë“¤ì´ ë‹´ê¸´ í…ì„œë¥¼ ì…ë ¥ìœ¼ë¡œ í•˜ëŠ” ë§¤ê°œë³€ìˆ˜ë‹¤. ì´ ë•Œ ì£¼ì˜í•  ì ì€ ëŒ€ìƒ í…ì„œ(`input`)ì™€ ì¸ë±ìŠ¤ í…ì„œì˜ ì°¨ì› í˜•íƒœê°€ ë°˜ë“œì‹œ ë™ì¼í•´ì•¼ í•œë‹¤ëŠ” ê²ƒì´ë‹¤. ì—­ì‹œ ë§ë¡œë§Œ ë“¤ìœ¼ë©´ ì´í•´í•˜ê¸° í˜ë“œë‹ˆ ì‚¬ìš© ì˜ˆì‹œë¥¼ í•¨ê¼ ì‚´í´ë³´ì.

```python
# torch.gather usage example
>>> q, kr = torch.randn(10, 1024, 64), torch.randn(10, 1024, 64) # [batch, sequence, dim_head], [batch, 2*sequence, dim_head]
>>> tmp_c2p = torch.matmul(q, kr.transpose(-1, -2))
>>> tmp_c2p, tmp_c2p.shape
(tensor([[-2.6477, -4.7478, -5.3250,  ...,  1.6062, -1.9717,  3.8004],
         [ 0.0662,  1.5240,  0.1182,  ...,  0.1653,  2.8476,  1.6337],
         [-0.5010, -4.2267, -1.1179,  ...,  1.1447,  1.7845, -0.1493],
         ...,
         [-2.1073, -1.2149, -4.8630,  ...,  0.8238, -0.5833, -1.2066],
         [ 2.1747,  3.2924,  6.5808,  ..., -0.2926, -0.2511,  2.6996],
         [-2.8362,  2.8700, -0.9729,  ..., -4.9913, -0.3616, -0.1708]],
        grad_fn=<MmBackward0>)
torch.Size([1024, 1024]))

>>> max_seq, max_relative_position = 1024, 512
>>> q_index, k_index = torch.arange(max_seq), torch.arange(2*max_relative_position)
>>> q_index, k_index
(tensor([   0,    1,    2,  ..., 1021, 1022, 1023]),
 tensor([   0,    1,    2,  ..., 1021, 1022, 1023]))

>>> tmp_pos = q_index.view(-1, 1) - k_index.view(1, -1)
>>> rel_pos_matrix = tmp_pos + max_relative_position
>>> rel_pos_matrix
tensor([[ 512,  511,  510,  ..., -509, -510, -511],
        [ 513,  512,  511,  ..., -508, -509, -510],
        [ 514,  513,  512,  ..., -507, -508, -509],
        ...,
        [1533, 1532, 1531,  ...,  512,  511,  510],
        [1534, 1533, 1532,  ...,  513,  512,  511],
        [1535, 1534, 1533,  ...,  514,  513,  512]])

>>> rel_pos_matrix = torch.clamp(rel_pos_matrix, 0, 2*max_relative_position - 1).repeat(10, 1, 1)
>>> tmp_c2p = tmp_c2p.repeat(10, 1, 1)
>>> rel_pos_matrix, rel_pos_matrix.shape, tmp_c2p.shape 
(tensor([[[ 512,  511,  510,  ...,    0,    0,    0],
          [ 513,  512,  511,  ...,    0,    0,    0],
          [ 514,  513,  512,  ...,    0,    0,    0],
          ...,
          [1023, 1023, 1023,  ...,  512,  511,  510],
          [1023, 1023, 1023,  ...,  513,  512,  511],
          [1023, 1023, 1023,  ...,  514,  513,  512]],
 
         [[ 512,  511,  510,  ...,    0,    0,    0],
          [ 513,  512,  511,  ...,    0,    0,    0],
          [ 514,  513,  512,  ...,    0,    0,    0],
          ...,
          [1023, 1023, 1023,  ...,  512,  511,  510],
          [1023, 1023, 1023,  ...,  513,  512,  511],
          [1023, 1023, 1023,  ...,  514,  513,  512]],
torch.Size([10, 1024, 1024]),
torch.Size([10, 1024, 1024]))

>>> torch.gather(tmp_c2p, dim=-1, index=rel_pos_matrix)
tensor([[[-0.8579, -0.2178,  1.6323,  ..., -2.6477, -2.6477, -2.6477],
         [ 1.1601,  2.1752,  0.7187,  ...,  0.0662,  0.0662,  0.0662],
         [ 3.4379, -1.2573,  0.1375,  ..., -0.5010, -0.5010, -0.5010],
         ...,
         [-1.2066, -1.2066, -1.2066,  ...,  0.5943, -0.5169, -3.0820],
         [ 2.6996,  2.6996,  2.6996,  ...,  0.2014,  1.1458,  3.2626],
         [-0.1708, -0.1708, -0.1708,  ...,  1.9955,  4.1549,  2.6356]],
```

ìœ„ ì½”ë“œëŠ” `DeBERTa` ì˜ `Disentangled Self-Attention`ì„ êµ¬í˜„í•œ ì½”ë“œì˜ ì¼ë¶€ë¶„ì´ë‹¤. ìì„¸í•œ ì›ë¦¬ëŠ” `DeBERTa` ë…¼ë¬¸ ë¦¬ë·° í¬ìŠ¤íŒ…ì—ì„œ í™•ì¸í•˜ë©´ ë˜ê³ , ìš°ë¦¬ê°€ ì§€ê¸ˆ ì£¼ëª©í•  ë¶€ë¶„ì€ ë°”ë¡œ `tmp_c2p`, `rel_pos_matrix` ê·¸ë¦¬ê³  ë§ˆì§€ë§‰ ì¤„ì— ìœ„ì¹˜í•œ `torch.gather` ë‹¤. `[10, 1024, 1024]` ëª¨ì–‘ì„ ê°€ì§„ ëŒ€ìƒ í…ì„œ `tmp_c2p` ì—ì„œ ë‚´ê°€ ì›í•˜ëŠ” ì›ì†Œë§Œ ì¶”ì¶œí•˜ë ¤ëŠ” ìƒí™©ì¸ë°, ì¶”ì¶œí•´ì•¼í•  ì›ì†Œì˜ ì¸ë±ìŠ¤ ê°’ì´ ë‹´ê¸´ í…ì„œë¥¼ `rel_pos_matrix` ë¡œ ì •ì˜í–ˆë‹¤. `rel_pos_matrix` ì˜ ì°¨ì›ì€ `[10, 1024, 1024]`ë¡œ `tmp_c2p`ì™€ ë™ì¼í•˜ë‹¤. ì°¸ê³ ë¡œ ì¶”ì¶œí•´ì•¼ í•˜ëŠ” ì°¨ì› ë°©í–¥ì€ ê°€ë¡œ ë°©í–¥(ë‘ ë²ˆì§¸ 1024)ì´ë‹¤.

ì´ì œ `torch.gather`ì˜ ë™ì‘ì„ ì‚´í´ë³´ì. ìš°ë¦¬ê°€ í˜„ì¬ ì¶”ì¶œí•˜ê³  ì‹¶ì€ ëŒ€ìƒì€ 3ì°¨ì› í…ì„œì˜ ê°€ë¡œ ë°©í–¥(ë‘ ë²ˆì§¸ 1024, í…ì„œì˜ í–‰ ë²¡í„°), ì¦‰ `2 * max_sequence_length` ë¥¼ ì˜ë¯¸í•˜ëŠ” ì°¨ì› ë°©í–¥ì˜ ì›ì†Œë‹¤. ë”°ë¼ì„œ `dim=-1`ìœ¼ë¡œ ì„¤ì •í•´ì¤€ë‹¤. ì´ì œ ë©”ì„œë“œê°€ ì˜ë„ëŒ€ë¡œ ì ìš©ë˜ì—ˆëŠ”ì§€ í™•ì¸í•´ë³´ì. `rel_pos_matrix` ì˜ 0ë²ˆ ë°°ì¹˜, 0ë²ˆì§¸ ì‹œí€€ìŠ¤ì˜ ê°€ì¥ ë§ˆì§€ë§‰ ì°¨ì›ì˜ ê°’ì€ `0`ìœ¼ë¡œ ì´ˆê¸°í™” ë˜ì–´ ìˆë‹¤. ë‹¤ì‹œ ë§í•´, ëŒ€ìƒ í…ì„œì˜ ëŒ€ìƒ ì°¨ì›ì—ì„œ 0ë²ˆì§¸ ì¸ë±ìŠ¤ì— í•´ë‹¹í•˜ëŠ” ê°’ì„ ê°€ì ¸ì˜¤ë¼ëŠ” ì˜ë¯¸ë¥¼ ë‹´ê³  ìˆë‹¤. ê·¸ë ‡ë‹¤ë©´ `torch.gather` ì‹¤í–‰ ê²°ê³¼ê°€ `tmp_c2p`ì˜ 0ë²ˆ ë°°ì¹˜, 0ë²ˆì§¸ ì‹œí€€ìŠ¤ì˜ 0ë²ˆì§¸ ì°¨ì› ê°’ê³¼ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸í•´ë³´ì. ë‘˜ ë‹¤ `-2.6477`, `-2.6477` ìœ¼ë¡œ ê°™ì€ ê°’ì„ ë‚˜íƒ€ë‚´ê³  ìˆë‹¤. ë”°ë¼ì„œ ìš°ë¦¬ ì˜ë„ëŒ€ë¡œ ì˜ ì‹¤í–‰ë˜ì—ˆë‹¤ëŠ” ì‚¬ì‹¤ì„ ì•Œ ìˆ˜ ìˆë‹¤. 

### `ğŸ‘©â€ğŸ‘©â€ğŸ‘§â€ğŸ‘¦Â torch.triu, torch.tril`

ê°ê° ì…ë ¥ í…ì„œë¥¼ `ìƒì‚¼ê°í–‰ë ¬`, `í•˜ì‚¼ê°í–‰ë ¬`ë¡œ ë§Œë“ ë‹¤. `triu`ë‚˜ `tril`ì€ ì‚¬ì‹¤ ë’¤ì§‘ìœ¼ë©´ ê°™ì€ ê²°ê³¼ë¥¼ ë°˜í™˜í•˜ê¸° ë•Œë¬¸ì— `tril`ì„ ê¸°ì¤€ìœ¼ë¡œ ì„¤ëª…ì„ í•˜ê² ë‹¤. ë©”ì„œë“œì˜ ë§¤ê°œë³€ìˆ˜ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.

```python
# torch.triu, tril params
upper_tri_matrix = torch.triu(input_tensor, diagonal=0, *, out=None)
lower_tri_matrix = torch.tril(input_tensors, diagonal=0, *, out=None)
```

`diagonal` ì— ì£¼ëª©í•´ë³´ì. ì–‘ìˆ˜ë¥¼ ì „ë‹¬í•˜ë©´ ì£¼ëŒ€ê°ì„±ë¶„ì—ì„œ í•´ë‹¹í•˜ëŠ” ê°’ë§Œí¼ ë–¨ì–´ì§„ ê³³ì˜ ëŒ€ê°ì„±ë¶„ê¹Œì§€ ê·¸ ê°’ì„ ì‚´ë ¤ë‘”ë‹¤. í•œí¸ ìŒìˆ˜ë¥¼ ì „ë‹¬í•˜ë©´ ì£¼ëŒ€ê°ì„±ë¶„ì„ í¬í•¨í•´ ì£¼ì–´ì§„ ê°’ë§Œí¼ ë–¨ì–´ì§„ ê³³ê¹Œì§€ì˜ ëŒ€ê°ì„±ë¶„ì„ ëª¨ë‘ 0ìœ¼ë¡œ ë§Œë“¤ì–´ë²„ë¦°ë‹¤. ê¸°ë³¸ì€ 0ìœ¼ë¡œ ì„¤ì •ë˜ì–´ ìˆìœ¼ë©°, ì´ëŠ” ì£¼ëŒ€ê°ì„±ë¶„ë¶€í„° ì™¼ìª½ í•˜ë‹¨ì˜ ì›ì†Œë¥¼ ëª¨ë‘ ì‚´ë ¤ë‘ê² ë‹¤ëŠ” ì˜ë¯¸ê°€ ëœë‹¤. 

```python
# torch.tril usage example
>>> lm_mask = torch.tril(torch.ones(x.shape[0], x.shape[-1], x.shape[-1]))
>>> lm_mask
1 0 0 0 0
1 1 0 0 0
1 1 1 0 0
1 1 1 1 0
```

ë‘ ë©”ì„œë“œëŠ” ì„ í˜•ëŒ€ìˆ˜í•™ì´ í•„ìš”í•œ ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ ì‚¬ìš©ë˜ëŠ”ë°, í•„ìì˜ ê²½ìš°, `GPT`ì²˜ëŸ¼ `Transformer`ì˜ `Decoder` ë¥¼ ì‚¬ìš©í•˜ëŠ” ëª¨ë¸ì„ ë¹Œë“œí•  ë•Œ ê°€ì¥ ë§ì´ ì‚¬ìš©í–ˆë˜ ê²ƒ ê°™ë‹¤. `Decoder`ë¥¼ ì‚¬ìš©í•˜ëŠ” ëª¨ë¸ì€ ëŒ€ë¶€ë¶„ êµ¬ì¡°ìƒ `Language Modeling`ì„ ìœ„í•´ì„œ `Masked Multi-Head Self-Attention Block`ì„ ì‚¬ìš©í•˜ëŠ”ë° ì´ ë•Œ ë¯¸ë˜ ì‹œì ì˜ í† í° ì„ë² ë”© ê°’ì— ë§ˆìŠ¤í‚¹ì„ í•´ì£¼ê¸° ìœ„í•´ `torch.tril` ì„ ì‚¬ìš©í•˜ê²Œ ë˜ë‹ˆ ì°¸ê³ í•˜ì. 

### `ğŸ‘©â€ğŸ‘©â€ğŸ‘§â€ğŸ‘¦Â torch.Tensor.masked_fill`

ì‚¬ìš©ìê°€ ì§€ì •í•œ ê°’ì— í•´ë‹¹ë˜ëŠ” ì›ì†Œë¥¼ ëª¨ë‘ ë§ˆìŠ¤í‚¹ ì²˜ë¦¬í•´ì£¼ëŠ” ë©”ì„œë“œë‹¤. ë¨¼ì € ë§¤ê°œë³€ìˆ˜ë¥¼ í™•ì¸í•´ë³´ì.

```python
# torch.Tensor.masked_fill params
input_tensors = torch.Tensor([[1,2,3], [4,5,6]])
input_tensors.masked_fill(mask: BoolTensor, value: float)
```

`masked_fill` ì€ í…ì„œ ê°ì²´ì˜ ë‚´ë¶€ `attribute` ë¡œ ì •ì˜ë˜ê¸° ë•Œë¬¸ì— í•´ë‹¹ ë©”ì„œë“œë¥¼ ì‚¬ìš©í•˜ê³  ì‹¶ë‹¤ë©´ ë¨¼ì € ë§ˆìŠ¤í‚¹ ëŒ€ìƒ í…ì„œë¥¼ ë§Œë“¤ì–´ì•¼ í•œë‹¤. í…ì„œë¥¼ ì •ì˜í–ˆë‹¤ë©´ í…ì„œ ê°ì²´ì˜ `attributes` ì ‘ê·¼ì„ í†µí•´ `masked_fill()` ì„ í˜¸ì¶œí•œ ë’¤, í•„ìš”í•œ ë§¤ê°œë³€ìˆ˜ë¥¼ ì „ë‹¬í•´ì£¼ëŠ” ë°©ì‹ìœ¼ë¡œ ì‚¬ìš©í•˜ë©´ ëœë‹¤. 

`mask` ë§¤ê°œë³€ìˆ˜ì—ëŠ” ë§ˆìŠ¤í‚¹ í…ì„œë¥¼ ì „ë‹¬í•´ì•¼ í•˜ëŠ”ë°, ì´ ë•Œ ë‚´ë¶€ ì›ì†ŒëŠ” ëª¨ë‘ `boolean`ì´ì–´ì•¼ í•˜ê³  í…ì„œì˜ í˜•íƒœëŠ” ëŒ€ìƒ í…ì„œì™€ ë™ì¼í•´ì•¼ í•œë‹¤(ì™„ì „íˆ ê°™ì„ í•„ìš”ëŠ” ì—†ê³ , ë¸Œë¡œë“œ ìºìŠ¤íŒ…ë§Œ ê°€ëŠ¥í•˜ë©´ ìƒê´€ ì—†ìŒ).

`value` ë§¤ê°œë³€ìˆ˜ì—ëŠ” ë§ˆìŠ¤í‚¹ ëŒ€ìƒ ì›ì†Œë“¤ì— ì¼ê´„ì ìœ¼ë¡œ ì ìš©í•´ì£¼ê³  ì‹¶ì€ ê°’ì„ ì „ë‹¬í•œë‹¤. ì´ê²Œ ë§ë¡œë§Œ ë“¤ìœ¼ë©´ ì´í•´í•˜ê¸° ì‰½ì§€ ì•Šë‹¤. ì•„ë˜ ì‚¬ìš© ì˜ˆì‹œë¥¼ í•¨ê»˜ ì²¨ë¶€í–ˆìœ¼ë‹ˆ ì°¸ê³  ë°”ë€ë‹¤.

```python
# torch.masked_fill usage

>>> lm_mask = torch.tril(torch.ones(x.shape[0], x.shape[-1], x.shape[-1]))
>>> lm_mask
1 0 0 0 0
1 1 0 0 0
1 1 1 0 0
1 1 1 1 0
>>> attention_matrix = torch.matmul(q, k.transpose(-1, -2)) / dot_scale
>>> attention_matrix
1.22 2.1 3.4 1.2 1.1
1.22 2.1 3.4 9.9 9.9
1.22 2.1 3.4 9.9 9.9
1.22 2.1 3.4 9.9 9.9

>>> attention_matrix = attention_matrix.masked_fill(lm_mask == 0, float('-inf'))
>>> attention_matrix
1.22 -inf -inf -inf -inf
1.22 2.1 -inf -inf -inf
1.22 2.1 3.4 -inf -inf
1.22 2.1 3.4 9.9 -inf
```