---
title: "🪢 [DeBERTa] DeBERTa: Decoding-Enhanced BERT with Disentangled-Attention"
excerpt: "Transformer Official Paper Review with Pytorch Implementation"
permalink: "/nlp/deberta"
toc: true  # option for table of contents
toc_sticky: true  # option for table of content
categories:
  - NLP
tags:
  - Natural Language Process
  - DeBERTa
  - BERT
  - Transformer
  - Self-Attention
  - Disentangled-Attention
  - Relative Position Embedding
  - EMD
  - Encoder
  
last_modified_at: 2023-08-04T12:00:00-05:00
---
### `🔭 Overview`

`DeBERTa`는 2020년 `Microsoft`가 `ICLR`에서 발표한 자연어 처리용 신경망 모델이다. `Disentangled Self-Attention`, `Enhanced Mask Decoder`라는 두가지 새로운 테크닉을 `BERT`, `RoBERTa`에 적용해 당시 `SOTA`를 달성했으며, 특히 영어처럼 문장에서 자리하는 위치에 따라 단어의 의미, 형태가 결정되는 굴절어 계열에 대한 성능이 좋아 꾸준히 사랑받고 있는 모델이다. 또한 인코딩 가능한 최대 시퀀스 길이가 `4096`으로 매우 긴 편 (`DeBERTa-V3-Large`) 에 속해, `Kaggle Competition`에서 자주 활용된다. 출시된지 2년이 넘도록 `SuperGLUE` 대시보드에서 꾸준히 상위권을 유지하고 있다는 점도 `DeBERTa`가 얼마나 잘 설계된 모델인지 알 수 있는 대목이다. 

한편, `DeBERTa`의 설계 철학은 `Inductive Bias` 다. 간단하게 `Inductive Bias`란, 주어진 데이터로부터 일반화 성능을 높이기 위해 `"입력되는 데이터는 ~ 할 것이다"`, `"이런 특징을 갖고 있을 것이다"`와 같은 가정, 가중치, 가설 등을 기계학습 알고리즘에 적용하는 것을 말한다. **[`ViT` 논문 리뷰](https://qcqced123.github.io/cv/vit)**에서도 밝혔듯, 퓨어한 `Self-Attention` 의 `Inductive Bias` 는 사실상 없으며, 전체 `Transformer` 구조 레벨에서 봐도 `Absolute Position Embedding`을 사용해 토큰의 위치 정보를 모델에 주입해주는 것이 그나마 약한 `Iniductive Bias`라고 볼 수 있다. 다른 포스팅에서는 분명 `Inductive Bias` 가 적기 때문에 자연어 처리에서 `Transformer` 가 성공을 거둘 수 있다고 해놓고 이게 지금 와서 말을 뒤집는다고 생각할 수 있다. 하지만 `Self-Attention`과 `Absolute Position Embedding`의 의미를 다시 한 번 상기해보면, `Inductive Bias` 추가를 주장하는 저자들의 생각이 꽤나 합리적이었음을 알 수 있게 된다. 구체적인 모델 구조를 파악하기 전에 먼저 `Inductive Bias` 추가가 왜 필요하며, 어떠한 가정이 필요한지 알아보자.

### `🪢 Inducitve Bias in DeBERTa`
- **`Absolute Position + Relative Position`을 모두 활용해 풍부하고 깊은 임베딩 추출**
- **`단어의 발생 순서` 임베딩과 `단어 분포 가설` 임베딩을 모두 추출하는 것을 목적으로 설계**
  
본 논문 초록에는 다음과 같은 문장이 서술되어 있다.

`motivated by the observation that the attention weight of a word pair depends on not only their contents but their relative positions. For example, the dependency between the words “deep” and “learning” is much stronger when they occur next to each other than when they occur in different sentences.`

위의 두 문장이 `DeBERTa`의 `Inducitve Bias` 를 가장 잘 설명하고 있다고 생각한다. 저자가 추가를 주장하는 `Inductive Bias`란, `relative position` 정보라는 것과 기존 모델링으로는 `relative position`이 주는 문맥 정보 포착이 불가능하다는 사실을 알 수 있다. 

그렇다면 `relative position` 가 제공하는 문맥 정보가 도대체 뭐길래 기존의 방식으로는 포착이 불가능하다는 것일까?? 자연어에서 포착 가능한 문맥들의 종류와 기존의 모델링 방식에 대한 정리부터 해보자. 여기서 말하는 기존 방식이란, 퓨어한 `Self-Attention`과 `Absolute Position Embedding` 을 사용하는 `Transformer-Encoder-Base`  모델(`BERT`, `RoBERTa`)을 뜻한다. 이번 포스팅에서는 `BERT`를 기준으로 설명하겠다.

**`📚 Types of Embedding`**  
**먼저 현존하는 모든 임베딩(`벡터에 문맥을 주입하는`)기법들을 정리해보자. 다음과 같이 3가지 카테고리로 분류가 가능하다.**

- **1) 단어의 빈도수:  시퀀스에서 사용된 토큰들의 빈도수를 측정(`Bag of words`)**
- **2) 단어의 발생 순서: `corpus` 내부의 특정 `sequence` 등장 빈도를 카운트(`N-Gram`), 주어진 시퀀스를 가지고 다음 시점에 등장할 토큰을 맞추는 방식(`LM`)**
- **3) 단어 분포 가설 :  단어의 의미는 주변 문맥에 의해 결정된다는 가정, 어떤 단어 쌍이 자주 같이 등장하는지 카운트해 `PMI`를 측정하는 방식(`Word2Vec`)**

기존의 모델링 방식은 어디에 포함될까?? `BERT` 는 대분류 상 신경망에 포함되고, `Language Modeling`을 통해 시퀀스를 학습한다는 점 그리고 `Self-Attention`과 `Absolute Position Embedding` 을 사용한다는 점에서 2번, `단어의 발생 순서` 에 포함된다고 볼 수 있다. `Absolute Position Embedding` 과 `Self-Attention`의 사용이 퓨어한 `BERT`가 분류상 2번이라는 사실을 뒷받침하는 증거라는 점에서 의아할 수 있다. 하지만 잘 생각해보자. 

`Absolute Position Embedding`은 주어진 시퀀스의 길이를 측정한 뒤, 나열된 순서 그대로 `forward`하게 `0`부터 `길이-1`의 번호를 개별 토큰에 할당한다. 다시 말해, 단어가 시퀀스에서 발생한 순서를 수학적으로 표현해 모델에 주입한다는 의미가 된다. `Self-Attention`은 `Absolute Position Embedding` 정보가 주입된 시퀀스 전체를 한 번에 병렬 처리한다. 따라서 충분히 `BERT` 같은 `Self-Attention`, `Absolute Position Embedding` 기반 모델을 2번에 분류할 수 있겠다. 

한편, 혹자는 `"BERT는 MLM 을 사용하는데 Language Modeling을 한다고 하는게 맞나요"`라고 말할 수 있다. 하지만 `MLM` 역시 대분류 상 `Language Modeling` 기법에 속한다. **다만, `Bi-Directional`하게 문맥을 파악하고 `LM`을 하니까 정말 엄밀히 따지면 3번의 속성도 어느 정도 있다고 보는게 무리는 아니라 생각한다.** `MLM` 사용으로 더 많은 정보를 포착해 임베딩을 만들기 때문에 초기 `BERT`가 `GPT`보다 `NLU`에서 상대적으로 강점을 가졌던 것 아닐까 싶다.

**`🔢 Relative Position Embedding`**  
이제 `Relative Position Embedding`이 무엇이고, 도대체 어떤 문맥 정보를 포착한다는 것인지 알아보자. `Relative Position Embedding` 이란, 시퀀스 내부 토큰 사이의 위치 관계 표현을 통해 토큰 사이의 `relation`을 `pairwise`하게 학습하는 위치 임베딩 기법을 말한다. 일반적으로 상대 위치 관계는 서로 다른 두 토큰의 시퀀스 인덱스 값의 차를 이용해 나타낸다. 포착하는 문맥 정보는 예시와 함깨 설명하겠다. 딥러닝이라는 단어는 영어로 `Deep Learning` 이다. 두 단어를 합쳐놓고 보면 `신경망을 사용하는 머신러닝 기법의 한 종류`라는 의미를 갖겠지만, 따로 따로 보면 `깊은`, `배움`이라는 개별적인 의미로 나뉜다.

- **`1) The Deep Learning is the Best Technique in Computer Science`**
- **`2) I’m learning how to swim in the deep ocean`**

`Deep`과 `Learning`의 상대적인 거리에 주목하면서 두 문장을 해석해보자. 첫 번째 문장에서 두 단어는 이웃하게 위치해 `신경망을 사용하는 머신러닝 기법의 한 종류` 라는 의미를 만들어내고 있다. 한편 두 번째 문장에서 두 단어는 띄어쓰기 기준 5개의 토큰만큼 떨어져 위치해 각각 `배움`, `깊은` 이라는 의미를 만들어 내고 있다. 이처럼 개별 토큰 사이의 위치 관계에 따라서 파생되는 문맥적 정보를 포착하려는 의도로 설계된 기법이 바로 `Relative Position Embedding` 이다. 

`pairwise` 하게 `relation` 을 포착한다는 점으로 보아 `skip-gram`의 `negative sampling`과 매우 유사한 느낌의 정보를 포착할 것이라고 예상되며 카테고리 분류상 **3번, `단어 분포 가설`**에 포함시킬 수 있을 것 같다. (필자의 개인적인 의견이니 이 부분에 대한 다른 의견이 있다면 꼭 댓글에 적어주시면 감사하겠습니당🥰).

`Relative Position Embedding` 을 실제 어떻게 코드로 구현하는지, 본 논문에서는 위치 관계를 어떻게 정의했는지 `Absolute Position Embedding`와 비교를 통해 알아보자. 다음과 같은 두 개의 문장이 있을 때, 개별 위치 임베딩 방식이 문장의 위치 정보를 인코딩하는 과정을 파이썬 코드로 작성해봤다. 함께 살펴보자.

- **`A) I love studying deep learning so much`**
- **`B) I love deep cheeze burguer so much`**

```python
# Absolute Position Embedding

>>> max_length = 7
>>> position_embedding = nn.Embedding(7, 512) # [max_seq, dim_model]
>>> pos_x = position_embedding(torch.arange(max_length))
>>> pos_x, pos_x.shape
(tensor([[ 0.4027,  0.9331,  1.0556,  ..., -1.7370,  0.7799,  1.9851],  # A,B의 0번 토큰: I
         [-0.2206,  2.1024, -0.6055,  ..., -1.1342,  1.3956,  0.9017],  # A,B의 1번 토큰: love
         [-0.9560, -0.0426, -1.8587,  ..., -0.9406, -0.1467,  0.1762],  # A,B의 2번 토큰: studying, deep
         ...,                                                           # A,B의 3번 토큰: deep, cheeze
         [ 0.5999,  0.5235, -0.3445,  ...,  1.9020, -1.5003,  0.7535],  # A,B의 4번 토큰: learning, burger
         [ 0.0688,  0.5867, -0.0340,  ...,  0.8547, -0.9196,  1.1193],  # A,B의 5번 토큰: so
         [-0.0751, -0.4133,  0.0256,  ...,  0.0788,  1.4665,  0.8196]], # A,B의 6번 토큰: much
        grad_fn=<EmbeddingBackward0>),
 torch.Size([7, 512]))
```

`Absolute Position Embedding`은 주위 문맥에 상관없이 같은 위치의 토큰이라면 같은 포지션 값으로 인코딩하기 때문에 `512`개의 원소로 구성된 행벡터들의 인덱스를 실제 문장에서 토큰의 등장 순서에 맵핑해주는 방식으로 위치 정보를 표현한다. 예를 들면, 문장에서 가장 먼저 등장하는 `0`번 토큰에 `0`번째 `행벡터`를 배정하고 가장 마지막에 등장하는 `N-1` 번째 토큰은 `N-1`번째 `행벡터`를 위치 정보값으로 갖는 방식이다. 전체 시퀀스 관점에서 개별 토큰에 번호를 부여하기 때문에 `syntactical`한 정보를 모델링 해주기 적합하다는 장점이 있다. 

`Absolute Position Embedding` 은 일반적으로 `Input Embedding`과 행렬합 연산을 통해 `Word Embedding` 으로 만들어 인코더의 입력으로 사용한다.

아래 코드는 저자가 논문에서 제시한 `DeBERTa`의 `Relative Position Embedding` 구현을 파이토치로 옮긴 것이다. `Relative Position Embedding` 은 절대 위치에 비해 꽤나 복잡한 과정을 거쳐야 하기 때문에 코드 역시 긴 편이다. 하나 하나 천천히 살펴보자.

```python
# Relative Position Embedding
>>> position_embedding = nn.Embedding(2*max_length, dim_model)
>>> x, p_x = torch.randn(max_length, dim_model), position_embedding(torch.arange(2*max_length))
>>> fc_q, fc_kr = nn.Linear(dim_model, dim_head), nn.Linear(dim_model, dim_head)
>>> q, kr = fc_q(x), fc_kr(p_x) # [batch, max_length, dim_head], [batch, 2*max_length, dim_head]

>>> tmp_c2p = torch.matmul(q, kr.transpose(-1, -2))
>>> tmp_c2p, tmp_c2p.shape
(tensor([[ 2.8118,  0.8449, -0.6240, -0.6516,  3.4009,  1.8296,  0.8304,  1.0164,
           3.5664, -1.4208, -2.0821,  1.5752, -0.9469, -7.1767],
         [-2.1907, -3.2801, -2.0628,  0.4443,  2.2272, -5.6653, -4.6036,  1.4134,
          -1.1742, -0.3361, -0.4586, -1.1827,  1.0878, -2.5657],
         [-4.8952, -1.5330,  0.0251,  3.5001,  4.1619,  1.7408, -0.5100, -3.4616,
          -1.6101, -1.8741,  1.1404,  4.9860, -2.5350,  1.0999],
         [-3.3437,  4.2276,  0.4509, -1.8911, -1.1069,  0.9540,  1.2045,  2.2194,
          -2.6509, -1.4076,  5.1599,  1.6591,  3.8764,  2.5126],
         [ 0.8164, -1.9171,  0.8217,  1.3953,  1.6260,  3.8104, -1.0303, -2.1631,
           3.9008,  0.5856, -1.6212,  1.7220,  2.7997, -1.8802],
         [ 3.4473,  0.9721,  3.9137, -3.2055,  0.6963,  1.2761, -0.2266, -3.7274,
          -1.4928, -1.9257, -5.4422, -1.8544,  1.8749, -3.4923],
         [ 2.6639, -1.4392, -3.8818, -1.4120,  1.7542, -0.8774, -3.0795, -1.2156,
          -1.0852,  3.7825, -3.5581, -3.6989, -2.6705, -1.2262]],
        grad_fn=<MmBackward0>),
 torch.Size([7, 14]))

>>> max_seq, max_pos = 7, max_seq * 2
>>> q_index, k_index = torch.arange(max_seq), torch.arange(max_seq)
>>> q_index, k_index
(tensor([0, 1, 2, 3, 4, 5, 6]), tensor([0, 1, 2, 3, 4, 5, 6]))

>>> tmp_pos = q_index.view(-1, 1) - k_index.view(1, -1)
>>> rel_pos_matrix = tmp_pos + max_relative_position
>>> rel_pos_matrix
tensor([[ 7,  6,  5,  4,  3,  2,  1],
        [ 8,  7,  6,  5,  4,  3,  2],
        [ 9,  8,  7,  6,  5,  4,  3],
        [10,  9,  8,  7,  6,  5,  4],
        [11, 10,  9,  8,  7,  6,  5],
        [12, 11, 10,  9,  8,  7,  6],
        [13, 12, 11, 10,  9,  8,  7]])

>>> rel_pos_matrix = torch.clamp(rel_pos_matrix, 0, max_pos - 1).repeat(10, 1, 1)
>>> tmp_c2p = tmp_c2p.repeat(10, 1, 1)
>>> rel_pos_matrix, rel_pos_matrix.shape, tmp_c2p.shape 
(tensor([[[ 7,  6,  5,  4,  3,  2,  1],
          [ 8,  7,  6,  5,  4,  3,  2],
          [ 9,  8,  7,  6,  5,  4,  3],
          [10,  9,  8,  7,  6,  5,  4],
          [11, 10,  9,  8,  7,  6,  5],
          [12, 11, 10,  9,  8,  7,  6],
          [13, 12, 11, 10,  9,  8,  7]],
torch.Size([10, 7, 14]),
torch.Size([10, 7, 14]))

>>> outputs = torch.gather(tmp_c2p, dim=-1, index=rel_pos_matrix)
>>> outputs, outputs.shape
(tensor([[[ 1.0164,  0.8304,  1.8296,  3.4009, -0.6516, -0.6240,  0.8449],
          [-1.1742,  1.4134, -4.6036, -5.6653,  2.2272,  0.4443, -2.0628],
          [-1.8741, -1.6101, -3.4616, -0.5100,  1.7408,  4.1619,  3.5001],
          [ 5.1599, -1.4076, -2.6509,  2.2194,  1.2045,  0.9540, -1.1069],
          [ 1.7220, -1.6212,  0.5856,  3.9008, -2.1631, -1.0303,  3.8104],
          [ 1.8749, -1.8544, -5.4422, -1.9257, -1.4928, -3.7274, -0.2266],
          [-1.2262, -2.6705, -3.6989, -3.5581,  3.7825, -1.0852, -1.2156]],
          .....
          [[ 1.0164,  0.8304,  1.8296,  3.4009, -0.6516, -0.6240,  0.8449],
          [-1.1742,  1.4134, -4.6036, -5.6653,  2.2272,  0.4443, -2.0628],
          [-1.8741, -1.6101, -3.4616, -0.5100,  1.7408,  4.1619,  3.5001],
          [ 5.1599, -1.4076, -2.6509,  2.2194,  1.2045,  0.9540, -1.1069],
          [ 1.7220, -1.6212,  0.5856,  3.9008, -2.1631, -1.0303,  3.8104],
          [ 1.8749, -1.8544, -5.4422, -1.9257, -1.4928, -3.7274, -0.2266],
          [-1.2262, -2.6705, -3.6989, -3.5581,  3.7825, -1.0852, -1.2156]]],
        grad_fn=<GatherBackward0>),
 torch.Size([10, 7, 7]))
```

일단 절대 위치와 동일하게 `nn.Embedding`을 사용해 임베딩 룩업 테이블(레이어)를 정의하지만, 입력 차원이 다르다. 절대 위치 임베딩은 `forward`하게 위치값을 맵핑해야 하는 반면에 상대 위치 임베딩 방식은 `Bi-Directional`한 맵핑을 해야 해서, 기존 `max_length` 값의 두 배를 입력 차원(`max_pos`)으로 사용했다. 예를 들어 `0`번 토큰과 나머지 토큰 사이의 위치 관계를 표현해야 하는 상황이다. 그렇다면 우리는 `0`번 토큰과 나머지 토큰과의 위치 관계를 `[0, -1, -2, -3, -4, -5, -6]` 으로 인코딩할 수 있다. 

반대로 마지막 `6`번 토큰과 나머지 토큰 사이의 위치 관계를 표현하는 경우라면 어떻게 될까?? `[6, 5, 4, 3, 2, 1, 0]` 으로 인코딩 될 것이다. 다시 말해, 위치 임베딩 원소 값은 `[-max_seq:max_seq]` 사이에서 정의된다는 것이다. 그러나 원소값의 범위를 그대로 사용할 수는 없다. 이유는 파이썬의 리스트, 텐서 같은 배열형 자료구조는 음이 아닌 정수를 인덱스로 활용해야 `forward` 하게 원소에 접근할 수 있기 때문이다. 일반적으로 배열 형태의 자료형은 모두 인덱스 `0`부터 `N-1`까지 순차적으로 맵핑된다. 그래서 의도한대로 토큰에 접근하려면 역시 토큰의 인덱스를 `forward` 한 형태로 만들어줘야 한다.

따라서 기존 `[-max_seq:max_seq]` 에  `max_seq`를 더해준 `[0:2*max_seq]` (`2 * max_seq`)을 원소 값의 범위로 사용하게 된다. 여기까지가 통상적으로 말하는 `Relative Position Embedding` 에 해당한다. 위 코드상으로는 `rel_pos_matrix` 를 만든 부분에 해당한다.

$$
∂(i,j)=
\begin{cases}
\ 0 & {(i - j ≤ k)} \\ 
\ 2k-1 & {(i - j ≥ k)} \\
\ i - j + k & {(others)} \\
\end{cases}
$$

이제부터 저자가 주장하는 위치 관계 표현 방식에 대해 알아보자. 일반적인 `Relative Position Embedding`과 거의 유사하지만, `rel_pos_matrix` 내부 원소 값이 음수가 되거나 `max_pos` 을 초과하는 경우를 처리 해주기 위해 후처리 과정을 도입해 사용했다. 예외 상황은 **`max_seq > 1/2 * max_pos(==k)`** 일 때 발생한다. `official repo` 의 코드를 보면 `max_seq`와 `k`를 일치시켜 모델링 하기 때문에 파인튜닝 하는 상황이라면 이것을 몰라도 상관없겠지만, 하나 하나 모델을 직접 만드는 입장이라면 예외 상황을 반드시 기억하자. 

한편, 이러한 인코딩 방식은 `word2vec` 의 `window size` 도입과 비슷한 원리(`의미는 주변 문맥에 의해 결정`)라고 생각하면 되는데, 윈도우 사이즈 범위에서 벗어난 토큰들은 주변 문맥으로 인정 하지 않겠다는(`negative sample`) 의도를 갖고 있다. 실제 구현은 텐서 내부 원소값의 범위를 사용자 지정 범위로 제한할 수 있는 `torch.clamp` 를 사용하면 `1`줄로 깔끔하게 만들 수 있으니 참고하자. 

`torch.clamp` 까지 적용하고 난 최종 결과를 살펴보자. 행백터, 열벡터 모두 `[0:2*max_seq]` 사이에서 정의되고 있으며, 개별 방향 벡터 원소의 최대값과 최소값의 차이가 항상 `k` 로 유지 된다. 의도대로 정확히 윈도우 사이즈만큼의 주변 맥락을 반영해 임베딩을 형성하고 있음을 알 수 있다.

정리하면, `Relative Position Embedding` 란 절대 위치 방식처럼 임베딩 룩업 테이블을 만들되, 사용자가 지정한 윈도우 사이즈에 해당하는 토큰의 임베딩 값만 추출해 새로운 행벡터를 여러 개 만들어 내는 기법이라고 할 수 있다. **이 때 행벡터는 대상 토큰과 그 나머지 토큰 사이의 위치 변화에 따라 발생하는 파생적인 맥락 정보를 담고 있다.** 

**`🤔 DeBERTa Inductive Bias`**  s
**결국** `DeBERTa`**는 두가지 위치 정보 포착 방식을 적절히 섞어서 모델이 더욱 풍부한 임베딩을 갖도록 하려는 의도로 설계 되었다.** 또한 우리는 이미 모델이 다양한 맥락 정보를 포착할수록 `NLU Task` 에서 더 나은 성능을 기록한다는 사실을 `BERT`와 `GPT` 사례에서 알 수 있었다. 따라서 `Relative Position Embedding` 을 추가하여 `단어의 발생 순서` 를 포착하는 모델에 `단어 분포 가설` 적인 특징을 더해주려는 저자의 아이디어는 매우 타당하다고 볼 수 있겠다. 

이제 관건은 **`“두가지 위치 정보를 어떤 방식으로 추출하고 섞어줄 것인가”`** 하는 물음에 답하는 것이다. 저자는 물음에 답하기 위해 `Disentangled Self-Attention` 과  `Enhanced Mask Decoder` 라는 새로운 기법 두가지를 제시한다. 전자는 `단어 분포 가설` 에 해당되는 맥락 정보를 추출하기 위한 기법이고, 후자는 `단어 발생 순서` 에 포함되는 임베딩을 모델에 주입하기 위해 설계되었다. 모델링 파트에서는 두가지 새로운 기법에 대해서 자세히 살펴본 뒤에 실제 모델을 코드로 빌드하는 과정을 설명하려 한다.

### `🌟 Modeling`

<p markdown="1" align="center">
![DeBERTa Model Structure](/assets/images/deberta/deberta_overview.png){: .align-center}{: width="60%", height="50%"}{: .image-caption}
__*[DeBERTa Model Structure](https://www.youtube.com/watch?v=gcMyKUXbY8s&t=838s&ab_channel=%EA%B3%A0%EB%A0%A4%EB%8C%80%ED%95%99%EA%B5%90%EC%82%B0%EC%97%85%EA%B2%BD%EC%98%81%EA%B3%B5%ED%95%99%EB%B6%80DSBA%EC%97%B0%EA%B5%AC%EC%8B%A4)*__
</p>
