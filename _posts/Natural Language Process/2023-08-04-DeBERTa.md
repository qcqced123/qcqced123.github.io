---
title: "ðŸª¢Â [DeBERTa] DeBERTa: Decoding-Ehanced BERT with Disentangled-Attention"
excerpt: "Transformer Official Paper Review with Pytorch Implementation"
permalink: "/nlp/deberta"
toc: true  # option for table of content
toc_sticky: true  # option for table of content
categories:
  - NLP
tags:
  - Natural Language Process
  - DeBERTa
  - BERT
  - Transformer
  - Self-Attention
  - Disentangled-Attention
  - Relative Position Embedding
  - EMD
  - Encoder
  
last_modified_at: 2023-08-04T12:00:00-05:00
---
