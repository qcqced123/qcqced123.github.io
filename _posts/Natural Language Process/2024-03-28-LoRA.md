---
title: "🔪 [LoRA] Low-Rank Adaptation of Large Language Models"
excerpt: "LoRA Official Paper Review with Pytorch Implementation"
permalink: "/nlp/lora"
toc: true  # option for table of contents
toc_sticky: true  # option for table of content
categories:
  - NLP
tags:
  - Natural Language Process
  - LoRA
  - Low-Rank Adaptation
  - Fine-Tune
  - Optimization
  - Pytorch

last_modified_at: 2024-03-28T12:00:00-05:00
---
### `🔭 Overview`

<p markdown="1" align="center">
![LoRA](/assets/images/lora/lora.png){: .align-center}{: width="50%", height="50%"}{: .image-caption}
__*[LoRA](https://arxiv.org/abs/2106.09685)*__
</p>

LoRA는 2021년 MS 연구진이 발표한 논문으로 원본(Full 파인튜닝)과 거의 유사한 성능(심지어 일부 벤치마크는 더 높음)으로 LLM 파인튜닝에 필요한 GPU 메모리를 획기적으로 줄이는데 성공해 주목을 받았다. 커뮤니티에서 `LoRA is All You Need` 라는 별명까지 얻으며 그 인기를 구가하고 있다. 

`DistilBERT` 리뷰에서도 살펴보았듯, BERT와 GPT의 등장 이후, 모든 NLP 도메인에서 비약적인 성능 향상이 이뤄줬음에도 불구하고, NLP용 딥러닝 모델을 실생활에 활용하기에는 너무 큰 리소스 요구량과 레이턴시가 발목을 잡았다. 하지만 `LoRA` 발표 이후, 파인튜닝 시점에 훈련해야 하는 파라미터 수가 현저히 줄어들면서 모델의 체크포인트 용량이 기하급수적으로 감소했다. 덕분에 요구 GPU VRAM이 현저히 낮아져, 리소스 제한 때문에 서빙하지 못하는 경우가 많이 사라졌다. 그래서 오늘날 `Mixed Precision`, `Quantization`과 함께 모델 경량•최적화 분야에서 가장 중요한 주제로 떠오르고 있다.

내용을 살펴보기전, `LoRA` 는 이미 사전학습을 완료한 모델을 파인튜닝할 때 사용해야함을 다시 한 번 명심하자. 이번 포스팅에서는 두가지를 집중적으로 다룰 것이다. 

**1) 모델 크기 줄인 방법, 2) 크기를 줄이면서도 비슷한 성능을 낼 수 있었던 이유**

### `🤔 Concept: Low-Rank Adaptation`

$$
h = W_0x + \Delta Wx =  W_0x + BAx
$$

아이디어는 상당히 간단하다. 사전학습을 마치고 수렴된 상태의 가중치 행렬을 의미하는 $W_0$과 새로운 가중치 행렬 $\Delta W$에 모두 입력을 통과시킨다. 그리고 나온 결과를 더해 다음층의 입력으로 사용한다. 오히려 새로운 가중치 행렬을 추가해 파인튜닝을 하는데 어떻게 훈련해야 하는 파라미터 수를 줄일 수 있었을까??

그 비밀은 `Freeze(Stop Gradient, require_grad=False)`와 `Matrix Factorization`에 숨어 있다. 먼저 사전 훈련된 가중치 행렬에 `Freeze(Stop Gradient, require_grad=False)` 를 적용해 그라디언트가 흐르지 않도록 한다. 이렇게 하면 파인튜닝 과정에서 가중치가 업데이트 되지 않아 사전 학습에서 습득한 지식을 유지할 수 있을 뿐만 아니라, 학습을 위해 그라디언트를 저장할 필요가 없어져 파인튜닝 때 필요한 GPU VRAM을 획기적으로 줄일 수 있다.

처음에 사전학습 가중치를 통과한 값과 새로운 가중치 행렬 $\Delta W$를 통과한 값을 서로 더한다고 언급했다. 그렇다면, 두 결과 행렬의 행렬 크기가 동일해야 한다는 것이다. 어떻게 기존보다 사이즈는 줄이면서 결과 행렬의 크기는 동일하게 만들어줄 수 있을까?? 바로 Low Rank value $r$을 도입해 Matrix Factorization 을 한다.

$$
W_{d \times d} = \begin{bmatrix}
w_{1,1} & w_{1,2} & \cdots & w_{1,d} \\
w_{2,1} & w_{2,2} & \cdots & w_{2,d} \\
\vdots & \vdots & \ddots & \vdots \\
w_{d,1} & w_{d,2} & \cdots & w_{d,d}
\end{bmatrix}

$$

행렬 곱셈(matrix multiplication)을 다시 한 번 상기해보자. `MxN` 의 크기를 갖는 행렬에 `NxK`의 크기를 갖는 행렬을 곱해주면 `MxK` 의 크기를 갖는 행렬을 만들어줄 수 있다. 마찬가지다. `dxd` 크기인 사전학습의 가중치 행렬 $W_{d \times d}$과 크기를 맞추기 위해, `dxd` 짜리 행렬을 각각 `dxr`, `rxd` 의 크기를 갖는 두 행렬 $A, B$로 분해한다. 이 때, 행렬 $A$의 열차원과 행렬 $B$차원의 행차원 크기를 표현하는 $r$에 바로 Low Rank value $r$을 대입하면 된다.

$$
\Delta W_{d \times d} = A_{d \times r}\ B_{r \times d} = \begin{bmatrix}
w_{1,1} & w_{1,2} & w_{1,r} \\
w_{2,1} & w_{2,2} & w_{2,r} \\
\vdots & \vdots & \vdots \\
w_{d,1} & w_{d,2} & w_{d,r}
\end{bmatrix}\begin{bmatrix}
w_{1,1} & w_{2,1} & w_{d,1} \\
w_{1,2} & w_{2,2} & w_{d,2} \\
\vdots & \vdots & \vdots \\
w_{1,r} & w_{2,r} & w_{d,r}
\end{bmatrix}

$$

$r=3$이라고 가정하고 `768x768` 짜리 기존 가중치 행렬 $W$과 `768x3`, `3x768`의 크기를 갖는 $\Delta W = BA$의 파라미터 개수를 비교해보자. 계산해보면 전자는 `589,824`개, 후자는 `4608`개가 된다. 정확하게 `128`배 차이가 난다. 트랜스포머 모델 속에는 행렬 $W$과 같은 크기를 갖는 가중치 행렬이 단일 인코더 내부, 하나의 어텐션 레이어만 해도 4개($W_q, W_k, W_v, W_o$)가 있다. `BERT-base` 모델을 기준으로 보면, 해당 모델이 `12`개의 인코더로 구성되어 있으니까 총 `48`개의 가중치 행렬이 있고, 어림잡아도 `48*128`배의 학습 파라미터 감소 효과를 낼 수 있다. 모델의 레이어가 많으면 많을수록 더 좋은 효율을 보인다.

<p markdown="1" align="center">
![Resnet50 Memeory Type in GPU](/assets/images/lora/cuda_memory.png){: .align-center}{: width="100%", height="50%"}{: .image-caption}
__*[Resnet50 Memeory Type in GPU](https://pytorch.org/blog/understanding-gpu-memory-1/)*__
</p>

위 그림은 파이토치 공식 블로그에서 퍼온 자료로, 학습 때 ResNet50의 GPU VRAM 점유율 추이는 물론 모델의 개별 구성요소의 메모리 비율까지 자세히 보여준다. 먼저 `Parameter`와 `Optimizer State`를 보자. `Parameter` 는 모델에서 훈련을 통해 업데이트가 필요한 모든 구성 요소를 말한다. `Freeze`, `require_grad=False` `@torch.no_grad()` , `torch.register_buffer()` 의 영향을 받지 않은 모델 내부의 모든 텐서라고 보면 된다. 

한편, `Optimizer State` 는 옵티마이저의 최적화 수행에 필요한 모든 정보들을 의미하는데, 예를 들어 업데이트 될 텐서의 메타 정보, 여러 하이퍼파라미터 값 같은 것들이 담겨 있다. 

이 두 요소가 모델의 GPU VRAM을 차지하는 비율이 상당히 크다. 하지만 두 요소 모두 파라미터 개수에 비례하므로 `LoRA` 적용으로 파라미터 개수를 줄이면, GPU VRAM을 획기적으로 줄일 수 있다.

또한 파이토치는 역전파 수행을 위해 그라디언트를 파라미터와 동일한 모양(shape)을 갖는 텐서로 저장된다는 점을 감안하면, 기존의 Full-Rank 텐서 대신 Low-Rank 텐서를 학습에 이용함으로서 그라디언트 텐서의 크기 역시 획기적으로 줄일 수 있겠다.

트랜스포머 계열의 모델들이 ResNet 대비 압도적으로 파라미터 개수가 많기 때문에 `LoRA`를 적용한다면 훨씬 큰 효과를 볼 수 있을 것이다.

지금까지 `LoRA` 가 제시하는 방법론이 어떻게 획기적으로 학습 파라미터를 줄이고 나아가 모델이 차지하는 GPU VRAM 크기를 감소시켰는지 알아 보았다. 이제 `LoRA`를 적용해도 일반적인 파인튜닝 방법과 비슷한 성능을 유지할 수 있었는지 알아보자.