---
title: "π” [LoRA] Low-Rank Adaptation of Large Language Models"
excerpt: "LoRA Official Paper Review with Pytorch Implementation"
permalink: "/nlp/lora"
toc: true  # option for table of contents
toc_sticky: true  # option for table of content
categories:
  - NLP
tags:
  - Natural Language Process
  - LoRA
  - Low-Rank Adaptation
  - Fine-Tune
  - Optimization
  - Pytorch

last_modified_at: 2024-03-28T12:00:00-05:00
---
### `π”­Β Overview`

<p markdown="1" align="center">
![LoRA](/assets/images/lora/lora.png){: .align-center}{: width="50%", height="50%"}{: .image-caption}
__*[LoRA](https://arxiv.org/abs/2106.09685)*__
</p>

LoRAλ” 2021λ…„ MS μ—°κµ¬μ§„μ΄ λ°ν‘ν• λ…Όλ¬ΈμΌλ΅ μ›λ³Έ(Full νμΈνλ‹)κ³Ό κ±°μ μ μ‚¬ν• μ„±λ¥(μ‹¬μ§€μ–΄ μΌλ¶€ λ²¤μΉλ§ν¬λ” λ” λ†’μ)μΌλ΅ LLM νμΈνλ‹μ— ν•„μ”ν• GPU λ©”λ¨λ¦¬λ¥Ό νκΈ°μ μΌλ΅ μ¤„μ΄λ”λ° μ„±κ³µν•΄ μ£Όλ©μ„ λ°›μ•λ‹¤. μ»¤λ®¤λ‹ν‹°μ—μ„ `LoRA is All You Need` λΌλ” λ³„λ…κΉμ§€ μ–»μΌλ©° κ·Έ μΈκΈ°λ¥Ό κµ¬κ°€ν•κ³  μλ‹¤. 

`DistilBERT` λ¦¬λ·°μ—μ„λ„ μ‚΄ν΄λ³΄μ•λ“―, BERTμ™€ GPTμ λ“±μ¥ μ΄ν›„, λ¨λ“  NLP λ„λ©”μΈμ—μ„ λΉ„μ•½μ μΈ μ„±λ¥ ν–¥μƒμ΄ μ΄λ¤„μ¤¬μμ—λ„ λ¶κµ¬ν•κ³ , NLPμ© λ”¥λ¬λ‹ λ¨λΈμ„ μ‹¤μƒν™μ— ν™μ©ν•κΈ°μ—λ” λ„λ¬΄ ν° λ¦¬μ†μ¤ μ”κµ¬λ‰κ³Ό λ μ΄ν„΄μ‹κ°€ λ°λ©μ„ μ΅μ•λ‹¤. ν•μ§€λ§ `LoRA` λ°ν‘ μ΄ν›„, νμΈνλ‹ μ‹μ μ— ν›λ ¨ν•΄μ•Ό ν•λ” νλΌλ―Έν„° μκ°€ ν„μ €ν μ¤„μ–΄λ“¤λ©΄μ„ λ¨λΈμ μ²΄ν¬ν¬μΈνΈ μ©λ‰μ΄ κΈ°ν•κΈ‰μμ μΌλ΅ κ°μ†ν–λ‹¤. λ•λ¶„μ— μ”κµ¬ GPU VRAMμ΄ ν„μ €ν λ‚®μ•„μ Έ, λ¦¬μ†μ¤ μ ν• λ•λ¬Έμ— μ„λΉ™ν•μ§€ λ»ν•λ” κ²½μ°κ°€ λ§μ΄ μ‚¬λΌμ΅λ‹¤. κ·Έλμ„ μ¤λλ‚  `Mixed Precision`, `Quantization`κ³Ό ν•¨κ» λ¨λΈ κ²½λ‰β€Άμµμ ν™” λ¶„μ•Όμ—μ„ κ°€μ¥ μ¤‘μ”ν• μ£Όμ λ΅ λ– μ¤λ¥΄κ³  μλ‹¤.

λ‚΄μ©μ„ μ‚΄ν΄λ³΄κΈ°μ „, `LoRA` λ” μ΄λ―Έ μ‚¬μ „ν•™μµμ„ μ™„λ£ν• λ¨λΈμ„ νμΈνλ‹ν•  λ• μ‚¬μ©ν•΄μ•Όν•¨μ„ λ‹¤μ‹ ν• λ² λ…μ‹¬ν•μ. μ΄λ² ν¬μ¤ν…μ—μ„λ” λ‘κ°€μ§€λ¥Ό μ§‘μ¤‘μ μΌλ΅ λ‹¤λ£° κ²ƒμ΄λ‹¤. 

**1) λ¨λΈ ν¬κΈ° μ¤„μΈ λ°©λ²•, 2) ν¬κΈ°λ¥Ό μ¤„μ΄λ©΄μ„λ„ λΉ„μ·ν• μ„±λ¥μ„ λ‚Ό μ μμ—λ μ΄μ **

### `π¤”Β Concept: Low-Rank Adaptation`

$$
h = W_0x + \Delta Wx =  W_0x + BAx
$$

μ•„μ΄λ””μ–΄λ” μƒλ‹Ήν κ°„λ‹¨ν•λ‹¤. μ‚¬μ „ν•™μµμ„ λ§μΉκ³  μλ ΄λ μƒνƒμ κ°€μ¤‘μΉ ν–‰λ ¬μ„ μλ―Έν•λ” $W_0$κ³Ό μƒλ΅μ΄ κ°€μ¤‘μΉ ν–‰λ ¬ $\Delta W$μ— λ¨λ‘ μ…λ ¥μ„ ν†µκ³Όμ‹ν‚¨λ‹¤. κ·Έλ¦¬κ³  λ‚μ¨ κ²°κ³Όλ¥Ό λ”ν•΄ λ‹¤μμΈµμ μ…λ ¥μΌλ΅ μ‚¬μ©ν•λ‹¤. μ¤νλ ¤ μƒλ΅μ΄ κ°€μ¤‘μΉ ν–‰λ ¬μ„ μ¶”κ°€ν•΄ νμΈνλ‹μ„ ν•λ”λ° μ–΄λ–»κ² ν›λ ¨ν•΄μ•Ό ν•λ” νλΌλ―Έν„° μλ¥Ό μ¤„μΌ μ μμ—μ„κΉ??

κ·Έ λΉ„λ°€μ€ `Freeze(Stop Gradient, require_grad=False)`μ™€ `Matrix Factorization`μ— μ¨μ–΄ μλ‹¤. λ¨Όμ € μ‚¬μ „ ν›λ ¨λ κ°€μ¤‘μΉ ν–‰λ ¬μ— `Freeze(Stop Gradient, require_grad=False)` λ¥Ό μ μ©ν•΄ κ·ΈλΌλ””μ–ΈνΈκ°€ νλ¥΄μ§€ μ•λ„λ΅ ν•λ‹¤. μ΄λ ‡κ² ν•λ©΄ νμΈνλ‹ κ³Όμ •μ—μ„ κ°€μ¤‘μΉκ°€ μ—…λ°μ΄νΈ λμ§€ μ•μ•„ μ‚¬μ „ ν•™μµμ—μ„ μµλ“ν• μ§€μ‹μ„ μ μ§€ν•  μ μμ„ λΏλ§ μ•„λ‹λΌ, ν•™μµμ„ μ„ν•΄ κ·ΈλΌλ””μ–ΈνΈλ¥Ό μ €μ¥ν•  ν•„μ”κ°€ μ—†μ–΄μ Έ νμΈνλ‹ λ• ν•„μ”ν• GPU VRAMμ„ νκΈ°μ μΌλ΅ μ¤„μΌ μ μλ‹¤.

μ²μμ— μ‚¬μ „ν•™μµ κ°€μ¤‘μΉλ¥Ό ν†µκ³Όν• κ°’κ³Ό μƒλ΅μ΄ κ°€μ¤‘μΉ ν–‰λ ¬ $\Delta W$λ¥Ό ν†µκ³Όν• κ°’μ„ μ„λ΅ λ”ν•λ‹¤κ³  μ–ΈκΈ‰ν–λ‹¤. κ·Έλ ‡λ‹¤λ©΄, λ‘ κ²°κ³Ό ν–‰λ ¬μ ν–‰λ ¬ ν¬κΈ°κ°€ λ™μΌν•΄μ•Ό ν•λ‹¤λ” κ²ƒμ΄λ‹¤. μ–΄λ–»κ² κΈ°μ΅΄λ³΄λ‹¤ μ‚¬μ΄μ¦λ” μ¤„μ΄λ©΄μ„ κ²°κ³Ό ν–‰λ ¬μ ν¬κΈ°λ” λ™μΌν•κ² λ§λ“¤μ–΄μ¤„ μ μμ„κΉ?? λ°”λ΅ Low Rank value $r$μ„ λ„μ…ν•΄ Matrix Factorization μ„ ν•λ‹¤.

$$
W_{d \times d} = \begin{bmatrix}
w_{1,1} & w_{1,2} & \cdots & w_{1,d} \\
w_{2,1} & w_{2,2} & \cdots & w_{2,d} \\
\vdots & \vdots & \ddots & \vdots \\
w_{d,1} & w_{d,2} & \cdots & w_{d,d}
\end{bmatrix}

$$

ν–‰λ ¬ κ³±μ…(matrix multiplication)μ„ λ‹¤μ‹ ν• λ² μƒκΈ°ν•΄λ³΄μ. `MxN` μ ν¬κΈ°λ¥Ό κ°–λ” ν–‰λ ¬μ— `NxK`μ ν¬κΈ°λ¥Ό κ°–λ” ν–‰λ ¬μ„ κ³±ν•΄μ£Όλ©΄ `MxK` μ ν¬κΈ°λ¥Ό κ°–λ” ν–‰λ ¬μ„ λ§λ“¤μ–΄μ¤„ μ μλ‹¤. λ§μ°¬κ°€μ§€λ‹¤. `dxd` ν¬κΈ°μΈ μ‚¬μ „ν•™μµμ κ°€μ¤‘μΉ ν–‰λ ¬ $W_{d \times d}$κ³Ό ν¬κΈ°λ¥Ό λ§μ¶”κΈ° μ„ν•΄, `dxd` μ§λ¦¬ ν–‰λ ¬μ„ κ°κ° `dxr`, `rxd` μ ν¬κΈ°λ¥Ό κ°–λ” λ‘ ν–‰λ ¬ $A, B$λ΅ λ¶„ν•΄ν•λ‹¤. μ΄ λ•, ν–‰λ ¬ $A$μ μ—΄μ°¨μ›κ³Ό ν–‰λ ¬ $B$μ°¨μ›μ ν–‰μ°¨μ› ν¬κΈ°λ¥Ό ν‘ν„ν•λ” $r$μ— λ°”λ΅ Low Rank value $r$μ„ λ€μ…ν•λ©΄ λλ‹¤.

$$
\Delta W_{d \times d} = A_{d \times r}\ B_{r \times d} = \begin{bmatrix}
w_{1,1} & w_{1,2} & w_{1,r} \\
w_{2,1} & w_{2,2} & w_{2,r} \\
\vdots & \vdots & \vdots \\
w_{d,1} & w_{d,2} & w_{d,r}
\end{bmatrix}\begin{bmatrix}
w_{1,1} & w_{2,1} & w_{d,1} \\
w_{1,2} & w_{2,2} & w_{d,2} \\
\vdots & \vdots & \vdots \\
w_{1,r} & w_{2,r} & w_{d,r}
\end{bmatrix}

$$

$r=3$μ΄λΌκ³  κ°€μ •ν•κ³  `768x768` μ§λ¦¬ κΈ°μ΅΄ κ°€μ¤‘μΉ ν–‰λ ¬ $W$κ³Ό `768x3`, `3x768`μ ν¬κΈ°λ¥Ό κ°–λ” $\Delta W = BA$μ νλΌλ―Έν„° κ°μλ¥Ό λΉ„κµν•΄λ³΄μ. κ³„μ‚°ν•΄λ³΄λ©΄ μ „μλ” `589,824`κ°, ν›„μλ” `4608`κ°κ°€ λλ‹¤. μ •ν™•ν•κ² `128`λ°° μ°¨μ΄κ°€ λ‚λ‹¤. νΈλμ¤ν¬λ¨Έ λ¨λΈ μ†μ—λ” ν–‰λ ¬ $W$κ³Ό κ°™μ€ ν¬κΈ°λ¥Ό κ°–λ” κ°€μ¤‘μΉ ν–‰λ ¬μ΄ λ‹¨μΌ μΈμ½”λ” λ‚΄λ¶€, ν•λ‚μ μ–΄ν…μ… λ μ΄μ–΄λ§ ν•΄λ„ 4κ°($W_q, W_k, W_v, W_o$)κ°€ μλ‹¤. `BERT-base` λ¨λΈμ„ κΈ°μ¤€μΌλ΅ λ³΄λ©΄, ν•΄λ‹Ή λ¨λΈμ΄ `12`κ°μ μΈμ½”λ”λ΅ κµ¬μ„±λμ–΄ μμΌλ‹κΉ μ΄ `48`κ°μ κ°€μ¤‘μΉ ν–‰λ ¬μ΄ μκ³ , μ–΄λ¦Όμ΅μ•„λ„ `48*128`λ°°μ ν•™μµ νλΌλ―Έν„° κ°μ† ν¨κ³Όλ¥Ό λ‚Ό μ μλ‹¤. λ¨λΈμ λ μ΄μ–΄κ°€ λ§μΌλ©΄ λ§μ„μλ΅ λ” μΆ‹μ€ ν¨μ¨μ„ λ³΄μΈλ‹¤.

<p markdown="1" align="center">
![Resnet50 Memeory Type in GPU](/assets/images/lora/cuda_memory.png){: .align-center}{: width="100%", height="50%"}{: .image-caption}
__*[Resnet50 Memeory Type in GPU](https://pytorch.org/blog/understanding-gpu-memory-1/)*__
</p>

μ„ κ·Έλ¦Όμ€ νμ΄ν† μΉ κ³µμ‹ λΈ”λ΅κ·Έμ—μ„ νΌμ¨ μλ£λ΅, ν•™μµ λ• ResNet50μ GPU VRAM μ μ μ¨ μ¶”μ΄λ” λ¬Όλ΅  λ¨λΈμ κ°λ³„ κµ¬μ„±μ”μ†μ λ©”λ¨λ¦¬ λΉ„μ¨κΉμ§€ μμ„Έν λ³΄μ—¬μ¤€λ‹¤. λ¨Όμ € `Parameter`μ™€ `Optimizer State`λ¥Ό λ³΄μ. `Parameter` λ” λ¨λΈμ—μ„ ν›λ ¨μ„ ν†µν•΄ μ—…λ°μ΄νΈκ°€ ν•„μ”ν• λ¨λ“  κµ¬μ„± μ”μ†λ¥Ό λ§ν•λ‹¤. `Freeze`, `require_grad=False` `@torch.no_grad()` , `torch.register_buffer()` μ μν–¥μ„ λ°›μ§€ μ•μ€ λ¨λΈ λ‚΄λ¶€μ λ¨λ“  ν…μ„λΌκ³  λ³΄λ©΄ λλ‹¤. 

ν•νΈ, `Optimizer State` λ” μµν‹°λ§μ΄μ €μ μµμ ν™” μν–‰μ— ν•„μ”ν• λ¨λ“  μ •λ³΄λ“¤μ„ μλ―Έν•λ”λ°, μλ¥Ό λ“¤μ–΄ μ—…λ°μ΄νΈ λ  ν…μ„μ λ©”νƒ€ μ •λ³΄, μ—¬λ¬ ν•μ΄νΌνλΌλ―Έν„° κ°’ κ°™μ€ κ²ƒλ“¤μ΄ λ‹΄κ²¨ μλ‹¤. 

μ΄ λ‘ μ”μ†κ°€ λ¨λΈμ GPU VRAMμ„ μ°¨μ§€ν•λ” λΉ„μ¨μ΄ μƒλ‹Ήν ν¬λ‹¤. ν•μ§€λ§ λ‘ μ”μ† λ¨λ‘ νλΌλ―Έν„° κ°μμ— λΉ„λ΅€ν•λ―€λ΅ `LoRA` μ μ©μΌλ΅ νλΌλ―Έν„° κ°μλ¥Ό μ¤„μ΄λ©΄, GPU VRAMμ„ νκΈ°μ μΌλ΅ μ¤„μΌ μ μλ‹¤.

λν• νμ΄ν† μΉλ” μ—­μ „ν μν–‰μ„ μ„ν•΄ κ·ΈλΌλ””μ–ΈνΈλ¥Ό νλΌλ―Έν„°μ™€ λ™μΌν• λ¨μ–‘(shape)μ„ κ°–λ” ν…μ„λ΅ μ €μ¥λλ‹¤λ” μ μ„ κ°μ•ν•λ©΄, κΈ°μ΅΄μ Full-Rank ν…μ„ λ€μ‹  Low-Rank ν…μ„λ¥Ό ν•™μµμ— μ΄μ©ν•¨μΌλ΅μ„ κ·ΈλΌλ””μ–ΈνΈ ν…μ„μ ν¬κΈ° μ—­μ‹ νκΈ°μ μΌλ΅ μ¤„μΌ μ μκ² λ‹¤.

νΈλμ¤ν¬λ¨Έ κ³„μ—΄μ λ¨λΈλ“¤μ΄ ResNet λ€λΉ„ μ••λ„μ μΌλ΅ νλΌλ―Έν„° κ°μκ°€ λ§κΈ° λ•λ¬Έμ— `LoRA`λ¥Ό μ μ©ν•λ‹¤λ©΄ ν›¨μ”¬ ν° ν¨κ³Όλ¥Ό λ³Ό μ μμ„ κ²ƒμ΄λ‹¤.

μ§€κΈκΉμ§€ `LoRA` κ°€ μ μ‹ν•λ” λ°©λ²•λ΅ μ΄ μ–΄λ–»κ² νκΈ°μ μΌλ΅ ν•™μµ νλΌλ―Έν„°λ¥Ό μ¤„μ΄κ³  λ‚μ•„κ°€ λ¨λΈμ΄ μ°¨μ§€ν•λ” GPU VRAM ν¬κΈ°λ¥Ό κ°μ†μ‹μΌ°λ”μ§€ μ•μ•„ λ³΄μ•λ‹¤. μ΄μ  `LoRA`λ¥Ό μ μ©ν•΄λ„ μΌλ°μ μΈ νμΈνλ‹ λ°©λ²•κ³Ό λΉ„μ·ν• μ„±λ¥μ„ μ μ§€ν•  μ μμ—λ”μ§€ μ•μ•„λ³΄μ.