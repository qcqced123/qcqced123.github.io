---
title: "ðŸŒ† [DistilBERT] DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter"
excerpt: "DistilBERT Official Paper Review with Pytorch Implementation"
permalink: "/nlp/distilbert"
toc: true  # option for table of contents
toc_sticky: true  # option for table of content
categories:
  - NLP
tags:
  - Natural Language Process
  - DistilBERT
  - BERT
  - Self-Attention
  - Pytorch

last_modified_at: 2024-03-11T12:00:00-05:00
---
### `ðŸ”­Â Overview`

`DistilBERT` ëŠ” í—ˆê¹… íŽ˜ì´ìŠ¤ ì—°êµ¬ì§„ì´ 2019ë…„ ë°œí‘œí•œ BERTì˜ ë³€í˜•ìœ¼ë¡œì„œ, On-Device Ai ê°œë°œì„ ëª©í‘œë¡œ ê²½ëŸ‰í™”ì— ì´ˆì ì„ ë§žì¶˜ ëª¨ë¸ì´ë‹¤. GPT, BERTì˜ ë“±ìž¥ ì´í›„, NLP ë¶„ì•¼ì—ì„œ ë¹„ì•½ì ì¸ ì„±ëŠ¥ í–¥ìƒì´ ì´ë¤„ì¡ŒìŒì—ë„ ë¶ˆêµ¬í•˜ê³ , í„°ë¬´ë‹ˆ ì—†ëŠ” ëª¨ë¸ ì‚¬ì´ì¦ˆì™€ ì»´í“¨íŒ… ë¦¬ì†ŒìŠ¤ ìš”êµ¬ë¡œ ì¸í•´ ì‹¤ìƒí™œ ì ìš© ê°™ì€ í™œìš©ì„±ì€ ì—¬ì „ížˆ í•´ê²°í•´ì•¼í•  ë¬¸ì œë¡œ ë‚¨ì•„ ìžˆì—ˆë‹¤. Googleì—ì„œ ë°œí‘œí•œ ì´ˆê¸° `BERT-base-uncased` ë§Œ í•´ë„ íŒŒë¼ë¯¸í„°ê°€ 1ì–µ 1ì²œë§Œê°œ ìˆ˜ì¤€ì— ë‹¬í•œë‹¤. 

ì´ë¥¼ ë‹¤ì–‘í•œ ë¹„ì¦ˆë‹ˆìŠ¤ ìš”êµ¬ ìƒí™©ì— ì ìš©í•  ìˆ˜ ìžˆìœ¼ë ¤ë©´ ìµœì†Œí•œ 8GB ì´ìƒì˜ ê°€ì†ê¸° ì „ìš© RAM ê³µê°„ì„ ìš”êµ¬ë¡œ í•œë‹¤. ì˜¤ëŠ˜ë‚  ê°œì¸ìš© PC í˜¹ì€ ì„œë²„ ì»´í“¨í„°ì˜ ê²½ìš°, 8GB ì´ìƒì˜ VRAMì´ ë‹¬ë¦° GPUê°€ ì¼ë°˜ì ìœ¼ë¡œ íƒ‘ìž¬ë˜ê¸° ë•Œë¬¸ì— í¬ê²Œ ë¬¸ì œ ë  ê²ƒ ì—†ëŠ” ìš”êµ¬ì‚¬í•­ì´ì§€ë§Œ, On-Device í™˜ê²½ì—ì„œëŠ” ì´ì•¼ê¸°ê°€ ë‹¬ë¼ì§„ë‹¤. ìµœì‹  í•˜ì´ì—”ë“œ ìŠ¤ë§ˆíŠ¸í°ì¸ Galaxy S24 Ultra, iPhone 15 Proì˜ ê²½ìš° 12GB, 8GBì˜ ëž¨ ìš©ëŸ‰ì„ ë³´ìœ í•˜ê³  ìžˆë‹¤. ê·¸ë§ˆì €ë„ ëŒ€ë¶€ë¶„ì˜ ì˜¨ë””ë°”ì´ìŠ¤ í™˜ê²½ì€ SoC êµ¬ì¡°ë¥¼ ì±„íƒí•˜ê³  ìžˆê¸° ë•Œë¬¸ì— ì „ìš© ê°€ì†ê¸°ê°€ ì˜¨ì „ížˆ ì € ëª¨ë“  ëž¨ ê³µê°„ì„ í™œìš©í•  ìˆ˜ ì—†ë‹¤. 

ë”°ë¼ì„œ ì˜¨ë””ë°”ì´ìŠ¤ì— Aië¥¼ ì ìš©í•˜ê¸° ìœ„í•´ì„œëŠ” íšê¸°ì ì¸ ëª¨ë¸ ê²½ëŸ‰í™”ê°€ í•„ìš”í•œ ìƒí™©ì´ê³  ê·¸ ì¶œë°œì ì´ ëœ ì—°êµ¬ê°€ ë°”ë¡œ `DistilBERT`ë‹¤. ë¡œì»¬ ë””ë°”ì´ìŠ¤ í™˜ê²½ì—ì„œë„ ì–¸ì–´ ëª¨ë¸ì„ í™œìš©í•˜ê¸° ìœ„í•´ í—ˆê¹… íŽ˜ì´ìŠ¤ ì—°êµ¬ì§„ì€ ì§€ì‹ ì¦ë¥˜ ê¸°ë²•ì„ í™œìš©í•´ ì¸ì½”ë” ê¸°ë°˜ ì–¸ì–´ ëª¨ë¸ì˜ íŒŒë¼ë¯¸í„°ë¥¼ íšê¸°ì ìœ¼ë¡œ ì¤„ì´ëŠ”ë° ì„±ê³µí•œë‹¤.


ì •ë¦¬í•˜ìžë©´, `DistilBERT` ëª¨ë¸ì€ ê¸°ì¡´ BERTì˜ êµ¬ì¡°ì  ì¸¡ë©´ ê°œì„ ì´ ì•„ë‹Œ, ì‚¬ì „í•™ìŠµ ë°©ë²• íŠ¹ížˆ ê²½ëŸ‰í™”ì— ì´ˆì ì„ ë§žì¶˜ ì‹œë„ë¼ê³  ë³¼ ìˆ˜ ìžˆë‹¤. ë”°ë¼ì„œ ì–´ë–¤ ëª¨ë¸ì´ë”ë¼ë„, ì¸ì½”ë” ì–¸ì–´ ëª¨ë¸ì´ë¼ë©´ ëª¨ë‘ `DistilBERT` êµ¬ì¡°ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìžˆìœ¼ë©°, ê¸°ì¡´ ë…¼ë¬¸ì—ì„œëŠ” ì›ë³¸ BERT êµ¬ì¡°ë¥¼ ì‚¬ìš©í–ˆë‹¤. ì´ë²ˆ í¬ìŠ¤íŒ…ì—ì„œë„ BERT êµ¬ì¡°ì— ëŒ€í•œ ì„¤ëª… ëŒ€ì‹ , `DistilBERT`ì˜ ì‚¬ì „ í•™ìŠµ ë°©ë²•ë¡ ì¸ `Knowledge Distillation`ì— ëŒ€í•´ì„œë§Œ ë‹¤ë£¨ë ¤ê³  í•œë‹¤.  

### `ðŸŒ†Â Knowledge Distillations`

$$
\min_{\theta}\sum_{x \in X} \alpha \mathcal{L}_{\text{KL}}(x, \theta) + \beta \mathcal{L}_{\text{MLM}}(x, \theta) + \gamma \mathcal{L}_{\text{Cos}}(x, \theta)
$$

`DistilBERT`ëŠ” Teacher-Student Architectureë¥¼ ì°¨ìš©í•´ ìƒëŒ€ì ìœ¼ë¡œ ìž‘ì€ íŒŒë¼ë¯¸í„° ì‚¬ì´ì¦ˆë¥¼ ê°–ëŠ” `Student` ëª¨ë¸ì—ê²Œ `Teacher`ì˜ ì§€ì‹ì„ ì „ìˆ˜í•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•œë‹¤. ë”°ë¼ì„œ `Teacher` ëª¨ë¸ì€ ì´ë¯¸ ì‚¬ì „ í•™ìŠµì„ ë§ˆì¹˜ê³  ìˆ˜ë ´ëœ ìƒíƒœì˜ ê°€ì¤‘ì¹˜ë¥¼ ê°–ê³  ìžˆëŠ” ëª¨ë¸ì„ ì‚¬ìš©í•´ì•¼ í•œë‹¤. ë”ë¶ˆì–´ Teacher ëª¨ë¸ì€ êµ¬ì¡°ë§Œ ê¸°ì¡´ BERTë¥¼ ë”°ë¥´ë˜, ì‚¬ì „ í•™ìŠµ ë°©ì‹ì€ RoBERTaì˜ ë°©ì‹ê³¼ ë™ì¼(NSP ì œê±°, Dynamic Masking ì ìš©)í•˜ê²Œ í›ˆë ¨ë˜ì–´ì•¼ í•œë‹¤.

í•œíŽ¸, `Student` ëª¨ë¸ì€ `Teacher`ì˜ 60%ì •ë„ íŒŒë¼ë¯¸í„° ì‚¬ì´ì¦ˆë¥¼ ê°–ë„ë¡ ì¶•ì†Œí•˜ì—¬ ì‚¬ìš©í•œë‹¤. ì´ ë•Œ ì¶•ì†ŒëŠ” ëª¨ë¸ì˜ `depth`(ë ˆì´ì–´ ê°œìˆ˜)ì—ë§Œ ì ìš©í•˜ëŠ”ë°, ì—°êµ¬ì§„ì— ë”°ë¥´ë©´ `width`(ì€ë‹‰ì¸µ í¬ê¸°)ëŠ” ì¶•ì†Œë¥¼ ì ìš©í•´ë„ ì—°ì‚° íš¨ìœ¨ì´ ì¦ê°€í•˜ì§€ ì•ŠëŠ”ë‹¤ê³  í•œë‹¤. ì •ë¦¬í•˜ë©´ `Teacher` ëª¨ë¸ì˜ `ë ˆì´ì–´ ê°œìˆ˜*0.6`ì˜ ê°œìˆ˜ë§Œí¼ ì¸ì½”ë”ë¥¼ ìŒ“ìœ¼ë©´ ëœë‹¤ëŠ” ê²ƒì´ë‹¤. 

ê·¸ë¦¬ê³  ìµœëŒ€í•œ `Teacher`ì˜ ì§€ì‹ì„ ì „ìˆ˜í•´ì•¼ í•˜ê¸° ë•Œë¬¸ì—, ë°ì´í„°ëŠ” `Teacher` ë¥¼ ìˆ˜ë ´ì‹œí‚¨ ê²ƒê³¼ ë™ì¼í•œ ì„¸íŠ¸ë¥¼ ì´ìš©í•´ì•¼ í•œë‹¤. ì´ ë•Œ, Teacher ëª¨ë¸ì€ ì´ë¯¸ MLE ë°©ì‹ìœ¼ë¡œ í›ˆë ¨ì´ ëœ ìƒíƒœë¼ì„œ ë¡œì§“ì´ ë‹¨ì¼ í† í° í•˜ë‚˜ ìª½ìœ¼ë¡œ ì ë ¤ ìžˆì„ ê°€ëŠ¥ì„±ì´ ë§¤ìš° ë†’ë‹¤. ì´ëŠ” `Student` ëª¨ë¸ì˜ ì¼ë°˜í™” ëŠ¥ë ¥ì— ì•…ì˜í–¥ì„ ë¯¸ì¹  ê°€ëŠ¥ì„±ì´ ë†’ë‹¤. ë”°ë¼ì„œ Temperature ë³€ìˆ˜ $T$ ë„ìž…í•´ ì†Œí”„íŠ¸ ë§¥ìŠ¤(ë¡œì§“)ì˜ ë¶„í¬ë¥¼ í‰íƒ„í™” í•œë‹¤. ì´ë ‡ê²Œ í•˜ë©´, `argmax()` ê°€ ì•„ë‹Œ ë‹¤ë¥¸ í† í° í‘œí˜„ì— ëŒ€í•´ì„œë„ `Student` ëª¨ë¸ì´ ì§€ì‹ì„ ìŠµë“í•  ìˆ˜ ìžˆì–´ì„œ í’ë¶€í•œ ë¬¸ë§¥ì„ í•™ìŠµí•˜ê³  ì¼ë°˜í™” ëŠ¥ë ¥ì„ ë†’ì´ëŠ”ë° ë„ì›€ì´ ëœë‹¤. ì´ë¥¼ `ì•”í‘ ì§€ì‹(Dark Knowledge)` ì„ í™œìš©í•œë‹¤ê³  í‘œí˜„í•œë‹¤. Temperature ë³€ìˆ˜ $T$ ë„ìž…í•œ ì†Œí”„íŠ¸ë§¥ìŠ¤ í•¨ìˆ˜ ìˆ˜ì‹ì€ ì•„ëž˜ì™€ ê°™ë‹¤.

$$
\text{softmax}(x_i) = \frac{e^{\frac{x_i}{\tau}}}{\sum_{j} e^{\frac{x_j}{\tau}}}
$$

ìˆ˜ì‹ìƒ ë³€ìˆ˜ $T$ì˜ ê°’ì„ 1ì´ìƒìœ¼ë¡œ ì„¸íŒ…í•´ì•¼ í‰íƒ„í™”ë¥¼ í•  ìˆ˜ ìžˆë‹¤. ë”°ë¼ì„œ ì—°êµ¬ì§„ì€ $T =2$ ë¡œ ë‘ê³  ì‚¬ì „ í•™ìŠµì„ ì§„í–‰í–ˆë‹¤(ë…¼ë¬¸ì— ê³µê°œì•ˆë¨, GitHubì— ìžˆìŒ). ì´ë²ˆ íŒŒíŠ¸ ë§¨ ì²˜ìŒì— ë“±ìž¥í•œ ìˆ˜ì‹ì„ ë‹¤ì‹œ ë³´ìž. ê²°êµ­ `DisilBERT`ì˜ ëª©ì í•¨ìˆ˜ëŠ” 3ê°€ì§€ ì†ì‹¤ì˜ ê°€ì¤‘í•©ìœ¼ë¡œ êµ¬ì„±ëœë‹¤. ì´ì œë¶€í„°ëŠ” ê°œë³„ ì†ì‹¤ì— ëŒ€í•´ì„œ ìžì„¸ížˆ ì‚´íŽ´ë³´ìž.

#### `ðŸŒ†Â Distillation Loss: KL-Divergence Loss`

$$
\text{KL-Divergence}(P || Q) = \sum_{i} P(i) \log \frac{P(i)}{Q(i)}
$$

ì¦ë¥˜ ì†ì‹¤ë¡œ ì‚¬ìš©ë˜ëŠ” `KL-Divergence Loss`ëŠ” ë‘ í™•ë¥  ë¶„í¬ ê°„ì˜ ì°¨ì´ë¥¼ ì¸¡ì •í•˜ëŠ” ì§€í‘œ ì¤‘ í•˜ë‚˜ë‹¤. ì£¼ë¡œ í™•ë¥  ë¶„í¬ Pì™€ Q ì‚¬ì´ì˜ ì°¨ì´ë¥¼ ë‚˜íƒ€ë‚´ëŠ”ë°, ê°œë³„ ìš”ì†Œì˜ í™•ë¥ ê°’ ì°¨ì´ê°€ í´ìˆ˜ë¡ í•©ì‚°ê°’ì€ ì»¤ì ¸ ì†ì‹¤ì´ ì»¤ì§€ê²Œ ëœë‹¤. ë°˜ëŒ€ë¡œ ë‘ ë¶„í¬ì˜ ê°œë³„ ìš”ì†Œ í™•ë¥ ê°’ ì°¨ì´ê°€ ìž‘ë‹¤ë©´ ë‹¹ì—°ížˆ, ë‘ ë¶„í¬ê°€ ìœ ì‚¬í•˜ë‹¤ëŠ” ì˜ë¯¸ì´ë¯€ë¡œ ì†ì‹¤ ì—­ì‹œ ìž‘ì•„ì§€ê²Œ ëœë‹¤. ì¼ë°˜ì ìœ¼ë¡œ `KL-Divergence Loss` ì—ì„œ í™•ë¥ ë¶„í¬ $P$ ê°€ ì´ìƒì ì¸ í™•ë¥  ë¶„í¬ë¥¼, $Q$ ê°€ ëª¨ë¸ì´ ì˜ˆì¸¡í•œ í™•ë¥ ë¶„í¬ë¥¼ ì˜ë¯¸í•œë‹¤. ë”°ë¼ì„œ `DistilBERT`ì˜ ê²½ìš° í™•ë¥ ë¶„í¬ $P$ ìžë¦¬ì—ëŠ” `Teacher` ëª¨ë¸ì˜ ì†Œí”„íŠ¸ë§¥ìŠ¤ ë¶„í¬ê°€, $Q$ ì—ëŠ” `Student` ëª¨ë¸ì˜ ì†Œí”„íŠ¸ë§¥ìŠ¤ ë¶„í¬ê°€ ëŒ€ìž…ë˜ë©´ ëœë‹¤. ì´ ë•Œ ë‘ í™•ë¥ ë¶„í¬ ëª¨ë‘, ì•”í‘ ì§€ì‹ íšë“ì„ ìœ„í•´ ì†Œí”„íŠ¸ë§¥ìŠ¤ í‰íƒ„í™”ë¥¼ ì ìš©í•œ ê²°ê³¼ë¥¼ ì‚¬ìš©í•œë‹¤. ë…¼ë¬¸ì—ì„œ, ì„ ìƒ ëª¨ë¸ ì˜ˆì¸¡ì— í‰íƒ„í™”ë¥¼ ì ìš©í•œ ê²ƒì„ `ì†Œí”„íŠ¸ ë¼ë²¨`, í•™ìƒ ëª¨ë¸ì˜ ê²ƒì— ì ìš©í•œ ê²°ê³¼ëŠ” `ì†Œí”„íŠ¸ ì˜ˆì¸¡`ì´ë¼ê³  ë¶€ë¥¸ë‹¤.

#### `ðŸŒ†Â Student Loss: MLM Loss`

$$
\mathcal{L}_{\text{MLM}} = - \sum_{i=1}^{N} \sum_{j=1}^{L} \mathbb{1}_{m_{ij}} \log \text{softmax}(x_{ij})
$$

í•™ìƒ ì†ì‹¤ì€ ë§ê·¸ëŒ€ë¡œ ê¸°ë³¸ì ì¸ MLM ì†ì‹¤ì„ ë§í•œë‹¤. ì •í™•í•œ ì†ì‹¤ê°’ ê³„ì‚°ì„ ìœ„í•´ì„œ í•™ìƒì˜ ì†Œí”„íŠ¸ë§¥ìŠ¤ ë¶„í¬ì— í‰íƒ„í™”ë¥¼ ì ìš©í•˜ì§€ ì•ŠëŠ”ë‹¤. ì´ë¥¼ ë…¼ë¬¸ì—ì„œëŠ” `í•˜ë“œ ì˜ˆì¸¡`ì´ë¼ê³  ë¶€ë¥¸ë‹¤. ë¼ë²¨ ì—­ì‹œ `Teacher`ë¡œë¶€í„° ë‚˜ì˜¨ ê²ƒì´ ì•„ë‹Œ ì›ëž˜ MLM ìˆ˜í–‰ì— ì‚¬ìš©ë˜ëŠ” ë§ˆìŠ¤í‚¹ ë¼ë²¨ì„ ì‚¬ìš©í•œë‹¤.

#### `ðŸŒ†Â Cosine Embedding Loss: Contrastive Loss by cosine similarity`

$$
\mathcal{L}_{\text{COS}}(x,y) = \begin{cases} 1 - \cos(x_1, x_2), & \text{if } y = 1 \\ \max(0, \cos(x_1, x_2) - \text{margin}), & \text{if } y = -1 \end{cases}

$$

`Teacher` ëª¨ë¸ê³¼ `Student` ëª¨ë¸ì˜ ë§ˆì§€ë§‰ ì¸ì½”ë” ëª¨ë¸ì´ ì¶œë ¥í•˜ëŠ” ì€ë‹‰ê°’ì— ëŒ€í•œ `Contrastive Loss`ë¥¼ ì˜ë¯¸í•œë‹¤. ì´ ë•Œ `Distance Metric`ì€ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ ì‚¬ìš©í•œë‹¤. ê·¸ëž˜ì„œ ì½”ì‚¬ì¸ ìž„ë² ë”© ì†ì‹¤ì´ë¼ê³  ë…¼ë¬¸ì—ì„œ ì •ì˜í•˜ëŠ” ê²ƒìœ¼ë¡œ ì¶”ì •ëœë‹¤. ìœ„ ìˆ˜ì‹ì„ ìµœì í™”í•˜ëŠ” ê²ƒì„ ëª©ì ìœ¼ë¡œ í•œë‹¤. ì´ ë•Œ ë¼ë²¨ì€ `[BS, Seq_len]`ì˜ í¬ê¸°ë¥¼ ê°–ë˜, ëª¨ë“  ì›ì†ŒëŠ” 1ì´ ë˜ë„ë¡ ë§Œë“ ë‹¤. ì´ìœ ëŠ” ê°„ë‹¨í•˜ë‹¤. `Student` ëª¨ë¸ì˜ ì€ë‹‰ê°’ì´ `Teacher` ëª¨ë¸ì˜ ê²ƒê³¼ ìµœëŒ€í•œ ë¹„ìŠ·í•´ì§€ë„ë¡ ë§Œë“œëŠ”ê²Œ ìš°ë¦¬ ëª©ì ì´ê¸° ë•Œë¬¸ì´ë‹¤.

### `ðŸ‘©â€ðŸ’»Â Implementation by Pytorch`
ë…¼ë¬¸ì˜ ë‚´ìš©ê³¼ ì˜¤í”¼ì…œë¡œ ê³µê°œëœ ì½”ë“œë¥¼ ì¢…í•©í•˜ì—¬ íŒŒì´í† ì¹˜ë¡œ `DistilBERT`ë¥¼ êµ¬í˜„í•´ë´¤ë‹¤. ë…¼ë¬¸ì— í¬í•¨ëœ ì•„ì´ë””ì–´ë¥¼ ì´í•´í•˜ëŠ”ë°ëŠ” ì—­ì‹œ ì–´ë µì§€ ì•Šì•˜ì§€ë§Œ, íŽ˜ì´í¼ì— hyper-param í…Œì´ë¸”ì´ ë”°ë¡œ ì œì‹œë˜ì–´ ìžˆì§€ ì•Šì•„ ê³µê°œëœ ì½”ë“œë¥¼ ì•ˆ ë³¼ìˆ˜ê°€ ì—†ì—ˆë‹¤.

ì „ì²´ ëª¨ë¸ êµ¬ì¡° ëŒ€í•œ ì½”ë“œëŠ” **[ì—¬ê¸° ë§í¬](https://github.com/qcqced123/model_study)**ë¥¼ í†µí•´ ì°¸ê³ ë°”ëž€ë‹¤.

#### `ðŸ‘©â€ðŸ’»Â Knowledge Distillation Pipeline`

```python
def train_val_fn(self, loader_train, model: nn.Module, criterion: Dict[str, nn.Module], optimizer,scheduler) -> Tuple[Any, Union[float, Any]]:
    """ Function for train loop with validation for each batch*N Steps
    DistillBERT has three loss:

        1) distillation loss, calculated by soft targets & soft predictions
            (nn.KLDIVLoss(reduction='batchmean'))

        2) student loss, calculated by hard targets & hard predictions
            (nn.CrossEntropyLoss(reduction='mean')), same as pure MLM Loss

        3) cosine similarity loss, calculated by student & teacher logit similarity
            (nn.CosineEmbeddingLoss(reduction='mean')), similar as contrastive loss

    Those 3 losses are summed jointly and then backward to student model
    """
    scaler = torch.cuda.amp.GradScaler(enabled=self.cfg.amp_scaler)
    model.train()
    for step, batch in enumerate(tqdm(loader_train)):
        optimizer.zero_grad(set_to_none=True)
        inputs = batch['input_ids'].to(self.cfg.device, non_blocking=True)
        labels = batch['labels'].to(self.cfg.device, non_blocking=True)
        padding_mask = batch['padding_mask'].to(self.cfg.device, non_blocking=True)

        mask = padding_mask.unsqueeze(-1).expand(-1, -1, self.cfg.dim_model)  # for hidden states dim
        with torch.no_grad():
            t_hidden_state, soft_target = model.teacher_fw(
                inputs=inputs,
                padding_mask=padding_mask,
                mask=mask
            )  # teacher model's pred => hard logit

        with torch.cuda.amp.autocast(enabled=self.cfg.amp_scaler):
            s_hidden_state, s_logit, soft_pred, c_labels = model.student_fw(
                inputs=inputs,
                padding_mask=padding_mask,
                mask=mask
            )
            d_loss = criterion["KLDivLoss"](soft_pred.log(), soft_target)  # nn.KLDIVLoss
            s_loss = criterion["CrossEntropyLoss"](s_logit.view(-1, self.cfg.vocab_size), labels.view(-1))  # nn.CrossEntropyLoss
            c_loss = criterion["CosineEmbeddingLoss"](s_hidden_state, t_hidden_state, c_labels)  # nn.CosineEmbeddingLoss
            loss = d_loss*self.cfg.alpha_distillation + s_loss*self.cfg.alpha_student + c_loss*self.cfg.alpha_cosine  # linear combination loss

        scaler.scale(loss).backward()
        scaler.step(optimizer)
        scaler.update()
        scheduler.step()
```

#### `ðŸ‘©â€ðŸ’»Â Knowledge Distillation Model`

```python
class DistillationKnowledge(nn.Module, AbstractTask):
    """ Custom Task Module for Knowledge Distillation by DistilBERT Style Architecture
    DistilBERT Style Architecture is Teacher-Student Framework for Knowledge Distillation,

    And then they have 3 objective functions:
        1) distillation loss, calculated by soft targets & soft predictions
            (nn.KLDIVLoss(reduction='batchmean'))
        2) student loss, calculated by hard targets & hard predictions
            (nn.CrossEntropyLoss(reduction='mean')), same as pure MLM Loss
        3) cosine similarity loss, calculated by student & teacher logit similarity
            (nn.CosineEmbeddingLoss(reduction='mean')), similar as contrastive loss

    References:
        https://arxiv.org/pdf/1910.01108.pdf
        https://github.com/huggingface/transformers/blob/main/examples/research_projects/distillation/distiller.py
    """
    def __init__(self, cfg: CFG) -> None:
        super(DistillationKnowledge, self).__init__()
        self.cfg = CFG
        self.model = DistilBERT(
            self.cfg,
            self.select_model
        )
        self._init_weights(self.model)
        if self.cfg.teacher_load_pretrained:  # for teacher model
            self.model.teacher.load_state_dict(
                torch.load(cfg.checkpoint_dir + cfg.teacher_state_dict),
                strict=False
            )
        if self.cfg.student_load_pretrained:  # for student model
            self.model.student.load_state_dict(
                torch.load(cfg.checkpoint_dir + cfg.student_state_dict),
                strict=True
            )
        if self.cfg.freeze:
            freeze(self.model.teacher)
            freeze(self.model.mlm_head)

        if self.cfg.gradient_checkpoint:
            self.model.gradient_checkpointing_enable()

    def teacher_fw(
        self,
        inputs: Tensor,
        padding_mask: Tensor,
        mask: Tensor,
        attention_mask: Tensor = None,
        is_valid: bool = False
    ) -> Tuple[Tensor, Tensor]:
        """ teacher forward pass to make soft target, last_hidden_state for distillation loss """
        # 1) make soft target
        temperature = 1.0 if is_valid else self.cfg.temperature
        last_hidden_state, t_logit = self.model.teacher_fw(
            inputs,
            padding_mask,
            attention_mask
        )
        last_hidden_state = torch.masked_select(last_hidden_state, ~mask)  # for inverse select
        last_hidden_state = last_hidden_state.view(-1, self.cfg.dim_model)  # flatten last_hidden_state
        soft_target = F.softmax(
            t_logit.view(-1, self.cfg.vocab_size) / temperature**2,  # flatten softmax distribution
            dim=-1
        )  # [bs* seq, vocab_size]
        return last_hidden_state, soft_target

    def student_fw(
        self,
        inputs: Tensor,
        padding_mask: Tensor,
        mask: Tensor,
        attention_mask: Tensor = None,
        is_valid: bool = False
    ) -> Tuple[Tensor, Tensor, Tensor, Tensor]:
        """ student forward pass to make soft prediction, hard prediction for student loss """
        temperature = 1.0 if is_valid else self.cfg.temperature
        last_hidden_state, s_logit = self.model.teacher_fw(
            inputs,
            padding_mask,
            attention_mask
        )
        last_hidden_state = torch.masked_select(last_hidden_state, ~mask)  # for inverse select
        last_hidden_state = last_hidden_state.view(-1, self.cfg.dim_model)  # flatten last_hidden_state
        c_labels = last_hidden_state.new(last_hidden_state.size(0)).fill_(1)
        soft_pred = F.softmax(
            s_logit.view(-1, self.cfg.vocab_size) / temperature**2,  # flatten softmax distribution
            dim=-1
        )
        return last_hidden_state, s_logit, soft_pred, c_labels
```

#### `ðŸ‘©â€ðŸ’»Â DistilBERT Model`

```python
class DistilBERT(nn.Module, AbstractModel):
    """ Main class for DistilBERT Style Model, Teacher-Student Framework
    for Knowledge Distillation aim to lighter Large Scale LLM model. This model have 3 objective functions:

        1) distillation loss, calculated by soft targets & soft predictions
            (nn.KLDIVLoss(reduction='batchmean'))

        2) student loss, calculated by hard targets & hard predictions
            (nn.CrossEntropyLoss(reduction='mean')), same as pure MLM Loss

        3) cosine similarity loss, calculated by student & teacher logit similarity
            (nn.CosineEmbeddingLoss(reduction='mean')), similar as contrastive loss

    soft targets & soft predictions are meaning that logit are passed through softmax function applied with temperature T
    temperature T aim to flatten softmax layer distribution for making "Dark Knowledge" from teacher model

    hard targets & hard predictions are meaning that logit are passed through softmax function without temperature T
    hard targets are same as just simple labels from MLM Collator returns for calculating cross entropy loss

    cosine similarity loss is calculated by cosine similarity between student & teacher
    in official repo, they mask padding tokens for calculating cosine similarity, target for this task is 1
    cosine similarity is calculated by nn.CosineSimilarity() function, values are range to [-1, 1]

    you can select any other backbone model architecture for Teacher & Student Model for knowledge distillation
    but, in original paper, BERT is used for Teacher Model & Student
    and you must select pretrained model for Teacher Model, because Teacher Model is used for knowledge distillation,
    which is containing pretrained mlm head

    Do not pass gradient backward to teacher model!!
    (teacher model must be frozen or register_buffer to model or use no_grad() context manager)

    Args:
        cfg: configuration.CFG
        model_func: make model instance in runtime from config.json

    References:
        https://arxiv.org/pdf/1910.01108.pdf
        https://github.com/huggingface/transformers/blob/main/examples/research_projects/distillation/distiller.py
    """
    def __init__(self, cfg: CFG, model_func: Callable) -> None:
        super(DistilBERT, self).__init__()
        self.cfg = cfg
        self.teacher = model_func(self.cfg.teacher_num_layers)  # must be loading pretrained model containing mlm head
        self.mlm_head = MLMHead(self.cfg)  # must be loading pretrained model's mlm head

        self.student = model_func(self.cfg.student_num_layers)
        self.s_mlm_head = MLMHead(self.cfg)

    def teacher_fw(
        self,
        inputs: Tensor,
        padding_mask: Tensor,
        attention_mask: Tensor = None,
    ) -> Tuple[Tensor, Tensor]:
        """ forward pass for teacher model
        """
        last_hidden_state, _ = self.teacher(
            inputs,
            padding_mask,
            attention_mask
        )
        t_logit = self.mlm_head(last_hidden_state)  # hard logit => to make soft logit
        return last_hidden_state, t_logit

    def student_fw(
        self,
        inputs: Tensor,
        padding_mask: Tensor,
        attention_mask: Tensor = None
    ) -> Tuple[Tensor, Tensor]:
        """ forward pass for student model
        """
        last_hidden_state, _ = self.student(
            inputs,
            padding_mask,
            attention_mask
        )
        s_logit = self.s_mlm_head(last_hidden_state)  # hard logit => to make soft logit
        return last_hidden_state, s_logit
```
