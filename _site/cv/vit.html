<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.24.0 by Michael Rose
  Copyright 2013-2020 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->
<html lang="en" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>🌆 [ViT] An Image Is Worth 16x16 Words: Transformers For Image Recognition At Scale - AI/Business Study Log</title>
<meta name="description" content="ViT Official Paper Review with Pytorch Implementation">


  <meta name="author" content="qcqced">
  
  <meta property="article:author" content="qcqced">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="AI/Business Study Log">
<meta property="og:title" content="🌆 [ViT] An Image Is Worth 16x16 Words: Transformers For Image Recognition At Scale">
<meta property="og:url" content="http://localhost:4000/cv/vit">


  <meta property="og:description" content="ViT Official Paper Review with Pytorch Implementation">







  <meta property="article:published_time" content="2023-07-26T00:00:00+09:00">



  <meta property="article:modified_time" content="2023-07-27T02:00:00+09:00">



  

  


<link rel="canonical" href="http://localhost:4000/cv/vit">




<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    
      "@type": "Person",
      "name": "qcqced",
      "url": "http://localhost:4000/"
    
  }
</script>







<!-- end _includes/seo.html -->



  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="AI/Business Study Log Feed">


<!-- https://t.co/dKP3o1e -->
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css"></noscript>



<!-- Latex -->
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->
<link rel="apple-touch-icon" sizes="180x180" href="/assets/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/assets/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/assets/favicon-16x16.png">
<link rel="manifest" href="/assets/site.webmanifest">
<link rel="mask-icon" href="/assets/safari-pinned-tab.svg" color="#5bbad5">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="theme-color" content="#ffffff">
<!-- end custom head snippets -->

    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      TeX: {
        equationNumbers: {
          autoNumber: "AMS"
        }
      },
      tex2jax: {
      inlineMath: [ ['$', '$'] ],
      displayMath: [ ['$$', '$$'] ],
      processEscapes: true
    }
  });
  MathJax.Hub.Register.MessageHook("Math Processing Error",function (message) {
      alert("Math Processing Error: "+message[1]);
    });
  MathJax.Hub.Register.MessageHook("TeX Jax - parse error",function (message) {
      alert("Math Processing Error: "+message[1]);
    });
  </script>
  <script type="text/javascript" async
    src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>
  </head>

  <body class="layout--single">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
          AI/Business Study Log
          <span class="site-subtitle">NLP, Marketing</span>
        </a>
        
        
        <ul class="visible-links">
              
              
                  <li class="masthead__menu-item">
                      <a href="https://qcqced123.github.io/">Home</a>
                  </li>
              
          
              
              
                  <li class="dropdown ">
                      <a class="dropdown-toggle" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">CS/AI  <i class="fa fa-caret-down fa-sm" aria-hidden="true"></i><span class="caret"></span></a>
       
                      <ul class="dropdown-content" >
                          
                              <a class = "dropdown-item" href="/nlp/">    Natural Language Process</a>
                          
                              <a class = "dropdown-item" href="/multi-modal/">    Multi Modal</a>
                          
                              <a class = "dropdown-item" href="/cv/">    Computer Vision</a>
                          
                              <a class = "dropdown-item" href="/ml/">    Machine Learning</a>
                          
                              <a class = "dropdown-item" href="/framework-library/">    Framework & Library</a>
                          
                              <a class = "dropdown-item" href="/python/">    Python</a>
                          
                              <a class = "dropdown-item" href="/algorithm/">    Data Structure & Algorithm</a>
                          
                              <a class = "dropdown-item" href="/ps/">    Problem Solving</a>
                          
                      </ul>
                  </li>
              
          
              
              
                  <li class="dropdown ">
                      <a class="dropdown-toggle" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Math  <i class="fa fa-caret-down fa-sm" aria-hidden="true"></i><span class="caret"></span></a>
       
                      <ul class="dropdown-content" >
                          
                              <a class = "dropdown-item" href="/linear-algebra/">    Linear Algebra</a>
                          
                              <a class = "dropdown-item" href="/optimization-theory/">    Optimization Theory/Calculus</a>
                          
                              <a class = "dropdown-item" href="/signal-system/">    Signal & System</a>
                          
                      </ul>
                  </li>
              
          
              
              
                  <li class="dropdown ">
                      <a class="dropdown-toggle" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Business/Marketing  <i class="fa fa-caret-down fa-sm" aria-hidden="true"></i><span class="caret"></span></a>
       
                      <ul class="dropdown-content" >
                          
                              <a class = "dropdown-item" href="/device/">    Device</a>
                          
                              <a class = "dropdown-item" href="/semi-conductor/">    Semi-Conductor</a>
                          
                              <a class = "dropdown-item" href="/ai/">    AI</a>
                          
                      </ul>
                  </li>
              
          
              
              
                  <li class="masthead__menu-item">
                      <a href="/categories/">Category</a>
                  </li>
              
          
              
              
                  <li class="masthead__menu-item">
                      <a href="/about/">About</a>
                  </li>
              
          
       </ul>
       
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      





<div id="main" role="main">
  
  <div class="sidebar sticky">
  


<div itemscope itemtype="https://schema.org/Person" class="h-card">

  
    <div class="author__avatar">
      <a href="http://localhost:4000/">
        <img src="/assets/images/huggingface_emoji.png" alt="qcqced" itemprop="image" class="u-photo">
      </a>
    </div>
  

  <div class="author__content">
    <h3 class="author__name p-name" itemprop="name">
      <a class="u-url" rel="me" href="http://localhost:4000/" itemprop="url">qcqced</a>
    </h3>
    
      <div class="author__bio p-note" itemprop="description">
        <p>Interested in NLP, Marketing</p>

      </div>
    
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">Follow</button>
    <ul class="author__urls social-icons">
      
        <li itemprop="homeLocation" itemscope itemtype="https://schema.org/Place">
          <i class="fas fa-fw fa-map-marker-alt" aria-hidden="true"></i> <span itemprop="name" class="p-locality">Seoul, South Korea</span>
        </li>
      

      
        
          
            <li><a href="https://qcqced123.github.io" rel="nofollow noopener noreferrer me" itemprop="sameAs"><i class="fas fa-fw fa-link" aria-hidden="true"></i><span class="label">Website</span></a></li>
          
        
          
            <li><a href="https://github.com/qcqced123" rel="nofollow noopener noreferrer me" itemprop="sameAs"><i class="fab fa-fw fa-github" aria-hidden="true"></i><span class="label">GitHub</span></a></li>
          
        
          
            <li><a href="https://www.kaggle.com/qcqced" rel="nofollow noopener noreferrer me" itemprop="sameAs"><i class="fab fa-fw fa-kaggle" aria-hidden="true"></i><span class="label">Kaggle</span></a></li>
          
        
      

      

      
        <li>
          <a href="mailto:qcqced123@gmail.com" rel="me" class="u-email">
            <meta itemprop="email" content="qcqced123@gmail.com" />
            <i class="fas fa-fw fa-envelope-square" aria-hidden="true"></i><span class="label">Email</span>
          </a>
        </li>
      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      <!--
  <li>
    <a href="http://link-to-whatever-social-network.com/user/" itemprop="sameAs" rel="nofollow noopener noreferrer me">
      <i class="fas fa-fw" aria-hidden="true"></i> Custom Social Profile Link
    </a>
  </li>
-->
    </ul>
  </div>
</div>

  
  </div>



  <article class="page h-entry" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="🌆 [ViT] An Image Is Worth 16x16 Words: Transformers For Image Recognition At Scale">
    <meta itemprop="description" content="ViT Official Paper Review with Pytorch Implementation">
    <meta itemprop="datePublished" content="2023-07-26T00:00:00+09:00">
    <meta itemprop="dateModified" content="2023-07-27T02:00:00+09:00">

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title p-name" itemprop="headline">
            <a href="http://localhost:4000/cv/vit" class="u-url" itemprop="url">🌆 [ViT] An Image Is Worth 16x16 Words: Transformers For Image Recognition At Scale
</a>
          </h1>
          <p class="page__date">
            <a href="https://hits.seeyoufarm.com/localhost:4000/cv/vit"target="_blank">
              <img src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https://localhost:4000/cv/vit&count_bg=%23399DE2&title_bg=%236D6D6D&icon=pytorch.svg&icon_color=%23E7E7E7&title=Views&edge_flat=false"/>
            </a>
            <i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> Updated: <time datetime="2023-07-26T00:00:00+09:00">July 26, 2023</time>
            <!-- <div style="text-align: left;"> -->
            <!-- </div> -->
          </p>
          
          
        </header>
      

      <section class="page__content e-content" itemprop="text">
        
          <aside class="sidebar__right sticky">
            <nav class="toc">
              <header><h4 class="nav__title"><i class="fas fa-file-alt"></i> On this page</h4></header>
              <ul class="toc__menu"><li><a href="#overview">🔭 Overview</a></li><li><a href="#scalability-in-vit">🧠 Scalability in ViT</a></li><li><a href="#modeling">🌟 Modeling</a><ul><li><a href="#linear-projection-of-flattened-patches">🔬 Linear Projection of Flattened Patches</a></li><li><a href="#positional-embedding">🔢 Positional Embedding</a></li><li><a href="#-multi-head-attention">👩‍👩‍👧‍👦 Multi-Head Attention</a></li><li><a href="#️-mlp">🗳️ MLP</a></li><li><a href="#-vision-encoder-layer">📘 Vision Encoder Layer</a></li><li><a href="#-visionencoder">📚 VisionEncoder</a></li><li><a href="#-visiontransformer">🤖 VisionTransformer</a></li></ul></li><li><a href="#insight-from-experiment">🔬 Insight from Experiment</a><ul><li><a href="#insight-1-vit의-scalability-증명">💡 Insight 1. ViT의 Scalability 증명</a></li><li><a href="#insight-2-pure-self-attention은-좋은-이미지-피처를-추출하기에-충분하다">💡 Insight 2. Pure Self-Attention은 좋은 이미지 피처를 추출하기에 충분하다</a></li><li><a href="#insight-3-bottom2general-information-top2specific-information">💡 Insight 3. Bottom2General Information, Top2Specific Information</a></li><li><a href="#insight-4-vit는-cls-pooling-사용하는게-효율적">💡 Insight 4. ViT는 CLS Pooling 사용하는게 효율적</a></li><li><a href="#insight-5-vit는-absolute-1d-position-embedding-사용하는게-가장-효율적">💡 Insight 5. ViT는 Absolute 1D-Position Embedding 사용하는게 가장 효율적</a></li></ul></li><li><a href="#️conclusion">🧑‍⚖️ Conclusion</a></li></ul>

            </nav>
          </aside>
        
        <h3 id="overview"><code class="language-plaintext highlighter-rouge">🔭 Overview</code></h3>

<p>시작하기 앞서, 본 논문 리뷰를 수월하게 읽으려면 <code class="language-plaintext highlighter-rouge">Transformer</code> 에 대한 선이해가 필수적이다. 아직 <code class="language-plaintext highlighter-rouge">Transformer</code> 에 대해서 잘 모른다면 필자가 작성한 포스트를 읽고 오길 권장한다. 또한 본문 내용을 작성하면서 참고한 논문과 여러 포스트의 링크를 맨 밑 하단에 첨부했으니 참고 바란다. 시간이 없으신 분들은 중간의 코드 구현부를 생략하고 <code class="language-plaintext highlighter-rouge">Insight</code> 부터 읽기를 권장한다.</p>

<p><code class="language-plaintext highlighter-rouge">Vision Transformer</code>(이하 <code class="language-plaintext highlighter-rouge">ViT</code>)는 2020년 10월 Google에서 발표한 컴퓨터 비전용 모델이다. 자연어 처리에서 대성공을 거둔 트렌스포머 구조와 기법을 거의 그대로 비전 분야에 이식했다는 점에서 큰 의의가 있으며, 이후 컴퓨터 비전 분야의 트렌스포머 전성시대가 열리게 된 계기로 작용한다.</p>

<p>한편, <code class="language-plaintext highlighter-rouge">ViT</code> 의 설계 철학은 바로 <code class="language-plaintext highlighter-rouge">scalability(범용성)</code>이다. 신경망 설계에서 범용성이란, 모델의 확장 가능성을 말한다. 예를 들면 학습 데이터보다 더 크고 복잡한 데이터 세트를 사용하거나 모델의 파라미터를 늘려 사이즈를 키워도 여전히 유효한 추론 결과를 도출하거나 더 나은 성능을 보여주고 나아가 개선의 여지가 여전히 남아있을 때 <code class="language-plaintext highlighter-rouge">“확장성이 높다”</code> 라고 표현한다. 저자들은 논문 초반에 콕 찝어서 컴퓨터 비전 분야의 <code class="language-plaintext highlighter-rouge">scalability</code> 높이는 것이 이번 모델 설계의 목표였다고 밝히고 있다. <code class="language-plaintext highlighter-rouge">범용성</code>은 신경망 모델 설계에서 가장 큰 화두가 되는데 도메인마다 정의하는 의미에 차이가 미세하게 존재한다. 따라서  <code class="language-plaintext highlighter-rouge">ViT</code>의 저자들이 말하는 <code class="language-plaintext highlighter-rouge">범용성</code>이 무엇을 의미하는지 알아보는 것은 구체적인 모델 구조를 이해하는데 큰 도움이 될 것이다.</p>

<h3 id="scalability-in-vit"><code class="language-plaintext highlighter-rouge">🧠 Scalability in ViT</code></h3>

<p>논문 초반부에서 다음과 같은 문장이 서술 되어있다.</p>

<p><code class="language-plaintext highlighter-rouge">“Our Vision Transformer (ViT) attains excellent results when pre-trained at sufficient scale and transferred to tasks with fewer datapoints"</code></p>

<p>이 구문이 <code class="language-plaintext highlighter-rouge">ViT</code> 의 <code class="language-plaintext highlighter-rouge">Scalability</code>를 가장 잘 설명하고 있다고 생각한다. 저자들이 말하는 범용성은 결국 <code class="language-plaintext highlighter-rouge">backbone</code> 구조의 활용을 의미한다. 자연어 처리에 익숙한 독자라면 쉽게 이해가 가능할 것이다. <code class="language-plaintext highlighter-rouge">Transformer</code>, <code class="language-plaintext highlighter-rouge">GPT</code>, <code class="language-plaintext highlighter-rouge">BERT</code>의 등장 이후, 자연어 처리는 범용성을 갖는 데이터 세트로 사전 훈련한 모델을 활용해 <code class="language-plaintext highlighter-rouge">Task-Agnostic</code>하게 하나의 <code class="language-plaintext highlighter-rouge">backbone</code>으로 거의 모든 Task를 수행할 수 있으며, 작은 사이즈의 데이터라도 상당히 높은 수준의 추론 성능을 낼 수 있었다. 그러나 당시 컴퓨터 비전의 메인이었던 <code class="language-plaintext highlighter-rouge">Conv</code> 기반 모델들은 파인튜닝해도 데이터 크기가 작으면 일반화 성능이 매우 떨어지고, Task에 따라서 다른 아키텍처를 갖는 모델을 새롭게 정의하거나 불러와 사용해야 하는 번거로움이 있었다. 예를 들면 <code class="language-plaintext highlighter-rouge">Image Classfication</code> 에는 <code class="language-plaintext highlighter-rouge">ResNet</code>, <code class="language-plaintext highlighter-rouge">Segmentation</code> 에는 <code class="language-plaintext highlighter-rouge">U-Net</code>, <code class="language-plaintext highlighter-rouge">Object Detection</code> 은 <code class="language-plaintext highlighter-rouge">YOLO</code> 를 사용하는 것처럼 말이다. 반면 자연어 처리는 사전 학습된 모델 하나로 모든 NLU, 심지어는 NLG Task도 수행할 수 있다. 저자들은 이러한 범용성을 컴퓨터 비전에도 이식 시키고 싶었던 것 같다. 그렇다면 먼저 자연어 처리에서 트랜스포머 계열이 범용성을 가질 수 있었던 이유는 무엇인지 간단히 살펴보자.</p>

<p>저자들은 <code class="language-plaintext highlighter-rouge">self-attention</code>(내적)의 효율성, 모델의 구조적 탁월성 그리고 <code class="language-plaintext highlighter-rouge">self-supervised task</code>의 존재를 꼽는다. 그럼 이것들이 왜 범용성을 높이는데 도움이 될까??</p>

<p><code class="language-plaintext highlighter-rouge">self-attention(내적)</code>은 행렬 간 곱셉으로 정의 되어 설계가 매우 간편하고 병렬로 한번에 처리하는 것이 가능하기 때문에 효율적으로 전체 데이터를 모두 고려한 연산 결과를 얻을 수 있다.</p>

<p><code class="language-plaintext highlighter-rouge">Multi-Head Attention</code> 구조는 여러 차원의 의미 관계를 동시에 포착하고 그것을 앙상블한 것과 같은(실제로는 MLP) 결과를 얻을 수 있다는 점에서 구조적으로 탁월하다.</p>

<p>마지막으로 <code class="language-plaintext highlighter-rouge">MLM</code>, <code class="language-plaintext highlighter-rouge">Auto-Regression(LM) Task</code>는 데이터 세트에 별도의 인간의 개입(라벨링)이 필요하지 않기 때문에 가성비 있게 데이터와 모델의 사이즈를 늘릴 수 있게 된다.<br />
이제 논문에서 트랜스포머 계열이 가진 범용성을 어떻게 비전 분야에 적용했는지 주목하면서 모델 구조를 하나 하나 살펴보자.</p>

<h3 id="modeling"><code class="language-plaintext highlighter-rouge">🌟 Modeling</code></h3>

<p align="center">
<img src="/assets/images/vision_transformer/modeling_overview.png" alt="ViT Model Structure" class="align-center image-caption" width="100%&quot;, height=&quot;50%" />
<strong><em><a href="https://arxiv.org/abs/2010.11929">ViT Model Structure</a></em></strong>
</p>

<ul>
  <li><strong>1) Transfer <code class="language-plaintext highlighter-rouge">Scalability</code> from pure <code class="language-plaintext highlighter-rouge">Transformer</code> to Computer Vision</strong>
    <ul>
      <li><strong>Overcome <code class="language-plaintext highlighter-rouge">reliance</code> on Convolution(<code class="language-plaintext highlighter-rouge">Inductive Bias</code>) in Computer Vision</strong></li>
      <li><strong>Apply Self-Attention &amp; Architecture from vanilla NLP Transformers as <code class="language-plaintext highlighter-rouge">closely</code> as possible</strong></li>
      <li><strong>Treat Image as sequence of text token</strong></li>
      <li><strong>Make $P$ sub-patches from whole image, playing same role as token in NLP Transformer</strong></li>
    </ul>
  </li>
</ul>

<p>저자들은 먼저 <code class="language-plaintext highlighter-rouge">Conv</code> 에 대한 의존을 버릴 것을 주장한다. <code class="language-plaintext highlighter-rouge">Conv</code>가 가진 <code class="language-plaintext highlighter-rouge">Inductive Bias</code> 때문에 파인튜닝 레벨에서 데이터 크기가 작으면 일반화 성능이 떨어지는 것이라고 설명하고 있다. 이 말을 이해하려면 <code class="language-plaintext highlighter-rouge">Inductive Bias</code>에 대해서 먼저 알아야 한다. <code class="language-plaintext highlighter-rouge">Inductive Bias</code>란, 주어진 데이터로부터 일반화 성능을 높이기 위해 <code class="language-plaintext highlighter-rouge">‘입력되는 데이터는 ~ 할 것이다’</code>, <code class="language-plaintext highlighter-rouge">‘이런 특징을 갖고 있을 것이다’</code>와 같은 가정, 가중치, 가설 등을 기계학습 알고리즘에 적용하는 것을 말한다.</p>

<p><code class="language-plaintext highlighter-rouge">Conv</code> 연산 자체 (가중치 공유, 풀링 있는 <code class="language-plaintext highlighter-rouge">Conv Block</code>이 <code class="language-plaintext highlighter-rouge">Invariance</code>)의 기본 가정은 <code class="language-plaintext highlighter-rouge">translation equivariance</code>, <code class="language-plaintext highlighter-rouge">locality</code>이다. 사실 저자의 주장을 이해하는데 <code class="language-plaintext highlighter-rouge">equivariance</code>와 <code class="language-plaintext highlighter-rouge">locality</code>의 뜻이 무엇인지 파악하는 것은 크게 의미가 없다 (<code class="language-plaintext highlighter-rouge">equivariance</code>와 <code class="language-plaintext highlighter-rouge">invariance</code>에 대해서는 다른 포스팅에서 자세히 살펴보도록 하겠다). <strong><u>중요한 것은 입력 데이터에 가정을 더한다는 점이다.</u></strong> 만약 주어진 입력이 미리 가정한 <code class="language-plaintext highlighter-rouge">Inductive Bias</code> 에 벗어난다면 어떻게 될까??</p>

<p>아마 오버피팅 되거나 모델 학습이 수렴성을 갖지 못하게 될 것이다. 이미지 데이터도 Task에 따라 필요한 <code class="language-plaintext highlighter-rouge">Inductive Bias</code>가 달라진다. 예를 들어 <code class="language-plaintext highlighter-rouge">Segmentation</code>, <code class="language-plaintext highlighter-rouge">Detection</code> 의 경우는 이미지 속 객체의 위치, 픽셀 사이의 <code class="language-plaintext highlighter-rouge">spatial variance</code> 정보가 매우 중요하다. 한편, <code class="language-plaintext highlighter-rouge">Classification</code>은 <code class="language-plaintext highlighter-rouge">spatial invariance</code>가 중요하다. 목표 객체의 위치와 주변 특징보다 타겟 자체를 신경망이 인식하는 것이 중요하기 때문이다. 따라서  <code class="language-plaintext highlighter-rouge">ViT</code> 저자들은 어떤 Bias던 상관없이 편향을 갖고 데이터를 본다는 것 자체에 의문을 표하며, 이미지 역시 <code class="language-plaintext highlighter-rouge">Inductive Bias</code>에서 벗어나, 주어진 데이터 전체 특징(패치) 사이의 관계를 파악하는 과정에서 <code class="language-plaintext highlighter-rouge">scalability</code>를 획득할 수 있다고 주장한다.</p>

<p>그래서 <code class="language-plaintext highlighter-rouge">Conv</code>의 대안으로 상대적으로 <code class="language-plaintext highlighter-rouge">Inductive Bias</code> 가 부족한 <code class="language-plaintext highlighter-rouge">Self-Attention</code>, <code class="language-plaintext highlighter-rouge">Transformer Architecture</code>를 사용한다. 두가지의 효용성에 대해서는 이미 위에서 언급했기 때문에 생략하고, 여기서 짚고 넘어가야할 점은 <code class="language-plaintext highlighter-rouge">Self-Attention</code>이 <code class="language-plaintext highlighter-rouge">Conv</code> 대비 <code class="language-plaintext highlighter-rouge">Inductive Bias</code>가 적다는 점이다. Self-Attention 과정에는 여러 연산, 스케일 조정값들이 포함되지만 본질적으로 <code class="language-plaintext highlighter-rouge">“내적”</code> 이 중심이다. 내적은 그 어떤 편향 (<code class="language-plaintext highlighter-rouge">Conv</code>와 대조하려고 이렇게 서술했지만 사실 <code class="language-plaintext highlighter-rouge">Position Embedding</code> 더하는 것도 일종의 약한 <code class="language-plaintext highlighter-rouge">Inductive Bias</code>)이 존재하지 않는다. 일단 주어진 모든 데이터에 대해서 내적값을 산출하고 그 다음에 관계가 있다고 생각되는 정보를 추리기 때문이다. <code class="language-plaintext highlighter-rouge">Conv</code> 때와 달리 <code class="language-plaintext highlighter-rouge">‘입력되는 데이터는 ~ 할 것이다’</code>, <code class="language-plaintext highlighter-rouge">‘이런 특징을 갖고 있을 것이다’</code> 라는 가정이 없다. 이번 포스팅의 마지막 쯤에서 다시 다루겠지만 그래서 <code class="language-plaintext highlighter-rouge">ViT</code>는 인스턴스 사이의 모든 관계를 뽑아보는 <code class="language-plaintext highlighter-rouge">Self-Attention(내적)</code> 을 기반으로 만들어졌기 때문에 이미지의 <code class="language-plaintext highlighter-rouge">Global Information</code>을 포착하는데 탁월한 성능을 보이고, <code class="language-plaintext highlighter-rouge">Conv</code> 는 <strong><u>“중요한 정보는 근처 픽셀에 몰려있다라는”</u></strong> <code class="language-plaintext highlighter-rouge">Inductive Bias</code>  덕분에 <code class="language-plaintext highlighter-rouge">Local Information</code>을 포착하는데 탁월한 성능을 낸다.</p>

<p>그렇다면 픽셀 하나 하나끼리 내적해준다는 것일까?? 아니다 여기서 논문의 제목이 <code class="language-plaintext highlighter-rouge">An Image Is Worth 16x16 Words</code> 인 이유가 드러난다. 일단 픽셀 하나 하나끼리 유사도를 측정하는 것이 유의미할까 생각해보자. 자연어의 토큰과 달리 이미지의 단일 픽셀 한 개는 큰 인사이트를 얻기 힘들다. 픽셀은 말 그대로 점 하나일 뿐이다. 픽셀을 여러 개 묶어 패치 단위로 묶는다면 이야기는 달라진다. 일정 크기 이상의 패치라면 자연어의 토큰처럼 그 자체로 어떤 의미를 담을 수 있다. 따라서 저자는 전체 이미지를 여러 개의 16x16 혹은 14x14 사이즈 패치로 나누어 하나 하나를 토큰으로 간주해 이미지 시퀀스를 만들고 그것을 모델의 Input으로 사용한다.</p>

<p align="center">
<img src="/assets/images/vision_transformer/class_diagram.png" alt="Class Diagram" class="align-center image-caption" width="50%&quot;, height=&quot;50%" />
<strong><em>Class Diagram</em></strong>
</p>

<p>모델 구조의 뼈대가 되는 내용들을 모두 살펴보았고, 위에서 서술한 내용을 구현하기 위해 어떤 블록들을 사용했는지 필자가 직접 논문을 보고 따라 구현한 코드와 함께 알아보도록 하자. 위에 첨부한 모델 모식도에 나와 있는 블록들 하나 하나 살펴볼 예정이다. 여담으로 Google Research의 Official Repo 역시 함께 참고했는데, 코드가 모두 구글이 요새 새롭게 미는 <code class="language-plaintext highlighter-rouge">Jax</code>, <code class="language-plaintext highlighter-rouge">Flax</code> 로 구현 되어 있었다. 파이토치나 좀 써본 필자 입장에서는 정말 … 지옥불을 경험했다. 오늘도 다시 한 번 페이스북 파이토치 개발팀에 큰절 드리고 싶다.</p>

<h4 id="linear-projection-of-flattened-patches"><code class="language-plaintext highlighter-rouge">🔬 Linear Projection of Flattened Patches</code></h4>

\[x_p \in R^{N * (P^2•C)}\]

\[z_{0} = [x_{class}; x_p^1E;x_p^2E;x_p^3E....x_p^NE]\]

\[N = \frac{H*W}{P*P}\]

<p><code class="language-plaintext highlighter-rouge">ViT</code>의 입력 임베딩을 생성하는 역할을 한다. <code class="language-plaintext highlighter-rouge">ViT</code>는 $x \in R^{H * W * C}$(H: height, W: width, C: channel)의 형상을 갖는 이미지를 입력으로 받아 가로 세로 길이가 $P$, 채널 개수 $C$인 $N$개의 패치로 <code class="language-plaintext highlighter-rouge">reshape</code> 한다. 필자가 코드 구현 중 가장 혼동한 부분이 바로 패치 개수 $N$이었다. 직관적으로 패치 개수라고 하면, 전체 이미지 사이즈에서 패치 크기를 나눈 값이라고 생각하기 쉽기 때문이다. 예를 들면 <code class="language-plaintext highlighter-rouge">512x512</code>짜리 이미지를 <code class="language-plaintext highlighter-rouge">16x16</code> 사이즈의 패치로 나눈다고 해보자. 필자는 단순히 <code class="language-plaintext highlighter-rouge">512/16=32</code> 라는 결과를 이용해 $N=32$로 설정하고 실험을 진행하다가 텐서 차원이 맞지 않아 발생하는 에러 로그를 마주했었다. 그러나 논문 속 수식을 확인해보면,  $H * W / P^2$이 바로 패치 개수$N$으로 정의된다. 그래서 만약 <code class="language-plaintext highlighter-rouge">512x512</code> 사이즈의 <code class="language-plaintext highlighter-rouge">RGB</code> 이미지 <code class="language-plaintext highlighter-rouge">10장</code>을 ViT 입력 임베딩에 맞게 차원 변환한다면 결과는 <code class="language-plaintext highlighter-rouge">[10, 3, 1024, 768]</code> 이 될 것이다. (이 예시를 앞으로 계속 이용하겠다)</p>

<p>이렇게 차원을 바꿔준 이미지를 <code class="language-plaintext highlighter-rouge">nn.Linear((channels * patch_size**2), dim_model)</code> 를 통해 <code class="language-plaintext highlighter-rouge">ViT</code>의 임베딩 레이어에 선형 투영해준다. 여기서 자연어 처리와 파이토치를 자주 사용하시는 독자라면 왜 <code class="language-plaintext highlighter-rouge">nn.Embedding</code>을 사용하지 않았는가 의문을 가질 수 있다.</p>

<p>자연어 처리에서 입력 임베딩을 만들때는 모델의 토크나이저에 의해 사전 정의된 vocab의 사이즈가 입력 문장에 속한 토큰 개수보다 훨씬 크기 때문에 데이터 룩업 테이블 방식의 <code class="language-plaintext highlighter-rouge">nn.Embedding</code> 을 사용하게 된다. 이게 무슨 말이냐면, 토크나이저에 의해 사전에 정의된 <code class="language-plaintext highlighter-rouge">vocab</code> 전체가 <code class="language-plaintext highlighter-rouge">nn.Embedding(vocab_size, dim_model)</code>로 투영 되어 가로는 vocab 사이즈, 세로는 모델의 차원 크기에 해당하는 룩업 테이블이 생성되고, 내가 입력한 토큰들은 전체 <code class="language-plaintext highlighter-rouge">vocab</code>의 일부분일테니 전체 임베딩 룩업 테이블에서 내가 임베딩하고 싶은 토큰들의 인덱스만 알아낸다는 것이다.</p>

<p>그래서 <code class="language-plaintext highlighter-rouge">nn.Embedding</code> 에 정의된 차원과 실제 입력 데이터의 차원이 맞지 않아도 함수가 동작하게 되는 것이다. 그러나 비전의 경우, 사전에 정의된 <code class="language-plaintext highlighter-rouge">vocab</code>이라는 개념이 전혀 없고 입력 이미지 역시 항상 고정된 크기의 차원으로 들어오기 때문에 <code class="language-plaintext highlighter-rouge">nn.Embedding</code>이 아닌  <code class="language-plaintext highlighter-rouge">nn.Linear</code> 을 사용해 곧바로 선형 투영을 구현한 것이다. 두 메서드에 대한 자세한 비교는 파이토치 관련 포스트에서 다시 한 번 자세히 다루도록 하겠다.</p>

<p>한편, <code class="language-plaintext highlighter-rouge">Position Embedding</code>을 더하기 전, <code class="language-plaintext highlighter-rouge">Input Embedding</code>의 차원은 <code class="language-plaintext highlighter-rouge">[10, 1024, 1024]</code> 이 된다. 지금까지 설명한 부분(<code class="language-plaintext highlighter-rouge">Linear Projection of Flattened Patches</code> )을 파이토치 코드로 구현하면 다음과 같다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">VisionTransformer</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="p">...</span>
    <span class="n">중략</span>
    <span class="p">...</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">num_patches</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">image_size</span> <span class="o">/</span> <span class="n">patch_size</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">input_embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">((</span><span class="n">channels</span> <span class="o">*</span> <span class="n">patch_size</span><span class="o">**</span><span class="mi">2</span><span class="p">),</span> <span class="n">dim_model</span><span class="p">)</span> <span class="c1"># Projection Layer for Input Embedding
</span>    <span class="p">...</span>
    <span class="n">중략</span>
    <span class="p">...</span>  
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">any</span><span class="p">:</span>
        <span class="s">""" For cls pooling """</span>
        <span class="k">assert</span> <span class="n">inputs</span><span class="p">.</span><span class="n">ndim</span> <span class="o">!=</span> <span class="mi">4</span><span class="p">,</span> <span class="sa">f</span><span class="s">"Input shape should be [BS, CHANNEL, IMAGE_SIZE, IMAGE_SIZE], but got </span><span class="si">{</span><span class="n">inputs</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="s">"</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">inputs</span> 
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">input_embedding</span><span class="p">(</span>
            <span class="n">x</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="p">.</span><span class="n">num_patches</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">patch_size</span><span class="o">**</span><span class="mi">2</span> <span class="o">*</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span> <span class="c1"># Projection Layer for Input Embedding
</span>        <span class="p">)</span>
        <span class="n">cls_token</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">,</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>  <span class="c1"># can change init method
</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cat</span><span class="p">([</span><span class="n">cls_token</span><span class="p">,</span> <span class="n">x</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="p">...</span>
</code></pre></div></div>

<p>임베딩 레이어를 객체로 따로 구현해도 되지만, 필자는 굳이 추상화가 필요하지 않다고 생각해 ViT의 최상위 클래스인 <code class="language-plaintext highlighter-rouge">VisionTransformer</code>의 <code class="language-plaintext highlighter-rouge">forward</code> 메서드 맨 초반부에 구현하게 되었다. 입력 받은 이미지 텐서를 <code class="language-plaintext highlighter-rouge">torch.reshape</code> 을 통해 <code class="language-plaintext highlighter-rouge">[패치 개수, 픽셀개수*채널개수]</code> 로 바꾼 뒤, 미리 정의해둔 <code class="language-plaintext highlighter-rouge">self.input_embedding</code> 에 매개변수로 전달해 <code class="language-plaintext highlighter-rouge">“위치 임베딩”</code> 값이 더해지기 전 <code class="language-plaintext highlighter-rouge">Input Embedding</code>을 만든다.</p>

<p>한편, <code class="language-plaintext highlighter-rouge">CLS Pooling</code>을 위해 마지막에 <code class="language-plaintext highlighter-rouge">[batch, 1, image_size]</code> 의 차원을 갖는 <code class="language-plaintext highlighter-rouge">cls_token</code> 을 정의해 패치 시퀀스와 <code class="language-plaintext highlighter-rouge">concat</code> (맨 앞에)해준다. 이 때 논문에 제시된 수식 상, <code class="language-plaintext highlighter-rouge">CLS Token</code>은 선형 투영하지 않으며, 패치 시퀀스에 선형 투영이 이뤄지고 난 뒤에 맨 앞에 <code class="language-plaintext highlighter-rouge">Concat</code> 하게 된다.</p>

<p><code class="language-plaintext highlighter-rouge">CLS Token</code>까지 더한 최종 <code class="language-plaintext highlighter-rouge">Input Embedding</code> 의 텐서 차원은 <code class="language-plaintext highlighter-rouge">[10, 1025, 1024]</code> 가 된다.</p>

<h4 id="positional-embedding"><code class="language-plaintext highlighter-rouge">🔢 Positional Embedding</code></h4>

\[E_{pos} \in R^{(N+1)*D}\]

<p>이미지를 패치 단위의 임베딩으로 만들었다면 이제 위치 임베딩을 정의해서 더해주면 모식도 속 <code class="language-plaintext highlighter-rouge">Embedded Patches</code> , 즉 인코더에 들어갈 최종 <code class="language-plaintext highlighter-rouge">Patch Embedding</code> 이 완성 된다. 위치 임베딩을 만드는 방식은 기존 <code class="language-plaintext highlighter-rouge">Transformer</code>, <code class="language-plaintext highlighter-rouge">BERT</code> 와 동일하다. 아래 <code class="language-plaintext highlighter-rouge">VisionEncoder</code> 클래스를 구현한 코드를 살펴보자.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="k">class</span> <span class="nc">VisionEncoder</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="p">...</span>
    <span class="n">중략</span>
    <span class="p">...</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">positional_embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Embedding</span><span class="p">((</span><span class="bp">self</span><span class="p">.</span><span class="n">num_patches</span> <span class="o">+</span> <span class="mi">1</span><span class="p">),</span> <span class="n">dim_model</span><span class="p">)</span>  <span class="c1"># add 1 for cls token
</span>    <span class="p">...</span>
    <span class="n">중략</span>
    <span class="p">...</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
        <span class="n">layer_output</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">pos_x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">num_patches</span> <span class="o">+</span> <span class="mi">1</span><span class="p">).</span><span class="n">repeat</span><span class="p">(</span><span class="n">inputs</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]).</span><span class="n">to</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>  <span class="c1"># inputs.shape[0] = Batch Size of Input
</span>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">dropout</span><span class="p">(</span>
            <span class="n">inputs</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">positional_embedding</span><span class="p">(</span><span class="n">pos_x</span><span class="p">)</span>
        <span class="p">)</span>
    <span class="p">...</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">Input Embedding</code>과 다르게 위치 임베딩은 <code class="language-plaintext highlighter-rouge">nn.Embedding</code>으로 구현했는데, 여기서도 사실 <code class="language-plaintext highlighter-rouge">nn.Linear</code>를 사용해도 무방하다. 그것보다 <code class="language-plaintext highlighter-rouge">nn.Embedding</code>의 입력 차원인 <code class="language-plaintext highlighter-rouge">self.num_patches + 1</code> 에 주목해보자. 왜 1을 더해준 값을 사용했을까??</p>

<p><code class="language-plaintext highlighter-rouge">ViT</code>는 BERT의 <code class="language-plaintext highlighter-rouge">CLS Token Pooling</code> 을 차용하기 위해 패치 시퀀스 맨 앞에 CLS 토큰을 추가하기 때문이다. 이렇게 추가된 <code class="language-plaintext highlighter-rouge">CLS Token</code>은 인코더를 거쳐 최종 <code class="language-plaintext highlighter-rouge">MLP Head</code>에 흘러들어가 로짓으로 변환된다. 만약 독자께서 <code class="language-plaintext highlighter-rouge">CLS Token Pooling</code> 대신 다른 풀링 방식을 사용할거라면 1을 추가해줄 필요는 없다.</p>

<p>애초에 객체 인스턴스 초기화 당시에 <code class="language-plaintext highlighter-rouge">CLS Token</code> 을 추가를 반영한 값을 전달하면 되지 않는가하는 의문이 들 수도 있다. 하지만 <code class="language-plaintext highlighter-rouge">VisionEncoder</code> 객체 인스턴스 초기화 당시에는 <code class="language-plaintext highlighter-rouge">num_patches</code> 값으로 <code class="language-plaintext highlighter-rouge">CLS Token</code>이 추가되기 이전 값(+1 반영이 안되어 있음)을 전달하도록 설계 되어 있어서  <code class="language-plaintext highlighter-rouge">CLS Pooling</code>을 사용할거라면 1 추가를 꼭 해줘야 한다.</p>

<p align="center">
<img src="/assets/images/vision_transformer/insight5.png" alt="Performance Table by making Position Embedding method" class="align-center image-caption" width="75%&quot;, height=&quot;50%" />
<strong><em><a href="https://arxiv.org/abs/2010.11929">Performance Table by making Position Embedding method</a></em></strong>
</p>

<p>한편 저자는 <code class="language-plaintext highlighter-rouge">2D Postion Embedding</code>, <code class="language-plaintext highlighter-rouge">Relative Position Embedding</code> 방식도 적용해봤지만, 구현 복잡도 &amp; 연산량 대비 성능 향상 폭이 매우 미미해 일반적인 <code class="language-plaintext highlighter-rouge">1D Position Embedding</code>을 사용할 것을 추천하고 있다.</p>

<h4 id="-multi-head-attention"><code class="language-plaintext highlighter-rouge">👩‍👩‍👧‍👦 Multi-Head Attention</code></h4>

\[z_t^{'} = MSA(LN(z_{t-1}) + z_{t-1})\]

\[MSA(z) = [SA_1();SA_2();SA_3()...SA_k()]*U_{msa}, \ \ U_{msa} \in R^{(k*D_h)*D} \\\]

<p>트랜스포머 계열 모델의 핵심 <code class="language-plaintext highlighter-rouge">Multi-Head Self-Attention</code> 모듈에 대해서 알아보자. 사실 기존 자연어 처리 <code class="language-plaintext highlighter-rouge">Transformer</code>, <code class="language-plaintext highlighter-rouge">BERT</code> 등의 동작 방식과 완전히 동일하며, 코드로 구현할 때 역시 동일하게 만들어주면 된다. 자세한 원리와 동작 방식은 <strong><u>Attention Is All You Need</u></strong> 리뷰 포스트에서 설명했기 때문에 생략하고 넘어가겠다. 한편 파이토치로 구현한 <code class="language-plaintext highlighter-rouge">Multi-Head Self-Attention</code> 블럭에 대한 코드는 다음과 같다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">scaled_dot_product_attention</span><span class="p">(</span><span class="n">q</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">k</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">v</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">dot_scale</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
    <span class="s">"""
    Scaled Dot-Product Attention
    Args:
        q: query matrix, shape (batch_size, seq_len, dim_head)
        k: key matrix, shape (batch_size, seq_len, dim_head)
        v: value matrix, shape (batch_size, seq_len, dim_head)
        dot_scale: scale factor for Q•K^T result, same as pure transformer
    Math:
        A = softmax(q•k^t/sqrt(D_h)), SA(z) = Av
    """</span>
    <span class="n">attention_dist</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="n">softmax</span><span class="p">(</span>
        <span class="n">torch</span><span class="p">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">))</span> <span class="o">/</span> <span class="n">dot_scale</span><span class="p">,</span>
        <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span>
    <span class="p">)</span>
    <span class="n">attention_matrix</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">attention_dist</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">attention_matrix</span>

<span class="k">class</span> <span class="nc">AttentionHead</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="s">"""
    In this class, we implement workflow of single attention head
    Args:
        dim_model: dimension of model's latent vector space, default 1024 from official paper
        dim_head: dimension of each attention head, default 64 from official paper (1024 / 16)
        dropout: dropout rate, default 0.1
    Math:
        [q,k,v]=z•U_qkv, A = softmax(q•k^t/sqrt(D_h)), SA(z) = Av
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim_model</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span>  <span class="mi">1024</span><span class="p">,</span> <span class="n">dim_head</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">64</span><span class="p">,</span> <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">AttentionHead</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span> <span class="o">=</span> <span class="n">dim_model</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dim_head</span> <span class="o">=</span> <span class="n">dim_head</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">dropout</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dot_scale</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">dim_head</span><span class="p">))</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">fc_q</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">dim_head</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">fc_k</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">dim_head</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">fc_v</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">dim_head</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="n">attention_matrix</span> <span class="o">=</span> <span class="n">scaled_dot_product_attention</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">fc_q</span><span class="p">(</span><span class="n">x</span><span class="p">),</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">fc_k</span><span class="p">(</span><span class="n">x</span><span class="p">),</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">fc_v</span><span class="p">(</span><span class="n">x</span><span class="p">),</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">dot_scale</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">attention_matrix</span>

<span class="k">class</span> <span class="nc">MultiHeadAttention</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="s">"""
    In this class, we implement workflow of Multi-Head Self-Attention
    Args:
        dim_model: dimension of model's latent vector space, default 1024 from official paper
        num_heads: number of heads in MHSA, default 16 from official paper for ViT-Large
        dim_head: dimension of each attention head, default 64 from official paper (1024 / 16)
        dropout: dropout rate, default 0.1
    Math:
        MSA(z) = [SA1(z); SA2(z); · · · ; SAk(z)]•Umsa
    Reference:
        https://arxiv.org/abs/2010.11929
        https://arxiv.org/abs/1706.03762
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim_model</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1024</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span> <span class="n">dim_head</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">64</span><span class="p">,</span> <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MultiHeadAttention</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span> <span class="o">=</span> <span class="n">dim_model</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dim_head</span> <span class="o">=</span> <span class="n">dim_head</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">dropout</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">attention_heads</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">ModuleList</span><span class="p">(</span>
            <span class="p">[</span><span class="n">AttentionHead</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">dim_head</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">dropout</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">num_heads</span><span class="p">)]</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">fc_concat</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="s">""" x is already passed nn.Layernorm """</span>
        <span class="k">assert</span> <span class="n">x</span><span class="p">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">3</span><span class="p">,</span> <span class="sa">f</span><span class="s">'Expected (batch, seq, hidden) got </span><span class="si">{</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="s">'</span>
        <span class="n">attention_output</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">fc_concat</span><span class="p">(</span>
            <span class="n">torch</span><span class="p">.</span><span class="n">cat</span><span class="p">([</span><span class="n">head</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">head</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">attention_heads</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># concat all dim_head = num_heads * dim_head
</span>        <span class="p">)</span>
        <span class="k">return</span> <span class="n">attention_output</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">MultiHeadAttention</code>을 가장 최상위 객체로 두고, 하위에 <code class="language-plaintext highlighter-rouge">AttentionHead</code>객체를 따로 구현했다. 이렇게 구현하면, 어텐션 해드별로 쿼리, 키, 벨류 선영 투영 행렬(<code class="language-plaintext highlighter-rouge">nn.Linear</code>)을 따로 구현해줄 필요가 없어지며, <code class="language-plaintext highlighter-rouge">nn.ModuleList</code> 를 통해 개별 해드를 한 번에 그룹핑하고 <code class="language-plaintext highlighter-rouge">loop</code> 를 통해 출력 결과를 <code class="language-plaintext highlighter-rouge">concat</code> 해줄 수 있어 복잡하고 많은 에러를 유발하는 <strong><u>텐서 차원 조작을 피할 수 있으며</u></strong>, 코드의 가독성이 올라가는 효과가 있다.</p>

<h4 id="️-mlp"><code class="language-plaintext highlighter-rouge">🗳️ MLP</code></h4>

\[z_{t} = MLP(LN(z_{t}^{'}) + z_{t}^{'})\]

<p>이름만 <code class="language-plaintext highlighter-rouge">MLP</code>로 바뀌었을 뿐, 기존 트랜스포머의 피드 포워드 블럭과 동일한 역할을 한다. 역시 자세한 동작 방식은 여기 포스트에서 확인하자. 파이토치로 구현한 코드는 다음과 같다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">MLP</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="s">"""
    Class for MLP module in ViT-Large
    Args:
        dim_model: dimension of model's latent vector space, default 512
        dim_mlp: dimension of FFN's hidden layer, default 2048 from official paper
        dropout: dropout rate, default 0.1
    Math:
        MLP(x) = MLP(LN(x))+x
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim_model</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1024</span><span class="p">,</span> <span class="n">dim_mlp</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">4096</span><span class="p">,</span> <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MLP</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">mlp</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">dim_model</span><span class="p">,</span> <span class="n">dim_mlp</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">GELU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">dropout</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">dim_mlp</span><span class="p">,</span> <span class="n">dim_model</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">dropout</span><span class="p">),</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">mlp</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</code></pre></div></div>

<p>특이한 점은 <code class="language-plaintext highlighter-rouge">Activation Function</code>으로 <code class="language-plaintext highlighter-rouge">GELU</code>를 사용(기존 트랜스포머는 <code class="language-plaintext highlighter-rouge">RELU</code>)했다는 점이다.</p>

<h4 id="-vision-encoder-layer"><code class="language-plaintext highlighter-rouge">📘 Vision Encoder Layer</code></h4>

<p><code class="language-plaintext highlighter-rouge">ViT</code> 인코더 블럭 1개에 해당하는 하위 모듈과 동작을 구현한 객체이다. 구현한 코드는 아래와 같다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">VisionEncoderLayer</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="s">"""
    Class for encoder_model module in ViT-Large
    In this class, we stack each encoder_model module (Multi-Head Attention, Residual-Connection, Layer Normalization, MLP)
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim_model</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1024</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">16</span><span class="p">,</span> <span class="n">dim_mlp</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">4096</span><span class="p">,</span> <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">VisionEncoderLayer</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">self_attention</span> <span class="o">=</span> <span class="n">MultiHeadAttention</span><span class="p">(</span>
            <span class="n">dim_model</span><span class="p">,</span>
            <span class="n">num_heads</span><span class="p">,</span>
            <span class="nb">int</span><span class="p">(</span><span class="n">dim_model</span> <span class="o">/</span> <span class="n">num_heads</span><span class="p">),</span>
            <span class="n">dropout</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">layer_norm1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">dim_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">layer_norm2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">dim_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">dropout</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">mlp</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">(</span>
            <span class="n">dim_model</span><span class="p">,</span>
            <span class="n">dim_mlp</span><span class="p">,</span>
            <span class="n">dropout</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="n">ln_x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">layer_norm1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">residual_x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">dropout</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">self_attention</span><span class="p">(</span><span class="n">ln_x</span><span class="p">))</span> <span class="o">+</span> <span class="n">x</span>

        <span class="n">ln_x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">layer_norm2</span><span class="p">(</span><span class="n">residual_x</span><span class="p">)</span>
        <span class="n">fx</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">mlp</span><span class="p">(</span><span class="n">ln_x</span><span class="p">)</span> <span class="o">+</span> <span class="n">residual_x</span>  <span class="c1"># from official paper &amp; code by Google Research
</span>        <span class="k">return</span> <span class="n">fx</span>
</code></pre></div></div>

<p><del>특이점은 마지막 <code class="language-plaintext highlighter-rouge">MLP Layer</code>와 <code class="language-plaintext highlighter-rouge">Residual</code> 결과를 더한 뒤, 다음 인코더 블록에 전달하기 전에 층 정규화를 한 번 더 적용한다는 것이다. 모델 모식도에는 나와 있지 않지만, 본문에 해당 내용이 실려 있다.</del>
마지막 인코더의 출력값에만 한번 더 <code class="language-plaintext highlighter-rouge">layernorm</code>을 적용한다.</p>

<h4 id="-visionencoder"><code class="language-plaintext highlighter-rouge">📚 VisionEncoder</code></h4>

<p>입력 이미지를 <code class="language-plaintext highlighter-rouge">Patch Embedding</code>으로 인코딩 하고 N개의 <code class="language-plaintext highlighter-rouge">VisionEncoderLayer</code>를 쌓기 위해 구현된 객체이다. <code class="language-plaintext highlighter-rouge">Patch Embedding</code>을 만드는 부분은 이미 위에서 설명했기 때문에 넘어가고, 인코더 블럭을 N개 쌓는 방법은 역시나 <code class="language-plaintext highlighter-rouge">nn.ModuleList</code> 를 사용하면 간편하게 구현할 수 있다. 아래 코드를 살펴보자.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">VisionEncoder</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="s">"""
    In this class, encode input sequence(Image) and then we stack N VisionEncoderLayer
    This model is implemented by cls pooling method for classification
    First, we define "positional embedding" and then add to input embedding for making patch embedding
    Second, forward patch embedding to N EncoderLayer and then get output embedding
    Args:
        num_patches: number of patches in input image =&gt; (image_size / patch_size)**2
        N: number of EncoderLayer, default 24 for large model
    """</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_patches</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">N</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">24</span><span class="p">,</span> <span class="n">dim_model</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1024</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">16</span><span class="p">,</span> <span class="n">dim_mlp</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">4096</span><span class="p">,</span> <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">VisionEncoder</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">num_patches</span> <span class="o">=</span> <span class="n">num_patches</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">positional_embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Embedding</span><span class="p">((</span><span class="bp">self</span><span class="p">.</span><span class="n">num_patches</span> <span class="o">+</span> <span class="mi">1</span><span class="p">),</span> <span class="n">dim_model</span><span class="p">)</span>  <span class="c1"># add 1 for cls token
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">num_layers</span> <span class="o">=</span> <span class="n">N</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span> <span class="o">=</span> <span class="n">dim_model</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dim_mlp</span> <span class="o">=</span> <span class="n">dim_mlp</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">dropout</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">encoder_layers</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">ModuleList</span><span class="p">(</span>
            <span class="p">[</span><span class="n">VisionEncoderLayer</span><span class="p">(</span><span class="n">dim_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">dim_mlp</span><span class="p">,</span> <span class="n">dropout</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">num_layers</span><span class="p">)]</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">layer_norm</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">dim_model</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
        <span class="n">layer_output</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">pos_x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">num_patches</span> <span class="o">+</span> <span class="mi">1</span><span class="p">).</span><span class="n">repeat</span><span class="p">(</span><span class="n">inputs</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]).</span><span class="n">to</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">dropout</span><span class="p">(</span>
            <span class="n">inputs</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">positional_embedding</span><span class="p">(</span><span class="n">pos_x</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">encoder_layers</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            <span class="n">layer_output</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">encoded_x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">layer_norm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># from official paper &amp; code by Google Research
</span>        <span class="n">layer_output</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">stack</span><span class="p">(</span><span class="n">layer_output</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">).</span><span class="n">to</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>  <span class="c1"># For Weighted Layer Pool: [N, BS, SEQ_LEN, DIM]
</span>        <span class="k">return</span> <span class="n">encoded_x</span><span class="p">,</span> <span class="n">layer_output</span>
</code></pre></div></div>
<p>마지막 층의 인코더 출력값에는 <code class="language-plaintext highlighter-rouge">layernorm</code>을 적용해줘야 함을 잊지 말자. 한편, <code class="language-plaintext highlighter-rouge">layer_output</code>는 레이어 별 어텐션 결과를 시각화 하거나 나중에 <code class="language-plaintext highlighter-rouge">WeightedLayerPool</code>에 사용하려고 만들었다.</p>
<h4 id="-visiontransformer"><code class="language-plaintext highlighter-rouge">🤖 VisionTransformer</code></h4>

<p align="center">
<img src="/assets/images/vision_transformer/model_variant.png" alt="ViT Model Variant" class="align-center image-caption" width="75%&quot;, height=&quot;50%" />
<strong><em><a href="https://arxiv.org/abs/2010.11929">ViT Model Variant</a></em></strong>
</p>

<p><code class="language-plaintext highlighter-rouge">ViT</code> 모델의 가장 최상위 객체로, 앞에서 설명한 모든 모듈들의 동작이 이뤄지는 곳이다. 사용자로부터 하이퍼파라미터를 입력 받아 모델의 크기, 깊이, 패치 크기, 이미지 임베딩 추출 방식을 지정한다. 그리고 입력 이미지를 전달받아 임베딩을 만들고 인코더에 전달한 뒤, <code class="language-plaintext highlighter-rouge">MLP Head</code> 를 통해 최종 예측 결과를 반환하는 역할을 한다.</p>

<p>이미지 임베딩 추출 방식은 <code class="language-plaintext highlighter-rouge">Linear Projection</code>과 <code class="language-plaintext highlighter-rouge">Convolution</code>이 있다. 전자가 논문에서 말하는 일반적인 <code class="language-plaintext highlighter-rouge">ViT</code>를 말하며 후자는 저자가 <code class="language-plaintext highlighter-rouge">Hybrid ViT</code>라고 따로 명명하는 모델이다. 임베딩 추출 방식 이외에 다른 차이는 전혀 없다. <code class="language-plaintext highlighter-rouge">extractor</code> 매개변수를 통해 임베딩 추출 방식을 지정할 수 있으니 아래 코드를 확인해보자.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">VisionTransformer</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="s">"""
    Main class for ViT of cls pooling, Pytorch implementation
    We implement pure ViT, Not hybrid version which is using CNN for extracting patch embedding
    input must be [BS, CHANNEL, IMAGE_SIZE, IMAGE_SIZE]
    In NLP, input_sequence is always smaller than vocab size
    But in vision, input_sequence is always same as image size, not concept of vocab in vision
    So, ViT use nn.Linear instead of nn.Embedding for input_embedding
    Args:
        num_classes: number of classes for classification task
        image_size: size of input image, default 512
        patch_size: size of patch, default 16 from official paper for ViT-Large
        extractor: option for feature extractor, default 'base' which is crop &amp; just flatten
                   if you want to use Convolution for feature extractor, set extractor='cnn' named hybrid ver in paper
        classifier: option for pooling method, default token meaning that do cls pooling
                    if you want to use mean pooling, set classifier='mean'
        mode: option for train type, default fine-tune, if you want pretrain, set mode='pretrain'
              In official paper &amp; code by Google Research, they use different classifier head for pretrain, fine-tune
    Math:
        image2sequence: [batch, channel, image_size, image_size] -&gt; [batch, patch, patch_size^2*channel]
        input_embedding: R^(P^2 ·C)×D
    Reference:
        https://arxiv.org/abs/2010.11929
        https://arxiv.org/abs/1706.03762
        https://github.com/google-research/vision_transformer/blob/main/vit_jax/models_vit.py#L184
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span>
            <span class="n">num_classes</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
            <span class="n">channels</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span>
            <span class="n">image_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">512</span><span class="p">,</span>
            <span class="n">patch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">16</span><span class="p">,</span>
            <span class="n">num_layers</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">24</span><span class="p">,</span>
            <span class="n">dim_model</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1024</span><span class="p">,</span>
            <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">16</span><span class="p">,</span>
            <span class="n">dim_mlp</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">4096</span><span class="p">,</span>
            <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
            <span class="n">extractor</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s">'base'</span><span class="p">,</span>
            <span class="n">classifier</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s">'token'</span><span class="p">,</span>
            <span class="n">mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s">'fine_tune'</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">VisionTransformer</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">num_patches</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">image_size</span> <span class="o">/</span> <span class="n">patch_size</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">num_layers</span> <span class="o">=</span> <span class="n">num_layers</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">patch_size</span> <span class="o">=</span> <span class="n">patch_size</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span> <span class="o">=</span> <span class="n">dim_model</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dim_mlp</span> <span class="o">=</span> <span class="n">dim_mlp</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">dropout</span><span class="p">)</span>

        <span class="c1"># Input Embedding
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">extractor</span> <span class="o">=</span> <span class="n">extractor</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">input_embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">((</span><span class="n">channels</span> <span class="o">*</span> <span class="n">patch_size</span><span class="o">**</span><span class="mi">2</span><span class="p">),</span> <span class="n">dim_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">conv</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Conv2d</span><span class="p">(</span>
            <span class="n">in_channels</span><span class="o">=</span><span class="n">channels</span><span class="p">,</span>
            <span class="n">out_channels</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span><span class="p">,</span>
            <span class="n">kernel_size</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">patch_size</span><span class="p">,</span>
            <span class="n">stride</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">patch_size</span>
        <span class="p">)</span>

        <span class="c1"># Encoder Multi-Head Self-Attention
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">VisionEncoder</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">num_patches</span><span class="p">,</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">num_layers</span><span class="p">,</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span><span class="p">,</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">num_heads</span><span class="p">,</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">dim_mlp</span><span class="p">,</span>
            <span class="n">dropout</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">classifier</span> <span class="o">=</span> <span class="n">classifier</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">pretrain_classifier</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">Tanh</span><span class="p">(),</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">fine_tune_classifier</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">mode</span> <span class="o">=</span> <span class="n">mode</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">any</span><span class="p">:</span>
        <span class="s">""" For cls pooling """</span>
        <span class="k">assert</span> <span class="n">inputs</span><span class="p">.</span><span class="n">ndim</span> <span class="o">!=</span> <span class="mi">4</span><span class="p">,</span> <span class="sa">f</span><span class="s">"Input shape should be [BS, CHANNEL, IMAGE_SIZE, IMAGE_SIZE], but got </span><span class="si">{</span><span class="n">inputs</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="s">"</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">inputs</span>

        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">extractor</span> <span class="o">==</span> <span class="s">'cnn'</span><span class="p">:</span>
            <span class="c1"># self.conv(x).shape == [batch, dim, image_size/patch_size, image_size/patch_size]
</span>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">conv</span><span class="p">(</span><span class="n">x</span><span class="p">).</span><span class="n">reshape</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">num_patches</span><span class="o">**</span><span class="mi">2</span><span class="p">).</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># self.extractor == 'base':
</span>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">input_embedding</span><span class="p">(</span>
                <span class="n">x</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="p">.</span><span class="n">num_patches</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">patch_size</span><span class="o">**</span><span class="mi">2</span> <span class="o">*</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
            <span class="p">)</span>
        <span class="n">cls_token</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">,</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>  <span class="c1"># can change init method
</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cat</span><span class="p">([</span><span class="n">cls_token</span><span class="p">,</span> <span class="n">x</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="n">x</span><span class="p">,</span> <span class="n">layer_output</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># output
</span>
        <span class="c1"># classification
</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">,</span> <span class="p">:]</span>  <span class="c1"># select cls token, which is position 0 in sequence
</span>        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">mode</span> <span class="o">==</span> <span class="s">'fine_tune'</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">fine_tune_classifier</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">mode</span> <span class="o">==</span> <span class="s">'pretrain'</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">fine_tune_classifier</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">pretrain_classifier</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">x</span>
</code></pre></div></div>

<p>한편, 코드에서 눈여겨봐야 할 점은 <code class="language-plaintext highlighter-rouge">MLP Head</code>로, 저자는 <code class="language-plaintext highlighter-rouge">pre-train</code> 시점과 <code class="language-plaintext highlighter-rouge">fine-tune</code> 시점에 서로 다른 <code class="language-plaintext highlighter-rouge">Classifier Head</code>를 사용한다. 전자에는 <code class="language-plaintext highlighter-rouge">Activation Function</code> 1개와 두 개의 <code class="language-plaintext highlighter-rouge">MLP Layer</code>를 사용하고, 후자에는 1개의 <code class="language-plaintext highlighter-rouge">MLP Layer</code>를 사용한다.</p>

<p>다만, <code class="language-plaintext highlighter-rouge">pretrain_classifier</code>의 입출력 차원에 대한 정확한 수치를 논문이나 official repo code를 확인해도 찾을 수 없었다, 그래서 임시로 모델의 차원과 똑같이 세팅하게 되었다.</p>

<p>또한 저자는 <code class="language-plaintext highlighter-rouge">CLS Pooling</code>과 더불어 <code class="language-plaintext highlighter-rouge">GAP</code> 방식도 제시하는데, <code class="language-plaintext highlighter-rouge">GAP</code> 방식은 추후에 따로 추가가 필요하다. 그리고 사전 훈련과 파인 튜닝 모두 분류 테스크를 수행했는데 (심지어 같은 데이터 세트를 사용함) 왜 굳이 서로 다른 <code class="language-plaintext highlighter-rouge">Classifier Head</code>를 정의했는지 의도를 알 수 없어 논문을 다시 읽어봤지만, 이유에 대해서 상세히 언급하는 부분이 없었다.</p>

<p><code class="language-plaintext highlighter-rouge">ViT</code>는 입력 임베딩을 정의하는 부분을 제외하면 저자의 의도대로 기존 트랜스포머와 동일한 모델 구조를 가졌다. 완전히 다른 데이터인 이미지와 텍스트에 같은 구조의 모델을 적용한다는 것이 정말 쉽지 않아 보였는데, 패치 개념을 만들어 자연어의 토큰처럼 간주하고 사용한 것이 의도대로 구현하는데 직관적이면서도 정말 효과적이었다고 생각한다. 이제 이렇게 만들어진 모델을 통해 진행한 여러 실험 결과에 어떤 인사이트가 담겨 있는지 알아보자.</p>

<h3 id="insight-from-experiment"><code class="language-plaintext highlighter-rouge">🔬 Insight from Experiment</code></h3>

<h4 id="insight-1-vit의-scalability-증명"><code class="language-plaintext highlighter-rouge">💡 Insight 1. ViT의 Scalability 증명</code></h4>
<ul>
  <li><strong><code class="language-plaintext highlighter-rouge">Pre-Train</code>에 사용되는 이미지 데이터 세트의 크기가 커질수록 <code class="language-plaintext highlighter-rouge">Fine-Tune Stage</code>에서 <code class="language-plaintext highlighter-rouge">ViT</code>가 <code class="language-plaintext highlighter-rouge">CNN</code>보다 높은 성능</strong></li>
  <li><strong>같은 성능이라면 <code class="language-plaintext highlighter-rouge">ViT</code>가 상대적으로 적은 연산량을 기록</strong></li>
</ul>

<p align="center">
<img src="/assets/images/vision_transformer/insight1.png" alt="Performance per Dataset Scale" class="align-center image-caption" width="75%&quot;, height=&quot;50%" />
<strong><em><a href="https://arxiv.org/abs/2010.11929">Performance per Dataset Scale</a></em></strong>
</p>

<p>위 도표는 <code class="language-plaintext highlighter-rouge">Pre-Train Stage</code>에 사용된 이미지 데이터 세트에 따른 모델의 <code class="language-plaintext highlighter-rouge">Fine-Tune</code> 성능 추이를 나타낸 자료다. 사전 훈련 데이터 스케일이 크지 않을 때는 <code class="language-plaintext highlighter-rouge">Conv</code> 기반의 <code class="language-plaintext highlighter-rouge">ResNet</code> 시리즈가 <code class="language-plaintext highlighter-rouge">ViT</code> 시리즈를 압도하는 모습을 보여준다. 하지만 데이터 세트의 크기가 커질수록 점점 <code class="language-plaintext highlighter-rouge">ViT</code> 시리즈의 성능이 <code class="language-plaintext highlighter-rouge">ResNet</code>을 능가하는 결과를 볼 수 있다.</p>

<p>한편, ViT &amp; ResNet 성능 결과 모두 ImageNet과 JFT-Image로 사전 훈련 및 파인 튜닝을 거쳐 나왔다고 하니 참고하자. <strong><u>추가로 파인 튜닝 과정에서 사전 훈련 때보다 이미지 사이즈를 키워서 훈련을 시켰다고 논문에서 밝히고 있는데, 이는 저자의 실험 결과에 기인한 것이다</u></strong>. 논문에 따르면 파인 튜닝 때 사전 훈련 당시보다 더 높은 해상도의 이미지를 사용하면 성능이 향상 된다고 하니 기억했다가  써먹어보자.</p>

<p align="center">
<img src="/assets/images/vision_transformer/insight1_2.png" alt="Performance per FLOPs Scale" class="align-center image-caption" width="75%&quot;, height=&quot;50%" />
<strong><em><a href="https://arxiv.org/abs/2010.11929">Performance per FLOPs</a></em></strong>
</p>

<p>위 도표는 연산량 변화에 따른 모델의 성능 추이를 나타낸 그림이다. 두 지표 모두 같은 점수라면 <code class="language-plaintext highlighter-rouge">ViT</code> 시리즈의 연산량이 현저히 적음을 알 수 있다. 또한 정확도 95% 이하 구간에서 같은 성능이라면  <code class="language-plaintext highlighter-rouge">ViT</code>의 <code class="language-plaintext highlighter-rouge">Hybrid</code> 버전 모델의 연산량이 일반 <code class="language-plaintext highlighter-rouge">ViT</code> 버전보다 현저히 적음을 확인할 수 있다. 이러한 사실은 추후에 <code class="language-plaintext highlighter-rouge">Swin-Transformer</code> 설계에 영감을 준다.</p>

<p>두 개의 실험 결과를 종합했을 때, <code class="language-plaintext highlighter-rouge">ViT</code>가 <code class="language-plaintext highlighter-rouge">ResNet</code>보다 일반화 성능이 더 높으며(도표 1) 모델의 <code class="language-plaintext highlighter-rouge">Saturation</code> 현상이 두드러지지 않아 성능의 한계치(도표 2) 역시 더 높다고 볼 수 있다. 따라서 기존 트랜스포머의 연산•구조적 측면에서 <code class="language-plaintext highlighter-rouge">Scalability</code>를 성공적으로 이식했다고 평가할 수 있겠다.</p>

<h4 id="insight-2-pure-self-attention은-좋은-이미지-피처를-추출하기에-충분하다"><code class="language-plaintext highlighter-rouge">💡 Insight 2. Pure Self-Attention은 좋은 이미지 피처를 추출하기에 충분하다</code></h4>
<ul>
  <li><strong>Patch Embedding Layer의 PCA 결과, 패치의 기저가 되는 차원과 유사한 모양을 추출</strong>
    <ul>
      <li><strong><code class="language-plaintext highlighter-rouge">Convolution</code> 없이 <code class="language-plaintext highlighter-rouge">Self-Attention</code>만으로도 충분히 이미지의 좋은 피처를 추출하는 것이 가능</strong></li>
      <li><strong><code class="language-plaintext highlighter-rouge">Vision</code>에서 <code class="language-plaintext highlighter-rouge">Convolution</code>에 대한 <code class="language-plaintext highlighter-rouge">reliance</code> 탈피 가능</strong></li>
    </ul>
  </li>
</ul>

<p align="center">
<img src="/assets/images/vision_transformer/insight2.png" alt="Patch Embedding Layer’s Filter" class="align-center image-caption" width="75%&quot;, height=&quot;50%" />
<strong><em><a href="https://arxiv.org/abs/2010.11929">Patch Embedding Layer’s Filter</a></em></strong>
</p>

<p>위 자료는 충분한 학습을 거치고 난 <code class="language-plaintext highlighter-rouge">ViT</code>의 <code class="language-plaintext highlighter-rouge">Patch Embedding Layer</code>의 필터를 <code class="language-plaintext highlighter-rouge">PCA</code>한 결과 중에서 특잇값이 높은 상위 28개의 피처를 나열한 그림이다. 이미지의 기본 뼈대가 되기에 적합해 보이는 피처들이 추출된 모습을 볼 수 있다.</p>

<p>따라서 <code class="language-plaintext highlighter-rouge">Inductive Bias</code> 없이, 단일  <code class="language-plaintext highlighter-rouge">Self-Attention</code>만으로 이미지의 피처를 추출하는 것이 충분히 가능하다. 비전 분야에 만연한 <code class="language-plaintext highlighter-rouge">Convolution</code> 의존에서 벗어나 새로운 아키텍처의 도입이 가능함을 시사한 부분이라고 할 수 있겠다.</p>

<h4 id="insight-3-bottom2general-information-top2specific-information"><code class="language-plaintext highlighter-rouge">💡 Insight 3. Bottom2General Information, Top2Specific Information</code></h4>

<ul>
  <li><strong><code class="language-plaintext highlighter-rouge">입력</code>과 가까운 인코더일수록 <code class="language-plaintext highlighter-rouge">Global &amp; General</code>한 Information을 포착</strong></li>
  <li><strong><code class="language-plaintext highlighter-rouge">출력</code>과 가까운 인코더일수록 <code class="language-plaintext highlighter-rouge">Local &amp; Specific</code>한 Information을 포착</strong></li>
</ul>
<p align="center">
<img src="/assets/images/vision_transformer/insight3.png" alt="Multi-Head Attention Distance per Network Depth" class="align-center image-caption" width="75%&quot;, height=&quot;50%" />
<strong><em><a href="https://arxiv.org/abs/2010.11929">Multi-Head Attention Distance per Network Depth</a></em></strong>
</p>

<p>다음 자료는 인코더의 개수 변화에 따른 개별 어텐션 해드의 어텐션 거리 변화 추이를 나타낸 그림이다. 여기서 어텐션 거리란, 해드가 얼마나 멀리 떨어진 패치를 어텐션했는지 픽셀 단위로 표현한 지표다. 해당 값이 높을수록 거리상 멀리 떨어진 패치와 어텐션을, 작을수록 가까운 패치와 어텐션 했다는 것을 의미한다. 다시 도표를 살펴보자. 입력과 가까운 인코더일수록(Depth 0) 해드별 어텐션 거리의 분산이 커지고, 출력과 가까운 인코더일수록(Depth 23) 분산이 점자 줄어들다가 거의 한 점에 수렴하는듯한 양상을 보여준다. 다시 말해, 입력과 가까운 <code class="language-plaintext highlighter-rouge">Bottom Encoder</code>는 멀리 떨어진 패치부터 가까운 패치까지 모두 전역적(<code class="language-plaintext highlighter-rouge">Global</code>)으로 어텐션을 수행해 <code class="language-plaintext highlighter-rouge">General</code> 한 정보를 포착하게 되고 출력과 가까운 <code class="language-plaintext highlighter-rouge">Top Encoder</code>는 개별 해드들이 모두 비슷한 거리에 위치한 패치(<code class="language-plaintext highlighter-rouge">Local</code>)에 어텐션을 수행해 <code class="language-plaintext highlighter-rouge">Specific</code> 한 정보를 포착하게 된다.</p>

<p>이 때 <code class="language-plaintext highlighter-rouge">Global</code>과 <code class="language-plaintext highlighter-rouge">Local</code>이라는 용어 때문에 <code class="language-plaintext highlighter-rouge">Bottom Encoder</code> 는 멀리 떨어진 패치와 어텐션하고, <code class="language-plaintext highlighter-rouge">Top Encoder</code>는 가까운 패치와 어텐션한다고 착각하기 쉽다. <strong><u>그러나 개별 해드들의 어텐션 거리가 얼마나 분산되어 있는가가 바로 </u></strong><code class="language-plaintext highlighter-rouge">Global</code>, <code class="language-plaintext highlighter-rouge">Local</code><strong><u>을 구분하는 기준이 된다.</u></strong> 입력부에 가까운 레이어들은 헤드들의 어텐션 거리 분산이 매우 큰 편인데, 이것을 이패치 저패치 모두 어텐션 해보고 비교해본다고 해석해서 <code class="language-plaintext highlighter-rouge">Global</code>이라고 부르고, 출력부에 가까운 레이어는 헤드들의 어텐션 거리 분산이 매우 작은 편인데, 이게 바로 각각의 헤드들이 어떤 정보에 주목해야할지(분류 손실이 가장 작아지는 패치) 범위를 충분히 좁힌 상태에서 특정 부분에만 집중한다는 의미로 해석해 <code class="language-plaintext highlighter-rouge">Local</code> 이라고 부르게 되었다.</p>

<p>&lt;<strong><a href="https://arxiv.org/abs/2006.05987">Revisiting Few-sample BERT Fine-tuning</a></strong>&gt;도 위와 비슷한 맥락의 사실에 대해 언급하고 있으니 참고해보자. 이러한 사실은 트랜스포머 인코더 계열 모델을 튜닝할 때 <code class="language-plaintext highlighter-rouge">Depth</code> 별로 다른 <code class="language-plaintext highlighter-rouge">Learning Rate</code>을 적용하는 <code class="language-plaintext highlighter-rouge">Layerwise Learning Rate Decay</code> 의 초석이 되기도 한다. <code class="language-plaintext highlighter-rouge">Layerwise Learning Rate Decay</code> 에 대해서는 <strong><a href="https://towardsdatascience.com/advanced-techniques-for-fine-tuning-transformers-82e4e61e16e">여기 포스트</a></strong>를 참고하도록 하자.</p>

<p align="center">
<img src="/assets/images/vision_transformer/insight3_2.png" alt="Output from Last Encoder" class="align-center image-caption" width="40%&quot;, height=&quot;10%" />
<strong><em><a href="https://arxiv.org/abs/2010.11929">Output from Last Encoder</a></em></strong>
</p>

<p>한편 논문에는 언급되지 않은, 필자의 뇌피셜에 가깝지만, <strong><u>출력에 가까운 인코더들의 해드가 가진</u></strong> <code class="language-plaintext highlighter-rouge">Attention Distance</code><strong><u>이 모두 비슷하다는 사실로 이미지 분류에 결정적인 역할을 하는 피처가 이미지의 특정 구역에 모여 있으며, 그 스팟은 이미지의 중앙 부근일 가능성이 높다고 추측 해볼 수 있다.</u></strong> 모든 해드의 픽셀 거리가 서로 비슷하려면 일단 비슷한 위치의 패치에 어텐션을 해야하기 때문에 분류 손실값을 최소로 줄여주는 피처는 보통 한 구역(패치)에 몰려 있을 것이라고 유추가 가능하다. 또한 특정 스팟이 중앙에 위치할수록 어텐션 거리의 분산이 줄어들것이라고 생각 해볼 수도 있었다. 저자는 <code class="language-plaintext highlighter-rouge">Attention Rollout</code>이라는 개념을 통해 <code class="language-plaintext highlighter-rouge">Attention Distance</code>을 산출했다고 언급하는데, 자세한 내용은 옆에 두 링크를 참고해보자(<a href="https://hongl.tistory.com/234">한국어 설명 블로그</a>,  <a href="https://arxiv.org/abs/2005.00928">원논문</a>). 이러한 필자의 가설이 맞다면, <code class="language-plaintext highlighter-rouge">Convolution</code> 의 <code class="language-plaintext highlighter-rouge">Inductive Bias</code>  중 <code class="language-plaintext highlighter-rouge">Locality</code> 의 효과성을 <code class="language-plaintext highlighter-rouge">Self-Attention</code>을 통해 입증이 가능하며, 반대로 <code class="language-plaintext highlighter-rouge">Convolution</code>에 대한 의존에서 벗어나 단일 <code class="language-plaintext highlighter-rouge">Self-Attention</code> 으로도 같은 효과를 낼 수 있다는 증거 중 하나가 될 것이다.</p>

<h4 id="insight-4-vit는-cls-pooling-사용하는게-효율적"><code class="language-plaintext highlighter-rouge">💡 Insight 4. ViT는 CLS Pooling 사용하는게 효율적</code></h4>

<ul>
  <li><strong><code class="language-plaintext highlighter-rouge">CLS Pooling</code>은 <code class="language-plaintext highlighter-rouge">GAP</code> 보다 2배 이상 큰 학습률을 사용해도 비슷한 성능을 기록</strong>
    <ul>
      <li><strong><u>학습 속도는 더 빠르되 성능이 비슷하기 때문에</u></strong> <code class="language-plaintext highlighter-rouge">CLS Pooling</code> <strong><u>이 더 효율적</u></strong></li>
    </ul>
  </li>
</ul>

<p align="center">
<img src="/assets/images/vision_transformer/insight4.png" alt="Performance Trend by Pooling Method with LR" class="align-center image-caption" width="75%&quot;, height=&quot;50%" />
<strong><em><a href="https://arxiv.org/abs/2010.11929">Performance Trend by Pooling Method with LR</a></em></strong>
</p>

<p>다음 도표는 풀링 방식과 학습률의 변동에 따른 정확도 변화 추이를 나타낸 그림이다. 비슷한 성능이라면 <code class="language-plaintext highlighter-rouge">CLS Pooling</code>이 <code class="language-plaintext highlighter-rouge">GAP</code>보다 2배 이상 큰 학습률을 사용했다. 학습률이 크면 모델의 수렴 속도가 빨라져 학습 속도가 빨라지는 장점이 있다. 그런데 성능까지 비슷하다면 <code class="language-plaintext highlighter-rouge">ViT</code>는 <code class="language-plaintext highlighter-rouge">CLS Pooling</code>을 사용하는 것이 더 효율적이라고 할 수 있겠다.</p>

<p>나중에 시간이 된다면 다른 풀링 방식, 예를 들면 <code class="language-plaintext highlighter-rouge">Weighted Layer Pooling</code>, <code class="language-plaintext highlighter-rouge">GeM Pooling</code>, <code class="language-plaintext highlighter-rouge">Attention Pooling</code> 같은 것을 적용해 실험해보겠다.</p>

<h4 id="insight-5-vit는-absolute-1d-position-embedding-사용하는게-가장-효율적"><code class="language-plaintext highlighter-rouge">💡 Insight 5. ViT는 Absolute 1D-Position Embedding 사용하는게 가장 효율적</code></h4>

<ul>
  <li><strong>어떤 형태로든 위치 임베딩 값을 정의해준다면, 형태와 종류에 상관없이 거의 비슷한 성능을 보임</strong></li>
  <li><strong>성능이 비슷하면, 직관적이고 구현이 간편한 <code class="language-plaintext highlighter-rouge">Absolute 1D-Position Embedding</code> 방법을 사용하는 것이 가장 효율적</strong></li>
  <li><strong><code class="language-plaintext highlighter-rouge">ViT</code>는 <code class="language-plaintext highlighter-rouge">Patch-Level</code> 사용해, <code class="language-plaintext highlighter-rouge">Pixel-Level</code>보다 상대적으로 시퀀스 길이가 짧아 위치•공간 정보를 인코딩하는 방식에 영향을 덜 받음</strong></li>
</ul>

<p align="center">
<img src="/assets/images/vision_transformer/insight5.png" alt="Performance Table by making Position Embedding method" class="align-center image-caption" width="75%&quot;, height=&quot;50%" />
<strong><em><a href="https://arxiv.org/abs/2010.11929">Performance Table by making Position Embedding method</a></em></strong>
</p>

<p>위 실험 결과는 <code class="language-plaintext highlighter-rouge">Position Embedding</code> 인코딩 방식에 따른 <code class="language-plaintext highlighter-rouge">ViT</code> 모델의 성능 변화 추이를 나타낸 자료다. 인코딩 형태와 상관없이 위치 임베딩의 유무가 성능에 큰 영향을 미친다는 사실을 알려주고 있다. 한편, 인코딩 형태 변화에 따른 유의미한 성능 변화는 없었다. 하지만 <code class="language-plaintext highlighter-rouge">Absolute 1D-Position Embedding</code>의 컨셉이 가장 직관적이며 구현하기 편하고 연산량이 다른 인코딩보다 적다는 것을 감안하면 ViT에 가장 효율적인 위치 임베딩 방식이라고 판단할 수 있다.</p>

<p>논문은 결과에 대해 <code class="language-plaintext highlighter-rouge">ViT</code>가 사용하는 <code class="language-plaintext highlighter-rouge">Patch-Level Embedding</code>이 <code class="language-plaintext highlighter-rouge">Pixel-Level</code>보다 상대적으로 짧은 시퀀스 길이를 갖기 때문이라고 설명한다. 예를 들어 <code class="language-plaintext highlighter-rouge">224x224</code> 사이즈의 이미지를 <code class="language-plaintext highlighter-rouge">16x16</code> 사이즈의 패치 여러장으로 만든다고 생각해보자. 임베딩 차원에 들어가는 $N$ 은 $(224/16)^2$ , 즉 <code class="language-plaintext highlighter-rouge">196</code>이 된다. 한편 이것을 <code class="language-plaintext highlighter-rouge">Pixel-Level</code>로 임베딩 하게 되면 $224^2$, 즉 <code class="language-plaintext highlighter-rouge">50176</code> 개의 시퀀스가 생긴다. 따라서 <code class="language-plaintext highlighter-rouge">Pixel-Level</code> 에 비하면 훨씬 짧은 시퀀스 길이를 갖기 때문에 <code class="language-plaintext highlighter-rouge">Absolute 1D-Position Embedding</code> 만으로도 충분히 <code class="language-plaintext highlighter-rouge">Spatial Relation</code>을 학습할 수 있는 것이다.</p>

<p align="center">
<img src="/assets/images/vision_transformer/insight5_2.png" alt="Absolute 1D-Position Embedding" class="align-center image-caption" width="50%&quot;, height=&quot;50%" />
<strong><em><a href="https://arxiv.org/abs/2010.11929">Absolute 1D-Position Embedding</a></em></strong>
</p>

<p>하지만, 필자는 자연어 처리의 <code class="language-plaintext highlighter-rouge">Transformer-XL</code>, <code class="language-plaintext highlighter-rouge">XLNet</code>, <code class="language-plaintext highlighter-rouge">DeBERTa</code> 같은 모델들이 <code class="language-plaintext highlighter-rouge">Relative Position Embedding</code> 방식을 적용해 큰 성공을 거둔 바가 있다는 점을 생각하면 이런 결과가 납득이 가면서도 의아했다.</p>

<p>저자는 실험에 사용한 모든 데이터 세트를 <code class="language-plaintext highlighter-rouge">224x224</code>로 <code class="language-plaintext highlighter-rouge">resize</code> 했다고 밝히고 있는데, 만약 이미지 사이즈가 <code class="language-plaintext highlighter-rouge">512x512</code>정도만 되더라도 $N$ 값이 <code class="language-plaintext highlighter-rouge">1024</code> 이라서 위 결과와 상당히 다른 양상이 나타나지 않을까 하는 생각이 든다. 추후에 시간이 된다면 이 부분도 꼭 실험해봐야겠다. 예측컨데 이미자 사이즈가 커질수록 <code class="language-plaintext highlighter-rouge">2D Position Embedding</code> 혹은 <code class="language-plaintext highlighter-rouge">Relative Position Embedding</code>이 더 효율적일 것이라 예상한다.</p>

<h3 id="️conclusion"><code class="language-plaintext highlighter-rouge">🧑‍⚖️ Conclusion</code></h3>

<p>이렇게 <code class="language-plaintext highlighter-rouge">ViT</code> 모델을 제안한 <a href="https://arxiv.org/abs/2010.11929">&lt;An Image Is Worth 16x16 Words: Transformers For Image Recognition At Scale&gt;</a>에 실린 내용을 모두 살펴보았다. <code class="language-plaintext highlighter-rouge">Conv</code> 에 대한 의존을 탈피 했다는 점에서 매우 의미가 있는 시도였으며, Self-Attention &amp; Transformer 구조 채택만으로도 컴퓨터 비전 영역에 어느 정도  <code class="language-plaintext highlighter-rouge">scalability</code> 를  이식하는데 성공했다는 점에서 후대 연구에 중요한 시사점을 남겼다. 상대적으로 정체(??)되어 있던 비전 영역이 성능의 한계를 한단계 뛰어넘을 수 있는 초석을 마련해준 셈이다.</p>

<p>하지만, <code class="language-plaintext highlighter-rouge">ViT</code>의 <code class="language-plaintext highlighter-rouge">Pretrain Stage</code>에 적합한 <code class="language-plaintext highlighter-rouge">Self-Supervised Learning</code> 방법을 찾지 못해 여전히 <code class="language-plaintext highlighter-rouge">Supervised Learning</code> 방식을 채택한 점은 매우 아쉬웠다. <strong><u>이는 결국 데이터</u></strong> <code class="language-plaintext highlighter-rouge">Scale</code> <strong><u>확장에 한계를 의미하기 때문이다.</u></strong> 오늘날 BERT와 GPT의 성공 신화는 비단 <code class="language-plaintext highlighter-rouge">Self-Attention</code>와 <code class="language-plaintext highlighter-rouge">Transformer</code>의 구조적 탁월성에 의해서만 탄생한게 아니다. 이에 못지 않게(개인적으로 제일 중요하다 생각) 주요했던 것이 바로 데이터 <code class="language-plaintext highlighter-rouge">Scale</code> 확장이다.  <code class="language-plaintext highlighter-rouge">MLM</code>, <code class="language-plaintext highlighter-rouge">AR</code> 등의 <code class="language-plaintext highlighter-rouge">Self-Supervised Learning</code> 덕분에 데이터 <code class="language-plaintext highlighter-rouge">Scale</code>을 효율적으로 스케일 업 시킬 수 있었고, 사전 훈련 데이터의 증가는 모델 깊이, 너비, 차원까지 더욱 크케 키우는데 기여했다.</p>

<p>또한 <code class="language-plaintext highlighter-rouge">ViT</code>는 선천적으로 <code class="language-plaintext highlighter-rouge">Patch-Level Embedding</code>을 사용하기 때문에 다양한 이미지 테스크에 적용하는 것이 힘들다. <code class="language-plaintext highlighter-rouge">Segmentation</code>, <code class="language-plaintext highlighter-rouge">Object Detection</code> 같은 Task는 픽셀 단위로 예측을 수행해 객체를 탐지하거나 분할해야 한다. 하지만 <code class="language-plaintext highlighter-rouge">Patch</code> 단위로 훈련을 수행했던 <code class="language-plaintext highlighter-rouge">ViT</code>는 <code class="language-plaintext highlighter-rouge">Pixel</code> 단위의 예측을 수행하는데 어려움을 겪는다.</p>

<p>마지막으로 <code class="language-plaintext highlighter-rouge">Self-Attention</code> 자체의 <code class="language-plaintext highlighter-rouge">Computational Overhead</code>가 너무 심해 고해상도의 이미지를 적절히 다루기 힘들다. 위에서도 언급했지만 이미지의 사이즈가 <code class="language-plaintext highlighter-rouge">512x512</code>만 되어도 이미 패치의 개수가 <code class="language-plaintext highlighter-rouge">1024</code>가 된다. 사이즈가 커질수록 시퀀스 길이 역시 기하급수적으로 커지는데다가 <code class="language-plaintext highlighter-rouge">Self-Attention</code> 는 쿼리와 키 행렬을 내적 (자기 자신과 곱이라 볼 수 있음) 하기 때문에 <code class="language-plaintext highlighter-rouge">Computational Overhead</code>가 $N^2$이 된다.</p>

<p>필자는 <code class="language-plaintext highlighter-rouge">ViT</code>를 절반의 성공이라고 평하고 싶다. 본래 <code class="language-plaintext highlighter-rouge">ViT</code>의 설계 목적은 비전 분야의 <code class="language-plaintext highlighter-rouge">Conv</code>에 대한 의존을 탈피하면서, 퓨어한 <code class="language-plaintext highlighter-rouge">Self-Attention</code>을 도입해 <code class="language-plaintext highlighter-rouge">Scalabilty</code> 를 이식하는 것이었다. <code class="language-plaintext highlighter-rouge">Self-Attention</code>을 도입하는데는 성공했지만, 여전히 다룰 수 있는 이미지 사이즈나 Task에는 한계가 분명하며 결정적으로 <code class="language-plaintext highlighter-rouge">Self-Supervised Learning</code> 방식을 도입하지 못했다. <code class="language-plaintext highlighter-rouge">Scalabilty</code> 라는 단어의 의미를 생각하면, 방금 말한 부분에서까지 확장성이 있어야 설계 의도에 부합하는 결과라고 생각한다.</p>

        
      </section>

      <footer class="page__meta">
        
        
  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      <a href="/tags/#computer-vision" class="page__taxonomy-item p-category" rel="tag">Computer Vision</a><span class="sep">, </span>
    
      <a href="/tags/#image-classification" class="page__taxonomy-item p-category" rel="tag">Image Classification</a><span class="sep">, </span>
    
      <a href="/tags/#self-attention" class="page__taxonomy-item p-category" rel="tag">Self-Attention</a><span class="sep">, </span>
    
      <a href="/tags/#transformer" class="page__taxonomy-item p-category" rel="tag">Transformer</a><span class="sep">, </span>
    
      <a href="/tags/#vision-transformer" class="page__taxonomy-item p-category" rel="tag">Vision Transformer</a><span class="sep">, </span>
    
      <a href="/tags/#vit" class="page__taxonomy-item p-category" rel="tag">ViT</a>
    
    </span>
  </p>




  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> Categories: </strong>
    <span itemprop="keywords">
    
      <a href="/categories/#computer-vision" class="page__taxonomy-item p-category" rel="tag">Computer Vision</a>
    
    </span>
  </p>


        

  <p class="page__date"><strong><i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> Updated:</strong> <time class="dt-published" datetime="2023-07-26">July 26, 2023</time></p>

      </footer>

      <section class="page__share">
  
    <h4 class="page__share-title">Share on</h4>
  

  <a href="https://twitter.com/intent/tweet?text=%F0%9F%8C%86%C2%A0%5BViT%5D+An+Image+Is+Worth+16x16+Words%3A+Transformers+For+Image+Recognition+At+Scale%20http%3A%2F%2Flocalhost%3A4000%2Fcv%2Fvit" class="btn btn--twitter" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Twitter"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=http%3A%2F%2Flocalhost%3A4000%2Fcv%2Fvit" class="btn btn--facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Facebook"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=http%3A%2F%2Flocalhost%3A4000%2Fcv%2Fvit" class="btn btn--linkedin" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on LinkedIn"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>


      
  <nav class="pagination">
    
      <a href="/linear-algebra/vector-subspace" class="pagination--pager" title="🔢 Vector Space: Column Space, Basis, Rank, Null Space
">Previous</a>
    
    
      <a href="/nlp/deberta" class="pagination--pager" title="🪢 [DeBERTa] DeBERTa: Decoding-Enhanced BERT with Disentangled-Attention
">Next</a>
    
  </nav>

    </div>

    
  </article>

  
  
    <div class="page__related">
      <h2 class="page__related-title">You may also enjoy</h2>
      <div class="grid__wrapper">
        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
      <div class="archive__item-teaser">
        <img src="/assets/images/huggingface_emoji.png" alt="">
      </div>
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/nlp/linear_attention" rel="permalink">🌆 [Linear Attention] Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention
</a>
      
    </h2>
    <!-- 

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          8 minute read
        
      </span>
    
  </p>

 -->
    
      <p class="page__meta"><i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i> March 14 2024</p>
    
    <p class="archive__item-excerpt" itemprop="description">Linear Attention Official Paper Review with Pytorch Implementation
</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
      <div class="archive__item-teaser">
        <img src="/assets/images/huggingface_emoji.png" alt="">
      </div>
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/nlp/spanbert" rel="permalink">🗂️[SpanBERT] SpanBERT: Improving Pre-training by Representing and Predicting Spans
</a>
      
    </h2>
    <!-- 

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          6 minute read
        
      </span>
    
  </p>

 -->
    
      <p class="page__meta"><i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i> March 11 2024</p>
    
    <p class="archive__item-excerpt" itemprop="description">SpanBERT Official Paper Review with Pytorch Implementation
</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
      <div class="archive__item-teaser">
        <img src="/assets/images/huggingface_emoji.png" alt="">
      </div>
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/nlp/roformer" rel="permalink">🎡 [Roformer] RoFormer: Enhanced Transformer with Rotary Position Embedding
</a>
      
    </h2>
    <!-- 

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          5 minute read
        
      </span>
    
  </p>

 -->
    
      <p class="page__meta"><i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i> March 11 2024</p>
    
    <p class="archive__item-excerpt" itemprop="description">Roformer Official Paper Review with Pytorch Implementation
</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
      <div class="archive__item-teaser">
        <img src="/assets/images/huggingface_emoji.png" alt="">
      </div>
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/nlp/electra" rel="permalink">👮 [ELECTRA] Pre-training Text Encoders as Discriminators Rather Than Generators
</a>
      
    </h2>
    <!-- 

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          7 minute read
        
      </span>
    
  </p>

 -->
    
      <p class="page__meta"><i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i> March 11 2024</p>
    
    <p class="archive__item-excerpt" itemprop="description">ELECTRA Official Paper Review with Pytorch Implementation
</p>
  </article>
</div>

        
      </div>
    </div>
  
  
</div>

    </div>

    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>Follow:</strong></li>
    

    
      
        
      
        
      
        
      
        
      
        
      
        
      
    

    
      <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
    
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2024 qcqced. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>







    <script>
  'use strict';

  (function() {
    var commentContainer = document.querySelector('#utterances-comments');

    if (!commentContainer) {
      return;
    }

    var script = document.createElement('script');
    script.setAttribute('src', 'https://utteranc.es/client.js');
    script.setAttribute('repo', 'qcqced123/qcqced123.github.io');
    script.setAttribute('issue-term', 'pathname');
    
    script.setAttribute('theme', 'github-light');
    script.setAttribute('crossorigin', 'anonymous');

    commentContainer.appendChild(script);
  })();
</script>

  




<script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML">
</script>

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"] ],
    displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
    processEscapes: true
    },
    "HTML-CSS": { availableFonts: ["TeX"] }
});
</script>

  </body>
</html>
