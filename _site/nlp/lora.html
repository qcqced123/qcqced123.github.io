<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.24.0 by Michael Rose
  Copyright 2013-2020 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->
<html lang="en" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>🔪 [LoRA] Low-Rank Adaptation of Large Language Models - AI/Business Study Log</title>
<meta name="description" content="LoRA Official Paper Review with Pytorch Implementation">


  <meta name="author" content="qcqced">
  
  <meta property="article:author" content="qcqced">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="AI/Business Study Log">
<meta property="og:title" content="🔪 [LoRA] Low-Rank Adaptation of Large Language Models">
<meta property="og:url" content="http://localhost:4000/nlp/lora">


  <meta property="og:description" content="LoRA Official Paper Review with Pytorch Implementation">







  <meta property="article:published_time" content="2024-03-28T00:00:00+09:00">



  <meta property="article:modified_time" content="2024-03-29T02:00:00+09:00">



  

  


<link rel="canonical" href="http://localhost:4000/nlp/lora">




<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    
      "@type": "Person",
      "name": "qcqced",
      "url": "http://localhost:4000/"
    
  }
</script>







<!-- end _includes/seo.html -->



  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="AI/Business Study Log Feed">


<!-- https://t.co/dKP3o1e -->
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css"></noscript>



<!-- Latex -->
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->
<link rel="apple-touch-icon" sizes="180x180" href="/assets/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/assets/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/assets/favicon-16x16.png">
<link rel="manifest" href="/assets/site.webmanifest">
<link rel="mask-icon" href="/assets/safari-pinned-tab.svg" color="#5bbad5">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="theme-color" content="#ffffff">
<!-- end custom head snippets -->

    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      TeX: {
        equationNumbers: {
          autoNumber: "AMS"
        }
      },
      tex2jax: {
      inlineMath: [ ['$', '$'] ],
      displayMath: [ ['$$', '$$'] ],
      processEscapes: true
    }
  });
  MathJax.Hub.Register.MessageHook("Math Processing Error",function (message) {
      alert("Math Processing Error: "+message[1]);
    });
  MathJax.Hub.Register.MessageHook("TeX Jax - parse error",function (message) {
      alert("Math Processing Error: "+message[1]);
    });
  </script>
  <script type="text/javascript" async
    src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>
  </head>

  <body class="layout--single">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
          AI/Business Study Log
          <span class="site-subtitle">NLP, Marketing</span>
        </a>
        
        
        <ul class="visible-links">
              
              
                  <li class="masthead__menu-item">
                      <a href="https://qcqced123.github.io/">Home</a>
                  </li>
              
          
              
              
                  <li class="dropdown ">
                      <a class="dropdown-toggle" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">CS/AI  <i class="fa fa-caret-down fa-sm" aria-hidden="true"></i><span class="caret"></span></a>
       
                      <ul class="dropdown-content" >
                          
                              <a class = "dropdown-item" href="/nlp/">    Natural Language Process</a>
                          
                              <a class = "dropdown-item" href="/multi-modal/">    Multi Modal</a>
                          
                              <a class = "dropdown-item" href="/cv/">    Computer Vision</a>
                          
                              <a class = "dropdown-item" href="/ml/">    Machine Learning</a>
                          
                              <a class = "dropdown-item" href="/framework-library/">    Framework & Library</a>
                          
                              <a class = "dropdown-item" href="/python/">    Python</a>
                          
                              <a class = "dropdown-item" href="/algorithm/">    Data Structure & Algorithm</a>
                          
                              <a class = "dropdown-item" href="/ps/">    Problem Solving</a>
                          
                      </ul>
                  </li>
              
          
              
              
                  <li class="dropdown ">
                      <a class="dropdown-toggle" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Math  <i class="fa fa-caret-down fa-sm" aria-hidden="true"></i><span class="caret"></span></a>
       
                      <ul class="dropdown-content" >
                          
                              <a class = "dropdown-item" href="/linear-algebra/">    Linear Algebra</a>
                          
                              <a class = "dropdown-item" href="/optimization-theory/">    Optimization Theory/Calculus</a>
                          
                              <a class = "dropdown-item" href="/signal-system/">    Signal & System</a>
                          
                      </ul>
                  </li>
              
          
              
              
                  <li class="dropdown ">
                      <a class="dropdown-toggle" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Business/Marketing  <i class="fa fa-caret-down fa-sm" aria-hidden="true"></i><span class="caret"></span></a>
       
                      <ul class="dropdown-content" >
                          
                              <a class = "dropdown-item" href="/device/">    Device</a>
                          
                              <a class = "dropdown-item" href="/semi-conductor/">    Semi-Conductor</a>
                          
                              <a class = "dropdown-item" href="/ai/">    AI</a>
                          
                      </ul>
                  </li>
              
          
              
              
                  <li class="masthead__menu-item">
                      <a href="/categories/">Category</a>
                  </li>
              
          
              
              
                  <li class="masthead__menu-item">
                      <a href="/about/">About</a>
                  </li>
              
          
       </ul>
       
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      





<div id="main" role="main">
  
  <div class="sidebar sticky">
  


<div itemscope itemtype="https://schema.org/Person" class="h-card">

  
    <div class="author__avatar">
      <a href="http://localhost:4000/">
        <img src="/assets/images/huggingface_emoji.png" alt="qcqced" itemprop="image" class="u-photo">
      </a>
    </div>
  

  <div class="author__content">
    <h3 class="author__name p-name" itemprop="name">
      <a class="u-url" rel="me" href="http://localhost:4000/" itemprop="url">qcqced</a>
    </h3>
    
      <div class="author__bio p-note" itemprop="description">
        <p>Interested in NLP, Marketing</p>

      </div>
    
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">Follow</button>
    <ul class="author__urls social-icons">
      
        <li itemprop="homeLocation" itemscope itemtype="https://schema.org/Place">
          <i class="fas fa-fw fa-map-marker-alt" aria-hidden="true"></i> <span itemprop="name" class="p-locality">Seoul, South Korea</span>
        </li>
      

      
        
          
            <li><a href="https://qcqced123.github.io" rel="nofollow noopener noreferrer me" itemprop="sameAs"><i class="fas fa-fw fa-link" aria-hidden="true"></i><span class="label">Website</span></a></li>
          
        
          
            <li><a href="https://github.com/qcqced123" rel="nofollow noopener noreferrer me" itemprop="sameAs"><i class="fab fa-fw fa-github" aria-hidden="true"></i><span class="label">GitHub</span></a></li>
          
        
          
            <li><a href="https://www.kaggle.com/qcqced" rel="nofollow noopener noreferrer me" itemprop="sameAs"><i class="fab fa-fw fa-kaggle" aria-hidden="true"></i><span class="label">Kaggle</span></a></li>
          
        
      

      

      
        <li>
          <a href="mailto:qcqced123@gmail.com" rel="me" class="u-email">
            <meta itemprop="email" content="qcqced123@gmail.com" />
            <i class="fas fa-fw fa-envelope-square" aria-hidden="true"></i><span class="label">Email</span>
          </a>
        </li>
      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      <!--
  <li>
    <a href="http://link-to-whatever-social-network.com/user/" itemprop="sameAs" rel="nofollow noopener noreferrer me">
      <i class="fas fa-fw" aria-hidden="true"></i> Custom Social Profile Link
    </a>
  </li>
-->
    </ul>
  </div>
</div>

  
  </div>



  <article class="page h-entry" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="🔪 [LoRA] Low-Rank Adaptation of Large Language Models">
    <meta itemprop="description" content="LoRA Official Paper Review with Pytorch Implementation">
    <meta itemprop="datePublished" content="2024-03-28T00:00:00+09:00">
    <meta itemprop="dateModified" content="2024-03-29T02:00:00+09:00">

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title p-name" itemprop="headline">
            <a href="http://localhost:4000/nlp/lora" class="u-url" itemprop="url">🔪 [LoRA] Low-Rank Adaptation of Large Language Models
</a>
          </h1>
          <p class="page__date">
            <a href="https://hits.seeyoufarm.com/localhost:4000/nlp/lora"target="_blank">
              <img src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https://localhost:4000/nlp/lora&count_bg=%23399DE2&title_bg=%236D6D6D&icon=pytorch.svg&icon_color=%23E7E7E7&title=Views&edge_flat=false"/>
            </a>
            <i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> Updated: <time datetime="2024-03-28T00:00:00+09:00">March 28, 2024</time>
            <!-- <div style="text-align: left;"> -->
            <!-- </div> -->
          </p>
          
          
        </header>
      

      <section class="page__content e-content" itemprop="text">
        
          <aside class="sidebar__right sticky">
            <nav class="toc">
              <header><h4 class="nav__title"><i class="fas fa-file-alt"></i> On this page</h4></header>
              <ul class="toc__menu"><li><a href="#overview">🔭 Overview</a></li><li><a href="#concept-low-rank-adaptation">🤔 Concept: Low-Rank Adaptation</a><ul><li><a href="#inisght-1-apply-to-lora-wq-wv-or-wq-wk-wv-wo">💡 Inisght 1. Apply to LoRA (Wq, Wv) or (Wq, Wk, Wv, Wo)</a></li><li><a href="#inisght-2-낮은-랭크로도-충분">💡 Inisght 2. 낮은 랭크로도 충분</a></li><li><a href="#inisght-3-w--delta-w">💡 Inisght 3. w ≠ delta w</a></li></ul></li><li><a href="#implementation-by-pytorch">👩‍💻 Implementation by Pytorch</a></li></ul>

            </nav>
          </aside>
        
        <h3 id="overview"><code class="language-plaintext highlighter-rouge">🔭 Overview</code></h3>

<p align="center">
<img src="/assets/images/lora/lora.png" alt="LoRA" class="align-center image-caption" width="50%&quot;, height=&quot;50%" />
<strong><em><a href="https://arxiv.org/abs/2106.09685">LoRA</a></em></strong>
</p>

<p>LoRA는 2021년 MS 연구진이 발표한 논문으로 원본(Full 파인튜닝)과 거의 유사한 성능(심지어 일부 벤치마크는 더 높음)으로 LLM 파인튜닝에 필요한 GPU 메모리를 획기적으로 줄이는데 성공해 주목을 받았다. 커뮤니티에서 <code class="language-plaintext highlighter-rouge">LoRA is All You Need</code> 라는 별명까지 얻으며 그 인기를 구가하고 있다.</p>

<p><code class="language-plaintext highlighter-rouge">DistilBERT</code> 리뷰에서도 살펴보았듯, BERT와 GPT의 등장 이후, 모든 NLP 도메인에서 비약적인 성능 향상이 이뤄줬음에도 불구하고, NLP용 딥러닝 모델을 실생활에 활용하기에는 너무 큰 리소스 요구량과 레이턴시가 발목을 잡았다. 하지만 <code class="language-plaintext highlighter-rouge">LoRA</code> 발표 이후, 파인튜닝 시점에 훈련해야 하는 파라미터 수가 현저히 줄어들면서 모델의 체크포인트 용량이 기하급수적으로 감소했다. 덕분에 요구 GPU VRAM이 현저히 낮아져, 리소스 제한 때문에 서빙하지 못하는 경우가 많이 사라졌다. 그래서 오늘날 <code class="language-plaintext highlighter-rouge">Mixed Precision</code>, <code class="language-plaintext highlighter-rouge">Quantization</code>과 함께 모델 경량•최적화 분야에서 가장 중요한 주제로 떠오르고 있다.</p>

<p>내용을 살펴보기전, <code class="language-plaintext highlighter-rouge">LoRA</code> 는 이미 사전학습을 완료한 모델을 파인튜닝할 때 사용해야함을 다시 한 번 명심하자. 이번 포스팅에서는 두가지를 집중적으로 다룰 것이다.</p>

<p><strong>1) 모델 크기 줄인 방법, 2) 크기를 줄이면서도 비슷한 성능을 낼 수 있었던 이유</strong></p>

<h3 id="concept-low-rank-adaptation"><code class="language-plaintext highlighter-rouge">🤔 Concept: Low-Rank Adaptation</code></h3>

\[h = W_0x + \Delta Wx =  W_0x + BAx\]

<p>아이디어는 상당히 간단하다. 사전학습을 마치고 수렴된 상태의 가중치 행렬을 의미하는 $W_0$과 새로운 가중치 행렬 $\Delta W$에 모두 입력을 통과시킨다. 그리고 나온 결과를 더해 다음층의 입력으로 사용한다. 오히려 새로운 가중치 행렬을 추가해 파인튜닝을 하는데 어떻게 훈련해야 하는 파라미터 수를 줄일 수 있었을까??</p>

<p>그 비밀은 <code class="language-plaintext highlighter-rouge">Freeze(Stop Gradient, require_grad=False)</code>와 <code class="language-plaintext highlighter-rouge">Matrix Factorization</code>에 숨어 있다. 먼저 사전 훈련된 가중치 행렬에 <code class="language-plaintext highlighter-rouge">Freeze(Stop Gradient, require_grad=False)</code> 를 적용해 그라디언트가 흐르지 않도록 한다. 이렇게 하면 파인튜닝 과정에서 가중치가 업데이트 되지 않아 사전 학습에서 습득한 지식을 유지할 수 있을 뿐만 아니라, 학습을 위해 그라디언트를 저장할 필요가 없어져 파인튜닝 때 필요한 GPU VRAM을 획기적으로 줄일 수 있다.</p>

<p>처음에 사전학습 가중치를 통과한 값과 새로운 가중치 행렬 $\Delta W$를 통과한 값을 서로 더한다고 언급했다. 그렇다면, 두 결과 행렬의 행렬 크기가 동일해야 한다는 것이다. 어떻게 기존보다 사이즈는 줄이면서 결과 행렬의 크기는 동일하게 만들어줄 수 있을까?? 바로 Low Rank value $r$을 도입해 Matrix Factorization 을 한다.</p>

\[W_{d \times d} = \begin{bmatrix}
w_{1,1} &amp; w_{1,2} &amp; \cdots &amp; w_{1,d} \\
w_{2,1} &amp; w_{2,2} &amp; \cdots &amp; w_{2,d} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
w_{d,1} &amp; w_{d,2} &amp; \cdots &amp; w_{d,d}
\end{bmatrix}\]

<p>행렬 곱셈(matrix multiplication)을 다시 한 번 상기해보자. <code class="language-plaintext highlighter-rouge">MxN</code> 의 크기를 갖는 행렬에 <code class="language-plaintext highlighter-rouge">NxK</code>의 크기를 갖는 행렬을 곱해주면 <code class="language-plaintext highlighter-rouge">MxK</code> 의 크기를 갖는 행렬을 만들어줄 수 있다. 마찬가지다. <code class="language-plaintext highlighter-rouge">dxd</code> 크기인 사전학습의 가중치 행렬 $W_{d \times d}$과 크기를 맞추기 위해, <code class="language-plaintext highlighter-rouge">dxd</code> 짜리 행렬을 각각 <code class="language-plaintext highlighter-rouge">dxr</code>, <code class="language-plaintext highlighter-rouge">rxd</code> 의 크기를 갖는 두 행렬 $B, A$로 분해한다. 이 때, 행렬 $B$의 열차원과 행렬 $A$차원의 행차원 크기를 표현하는 $r$에 바로 Low Rank value $r$을 대입하면 된다.</p>

\[\Delta W_{d \times d} = B_{d \times r}\ A_{r \times d} = \begin{bmatrix}
w_{1,1} &amp; w_{1,2} &amp; w_{1,r} \\
w_{2,1} &amp; w_{2,2} &amp; w_{2,r} \\
\vdots &amp; \vdots &amp; \vdots \\
w_{d,1} &amp; w_{d,2} &amp; w_{d,r}
\end{bmatrix}\begin{bmatrix}
w_{1,1} &amp; w_{2,1} &amp; w_{d,1} \\
w_{1,2} &amp; w_{2,2} &amp; w_{d,2} \\
\vdots &amp; \vdots &amp; \vdots \\
w_{1,r} &amp; w_{2,r} &amp; w_{d,r}
\end{bmatrix}\]

<p>$r=3$이라고 가정하고 <code class="language-plaintext highlighter-rouge">768x768</code> 짜리 기존 가중치 행렬 $W$과 <code class="language-plaintext highlighter-rouge">768x3</code>, <code class="language-plaintext highlighter-rouge">3x768</code>의 크기를 갖는 $\Delta W = BA$의 파라미터 개수를 비교해보자. 계산해보면 전자는 <code class="language-plaintext highlighter-rouge">589,824</code>개, 후자는 <code class="language-plaintext highlighter-rouge">4608</code>개가 된다. 정확하게 <code class="language-plaintext highlighter-rouge">128</code>배 차이가 난다. 트랜스포머 모델 속에는 행렬 $W$과 같은 크기를 갖는 가중치 행렬이 단일 인코더 내부, 하나의 어텐션 레이어만 해도 4개($W_q, W_k, W_v, W_o$)가 있다. <code class="language-plaintext highlighter-rouge">BERT-base</code> 모델을 기준으로 보면, 해당 모델이 <code class="language-plaintext highlighter-rouge">12</code>개의 인코더로 구성되어 있으니까 총 <code class="language-plaintext highlighter-rouge">48</code>개의 가중치 행렬이 있고, 어림잡아도 <code class="language-plaintext highlighter-rouge">48*128</code>배의 학습 파라미터 감소 효과를 낼 수 있다. 모델의 레이어가 많으면 많을수록 더 좋은 효율을 보인다.</p>

<p align="center">
<img src="/assets/images/lora/cuda_memory.png" alt="Resnet50 Memeory Type in GPU" class="align-center image-caption" width="100%&quot;, height=&quot;50%" />
<strong><em><a href="https://pytorch.org/blog/understanding-gpu-memory-1/">Resnet50 Memeory Type in GPU</a></em></strong>
</p>

<p>위 그림은 파이토치 공식 블로그에서 퍼온 자료로, 학습 때 ResNet50의 GPU VRAM 점유율 추이는 물론 모델의 개별 구성요소의 메모리 비율까지 자세히 보여준다. 먼저 <code class="language-plaintext highlighter-rouge">Parameter</code>와 <code class="language-plaintext highlighter-rouge">Optimizer State</code>를 보자. <code class="language-plaintext highlighter-rouge">Parameter</code> 는 모델에서 훈련을 통해 업데이트가 필요한 모든 구성 요소를 말한다. <code class="language-plaintext highlighter-rouge">Freeze</code>, <code class="language-plaintext highlighter-rouge">require_grad=False</code> <code class="language-plaintext highlighter-rouge">@torch.no_grad()</code> , <code class="language-plaintext highlighter-rouge">torch.register_buffer()</code> 의 영향을 받지 않은 모델 내부의 모든 텐서라고 보면 된다.</p>

<p>한편, <code class="language-plaintext highlighter-rouge">Optimizer State</code> 는 옵티마이저의 최적화 수행에 필요한 모든 정보들을 의미하는데, 예를 들어 업데이트 될 텐서의 메타 정보, 여러 하이퍼파라미터 값 같은 것들이 담겨 있다.</p>

<p>이 두 요소가 모델의 GPU VRAM을 차지하는 비율이 상당히 크다. 하지만 두 요소 모두 파라미터 개수에 비례하므로 <code class="language-plaintext highlighter-rouge">LoRA</code> 적용으로 파라미터 개수를 줄이면, GPU VRAM을 획기적으로 줄일 수 있다.</p>

<p>또한 파이토치는 역전파 수행을 위해 그라디언트를 파라미터와 동일한 모양(shape)을 갖는 텐서로 저장된다는 점을 감안하면, 기존의 Full-Rank 텐서 대신 Low-Rank 텐서를 학습에 이용함으로서 그라디언트 텐서의 크기 역시 획기적으로 줄일 수 있겠다.</p>

<p>트랜스포머 계열의 모델들이 ResNet 대비 압도적으로 파라미터 개수가 많기 때문에 <code class="language-plaintext highlighter-rouge">LoRA</code>를 적용한다면 훨씬 큰 효과를 볼 수 있을 것이다.</p>

<figure class="half">
  <a href="https://arxiv.org/abs/2106.09685"><img src="/assets/images/lora/Encoder_LoRA.png" title="Encoder LoRA Result" /></a>
  <a href="https://arxiv.org/abs/2106.09685"><img src="/assets/images/lora/Decoder_LoRA.png" title="Decoder LoRA Result" /></a>
</figure>

<p>왼쪽 그림은 논문에서 제시한, <code class="language-plaintext highlighter-rouge">BERT</code> 계열의 <code class="language-plaintext highlighter-rouge">LM</code>에 <code class="language-plaintext highlighter-rouge">LoRA</code>를 적용한 파인튜닝 결과다. 표의 <code class="language-plaintext highlighter-rouge">FT</code> 가 일반적인 파인튜닝 방법에 의해 나온 결과다. 엎치락뒤치락하면서 거의 비슷한 양상을 보인다. 벤치마크 평균 성능은 <code class="language-plaintext highlighter-rouge">LoRA</code>가 더 높다. 아마, 적당히 성능 차이를 보여주기 위해 취사선택된 벤치마크일 가능성이 높지만, 그래도 상당히 유의미한 결과라고 생각한다. 우측은 <code class="language-plaintext highlighter-rouge">GPT2</code>에 <code class="language-plaintext highlighter-rouge">LoRA</code>를 적용한 결과다. 마찬가지로, 엇비슷한 성능 추이를 보여준다.</p>

<p>지금까지 <code class="language-plaintext highlighter-rouge">LoRA</code> 가 제시하는 방법론이 어떻게 획기적으로 학습 파라미터를 줄이고 나아가 모델이 차지하는 <code class="language-plaintext highlighter-rouge">GPU VRAM</code> 크기를 감소시켰는지 알아 보았다. 이제 <code class="language-plaintext highlighter-rouge">LoRA</code>를 적용해도 일반적인 파인튜닝 방법과 비슷한 성능을 유지할 수 있었는지 그 결과에 대해 해석해보자. 논문의 <code class="language-plaintext highlighter-rouge">Chapter 7. UNDERSTANDIGN THE LOW-RANK UPDATES</code> 내용에 해당된다. 해당 파트는 3가지 인사이트를 제시한다.</p>

<h4 id="inisght-1-apply-to-lora-wq-wv-or-wq-wk-wv-wo"><code class="language-plaintext highlighter-rouge">💡 Inisght 1. Apply to LoRA (Wq, Wv) or (Wq, Wk, Wv, Wo)</code></h4>

<p align="center">
<img src="/assets/images/lora/applying.png" alt="Which Matrix is the BEST" class="align-center image-caption" width="100%&quot;, height=&quot;50%" />
<strong><em><a href="https://arxiv.org/abs/2106.09685">Which Matrix is the BEST</a></em></strong>
</p>

<p>필자는 논문을 읽는 내내, <code class="language-plaintext highlighter-rouge">‘그래서 어떤 가중치 행렬에 적용해야 할까?? 모든 가중치 행렬에 적용해도 되는걸까??’</code>하는 의문을 갖고 있었다. 근데 마침 저자들이 이러한 의문들을 에상한 듯, 위와 같이 적용 가중치 행렬에 따른 벤치마크 성능 결과를 표로 정리해주었다. 모델은 <code class="language-plaintext highlighter-rouge">GPT3</code>을 사용했다고 논문에서 밝히고 있다.</p>

<p>보이는 것과 같이, ($W_q, W_v$) 혹은 ($W_q, W_k, W_v, W_o$)에 <code class="language-plaintext highlighter-rouge">LoRA</code>를 적용하는게 가장 좋은 벤치마크 성능을 보여준다. 주목할 점은 랭크가 가장 낮으면서, 가장 많은 가중치 행렬에 <code class="language-plaintext highlighter-rouge">LoRA</code>를 적용하는게 가장 성능이 좋다는 것이다. 실험결과 제시 이외에 다른 증명이나 인사이트 제시가 없는게 아쉽지만, 이를 통해 다음과 같은 사실들을 떠올려 보았다.</p>

<ul>
  <li><strong>1) <code class="language-plaintext highlighter-rouge">FT</code> 문제 해결에 필요한 문맥 정보들이 <code class="language-plaintext highlighter-rouge">쿼리</code>, <code class="language-plaintext highlighter-rouge">키</code>, <code class="language-plaintext highlighter-rouge">벨류</code> 행렬에 적절히 분산</strong>
    <ul>
      <li><strong>세가지 가중치 행렬이 모두 유의미한 문맥 표현을 학습</strong></li>
    </ul>
  </li>
  <li><strong>2) 낮은 랭크로도 충분히, <code class="language-plaintext highlighter-rouge">FT</code>에 필요한 임베딩 추출 가능</strong>
    <ul>
      <li><strong>그만큼, 사전 학습에서 포착할 수 있는 임베딩이 풍부하며 일반화 능력이 좋다고 판단할 수 있음</strong>
        <ul>
          <li><strong>사전 학습 단계에서 최대한 깊게 많이 학습시킬수록 FT 단계가 간소화 될 수 있지 않을까??</strong></li>
          <li><strong>다만, 사전학습과 파인튜닝 사이의 괴리가 큰 경우라면??</strong></li>
          <li><strong>사전 학습은 영어로, 파인튜닝은 한국어 데이터 세트로 하는 경우라면??</strong></li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p>마침 주석에 <code class="language-plaintext highlighter-rouge">However, we do not expect a small r to work for every task or dataset. Consider the following thought experiment: if the downstream task were in a different language than the one used for pre-training, retraining the entire model</code> 이라는 언급이 있는 것으로 보아, 랭크 값은 되도록 낮은 값을 선정하되, 사전학습과 파인 튜닝의 괴리가 심하다고 판단되는 경우, 높은 랭크값과 실험 결과 비교를 통해 적절한 값을 선정해야겠다.</p>

<h4 id="inisght-2-낮은-랭크로도-충분"><code class="language-plaintext highlighter-rouge">💡 Inisght 2. 낮은 랭크로도 충분</code></h4>

<p align="center">
<img src="/assets/images/lora/insight2.png" alt="Insight 2" class="align-center image-caption" width="70%&quot;, height=&quot;50%" />
<strong><em><a href="https://arxiv.org/abs/2106.09685">Insight 2</a></em></strong>
</p>

<p>낮은 랭크로도 충분히, <code class="language-plaintext highlighter-rouge">FT</code>에 필요한 임베딩 추출 가능하다는 것을 좀 더 구체적인 실험으로 증명하고 있다. 그래프 $y$축과 $x$축은 각각 $A_r = 8$, $A_r = 64$인 (텐서 모양 <code class="language-plaintext highlighter-rouge">[r, dim_model]</code>) 가중치 행렬을 <code class="language-plaintext highlighter-rouge">SVD</code>하여 얻은 <code class="language-plaintext highlighter-rouge">right-singular matrix</code> 에서 <code class="language-plaintext highlighter-rouge">top-i(1 ≤ i ≤ 8)</code>, <code class="language-plaintext highlighter-rouge">top-j (1 ≤ j ≤ 64)</code>개의 특이값을 추출한 뒤, <code class="language-plaintext highlighter-rouge">Grassmann Distance</code>를 거리 매트릭으로 이용해 부분 공간 사이의 유사도를 측정한 결과다.</p>

<p>한편, 왜 <code class="language-plaintext highlighter-rouge">right-singular matrix</code> 일까 다시 한 번 생각해봤다. 전체 벡터 공간에서 <code class="language-plaintext highlighter-rouge">top-i(1 ≤ i ≤ 8)</code>, <code class="language-plaintext highlighter-rouge">top-j (1 ≤ j ≤ 64)</code>개의 특이값을 뽑아내 서로 비교하려면, 두 행렬 $A_{r = 8}$, $A_{r = 64}$이 같은 부분 공간에서 정의 되어야 한다. <code class="language-plaintext highlighter-rouge">SVD</code> 정의상, 왼쪽 특이벡터는 각각 <code class="language-plaintext highlighter-rouge">8x8</code>, <code class="language-plaintext highlighter-rouge">64x64</code>차원이 되어 비교하기 어렵다. 한편, 오른쪽 벡터는 두 행렬 모두 <code class="language-plaintext highlighter-rouge">dxd</code>로 정의된다. 만약 행렬 $A$ 대신 $B$를 사용하고 싶다면 왼쪽 특이 벡터를 사용하면 된다.</p>

<p>오랜지 색에 가까울수록 서로 겹치는 정보가 많다는 의미를 갖는데, 사용한 인코더 위치와 상관없이 $A_{r = 8}$의 <code class="language-plaintext highlighter-rouge">top</code> 열벡터일수록, $A_{r = 64}$의 나머지 열벡터들과 높은 유사도(오랜지색에 가까움)을 기록하고 있다.(헷갈리니까 왼쪽 두 개 그래프만 보는게 낫다). 그리고 $A_{r = 8}$의 <code class="language-plaintext highlighter-rouge">bottom</code> 열벡터일수록, 거무죽죽한 색깔을 가지며 $A_{r = 64}$의 나머지 열벡터들과 낮은 유사도를 보인다.</p>

<p><strong>결국 알고봤더니 사전학습을 충분히 수행한 모델의 경우, 파인튜닝 <code class="language-plaintext highlighter-rouge">Task</code>에 대해 적응시키는데 필요한 공간은 소수, 굳이 전체 공간을 학습 파라미터로 두고 파인튜닝해봐야 대부분의 열벡터는 쓰잘데기 없는 표현을 인코딩하는데 쓰이고 있었다고 볼 수 있겠다.</strong></p>

<p>물론 여기서도 주의할 점은, <code class="language-plaintext highlighter-rouge">GPT3</code>의 사전학습과 궤가 비슷한 <code class="language-plaintext highlighter-rouge">WikiSQL</code>, <code class="language-plaintext highlighter-rouge">MNLU</code>에 대해 파인튜닝한 결과라는 점이다. 다국어로 구성된 데이터 세트를 활용하게 되면, 이 결과가 어떻게 바뀔지 모른다.</p>

<p><code class="language-plaintext highlighter-rouge">Grassmann Distance</code> 는 선형 부분공간(linear subspace) 간의 거리를 측정하는 데 사용되는 개념이라고 하는데, 여기서 이것까지 다루면 포스팅 길이가 너무 길어질 것 같아서, 나중에 다른 포스트에서 다루도록 하겠다.</p>

<h4 id="inisght-3-w--delta-w"><code class="language-plaintext highlighter-rouge">💡 Inisght 3. w ≠ delta w</code></h4>

<p align="center">
<img src="/assets/images/lora/insight3.png" alt="Insight 3" class="align-center image-caption" width="100%&quot;, height=&quot;50%" />
<strong><em><a href="https://arxiv.org/abs/2106.09685">Insight 3</a></em></strong>
</p>

<p>사전 학습한 가중치 행렬 $W$과 <code class="language-plaintext highlighter-rouge">LoRA</code> 의 $\Delta W$가 서로 얼마나 유사한지, 실험적으로 증명하고 있다. 논문에서 제공한 실험 방식을 정리하면 다음과 같다.</p>

<ul>
  <li>1) 사전 학습으로 수렴된 쿼리 행렬, $W_q$를 <code class="language-plaintext highlighter-rouge">task-specific</code>한 공간($U^T, V^T$: $\Delta W$의 <code class="language-plaintext highlighter-rouge">Top-r</code>개의 <code class="language-plaintext highlighter-rouge">Left, Right-Singular Vector</code>)으로 투영</li>
  <li>2) LoRA에 의해 수렴된 델타 쿼리행렬, $\Delta W_q$은 이미 행렬 전반에 <code class="language-plaintext highlighter-rouge">task-specific</code>한 정보를 담고 있음.
    <ul>
      <li>그래서 <code class="language-plaintext highlighter-rouge">top-r</code> 추출하지 않고 전체에 대해서 <code class="language-plaintext highlighter-rouge">프로베니우스 놈</code> 구하기</li>
    </ul>
  </li>
  <li>3) 1번 스탭에서 구한 투영 행렬, $U^TW_qV^T$에 대해 <code class="language-plaintext highlighter-rouge">프로베니우스 놈</code> 계산</li>
  <li>4) 2번/3번 수행: <code class="language-plaintext highlighter-rouge">task-specific</code>한 공간을 <code class="language-plaintext highlighter-rouge">LoRA</code>가 사전 학습 가중치에 비해 얼마나 많이 강조했는지 나타내는 지표
    <ul>
      <li>논문에서는 <code class="language-plaintext highlighter-rouge">Feature Amplication Factor</code> 라고 정의</li>
    </ul>
  </li>
</ul>

<p><code class="language-plaintext highlighter-rouge">프로베니우스 놈</code>은 기하학적으로 행렬의 크기, 즉 <code class="language-plaintext highlighter-rouge">선형변환</code>의 크기를 의미한다. 그래서 곧 <code class="language-plaintext highlighter-rouge">Feature Amplication Factor</code>가 행렬의 크기/행렬의 크기를 나타내는 지표가 되고, 분자와 분모의 행렬은 모두 <code class="language-plaintext highlighter-rouge">task-specific</code>한 공간으로의 변환 크기를 의미하기 때문에, 같은 특징을 분자($\Delta W$)가 분모($W$)에 비해서 얼마나 더 강조하는지를 뜻하게 된다. 띠리서 <code class="language-plaintext highlighter-rouge">factor</code> 값이 클수록 <code class="language-plaintext highlighter-rouge">LoRA</code>가 사전 학습에서 강조하지 않았던 특징을 더욱 강조한다고 해석할 수 있게 된다. 이제 다시 표를 분석해보자.</p>

<p><code class="language-plaintext highlighter-rouge">Low Rank value</code> $r=4$일 때, <code class="language-plaintext highlighter-rouge">Feature Amplication Factor</code>의 분모는 <code class="language-plaintext highlighter-rouge">0.32</code>, 분자는 <code class="language-plaintext highlighter-rouge">6.91</code>이 된다. 따라서 <code class="language-plaintext highlighter-rouge">factor</code> 값은 대략 <code class="language-plaintext highlighter-rouge">21.5</code>가 된다. 다시 말해 <code class="language-plaintext highlighter-rouge">GPT3</code>의 48번째 레이어의 경우, <code class="language-plaintext highlighter-rouge">FT</code> 적응에 필요한 <code class="language-plaintext highlighter-rouge">task-specific</code>한 공간을 <code class="language-plaintext highlighter-rouge">LoRA</code>가 <code class="language-plaintext highlighter-rouge">사전학습 쿼리 행렬</code>보다 <code class="language-plaintext highlighter-rouge">21.5</code>배 강조하고 있다는 것이다.</p>

<p><code class="language-plaintext highlighter-rouge">Low Rank value</code> $r=64$일 때는 <code class="language-plaintext highlighter-rouge">factor</code>가 대략 <code class="language-plaintext highlighter-rouge">1.9</code>가 된다. $r=4$일 때보다 <code class="language-plaintext highlighter-rouge">factor</code> 값이 현저히 낮은 이유는 <code class="language-plaintext highlighter-rouge">Insight 2</code>의 결과(낮은 랭크로도 충분히 <code class="language-plaintext highlighter-rouge">FT</code>의 <code class="language-plaintext highlighter-rouge">task-specific</code> 정보 표현 가능)와 일맥상통한다고 볼 수 있다.</p>

<p>처음 읽었을 때 이 부분에 대한 해석이 너무 난해해, 저자들이 깃허브에 공개한, <code class="language-plaintext highlighter-rouge">RoBERTa</code>를 LoRA와 함께 <code class="language-plaintext highlighter-rouge">MRPC</code> 벤치마크에 파인튜닝한 가중치를 불러와 똑같은 방식으로 실험을 진행해봤다. 먼저 전체 실험 방식을 요약하면 다음과 같다.</p>

<ul>
  <li>1) <code class="language-plaintext highlighter-rouge">Huggingface Hub</code>에서 <code class="language-plaintext highlighter-rouge">RoBERTa-base</code>의 사전학습 가중치 불러오기</li>
  <li>2) <code class="language-plaintext highlighter-rouge">LoRA official github</code>에서 <code class="language-plaintext highlighter-rouge">roberta_base_lora_mrpc.bin</code> 불러오기</li>
  <li>3) <code class="language-plaintext highlighter-rouge">1,2</code>번에서 모두 <code class="language-plaintext highlighter-rouge">6</code>번째 <code class="language-plaintext highlighter-rouge">인코더 레이어</code>의 <code class="language-plaintext highlighter-rouge">쿼리 행렬</code>에 대한 가중치 추출</li>
  <li>4) 이하 나머지 과정은 위에 논문의 실험 방식을 따름</li>
</ul>

<p>전체 과정을 코드로 정리하면 다음과 같다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="s">""" Insight 3 Experiment Code Exanple """</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModel</span><span class="p">,</span> <span class="n">AutoConfig</span>

<span class="s">""" LoRA 결과 해석 재현 """</span>
<span class="n">pt_config</span> <span class="o">=</span> <span class="n">AutoConfig</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s">'FacebookAI/roberta-base'</span><span class="p">)</span>  
<span class="n">pt_model</span> <span class="o">=</span> <span class="n">AutoModel</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">(</span> <span class="c1"># pretrained model
</span>    <span class="s">'roberta-base'</span><span class="p">,</span>
    <span class="n">config</span><span class="o">=</span><span class="n">pt_config</span>
<span class="p">)</span>

<span class="n">lora_checkpoint</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="s">'model/roberta_base_lora_mrpc.bin'</span><span class="p">,</span> <span class="n">map_location</span><span class="o">=</span><span class="s">'cpu'</span><span class="p">)</span>
<span class="n">lora_checkpoint</span>

<span class="s">""" Select Wq in 6-th encoder layer """</span>
<span class="n">pt_wq</span><span class="p">,</span> <span class="n">lora_a</span><span class="p">,</span> <span class="n">lora_b</span> <span class="o">=</span> <span class="n">pt_model</span><span class="p">.</span><span class="n">encoder</span><span class="p">.</span><span class="n">layer</span><span class="p">[</span><span class="mi">6</span><span class="p">].</span><span class="n">attention</span><span class="p">.</span><span class="bp">self</span><span class="p">.</span><span class="n">query</span><span class="p">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">lora_checkpoint</span><span class="p">[</span><span class="s">'roberta.encoder.layer.6.attention.self.query.lora_A'</span><span class="p">],</span> <span class="n">lora_checkpoint</span><span class="p">[</span><span class="s">'roberta.encoder.layer.6.attention.self.query.lora_B'</span><span class="p">]</span>
<span class="n">delta_wq</span> <span class="o">=</span> <span class="n">lora_b</span> <span class="o">@</span> <span class="n">lora_a</span>
<span class="n">pt_wq</span><span class="p">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">lora_a</span><span class="p">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">lora_b</span><span class="p">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">delta_wq</span><span class="p">.</span><span class="n">shape</span>

<span class="o">&gt;&gt;&gt;</span> <span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">768</span><span class="p">,</span> <span class="mi">768</span><span class="p">]),</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">8</span><span class="p">,</span> <span class="mi">768</span><span class="p">]),</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">768</span><span class="p">,</span> <span class="mi">8</span><span class="p">]),</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">768</span><span class="p">,</span> <span class="mi">768</span><span class="p">]))</span>

<span class="s">""" Let's SVD, select top-r singular vector, 분자  """</span>
<span class="n">U</span><span class="p">,</span> <span class="n">S</span><span class="p">,</span> <span class="n">V</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">svd</span><span class="p">(</span><span class="n">delta_wq</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Delta W U: </span><span class="si">{</span><span class="n">U</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Delta W S: </span><span class="si">{</span><span class="n">S</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Delta W V: </span><span class="si">{</span><span class="n">V</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">Delta</span> <span class="n">W</span> <span class="n">U</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">768</span><span class="p">,</span> <span class="mi">768</span><span class="p">])</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">Delta</span> <span class="n">W</span> <span class="n">S</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">768</span><span class="p">])</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">Delta</span> <span class="n">W</span> <span class="n">V</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">768</span><span class="p">,</span> <span class="mi">768</span><span class="p">])</span>

<span class="n">r</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">r_U</span><span class="p">,</span> <span class="n">r_V</span> <span class="o">=</span> <span class="n">U</span><span class="p">[:,</span> <span class="p">:</span><span class="n">r</span><span class="p">],</span> <span class="n">V</span><span class="p">[:</span><span class="n">r</span><span class="p">,</span> <span class="p">:]</span>
<span class="n">result1</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">r_U</span><span class="p">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">pt_wq</span><span class="p">,</span> <span class="n">r_V</span><span class="p">.</span><span class="n">T</span><span class="p">)</span>
<span class="n">fwq_norm</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">norm</span><span class="p">(</span><span class="n">result1</span><span class="p">)</span>  <span class="c1"># 분자값
</span><span class="n">result1</span><span class="p">,</span> <span class="n">fwq_norm</span>

<span class="o">&gt;&gt;&gt;</span> <span class="p">(</span><span class="n">tensor</span><span class="p">([[</span><span class="o">-</span><span class="mf">0.0441</span><span class="p">,</span>  <span class="mf">0.0447</span><span class="p">,</span>  <span class="mf">0.0323</span><span class="p">,</span>  <span class="mf">0.0963</span><span class="p">],</span>
         <span class="p">[</span><span class="o">-</span><span class="mf">0.0038</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0412</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0903</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0949</span><span class="p">],</span>
         <span class="p">[</span><span class="o">-</span><span class="mf">0.0314</span><span class="p">,</span>  <span class="mf">0.1003</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0599</span><span class="p">,</span>  <span class="mf">0.0023</span><span class="p">],</span>
         <span class="p">[</span><span class="o">-</span><span class="mf">0.0222</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1090</span><span class="p">,</span>  <span class="mf">0.0315</span><span class="p">,</span>  <span class="mf">0.0575</span><span class="p">]],</span> <span class="n">grad_fn</span><span class="o">=&lt;</span><span class="n">MmBackward0</span><span class="o">&gt;</span><span class="p">),</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">tensor</span><span class="p">(</span><span class="mf">0.2539</span><span class="p">,</span> <span class="n">grad_fn</span><span class="o">=&lt;</span><span class="n">LinalgVectorNormBackward0</span><span class="o">&gt;</span><span class="p">))</span>

<span class="s">""" 분모 """</span>
<span class="n">fdwq_norm</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">norm</span><span class="p">(</span><span class="n">delta_wq</span><span class="p">)</span>  <span class="c1"># 분모값
</span><span class="n">fdwq_norm</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">tensor</span><span class="p">(</span><span class="mf">5.0820</span><span class="p">)</span>

<span class="s">"""결과: Feature Amplication Factor """</span>
<span class="n">fdwq_norm</span> <span class="o">/</span> <span class="n">fwq_norm</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">tensor</span><span class="p">(</span><span class="mf">20.0170</span><span class="p">,</span> <span class="n">grad_fn</span><span class="o">=&lt;</span><span class="n">DivBackward0</span><span class="o">&gt;</span><span class="p">)</span>

</code></pre></div></div>

<h3 id="implementation-by-pytorch"><code class="language-plaintext highlighter-rouge">👩‍💻 Implementation by Pytorch</code></h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">Tensor</span>

<span class="k">class</span> <span class="nc">LoRA</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="s">""" class module for Low-Rank adaptation of LLM SFT
    This module return result of "BAx*(a/r)" in mathematical expression in official paper

    Args:
        dim: dimension of input tensor
        rank: rank of tensor, which is hyperparameter for LoRA
        alpha: hyperparameter for LoRA, trainable parameter, which is initialized by rank value

    Math:
        h = W0x + ∆Wx = W0x + BAx*(a/r)

    References:
        https://arxiv.org/abs/2106.09685
        https://pytorch.org/blog/understanding-gpu-memory-1/
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">rank</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">a</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">dim</span><span class="p">))</span>  <span class="c1"># init by random Gaussian distribution (normal distribution)
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">b</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">rank</span><span class="p">))</span>  <span class="c1"># init by zero
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">alpha</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">rank</span><span class="p">))</span> <span class="o">/</span> <span class="n">rank</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">b</span> <span class="o">@</span> <span class="bp">self</span><span class="p">.</span><span class="n">a</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">alpha</span>

</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">LoRA</code> 객체 구현 자체는 매우 간단하다. 중요한 점은 사전 학습 모델에 <code class="language-plaintext highlighter-rouge">LoRA</code> 객체를 모델 전체 코드에 적용하는게 어렵다는 것이다. 아래 코드처럼,</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="s">""" before MHA """</span>

<span class="n">q</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">fc_q</span><span class="p">(</span><span class="n">x</span><span class="p">).</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="bp">self</span><span class="p">.</span><span class="n">num_attention_heads</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">dim_head</span><span class="p">).</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">).</span><span class="n">contiguous</span><span class="p">()</span>
<span class="n">k</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">fc_k</span><span class="p">(</span><span class="n">x</span><span class="p">).</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="bp">self</span><span class="p">.</span><span class="n">num_attention_heads</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">dim_head</span><span class="p">).</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">).</span><span class="n">contiguous</span><span class="p">()</span>
<span class="n">v</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">fc_v</span><span class="p">(</span><span class="n">x</span><span class="p">).</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="bp">self</span><span class="p">.</span><span class="n">num_attention_heads</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">dim_head</span><span class="p">).</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">).</span><span class="n">contiguous</span><span class="p">()</span>

<span class="n">attention_output</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">fc_concat</span><span class="p">(</span><span class="n">attention_matrix</span><span class="p">)</span>

<span class="s">""" after MHA """</span>
<span class="bp">self</span><span class="p">.</span><span class="n">lora_q</span> <span class="o">=</span> <span class="n">lora</span><span class="p">()</span>
<span class="bp">self</span><span class="p">.</span><span class="n">lora_k</span> <span class="o">=</span> <span class="n">lora</span><span class="p">()</span>
<span class="bp">self</span><span class="p">.</span><span class="n">lora_v</span> <span class="o">=</span> <span class="n">lora</span><span class="p">()</span>
<span class="bp">self</span><span class="p">.</span><span class="n">lora_o</span> <span class="o">=</span> <span class="n">lora</span><span class="p">()</span>

<span class="n">q</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">fc_q</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">lora_q</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># freeze + trainable
</span><span class="n">q</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="bp">self</span><span class="p">.</span><span class="n">num_attention_heads</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">dim_head</span><span class="p">).</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">).</span><span class="n">contiguous</span><span class="p">()</span>

<span class="n">k</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">fc_k</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">lora_k</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># freeze + trainable
</span><span class="n">k</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="bp">self</span><span class="p">.</span><span class="n">num_attention_heads</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">dim_head</span><span class="p">).</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">).</span><span class="n">contiguous</span><span class="p">()</span>

<span class="n">v</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">fc_v</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">lora_v</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># freeze + trainable
</span><span class="n">v</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="bp">self</span><span class="p">.</span><span class="n">num_attention_heads</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">dim_head</span><span class="p">).</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">).</span><span class="n">contiguous</span><span class="p">()</span>

<span class="n">attention_output</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">fc_concat</span><span class="p">(</span><span class="n">attention_matrix</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">lora_o</span><span class="p">(</span><span class="n">attention_matrix</span><span class="p">)</span> <span class="c1"># freeze + trainable
</span>
</code></pre></div></div>

<p>선형 투영된 쿼리, 키, 벨류 행렬과 각각의 <code class="language-plaintext highlighter-rouge">LoRA</code> 객체를 더해줄 수만 있다면 매우 간단하게 해결될 문제지만, 사전학습 모델의 <code class="language-plaintext highlighter-rouge">Multi-Head Attention</code>객체를 처음부터 저런식으로 정의해야만 가능한 일이다. 필자가 작성한 모델 코드를 비롯해 대부분의 오픈소스로 풀려있는 트랜스포머 모델들은 저런식으로 작성되어 있지 않다. 따라서 다른 방법을 떠올려하는데, 당장은 너무 복잡한 작업이 될 것 같아(실험 어플리케이션 구조를 뒤엎어야 가능할 것으로 예측) 일단은 여기서 마무리하려고 한다. 만약 <code class="language-plaintext highlighter-rouge">LoRA</code>를 사전 학습 모델에 적용해 파인튜닝을 해보고 싶다면, <code class="language-plaintext highlighter-rouge">Huggingface</code>의 <code class="language-plaintext highlighter-rouge">PEFT</code> 라이브러리를 이용해보자. <code class="language-plaintext highlighter-rouge">Hugginface</code>의 <code class="language-plaintext highlighter-rouge">Automodel</code>, <code class="language-plaintext highlighter-rouge">Trainer</code> 객체와 유연하게 연동이 가능하다. 아래에 <code class="language-plaintext highlighter-rouge">PEFT</code> 공식 문서에서 참고한 <code class="language-plaintext highlighter-rouge">Usage Example</code> 코드를 첨부했으니 참고 부탁바란다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="s">""" PEFT LoRA Usage Example

Reference:
		https://github.com/huggingface/peft/blob/main/src/peft/tuners/lora/model.py
"""</span>
<span class="o">&gt;&gt;&gt;</span> <span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForSeq2SeqLM</span>
<span class="o">&gt;&gt;&gt;</span> <span class="kn">from</span> <span class="nn">peft</span> <span class="kn">import</span> <span class="n">LoraModel</span><span class="p">,</span> <span class="n">LoraConfig</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">config</span> <span class="o">=</span> <span class="n">LoraConfig</span><span class="p">(</span>
<span class="p">...</span>     <span class="n">task_type</span><span class="o">=</span><span class="s">"SEQ_2_SEQ_LM"</span><span class="p">,</span>
<span class="p">...</span>     <span class="n">r</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>  <span class="c1"># rank value in official paper
</span><span class="p">...</span>     <span class="n">lora_alpha</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>  <span class="c1"># alpha value in official paper
</span><span class="p">...</span>     <span class="n">target_modules</span><span class="o">=</span><span class="p">[</span><span class="s">"q"</span><span class="p">,</span> <span class="s">"v"</span><span class="p">],</span>
<span class="p">...</span>     <span class="n">lora_dropout</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span>
<span class="p">...</span> <span class="p">)</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForSeq2SeqLM</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s">"t5-base"</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">lora_model</span> <span class="o">=</span> <span class="n">LoraModel</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">config</span><span class="p">,</span> <span class="s">"default"</span><span class="p">)</span>

<span class="o">&gt;&gt;&gt;</span> <span class="kn">import</span> <span class="nn">torch</span>
<span class="o">&gt;&gt;&gt;</span> <span class="kn">import</span> <span class="nn">transformers</span>
<span class="o">&gt;&gt;&gt;</span> <span class="kn">from</span> <span class="nn">peft</span> <span class="kn">import</span> <span class="n">LoraConfig</span><span class="p">,</span> <span class="n">PeftModel</span><span class="p">,</span> <span class="n">get_peft_model</span><span class="p">,</span> <span class="n">prepare_model_for_kbit_training</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">rank</span> <span class="o">=</span> <span class="p">...</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">target_modules</span> <span class="o">=</span> <span class="p">[</span><span class="s">"q_proj"</span><span class="p">,</span> <span class="s">"k_proj"</span><span class="p">,</span> <span class="s">"v_proj"</span><span class="p">,</span> <span class="s">"out_proj"</span><span class="p">,</span> <span class="s">"fc_in"</span><span class="p">,</span> <span class="s">"fc_out"</span><span class="p">,</span> <span class="s">"wte"</span><span class="p">]</span>  <span class="c1"># target for projection matrix, MLP
</span><span class="o">&gt;&gt;&gt;</span> <span class="n">config</span> <span class="o">=</span> <span class="n">LoraConfig</span><span class="p">(</span>
<span class="p">...</span>     <span class="n">r</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">lora_alpha</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">target_modules</span><span class="o">=</span><span class="n">target_modules</span><span class="p">,</span> <span class="n">lora_dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="s">"none"</span><span class="p">,</span> <span class="n">task_type</span><span class="o">=</span><span class="s">"CAUSAL_LM"</span>
<span class="p">...</span> <span class="p">)</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">quantization_config</span> <span class="o">=</span> <span class="n">transformers</span><span class="p">.</span><span class="n">BitsAndBytesConfig</span><span class="p">(</span><span class="n">load_in_8bit</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">transformers</span><span class="p">.</span><span class="n">AutoTokenizer</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">(</span>
<span class="p">...</span>     <span class="s">"kakaobrain/kogpt"</span><span class="p">,</span>
<span class="p">...</span>     <span class="n">revision</span><span class="o">=</span><span class="s">"KoGPT6B-ryan1.5b-float16"</span><span class="p">,</span>  <span class="c1"># or float32 version: revision=KoGPT6B-ryan1.5b
</span><span class="p">...</span>     <span class="n">bos_token</span><span class="o">=</span><span class="s">"[BOS]"</span><span class="p">,</span>
<span class="p">...</span>     <span class="n">eos_token</span><span class="o">=</span><span class="s">"[EOS]"</span><span class="p">,</span>
<span class="p">...</span>     <span class="n">unk_token</span><span class="o">=</span><span class="s">"[UNK]"</span><span class="p">,</span>
<span class="p">...</span>     <span class="n">pad_token</span><span class="o">=</span><span class="s">"[PAD]"</span><span class="p">,</span>
<span class="p">...</span>     <span class="n">mask_token</span><span class="o">=</span><span class="s">"[MASK]"</span><span class="p">,</span>
<span class="p">...</span> <span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">model</span> <span class="o">=</span> <span class="n">transformers</span><span class="p">.</span><span class="n">GPTJForCausalLM</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">(</span>
<span class="p">...</span>     <span class="s">"kakaobrain/kogpt"</span><span class="p">,</span>
<span class="p">...</span>     <span class="n">revision</span><span class="o">=</span><span class="s">"KoGPT6B-ryan1.5b-float16"</span><span class="p">,</span>  <span class="c1"># or float32 version: revision=KoGPT6B-ryan1.5b
</span><span class="p">...</span>     <span class="n">pad_token_id</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">.</span><span class="n">eos_token_id</span><span class="p">,</span>
<span class="p">...</span>     <span class="n">use_cache</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
<span class="p">...</span>     <span class="n">device_map</span><span class="o">=</span><span class="p">{</span><span class="s">""</span><span class="p">:</span> <span class="n">rank</span><span class="p">},</span>
<span class="p">...</span>     <span class="n">torch_dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float16</span><span class="p">,</span>
<span class="p">...</span>     <span class="n">quantization_config</span><span class="o">=</span><span class="n">quantization_config</span><span class="p">,</span>
<span class="p">...</span> <span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">model</span> <span class="o">=</span> <span class="n">prepare_model_for_kbit_training</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">lora_model</span> <span class="o">=</span> <span class="n">get_peft_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">config</span><span class="p">)</span> 
</code></pre></div></div>

        
      </section>

      <footer class="page__meta">
        
        
  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      <a href="/tags/#fine-tune" class="page__taxonomy-item p-category" rel="tag">Fine-Tune</a><span class="sep">, </span>
    
      <a href="/tags/#huggingface" class="page__taxonomy-item p-category" rel="tag">Huggingface</a><span class="sep">, </span>
    
      <a href="/tags/#lora" class="page__taxonomy-item p-category" rel="tag">LoRA</a><span class="sep">, </span>
    
      <a href="/tags/#low-rank-adaptation" class="page__taxonomy-item p-category" rel="tag">Low-Rank Adaptation</a><span class="sep">, </span>
    
      <a href="/tags/#natural-language-process" class="page__taxonomy-item p-category" rel="tag">Natural Language Process</a><span class="sep">, </span>
    
      <a href="/tags/#optimization" class="page__taxonomy-item p-category" rel="tag">Optimization</a><span class="sep">, </span>
    
      <a href="/tags/#peft" class="page__taxonomy-item p-category" rel="tag">PEFT</a><span class="sep">, </span>
    
      <a href="/tags/#pytorch" class="page__taxonomy-item p-category" rel="tag">Pytorch</a>
    
    </span>
  </p>




  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> Categories: </strong>
    <span itemprop="keywords">
    
      <a href="/categories/#nlp" class="page__taxonomy-item p-category" rel="tag">NLP</a>
    
    </span>
  </p>


        

  <p class="page__date"><strong><i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> Updated:</strong> <time class="dt-published" datetime="2024-03-28">March 28, 2024</time></p>

      </footer>

      <section class="page__share">
  
    <h4 class="page__share-title">Share on</h4>
  

  <a href="https://twitter.com/intent/tweet?text=%F0%9F%94%AA+%5BLoRA%5D+Low-Rank+Adaptation+of+Large+Language+Models%20http%3A%2F%2Flocalhost%3A4000%2Fnlp%2Flora" class="btn btn--twitter" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Twitter"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=http%3A%2F%2Flocalhost%3A4000%2Fnlp%2Flora" class="btn btn--facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Facebook"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=http%3A%2F%2Flocalhost%3A4000%2Fnlp%2Flora" class="btn btn--linkedin" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on LinkedIn"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>


      
  <nav class="pagination">
    
      <a href="/python/time_complexity2" class="pagination--pager" title="👨⏰🐍 [Python] 시간복잡도 2
">Previous</a>
    
    
      <a href="#" class="pagination--pager disabled">Next</a>
    
  </nav>

    </div>

    
  </article>

  
  
    <div class="page__related">
      <h2 class="page__related-title">You may also enjoy</h2>
      <div class="grid__wrapper">
        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
      <div class="archive__item-teaser">
        <img src="/assets/images/huggingface_emoji.png" alt="">
      </div>
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/python/time_complexity2" rel="permalink">👨⏰🐍 [Python] 시간복잡도 2
</a>
      
    </h2>
    <!-- 

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          4 minute read
        
      </span>
    
  </p>

 -->
    
      <p class="page__meta"><i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i> March 26 2024</p>
    
    <p class="archive__item-excerpt" itemprop="description">시간 복잡도 줄이기
</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
      <div class="archive__item-teaser">
        <img src="/assets/images/huggingface_emoji.png" alt="">
      </div>
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/python/time_complexity1" rel="permalink">👨⏰🐍 [Python] 시간복잡도 1
</a>
      
    </h2>
    <!-- 

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          1 minute read
        
      </span>
    
  </p>

 -->
    
      <p class="page__meta"><i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i> March 26 2024</p>
    
    <p class="archive__item-excerpt" itemprop="description">시간 복잡도에 대한 이해
</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
      <div class="archive__item-teaser">
        <img src="/assets/images/huggingface_emoji.png" alt="">
      </div>
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/nlp/linear_attention" rel="permalink">🌆 [Linear Attention] Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention
</a>
      
    </h2>
    <!-- 

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          8 minute read
        
      </span>
    
  </p>

 -->
    
      <p class="page__meta"><i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i> March 14 2024</p>
    
    <p class="archive__item-excerpt" itemprop="description">Linear Attention Official Paper Review with Pytorch Implementation
</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
      <div class="archive__item-teaser">
        <img src="/assets/images/huggingface_emoji.png" alt="">
      </div>
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/nlp/spanbert" rel="permalink">🗂️[SpanBERT] SpanBERT: Improving Pre-training by Representing and Predicting Spans
</a>
      
    </h2>
    <!-- 

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          8 minute read
        
      </span>
    
  </p>

 -->
    
      <p class="page__meta"><i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i> March 11 2024</p>
    
    <p class="archive__item-excerpt" itemprop="description">SpanBERT Official Paper Review with Pytorch Implementation
</p>
  </article>
</div>

        
      </div>
    </div>
  
  
</div>

    </div>

    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>Follow:</strong></li>
    

    
      
        
      
        
      
        
      
        
      
        
      
        
      
    

    
      <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
    
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2024 qcqced. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>







    <script>
  'use strict';

  (function() {
    var commentContainer = document.querySelector('#utterances-comments');

    if (!commentContainer) {
      return;
    }

    var script = document.createElement('script');
    script.setAttribute('src', 'https://utteranc.es/client.js');
    script.setAttribute('repo', 'qcqced123/qcqced123.github.io');
    script.setAttribute('issue-term', 'pathname');
    
    script.setAttribute('theme', 'github-light');
    script.setAttribute('crossorigin', 'anonymous');

    commentContainer.appendChild(script);
  })();
</script>

  




<script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML">
</script>

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"] ],
    displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
    processEscapes: true
    },
    "HTML-CSS": { availableFonts: ["TeX"] }
});
</script>

  </body>
</html>
