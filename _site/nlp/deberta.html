<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.24.0 by Michael Rose
  Copyright 2013-2020 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->
<html lang="en" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>🪢 [DeBERTa] DeBERTa: Decoding-Enhanced BERT with Disentangled-Attention - AI/Business Study Log</title>
<meta name="description" content="Transformer Official Paper Review with Pytorch Implementation">


  <meta name="author" content="qcqced">
  
  <meta property="article:author" content="qcqced">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="AI/Business Study Log">
<meta property="og:title" content="🪢 [DeBERTa] DeBERTa: Decoding-Enhanced BERT with Disentangled-Attention">
<meta property="og:url" content="http://localhost:4000/nlp/deberta">


  <meta property="og:description" content="Transformer Official Paper Review with Pytorch Implementation">







  <meta property="article:published_time" content="2023-08-04T00:00:00+09:00">



  <meta property="article:modified_time" content="2023-08-05T02:00:00+09:00">



  

  


<link rel="canonical" href="http://localhost:4000/nlp/deberta">




<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    
      "@type": "Person",
      "name": "qcqced",
      "url": "http://localhost:4000/"
    
  }
</script>







<!-- end _includes/seo.html -->



  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="AI/Business Study Log Feed">


<!-- https://t.co/dKP3o1e -->
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css"></noscript>



<!-- Latex -->
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->
<link rel="apple-touch-icon" sizes="180x180" href="/assets/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/assets/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/assets/favicon-16x16.png">
<link rel="manifest" href="/assets/site.webmanifest">
<link rel="mask-icon" href="/assets/safari-pinned-tab.svg" color="#5bbad5">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="theme-color" content="#ffffff">
<!-- end custom head snippets -->

    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      TeX: {
        equationNumbers: {
          autoNumber: "AMS"
        }
      },
      tex2jax: {
      inlineMath: [ ['$', '$'] ],
      displayMath: [ ['$$', '$$'] ],
      processEscapes: true
    }
  });
  MathJax.Hub.Register.MessageHook("Math Processing Error",function (message) {
      alert("Math Processing Error: "+message[1]);
    });
  MathJax.Hub.Register.MessageHook("TeX Jax - parse error",function (message) {
      alert("Math Processing Error: "+message[1]);
    });
  </script>
  <script type="text/javascript" async
    src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>
  </head>

  <body class="layout--single">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
          AI/Business Study Log
          <span class="site-subtitle">NLP, Marketing</span>
        </a>
        
        
        <ul class="visible-links">
              
              
                  <li class="masthead__menu-item">
                      <a href="https://qcqced123.github.io/">Home</a>
                  </li>
              
          
              
              
                  <li class="dropdown ">
                      <a class="dropdown-toggle" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">CS/AI  <i class="fa fa-caret-down fa-sm" aria-hidden="true"></i><span class="caret"></span></a>
       
                      <ul class="dropdown-content" >
                          
                              <a class = "dropdown-item" href="/nlp/">    Natural Language Process</a>
                          
                              <a class = "dropdown-item" href="/multi-modal/">    Multi Modal</a>
                          
                              <a class = "dropdown-item" href="/cv/">    Computer Vision</a>
                          
                              <a class = "dropdown-item" href="/ml/">    Machine Learning</a>
                          
                              <a class = "dropdown-item" href="/framework-library/">    Framework & Library</a>
                          
                              <a class = "dropdown-item" href="/python/">    Python</a>
                          
                              <a class = "dropdown-item" href="/algorithm/">    Algorithm</a>
                          
                      </ul>
                  </li>
              
          
              
              
                  <li class="dropdown ">
                      <a class="dropdown-toggle" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Math  <i class="fa fa-caret-down fa-sm" aria-hidden="true"></i><span class="caret"></span></a>
       
                      <ul class="dropdown-content" >
                          
                              <a class = "dropdown-item" href="/linear-algebra/">    Linear Algebra</a>
                          
                              <a class = "dropdown-item" href="/calculus/">    Calculus</a>
                          
                              <a class = "dropdown-item" href="/optimization-theory/">    Optimization Theory</a>
                          
                              <a class = "dropdown-item" href="/signal-system/">    Signal & System</a>
                          
                      </ul>
                  </li>
              
          
              
              
                  <li class="dropdown ">
                      <a class="dropdown-toggle" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Business/Marketing  <i class="fa fa-caret-down fa-sm" aria-hidden="true"></i><span class="caret"></span></a>
       
                      <ul class="dropdown-content" >
                          
                              <a class = "dropdown-item" href="/device/">    Device</a>
                          
                              <a class = "dropdown-item" href="/semi-conductor/">    Semi-Conductor</a>
                          
                              <a class = "dropdown-item" href="/ai/">    AI</a>
                          
                      </ul>
                  </li>
              
          
              
              
                  <li class="masthead__menu-item">
                      <a href="/categories/">Category</a>
                  </li>
              
          
              
              
                  <li class="masthead__menu-item">
                      <a href="/about/">About</a>
                  </li>
              
          
       </ul>
       
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      





<div id="main" role="main">
  
  <div class="sidebar sticky">
  


<div itemscope itemtype="https://schema.org/Person" class="h-card">

  
    <div class="author__avatar">
      <a href="http://localhost:4000/">
        <img src="/assets/images/huggingface_emoji.png" alt="qcqced" itemprop="image" class="u-photo">
      </a>
    </div>
  

  <div class="author__content">
    <h3 class="author__name p-name" itemprop="name">
      <a class="u-url" rel="me" href="http://localhost:4000/" itemprop="url">qcqced</a>
    </h3>
    
      <div class="author__bio p-note" itemprop="description">
        <p>Interested in NLP, Marketing</p>

      </div>
    
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">Follow</button>
    <ul class="author__urls social-icons">
      
        <li itemprop="homeLocation" itemscope itemtype="https://schema.org/Place">
          <i class="fas fa-fw fa-map-marker-alt" aria-hidden="true"></i> <span itemprop="name" class="p-locality">Seoul, South Korea</span>
        </li>
      

      
        
          
            <li><a href="https://qcqced123.github.io" rel="nofollow noopener noreferrer me" itemprop="sameAs"><i class="fas fa-fw fa-link" aria-hidden="true"></i><span class="label">Website</span></a></li>
          
        
          
            <li><a href="https://github.com/qcqced123" rel="nofollow noopener noreferrer me" itemprop="sameAs"><i class="fab fa-fw fa-github" aria-hidden="true"></i><span class="label">GitHub</span></a></li>
          
        
          
            <li><a href="https://www.kaggle.com/qcqced" rel="nofollow noopener noreferrer me" itemprop="sameAs"><i class="fab fa-fw fa-kaggle" aria-hidden="true"></i><span class="label">Kaggle</span></a></li>
          
        
      

      

      
        <li>
          <a href="mailto:qcqced123@gmail.com" rel="me" class="u-email">
            <meta itemprop="email" content="qcqced123@gmail.com" />
            <i class="fas fa-fw fa-envelope-square" aria-hidden="true"></i><span class="label">Email</span>
          </a>
        </li>
      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      <!--
  <li>
    <a href="http://link-to-whatever-social-network.com/user/" itemprop="sameAs" rel="nofollow noopener noreferrer me">
      <i class="fas fa-fw" aria-hidden="true"></i> Custom Social Profile Link
    </a>
  </li>
-->
    </ul>
  </div>
</div>

  
  </div>



  <article class="page h-entry" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="🪢 [DeBERTa] DeBERTa: Decoding-Enhanced BERT with Disentangled-Attention">
    <meta itemprop="description" content="Transformer Official Paper Review with Pytorch Implementation">
    <meta itemprop="datePublished" content="2023-08-04T00:00:00+09:00">
    <meta itemprop="dateModified" content="2023-08-05T02:00:00+09:00">

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title p-name" itemprop="headline">
            <a href="http://localhost:4000/nlp/deberta" class="u-url" itemprop="url">🪢 [DeBERTa] DeBERTa: Decoding-Enhanced BERT with Disentangled-Attention
</a>
          </h1>
          <p class="page__date">
            <a href="https://hits.seeyoufarm.com/localhost:4000/nlp/deberta"target="_blank">
              <img src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https://localhost:4000/nlp/deberta&count_bg=%23399DE2&title_bg=%236D6D6D&icon=pytorch.svg&icon_color=%23E7E7E7&title=Views&edge_flat=false"/>
            </a>
            <i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> Updated: <time datetime="2023-08-04T00:00:00+09:00">August 04, 2023</time>
            <!-- <div style="text-align: left;"> -->
            <!-- </div> -->
          </p>
          
          
        </header>
      

      <section class="page__content e-content" itemprop="text">
        
          <aside class="sidebar__right sticky">
            <nav class="toc">
              <header><h4 class="nav__title"><i class="fas fa-file-alt"></i> On this page</h4></header>
              <ul class="toc__menu"><li><a href="#overview">🔭 Overview</a></li><li><a href="#inducitve-bias-in-deberta">🪢 Inducitve Bias in DeBERTa</a><ul><li><a href="#-types-of-embedding">📚 Types of Embedding</a></li><li><a href="#-relative-position-embedding">🔢 Relative Position Embedding</a></li><li><a href="#-deberta-inductive-bias">🤔 DeBERTa Inductive Bias</a></li></ul></li></ul></li></ul></li><li><a href="#modeling">🌟 Modeling</a></li></ul>

            </nav>
          </aside>
        
        <h3 id="overview"><code class="language-plaintext highlighter-rouge">🔭 Overview</code></h3>

<p><code class="language-plaintext highlighter-rouge">DeBERTa</code>는 2020년 <code class="language-plaintext highlighter-rouge">Microsoft</code>가 <code class="language-plaintext highlighter-rouge">ICLR</code>에서 발표한 자연어 처리용 신경망 모델이다. <code class="language-plaintext highlighter-rouge">Disentangled Self-Attention</code>, <code class="language-plaintext highlighter-rouge">Enhanced Mask Decoder</code>라는 두가지 새로운 테크닉을 <code class="language-plaintext highlighter-rouge">BERT</code>, <code class="language-plaintext highlighter-rouge">RoBERTa</code>에 적용해 당시 <code class="language-plaintext highlighter-rouge">SOTA</code>를 달성했으며, 특히 영어처럼 문장에서 자리하는 위치에 따라 단어의 의미, 형태가 결정되는 굴절어 계열에 대한 성능이 좋아 꾸준히 사랑받고 있는 모델이다. 또한 인코딩 가능한 최대 시퀀스 길이가 <code class="language-plaintext highlighter-rouge">4096</code>으로 매우 긴 편 (<code class="language-plaintext highlighter-rouge">DeBERTa-V3-Large</code>) 에 속해, <code class="language-plaintext highlighter-rouge">Kaggle Competition</code>에서 자주 활용된다. 출시된지 2년이 넘도록 <code class="language-plaintext highlighter-rouge">SuperGLUE</code> 대시보드에서 꾸준히 상위권을 유지하고 있다는 점도 <code class="language-plaintext highlighter-rouge">DeBERTa</code>가 얼마나 잘 설계된 모델인지 알 수 있는 대목이다.</p>

<p>한편, <code class="language-plaintext highlighter-rouge">DeBERTa</code>의 설계 철학은 <code class="language-plaintext highlighter-rouge">Inductive Bias</code> 다. 간단하게 <code class="language-plaintext highlighter-rouge">Inductive Bias</code>란, 주어진 데이터로부터 일반화 성능을 높이기 위해 <code class="language-plaintext highlighter-rouge">"입력되는 데이터는 ~ 할 것이다"</code>, <code class="language-plaintext highlighter-rouge">"이런 특징을 갖고 있을 것이다"</code>와 같은 가정, 가중치, 가설 등을 기계학습 알고리즘에 적용하는 것을 말한다. <strong><a href="https://qcqced123.github.io/cv/vit"><code class="language-plaintext highlighter-rouge">ViT</code> 논문 리뷰</a></strong>에서도 밝혔듯, 퓨어한 <code class="language-plaintext highlighter-rouge">Self-Attention</code> 의 <code class="language-plaintext highlighter-rouge">Inductive Bias</code> 는 사실상 없으며, 전체 <code class="language-plaintext highlighter-rouge">Transformer</code> 구조 레벨에서 봐도 <code class="language-plaintext highlighter-rouge">Absolute Position Embedding</code>을 사용해 토큰의 위치 정보를 모델에 주입해주는 것이 그나마 약한 <code class="language-plaintext highlighter-rouge">Iniductive Bias</code>라고 볼 수 있다. 다른 포스팅에서는 분명 <code class="language-plaintext highlighter-rouge">Inductive Bias</code> 가 적기 때문에 자연어 처리에서 <code class="language-plaintext highlighter-rouge">Transformer</code> 가 성공을 거둘 수 있다고 해놓고 이게 지금 와서 말을 뒤집는다고 생각할 수 있다. 하지만 <code class="language-plaintext highlighter-rouge">Self-Attention</code>과 <code class="language-plaintext highlighter-rouge">Absolute Position Embedding</code>의 의미를 다시 한 번 상기해보면, <code class="language-plaintext highlighter-rouge">Inductive Bias</code> 추가를 주장하는 저자들의 생각이 꽤나 합리적이었음을 알 수 있게 된다. 구체적인 모델 구조를 파악하기 전에 먼저 <code class="language-plaintext highlighter-rouge">Inductive Bias</code> 추가가 왜 필요하며, 어떠한 가정이 필요한지 알아보자.</p>

<h3 id="inducitve-bias-in-deberta"><code class="language-plaintext highlighter-rouge">🪢 Inducitve Bias in DeBERTa</code></h3>
<ul>
  <li><strong><code class="language-plaintext highlighter-rouge">Absolute Position + Relative Position</code>을 모두 활용해 풍부하고 깊은 임베딩 추출</strong></li>
  <li><strong><code class="language-plaintext highlighter-rouge">단어의 발생 순서</code> 임베딩과 <code class="language-plaintext highlighter-rouge">단어 분포 가설</code> 임베딩을 모두 추출하는 것을 목적으로 설계</strong></li>
</ul>

<p>본 논문 초록에는 다음과 같은 문장이 서술되어 있다.</p>

<p><code class="language-plaintext highlighter-rouge">motivated by the observation that the attention weight of a word pair depends on not only their contents but their relative positions. For example, the dependency between the words “deep” and “learning” is much stronger when they occur next to each other than when they occur in different sentences.</code></p>

<p>위의 두 문장이 <code class="language-plaintext highlighter-rouge">DeBERTa</code>의 <code class="language-plaintext highlighter-rouge">Inducitve Bias</code> 를 가장 잘 설명하고 있다고 생각한다. 저자가 추가를 주장하는 <code class="language-plaintext highlighter-rouge">Inductive Bias</code>란, <code class="language-plaintext highlighter-rouge">relative position</code> 정보라는 것과 기존 모델링으로는 <code class="language-plaintext highlighter-rouge">relative position</code>이 주는 문맥 정보 포착이 불가능하다는 사실을 알 수 있다.</p>

<p>그렇다면 <code class="language-plaintext highlighter-rouge">relative position</code> 가 제공하는 문맥 정보가 도대체 뭐길래 기존의 방식으로는 포착이 불가능하다는 것일까?? 자연어에서 포착 가능한 문맥들의 종류와 기존의 모델링 방식에 대한 정리부터 해보자. 여기서 말하는 기존 방식이란, 퓨어한 <code class="language-plaintext highlighter-rouge">Self-Attention</code>과 <code class="language-plaintext highlighter-rouge">Absolute Position Embedding</code> 을 사용하는 <code class="language-plaintext highlighter-rouge">Transformer-Encoder-Base</code>  모델(<code class="language-plaintext highlighter-rouge">BERT</code>, <code class="language-plaintext highlighter-rouge">RoBERTa</code>)을 뜻한다. 이번 포스팅에서는 <code class="language-plaintext highlighter-rouge">BERT</code>를 기준으로 설명하겠다.</p>

<h6 id="-types-of-embedding"><code class="language-plaintext highlighter-rouge">📚 Types of Embedding</code></h6>

<p><strong>먼저 현존하는 모든 임베딩(<code class="language-plaintext highlighter-rouge">벡터에 문맥을 주입하는</code>)기법들을 정리해보자. 다음과 같이 3가지 카테고리로 분류가 가능하다.</strong></p>

<ul>
  <li><strong>1) 단어의 빈도수:  시퀀스에서 사용된 토큰들의 빈도수를 측정(<code class="language-plaintext highlighter-rouge">Bag of words</code>)</strong></li>
  <li><strong>2) 단어의 발생 순서: <code class="language-plaintext highlighter-rouge">corpus</code> 내부의 특정 <code class="language-plaintext highlighter-rouge">sequence</code> 등장 빈도를 카운트(<code class="language-plaintext highlighter-rouge">N-Gram</code>), 주어진 시퀀스를 가지고 다음 시점에 등장할 토큰을 맞추는 방식(<code class="language-plaintext highlighter-rouge">LM</code>)</strong></li>
  <li><strong>3) 단어 분포 가설 :  단어의 의미는 주변 문맥에 의해 결정된다는 가정, 어떤 단어 쌍이 자주 같이 등장하는지 카운트해 <code class="language-plaintext highlighter-rouge">PMI</code>를 측정하는 방식(<code class="language-plaintext highlighter-rouge">Word2Vec</code>)</strong></li>
</ul>

<p>기존의 모델링 방식은 어디에 포함될까?? <code class="language-plaintext highlighter-rouge">BERT</code> 는 대분류 상 신경망에 포함되고, <code class="language-plaintext highlighter-rouge">Language Modeling</code>을 통해 시퀀스를 학습한다는 점 그리고 <code class="language-plaintext highlighter-rouge">Self-Attention</code>과 <code class="language-plaintext highlighter-rouge">Absolute Position Embedding</code> 을 사용한다는 점에서 2번, <code class="language-plaintext highlighter-rouge">단어의 발생 순서</code> 에 포함된다고 볼 수 있다. <code class="language-plaintext highlighter-rouge">Absolute Position Embedding</code> 과 <code class="language-plaintext highlighter-rouge">Self-Attention</code>의 사용이 퓨어한 <code class="language-plaintext highlighter-rouge">BERT</code>가 분류상 2번이라는 사실을 뒷받침하는 증거라는 점에서 의아할 수 있다. 하지만 잘 생각해보자.</p>

<p><code class="language-plaintext highlighter-rouge">Absolute Position Embedding</code>은 주어진 시퀀스의 길이를 측정한 뒤, 나열된 순서 그대로 <code class="language-plaintext highlighter-rouge">forward</code>하게 <code class="language-plaintext highlighter-rouge">0</code>부터 <code class="language-plaintext highlighter-rouge">길이-1</code>의 번호를 개별 토큰에 할당한다. 다시 말해, 단어가 시퀀스에서 발생한 순서를 수학적으로 표현해 모델에 주입한다는 의미가 된다. <code class="language-plaintext highlighter-rouge">Self-Attention</code>은 <code class="language-plaintext highlighter-rouge">Absolute Position Embedding</code> 정보가 주입된 시퀀스 전체를 한 번에 병렬 처리한다. 따라서 충분히 <code class="language-plaintext highlighter-rouge">BERT</code> 같은 <code class="language-plaintext highlighter-rouge">Self-Attention</code>, <code class="language-plaintext highlighter-rouge">Absolute Position Embedding</code> 기반 모델을 2번에 분류할 수 있겠다.</p>

<p>한편, 혹자는 <code class="language-plaintext highlighter-rouge">"BERT는 MLM 을 사용하는데 Language Modeling을 한다고 하는게 맞나요"</code>라고 말할 수 있다. 하지만 <code class="language-plaintext highlighter-rouge">MLM</code> 역시 대분류 상 <code class="language-plaintext highlighter-rouge">Language Modeling</code> 기법에 속한다. <strong>다만, <code class="language-plaintext highlighter-rouge">Bi-Directional</code>하게 문맥을 파악하고 <code class="language-plaintext highlighter-rouge">LM</code>을 하니까 정말 엄밀히 따지면 3번의 속성도 어느 정도 있다고 보는게 무리는 아니라 생각한다.</strong> <code class="language-plaintext highlighter-rouge">MLM</code> 사용으로 더 많은 정보를 포착해 임베딩을 만들기 때문에 초기 <code class="language-plaintext highlighter-rouge">BERT</code>가 <code class="language-plaintext highlighter-rouge">GPT</code>보다 <code class="language-plaintext highlighter-rouge">NLU</code>에서 상대적으로 강점을 가졌던 것 아닐까 싶다.</p>

<h6 id="-relative-position-embedding"><code class="language-plaintext highlighter-rouge">🔢 Relative Position Embedding</code></h6>

<p>이제 <code class="language-plaintext highlighter-rouge">Relative Position Embedding</code>이 무엇이고, 도대체 어떤 문맥 정보를 포착한다는 것인지 알아보자. <code class="language-plaintext highlighter-rouge">Relative Position Embedding</code> 이란, 시퀀스 내부 토큰 사이의 위치 관계 표현을 통해 토큰 사이의 <code class="language-plaintext highlighter-rouge">relation</code>을 <code class="language-plaintext highlighter-rouge">pairwise</code>하게 학습하는 위치 임베딩 기법을 말한다. 일반적으로 상대 위치 관계는 서로 다른 두 토큰의 시퀀스 인덱스 값의 차를 이용해 나타낸다. 포착하는 문맥 정보는 예시와 함깨 설명하겠다. 딥러닝이라는 단어는 영어로 <code class="language-plaintext highlighter-rouge">Deep Learning</code> 이다. 두 단어를 합쳐놓고 보면 <code class="language-plaintext highlighter-rouge">신경망을 사용하는 머신러닝 기법의 한 종류</code>라는 의미를 갖겠지만, 따로 따로 보면 <code class="language-plaintext highlighter-rouge">깊은</code>, <code class="language-plaintext highlighter-rouge">배움</code>이라는 개별적인 의미로 나뉜다.</p>

<ul>
  <li><strong><code class="language-plaintext highlighter-rouge">1) The Deep Learning is the Best Technique in Computer Science</code></strong></li>
  <li><strong><code class="language-plaintext highlighter-rouge">2) I’m learning how to swim in the deep ocean</code></strong></li>
</ul>

<p><code class="language-plaintext highlighter-rouge">Deep</code>과 <code class="language-plaintext highlighter-rouge">Learning</code>의 상대적인 거리에 주목하면서 두 문장을 해석해보자. 첫 번째 문장에서 두 단어는 이웃하게 위치해 <code class="language-plaintext highlighter-rouge">신경망을 사용하는 머신러닝 기법의 한 종류</code> 라는 의미를 만들어내고 있다. 한편 두 번째 문장에서 두 단어는 띄어쓰기 기준 5개의 토큰만큼 떨어져 위치해 각각 <code class="language-plaintext highlighter-rouge">배움</code>, <code class="language-plaintext highlighter-rouge">깊은</code> 이라는 의미를 만들어 내고 있다. 이처럼 개별 토큰 사이의 위치 관계에 따라서 파생되는 문맥적 정보를 포착하려는 의도로 설계된 기법이 바로 <code class="language-plaintext highlighter-rouge">Relative Position Embedding</code> 이다.</p>

<p><code class="language-plaintext highlighter-rouge">pairwise</code> 하게 <code class="language-plaintext highlighter-rouge">relation</code> 을 포착한다는 점으로 보아 <code class="language-plaintext highlighter-rouge">skip-gram</code>의 <code class="language-plaintext highlighter-rouge">negative sampling</code>과 매우 유사한 느낌의 정보를 포착할 것이라고 예상되며 카테고리 분류상 <strong>3번, <code class="language-plaintext highlighter-rouge">단어 분포 가설</code></strong>에 포함시킬 수 있을 것 같다. (필자의 개인적인 의견이니 이 부분에 대한 다른 의견이 있다면 꼭 댓글에 적어주시면 감사하겠습니당🥰).</p>

<p><code class="language-plaintext highlighter-rouge">Relative Position Embedding</code> 을 실제 어떻게 코드로 구현하는지, 본 논문에서는 위치 관계를 어떻게 정의했는지 <code class="language-plaintext highlighter-rouge">Absolute Position Embedding</code>와 비교를 통해 알아보자. 다음과 같은 두 개의 문장이 있을 때, 개별 위치 임베딩 방식이 문장의 위치 정보를 인코딩하는 과정을 파이썬 코드로 작성해봤다. 함께 살펴보자.</p>

<ul>
  <li><strong><code class="language-plaintext highlighter-rouge">A) I love studying deep learning so much</code></strong></li>
  <li><strong><code class="language-plaintext highlighter-rouge">B) I love deep cheeze burguer so much</code></strong></li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Absolute Position Embedding
</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">max_length</span> <span class="o">=</span> <span class="mi">7</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">position_embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Embedding</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">512</span><span class="p">)</span> <span class="c1"># [max_seq, dim_model]
</span><span class="o">&gt;&gt;&gt;</span> <span class="n">pos_x</span> <span class="o">=</span> <span class="n">position_embedding</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="n">max_length</span><span class="p">))</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">pos_x</span><span class="p">,</span> <span class="n">pos_x</span><span class="p">.</span><span class="n">shape</span>
<span class="p">(</span><span class="n">tensor</span><span class="p">([[</span> <span class="mf">0.4027</span><span class="p">,</span>  <span class="mf">0.9331</span><span class="p">,</span>  <span class="mf">1.0556</span><span class="p">,</span>  <span class="p">...,</span> <span class="o">-</span><span class="mf">1.7370</span><span class="p">,</span>  <span class="mf">0.7799</span><span class="p">,</span>  <span class="mf">1.9851</span><span class="p">],</span>  <span class="c1"># A,B의 0번 토큰: I
</span>         <span class="p">[</span><span class="o">-</span><span class="mf">0.2206</span><span class="p">,</span>  <span class="mf">2.1024</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6055</span><span class="p">,</span>  <span class="p">...,</span> <span class="o">-</span><span class="mf">1.1342</span><span class="p">,</span>  <span class="mf">1.3956</span><span class="p">,</span>  <span class="mf">0.9017</span><span class="p">],</span>  <span class="c1"># A,B의 1번 토큰: love
</span>         <span class="p">[</span><span class="o">-</span><span class="mf">0.9560</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0426</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.8587</span><span class="p">,</span>  <span class="p">...,</span> <span class="o">-</span><span class="mf">0.9406</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1467</span><span class="p">,</span>  <span class="mf">0.1762</span><span class="p">],</span>  <span class="c1"># A,B의 2번 토큰: studying, deep
</span>         <span class="p">...,</span>                                                           <span class="c1"># A,B의 3번 토큰: deep, cheeze
</span>         <span class="p">[</span> <span class="mf">0.5999</span><span class="p">,</span>  <span class="mf">0.5235</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3445</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mf">1.9020</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.5003</span><span class="p">,</span>  <span class="mf">0.7535</span><span class="p">],</span>  <span class="c1"># A,B의 4번 토큰: learning, burger
</span>         <span class="p">[</span> <span class="mf">0.0688</span><span class="p">,</span>  <span class="mf">0.5867</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0340</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mf">0.8547</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.9196</span><span class="p">,</span>  <span class="mf">1.1193</span><span class="p">],</span>  <span class="c1"># A,B의 5번 토큰: so
</span>         <span class="p">[</span><span class="o">-</span><span class="mf">0.0751</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4133</span><span class="p">,</span>  <span class="mf">0.0256</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mf">0.0788</span><span class="p">,</span>  <span class="mf">1.4665</span><span class="p">,</span>  <span class="mf">0.8196</span><span class="p">]],</span> <span class="c1"># A,B의 6번 토큰: much
</span>        <span class="n">grad_fn</span><span class="o">=&lt;</span><span class="n">EmbeddingBackward0</span><span class="o">&gt;</span><span class="p">),</span>
 <span class="n">torch</span><span class="p">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">7</span><span class="p">,</span> <span class="mi">512</span><span class="p">]))</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">Absolute Position Embedding</code>은 주위 문맥에 상관없이 같은 위치의 토큰이라면 같은 포지션 값으로 인코딩하기 때문에 <code class="language-plaintext highlighter-rouge">512</code>개의 원소로 구성된 행벡터들의 인덱스를 실제 문장에서 토큰의 등장 순서에 맵핑해주는 방식으로 위치 정보를 표현한다. 예를 들면, 문장에서 가장 먼저 등장하는 <code class="language-plaintext highlighter-rouge">0</code>번 토큰에 <code class="language-plaintext highlighter-rouge">0</code>번째 <code class="language-plaintext highlighter-rouge">행벡터</code>를 배정하고 가장 마지막에 등장하는 <code class="language-plaintext highlighter-rouge">N-1</code> 번째 토큰은 <code class="language-plaintext highlighter-rouge">N-1</code>번째 <code class="language-plaintext highlighter-rouge">행벡터</code>를 위치 정보값으로 갖는 방식이다. 전체 시퀀스 관점에서 개별 토큰에 번호를 부여하기 때문에 <code class="language-plaintext highlighter-rouge">syntactical</code>한 정보를 모델링 해주기 적합하다는 장점이 있다.</p>

<p><code class="language-plaintext highlighter-rouge">Absolute Position Embedding</code> 은 일반적으로 <code class="language-plaintext highlighter-rouge">Input Embedding</code>과 행렬합 연산을 통해 <code class="language-plaintext highlighter-rouge">Word Embedding</code> 으로 만들어 인코더의 입력으로 사용한다.</p>

<p>아래 코드는 저자가 논문에서 제시한 <code class="language-plaintext highlighter-rouge">DeBERTa</code>의 <code class="language-plaintext highlighter-rouge">Relative Position Embedding</code> 구현을 파이토치로 옮긴 것이다. <code class="language-plaintext highlighter-rouge">Relative Position Embedding</code> 은 절대 위치에 비해 꽤나 복잡한 과정을 거쳐야 하기 때문에 코드 역시 긴 편이다. 하나 하나 천천히 살펴보자.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Relative Position Embedding
</span><span class="o">&gt;&gt;&gt;</span> <span class="n">position_embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Embedding</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">max_length</span><span class="p">,</span> <span class="n">dim_model</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">x</span><span class="p">,</span> <span class="n">p_x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="n">max_length</span><span class="p">,</span> <span class="n">dim_model</span><span class="p">),</span> <span class="n">position_embedding</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">max_length</span><span class="p">))</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">fc_q</span><span class="p">,</span> <span class="n">fc_kr</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">dim_model</span><span class="p">,</span> <span class="n">dim_head</span><span class="p">),</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">dim_model</span><span class="p">,</span> <span class="n">dim_head</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">q</span><span class="p">,</span> <span class="n">kr</span> <span class="o">=</span> <span class="n">fc_q</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">fc_kr</span><span class="p">(</span><span class="n">p_x</span><span class="p">)</span> <span class="c1"># [batch, max_length, dim_head], [batch, 2*max_length, dim_head]
</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">tmp_c2p</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">kr</span><span class="p">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">))</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">tmp_c2p</span><span class="p">,</span> <span class="n">tmp_c2p</span><span class="p">.</span><span class="n">shape</span>
<span class="p">(</span><span class="n">tensor</span><span class="p">([[</span> <span class="mf">2.8118</span><span class="p">,</span>  <span class="mf">0.8449</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6240</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6516</span><span class="p">,</span>  <span class="mf">3.4009</span><span class="p">,</span>  <span class="mf">1.8296</span><span class="p">,</span>  <span class="mf">0.8304</span><span class="p">,</span>  <span class="mf">1.0164</span><span class="p">,</span>
           <span class="mf">3.5664</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.4208</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.0821</span><span class="p">,</span>  <span class="mf">1.5752</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.9469</span><span class="p">,</span> <span class="o">-</span><span class="mf">7.1767</span><span class="p">],</span>
         <span class="p">[</span><span class="o">-</span><span class="mf">2.1907</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.2801</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.0628</span><span class="p">,</span>  <span class="mf">0.4443</span><span class="p">,</span>  <span class="mf">2.2272</span><span class="p">,</span> <span class="o">-</span><span class="mf">5.6653</span><span class="p">,</span> <span class="o">-</span><span class="mf">4.6036</span><span class="p">,</span>  <span class="mf">1.4134</span><span class="p">,</span>
          <span class="o">-</span><span class="mf">1.1742</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3361</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4586</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.1827</span><span class="p">,</span>  <span class="mf">1.0878</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.5657</span><span class="p">],</span>
         <span class="p">[</span><span class="o">-</span><span class="mf">4.8952</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.5330</span><span class="p">,</span>  <span class="mf">0.0251</span><span class="p">,</span>  <span class="mf">3.5001</span><span class="p">,</span>  <span class="mf">4.1619</span><span class="p">,</span>  <span class="mf">1.7408</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5100</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.4616</span><span class="p">,</span>
          <span class="o">-</span><span class="mf">1.6101</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.8741</span><span class="p">,</span>  <span class="mf">1.1404</span><span class="p">,</span>  <span class="mf">4.9860</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.5350</span><span class="p">,</span>  <span class="mf">1.0999</span><span class="p">],</span>
         <span class="p">[</span><span class="o">-</span><span class="mf">3.3437</span><span class="p">,</span>  <span class="mf">4.2276</span><span class="p">,</span>  <span class="mf">0.4509</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.8911</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.1069</span><span class="p">,</span>  <span class="mf">0.9540</span><span class="p">,</span>  <span class="mf">1.2045</span><span class="p">,</span>  <span class="mf">2.2194</span><span class="p">,</span>
          <span class="o">-</span><span class="mf">2.6509</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.4076</span><span class="p">,</span>  <span class="mf">5.1599</span><span class="p">,</span>  <span class="mf">1.6591</span><span class="p">,</span>  <span class="mf">3.8764</span><span class="p">,</span>  <span class="mf">2.5126</span><span class="p">],</span>
         <span class="p">[</span> <span class="mf">0.8164</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.9171</span><span class="p">,</span>  <span class="mf">0.8217</span><span class="p">,</span>  <span class="mf">1.3953</span><span class="p">,</span>  <span class="mf">1.6260</span><span class="p">,</span>  <span class="mf">3.8104</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0303</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.1631</span><span class="p">,</span>
           <span class="mf">3.9008</span><span class="p">,</span>  <span class="mf">0.5856</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.6212</span><span class="p">,</span>  <span class="mf">1.7220</span><span class="p">,</span>  <span class="mf">2.7997</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.8802</span><span class="p">],</span>
         <span class="p">[</span> <span class="mf">3.4473</span><span class="p">,</span>  <span class="mf">0.9721</span><span class="p">,</span>  <span class="mf">3.9137</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.2055</span><span class="p">,</span>  <span class="mf">0.6963</span><span class="p">,</span>  <span class="mf">1.2761</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2266</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.7274</span><span class="p">,</span>
          <span class="o">-</span><span class="mf">1.4928</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.9257</span><span class="p">,</span> <span class="o">-</span><span class="mf">5.4422</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.8544</span><span class="p">,</span>  <span class="mf">1.8749</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.4923</span><span class="p">],</span>
         <span class="p">[</span> <span class="mf">2.6639</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.4392</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.8818</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.4120</span><span class="p">,</span>  <span class="mf">1.7542</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.8774</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.0795</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2156</span><span class="p">,</span>
          <span class="o">-</span><span class="mf">1.0852</span><span class="p">,</span>  <span class="mf">3.7825</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.5581</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.6989</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.6705</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2262</span><span class="p">]],</span>
        <span class="n">grad_fn</span><span class="o">=&lt;</span><span class="n">MmBackward0</span><span class="o">&gt;</span><span class="p">),</span>
 <span class="n">torch</span><span class="p">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">7</span><span class="p">,</span> <span class="mi">14</span><span class="p">]))</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">max_seq</span><span class="p">,</span> <span class="n">max_pos</span> <span class="o">=</span> <span class="mi">7</span><span class="p">,</span> <span class="n">max_seq</span> <span class="o">*</span> <span class="mi">2</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">q_index</span><span class="p">,</span> <span class="n">k_index</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="n">max_seq</span><span class="p">),</span> <span class="n">torch</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="n">max_seq</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">q_index</span><span class="p">,</span> <span class="n">k_index</span>
<span class="p">(</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">]),</span> <span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">]))</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">tmp_pos</span> <span class="o">=</span> <span class="n">q_index</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="o">-</span> <span class="n">k_index</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">rel_pos_matrix</span> <span class="o">=</span> <span class="n">tmp_pos</span> <span class="o">+</span> <span class="n">max_relative_position</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">rel_pos_matrix</span>
<span class="n">tensor</span><span class="p">([[</span> <span class="mi">7</span><span class="p">,</span>  <span class="mi">6</span><span class="p">,</span>  <span class="mi">5</span><span class="p">,</span>  <span class="mi">4</span><span class="p">,</span>  <span class="mi">3</span><span class="p">,</span>  <span class="mi">2</span><span class="p">,</span>  <span class="mi">1</span><span class="p">],</span>
        <span class="p">[</span> <span class="mi">8</span><span class="p">,</span>  <span class="mi">7</span><span class="p">,</span>  <span class="mi">6</span><span class="p">,</span>  <span class="mi">5</span><span class="p">,</span>  <span class="mi">4</span><span class="p">,</span>  <span class="mi">3</span><span class="p">,</span>  <span class="mi">2</span><span class="p">],</span>
        <span class="p">[</span> <span class="mi">9</span><span class="p">,</span>  <span class="mi">8</span><span class="p">,</span>  <span class="mi">7</span><span class="p">,</span>  <span class="mi">6</span><span class="p">,</span>  <span class="mi">5</span><span class="p">,</span>  <span class="mi">4</span><span class="p">,</span>  <span class="mi">3</span><span class="p">],</span>
        <span class="p">[</span><span class="mi">10</span><span class="p">,</span>  <span class="mi">9</span><span class="p">,</span>  <span class="mi">8</span><span class="p">,</span>  <span class="mi">7</span><span class="p">,</span>  <span class="mi">6</span><span class="p">,</span>  <span class="mi">5</span><span class="p">,</span>  <span class="mi">4</span><span class="p">],</span>
        <span class="p">[</span><span class="mi">11</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span>  <span class="mi">9</span><span class="p">,</span>  <span class="mi">8</span><span class="p">,</span>  <span class="mi">7</span><span class="p">,</span>  <span class="mi">6</span><span class="p">,</span>  <span class="mi">5</span><span class="p">],</span>
        <span class="p">[</span><span class="mi">12</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span>  <span class="mi">9</span><span class="p">,</span>  <span class="mi">8</span><span class="p">,</span>  <span class="mi">7</span><span class="p">,</span>  <span class="mi">6</span><span class="p">],</span>
        <span class="p">[</span><span class="mi">13</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span>  <span class="mi">9</span><span class="p">,</span>  <span class="mi">8</span><span class="p">,</span>  <span class="mi">7</span><span class="p">]])</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">rel_pos_matrix</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">rel_pos_matrix</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">max_pos</span> <span class="o">-</span> <span class="mi">1</span><span class="p">).</span><span class="n">repeat</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">tmp_c2p</span> <span class="o">=</span> <span class="n">tmp_c2p</span><span class="p">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">rel_pos_matrix</span><span class="p">,</span> <span class="n">rel_pos_matrix</span><span class="p">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">tmp_c2p</span><span class="p">.</span><span class="n">shape</span> 
<span class="p">(</span><span class="n">tensor</span><span class="p">([[[</span> <span class="mi">7</span><span class="p">,</span>  <span class="mi">6</span><span class="p">,</span>  <span class="mi">5</span><span class="p">,</span>  <span class="mi">4</span><span class="p">,</span>  <span class="mi">3</span><span class="p">,</span>  <span class="mi">2</span><span class="p">,</span>  <span class="mi">1</span><span class="p">],</span>
          <span class="p">[</span> <span class="mi">8</span><span class="p">,</span>  <span class="mi">7</span><span class="p">,</span>  <span class="mi">6</span><span class="p">,</span>  <span class="mi">5</span><span class="p">,</span>  <span class="mi">4</span><span class="p">,</span>  <span class="mi">3</span><span class="p">,</span>  <span class="mi">2</span><span class="p">],</span>
          <span class="p">[</span> <span class="mi">9</span><span class="p">,</span>  <span class="mi">8</span><span class="p">,</span>  <span class="mi">7</span><span class="p">,</span>  <span class="mi">6</span><span class="p">,</span>  <span class="mi">5</span><span class="p">,</span>  <span class="mi">4</span><span class="p">,</span>  <span class="mi">3</span><span class="p">],</span>
          <span class="p">[</span><span class="mi">10</span><span class="p">,</span>  <span class="mi">9</span><span class="p">,</span>  <span class="mi">8</span><span class="p">,</span>  <span class="mi">7</span><span class="p">,</span>  <span class="mi">6</span><span class="p">,</span>  <span class="mi">5</span><span class="p">,</span>  <span class="mi">4</span><span class="p">],</span>
          <span class="p">[</span><span class="mi">11</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span>  <span class="mi">9</span><span class="p">,</span>  <span class="mi">8</span><span class="p">,</span>  <span class="mi">7</span><span class="p">,</span>  <span class="mi">6</span><span class="p">,</span>  <span class="mi">5</span><span class="p">],</span>
          <span class="p">[</span><span class="mi">12</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span>  <span class="mi">9</span><span class="p">,</span>  <span class="mi">8</span><span class="p">,</span>  <span class="mi">7</span><span class="p">,</span>  <span class="mi">6</span><span class="p">],</span>
          <span class="p">[</span><span class="mi">13</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span>  <span class="mi">9</span><span class="p">,</span>  <span class="mi">8</span><span class="p">,</span>  <span class="mi">7</span><span class="p">]],</span>
<span class="n">torch</span><span class="p">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">10</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">14</span><span class="p">]),</span>
<span class="n">torch</span><span class="p">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">10</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">14</span><span class="p">]))</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">outputs</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">gather</span><span class="p">(</span><span class="n">tmp_c2p</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">rel_pos_matrix</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">.</span><span class="n">shape</span>
<span class="p">(</span><span class="n">tensor</span><span class="p">([[[</span> <span class="mf">1.0164</span><span class="p">,</span>  <span class="mf">0.8304</span><span class="p">,</span>  <span class="mf">1.8296</span><span class="p">,</span>  <span class="mf">3.4009</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6516</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6240</span><span class="p">,</span>  <span class="mf">0.8449</span><span class="p">],</span>
          <span class="p">[</span><span class="o">-</span><span class="mf">1.1742</span><span class="p">,</span>  <span class="mf">1.4134</span><span class="p">,</span> <span class="o">-</span><span class="mf">4.6036</span><span class="p">,</span> <span class="o">-</span><span class="mf">5.6653</span><span class="p">,</span>  <span class="mf">2.2272</span><span class="p">,</span>  <span class="mf">0.4443</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.0628</span><span class="p">],</span>
          <span class="p">[</span><span class="o">-</span><span class="mf">1.8741</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.6101</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.4616</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5100</span><span class="p">,</span>  <span class="mf">1.7408</span><span class="p">,</span>  <span class="mf">4.1619</span><span class="p">,</span>  <span class="mf">3.5001</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">5.1599</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.4076</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.6509</span><span class="p">,</span>  <span class="mf">2.2194</span><span class="p">,</span>  <span class="mf">1.2045</span><span class="p">,</span>  <span class="mf">0.9540</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.1069</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">1.7220</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.6212</span><span class="p">,</span>  <span class="mf">0.5856</span><span class="p">,</span>  <span class="mf">3.9008</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.1631</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0303</span><span class="p">,</span>  <span class="mf">3.8104</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">1.8749</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.8544</span><span class="p">,</span> <span class="o">-</span><span class="mf">5.4422</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.9257</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.4928</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.7274</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2266</span><span class="p">],</span>
          <span class="p">[</span><span class="o">-</span><span class="mf">1.2262</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.6705</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.6989</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.5581</span><span class="p">,</span>  <span class="mf">3.7825</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0852</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2156</span><span class="p">]],</span>
          <span class="p">.....</span>
          <span class="p">[[</span> <span class="mf">1.0164</span><span class="p">,</span>  <span class="mf">0.8304</span><span class="p">,</span>  <span class="mf">1.8296</span><span class="p">,</span>  <span class="mf">3.4009</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6516</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6240</span><span class="p">,</span>  <span class="mf">0.8449</span><span class="p">],</span>
          <span class="p">[</span><span class="o">-</span><span class="mf">1.1742</span><span class="p">,</span>  <span class="mf">1.4134</span><span class="p">,</span> <span class="o">-</span><span class="mf">4.6036</span><span class="p">,</span> <span class="o">-</span><span class="mf">5.6653</span><span class="p">,</span>  <span class="mf">2.2272</span><span class="p">,</span>  <span class="mf">0.4443</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.0628</span><span class="p">],</span>
          <span class="p">[</span><span class="o">-</span><span class="mf">1.8741</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.6101</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.4616</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5100</span><span class="p">,</span>  <span class="mf">1.7408</span><span class="p">,</span>  <span class="mf">4.1619</span><span class="p">,</span>  <span class="mf">3.5001</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">5.1599</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.4076</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.6509</span><span class="p">,</span>  <span class="mf">2.2194</span><span class="p">,</span>  <span class="mf">1.2045</span><span class="p">,</span>  <span class="mf">0.9540</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.1069</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">1.7220</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.6212</span><span class="p">,</span>  <span class="mf">0.5856</span><span class="p">,</span>  <span class="mf">3.9008</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.1631</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0303</span><span class="p">,</span>  <span class="mf">3.8104</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">1.8749</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.8544</span><span class="p">,</span> <span class="o">-</span><span class="mf">5.4422</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.9257</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.4928</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.7274</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2266</span><span class="p">],</span>
          <span class="p">[</span><span class="o">-</span><span class="mf">1.2262</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.6705</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.6989</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.5581</span><span class="p">,</span>  <span class="mf">3.7825</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0852</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2156</span><span class="p">]]],</span>
        <span class="n">grad_fn</span><span class="o">=&lt;</span><span class="n">GatherBackward0</span><span class="o">&gt;</span><span class="p">),</span>
 <span class="n">torch</span><span class="p">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">10</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">7</span><span class="p">]))</span>
</code></pre></div></div>

<p>일단 절대 위치와 동일하게 <code class="language-plaintext highlighter-rouge">nn.Embedding</code>을 사용해 임베딩 룩업 테이블(레이어)를 정의하지만, 입력 차원이 다르다. 절대 위치 임베딩은 <code class="language-plaintext highlighter-rouge">forward</code>하게 위치값을 맵핑해야 하는 반면에 상대 위치 임베딩 방식은 <code class="language-plaintext highlighter-rouge">Bi-Directional</code>한 맵핑을 해야 해서, 기존 <code class="language-plaintext highlighter-rouge">max_length</code> 값의 두 배를 입력 차원(<code class="language-plaintext highlighter-rouge">max_pos</code>)으로 사용했다. 예를 들어 <code class="language-plaintext highlighter-rouge">0</code>번 토큰과 나머지 토큰 사이의 위치 관계를 표현해야 하는 상황이다. 그렇다면 우리는 <code class="language-plaintext highlighter-rouge">0</code>번 토큰과 나머지 토큰과의 위치 관계를 <code class="language-plaintext highlighter-rouge">[0, -1, -2, -3, -4, -5, -6]</code> 으로 인코딩할 수 있다.</p>

<p>반대로 마지막 <code class="language-plaintext highlighter-rouge">6</code>번 토큰과 나머지 토큰 사이의 위치 관계를 표현하는 경우라면 어떻게 될까?? <code class="language-plaintext highlighter-rouge">[6, 5, 4, 3, 2, 1, 0]</code> 으로 인코딩 될 것이다. 다시 말해, 위치 임베딩 원소 값은 <code class="language-plaintext highlighter-rouge">[-max_seq:max_seq]</code> 사이에서 정의된다는 것이다. 그러나 원소값의 범위를 그대로 사용할 수는 없다. 이유는 파이썬의 리스트, 텐서 같은 배열형 자료구조는 음이 아닌 정수를 인덱스로 활용해야 <code class="language-plaintext highlighter-rouge">forward</code> 하게 원소에 접근할 수 있기 때문이다. 일반적으로 배열 형태의 자료형은 모두 인덱스 <code class="language-plaintext highlighter-rouge">0</code>부터 <code class="language-plaintext highlighter-rouge">N-1</code>까지 순차적으로 맵핑된다. 그래서 의도한대로 토큰에 접근하려면 역시 토큰의 인덱스를 <code class="language-plaintext highlighter-rouge">forward</code> 한 형태로 만들어줘야 한다.</p>

<p>따라서 기존 <code class="language-plaintext highlighter-rouge">[-max_seq:max_seq]</code> 에  <code class="language-plaintext highlighter-rouge">max_seq</code>를 더해준 <code class="language-plaintext highlighter-rouge">[0:2*max_seq]</code> (<code class="language-plaintext highlighter-rouge">2 * max_seq</code>)을 원소 값의 범위로 사용하게 된다. 여기까지가 통상적으로 말하는 <code class="language-plaintext highlighter-rouge">Relative Position Embedding</code> 에 해당한다. 위 코드상으로는 <code class="language-plaintext highlighter-rouge">rel_pos_matrix</code> 를 만든 부분에 해당한다.</p>

\[∂(i,j)=
\begin{cases}
\ 0 &amp; {(i - j ≤ k)} \\ 
\ 2k-1 &amp; {(i - j ≥ k)} \\
\ i - j + k &amp; {(others)} \\
\end{cases}\]

<p>이제부터 저자가 주장하는 위치 관계 표현 방식에 대해 알아보자. 일반적인 <code class="language-plaintext highlighter-rouge">Relative Position Embedding</code>과 거의 유사하지만, <code class="language-plaintext highlighter-rouge">rel_pos_matrix</code> 내부 원소 값이 음수가 되거나 <code class="language-plaintext highlighter-rouge">max_pos</code> 을 초과하는 경우를 처리 해주기 위해 후처리 과정을 도입해 사용했다. 예외 상황은 <strong><code class="language-plaintext highlighter-rouge">max_seq &gt; 1/2 * max_pos(==k)</code></strong> 일 때 발생한다. <code class="language-plaintext highlighter-rouge">official repo</code> 의 코드를 보면 <code class="language-plaintext highlighter-rouge">max_seq</code>와 <code class="language-plaintext highlighter-rouge">k</code>를 일치시켜 모델링 하기 때문에 파인튜닝 하는 상황이라면 이것을 몰라도 상관없겠지만, 하나 하나 모델을 직접 만드는 입장이라면 예외 상황을 반드시 기억하자.</p>

<p>한편, 이러한 인코딩 방식은 <code class="language-plaintext highlighter-rouge">word2vec</code> 의 <code class="language-plaintext highlighter-rouge">window size</code> 도입과 비슷한 원리(<code class="language-plaintext highlighter-rouge">의미는 주변 문맥에 의해 결정</code>)라고 생각하면 되는데, 윈도우 사이즈 범위에서 벗어난 토큰들은 주변 문맥으로 인정 하지 않겠다는(<code class="language-plaintext highlighter-rouge">negative sample</code>) 의도를 갖고 있다. 실제 구현은 텐서 내부 원소값의 범위를 사용자 지정 범위로 제한할 수 있는 <code class="language-plaintext highlighter-rouge">torch.clamp</code> 를 사용하면 <code class="language-plaintext highlighter-rouge">1</code>줄로 깔끔하게 만들 수 있으니 참고하자.</p>

<p><code class="language-plaintext highlighter-rouge">torch.clamp</code> 까지 적용하고 난 최종 결과를 살펴보자. 행백터, 열벡터 모두 <code class="language-plaintext highlighter-rouge">[0:2*max_seq]</code> 사이에서 정의되고 있으며, 개별 방향 벡터 원소의 최대값과 최소값의 차이가 항상 <code class="language-plaintext highlighter-rouge">k</code> 로 유지 된다. 의도대로 정확히 윈도우 사이즈만큼의 주변 맥락을 반영해 임베딩을 형성하고 있음을 알 수 있다.</p>

<p>정리하면, <code class="language-plaintext highlighter-rouge">Relative Position Embedding</code> 란 절대 위치 방식처럼 임베딩 룩업 테이블을 만들되, 사용자가 지정한 윈도우 사이즈에 해당하는 토큰의 임베딩 값만 추출해 새로운 행벡터를 여러 개 만들어 내는 기법이라고 할 수 있다. <strong>이 때 행벡터는 대상 토큰과 그 나머지 토큰 사이의 위치 변화에 따라 발생하는 파생적인 맥락 정보를 담고 있다.</strong></p>

<h6 id="-deberta-inductive-bias"><code class="language-plaintext highlighter-rouge">🤔 DeBERTa Inductive Bias</code></h6>

<p><strong>결국</strong> <code class="language-plaintext highlighter-rouge">DeBERTa</code><strong>는 두가지 위치 정보 포착 방식을 적절히 섞어서 모델이 더욱 풍부한 임베딩을 갖도록 하려는 의도로 설계 되었다.</strong> 또한 우리는 이미 모델이 다양한 맥락 정보를 포착할수록 <code class="language-plaintext highlighter-rouge">NLU Task</code> 에서 더 나은 성능을 기록한다는 사실을 <code class="language-plaintext highlighter-rouge">BERT</code>와 <code class="language-plaintext highlighter-rouge">GPT</code> 사례에서 알 수 있었다. 따라서 <code class="language-plaintext highlighter-rouge">Relative Position Embedding</code> 을 추가하여 <code class="language-plaintext highlighter-rouge">단어의 발생 순서</code> 를 포착하는 모델에 <code class="language-plaintext highlighter-rouge">단어 분포 가설</code> 적인 특징을 더해주려는 저자의 아이디어는 매우 타당하다고 볼 수 있겠다.</p>

<p>이제 관건은 <strong><code class="language-plaintext highlighter-rouge">“두가지 위치 정보를 어떤 방식으로 추출하고 섞어줄 것인가”</code></strong> 하는 물음에 답하는 것이다. 저자는 물음에 답하기 위해 <code class="language-plaintext highlighter-rouge">Disentangled Self-Attention</code> 과  <code class="language-plaintext highlighter-rouge">Enhanced Mask Decoder</code> 라는 새로운 기법 두가지를 제시한다. 전자는 <code class="language-plaintext highlighter-rouge">단어 분포 가설</code> 에 해당되는 맥락 정보를 추출하기 위한 기법이고, 후자는 <code class="language-plaintext highlighter-rouge">단어 발생 순서</code> 에 포함되는 임베딩을 모델에 주입하기 위해 설계되었다. 모델링 파트에서는 두가지 새로운 기법에 대해서 자세히 살펴본 뒤에 실제 모델을 코드로 빌드하는 과정을 설명하려 한다.</p>

<h3 id="modeling"><code class="language-plaintext highlighter-rouge">🌟 Modeling</code></h3>


        
      </section>

      <footer class="page__meta">
        
        
  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      <a href="/tags/#bert" class="page__taxonomy-item p-category" rel="tag">BERT</a><span class="sep">, </span>
    
      <a href="/tags/#deberta" class="page__taxonomy-item p-category" rel="tag">DeBERTa</a><span class="sep">, </span>
    
      <a href="/tags/#disentangled-attention" class="page__taxonomy-item p-category" rel="tag">Disentangled-Attention</a><span class="sep">, </span>
    
      <a href="/tags/#emd" class="page__taxonomy-item p-category" rel="tag">EMD</a><span class="sep">, </span>
    
      <a href="/tags/#encoder" class="page__taxonomy-item p-category" rel="tag">Encoder</a><span class="sep">, </span>
    
      <a href="/tags/#natural-language-process" class="page__taxonomy-item p-category" rel="tag">Natural Language Process</a><span class="sep">, </span>
    
      <a href="/tags/#relative-position-embedding" class="page__taxonomy-item p-category" rel="tag">Relative Position Embedding</a><span class="sep">, </span>
    
      <a href="/tags/#self-attention" class="page__taxonomy-item p-category" rel="tag">Self-Attention</a><span class="sep">, </span>
    
      <a href="/tags/#transformer" class="page__taxonomy-item p-category" rel="tag">Transformer</a>
    
    </span>
  </p>




  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> Categories: </strong>
    <span itemprop="keywords">
    
      <a href="/categories/#nlp" class="page__taxonomy-item p-category" rel="tag">NLP</a>
    
    </span>
  </p>


        

  <p class="page__date"><strong><i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> Updated:</strong> <time class="dt-published" datetime="2023-08-04">August 4, 2023</time></p>

      </footer>

      <section class="page__share">
  
    <h4 class="page__share-title">Share on</h4>
  

  <a href="https://twitter.com/intent/tweet?text=%F0%9F%AA%A2%C2%A0%5BDeBERTa%5D+DeBERTa%3A+Decoding-Enhanced+BERT+with+Disentangled-Attention%20http%3A%2F%2Flocalhost%3A4000%2Fnlp%2Fdeberta" class="btn btn--twitter" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Twitter"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=http%3A%2F%2Flocalhost%3A4000%2Fnlp%2Fdeberta" class="btn btn--facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Facebook"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=http%3A%2F%2Flocalhost%3A4000%2Fnlp%2Fdeberta" class="btn btn--linkedin" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on LinkedIn"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>


      
  <nav class="pagination">
    
      <a href="/framework-library/torch-indexing-function" class="pagination--pager" title="🔥 Pytorch Tensor Indexing 자주 사용하는 메서드 모음집
">Previous</a>
    
    
      <a href="/nlp/transformer" class="pagination--pager" title="🤖 [Transformer] Attention Is All You Need
">Next</a>
    
  </nav>

    </div>

    
  </article>

  
  
    <div class="page__related">
      <h2 class="page__related-title">You may also enjoy</h2>
      <div class="grid__wrapper">
        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
      <div class="archive__item-teaser">
        <img src="/assets/images/huggingface_emoji.png" alt="">
      </div>
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/nlp/transformer" rel="permalink">🤖 [Transformer] Attention Is All You Need
</a>
      
    </h2>
    <!-- 

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          35 minute read
        
      </span>
    
  </p>

 -->
    
      <p class="page__meta"><i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i> August 04 2023</p>
    
    <p class="archive__item-excerpt" itemprop="description">Transformer Official Paper Review with Pytorch Implementation
</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
      <div class="archive__item-teaser">
        <img src="/assets/images/huggingface_emoji.png" alt="">
      </div>
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/framework-library/torch-indexing-function" rel="permalink">🔥 Pytorch Tensor Indexing 자주 사용하는 메서드 모음집
</a>
      
    </h2>
    <!-- 

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          9 minute read
        
      </span>
    
  </p>

 -->
    
      <p class="page__meta"><i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i> August 04 2023</p>
    
    <p class="archive__item-excerpt" itemprop="description">파이토치에서 자주 사용하는 텐서 인덱싱 관련 메서드 모음
</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
      <div class="archive__item-teaser">
        <img src="/assets/images/huggingface_emoji.png" alt="">
      </div>
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/calculus/gradient" rel="permalink">📈 Gradient: Directional Derivative
</a>
      
    </h2>
    <!-- 

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          2 minute read
        
      </span>
    
  </p>

 -->
    
      <p class="page__meta"><i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i> July 31 2023</p>
    
    <p class="archive__item-excerpt" itemprop="description">Proof of gradient direction with Total Derivative
</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
      <div class="archive__item-teaser">
        <img src="/assets/images/huggingface_emoji.png" alt="">
      </div>
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/cv/vit" rel="permalink">🌆 [ViT] An Image Is Worth 16x16 Words: Transformers For Image Recognition At Scale
</a>
      
    </h2>
    <!-- 

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          24 minute read
        
      </span>
    
  </p>

 -->
    
      <p class="page__meta"><i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i> July 26 2023</p>
    
    <p class="archive__item-excerpt" itemprop="description">ViT Official Paper Review with Pytorch Implementation
</p>
  </article>
</div>

        
      </div>
    </div>
  
  
</div>

    </div>

    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>Follow:</strong></li>
    

    
      
        
      
        
      
        
      
        
      
        
      
        
      
    

    
      <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
    
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2023 qcqced. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>







    <script>
  'use strict';

  (function() {
    var commentContainer = document.querySelector('#utterances-comments');

    if (!commentContainer) {
      return;
    }

    var script = document.createElement('script');
    script.setAttribute('src', 'https://utteranc.es/client.js');
    script.setAttribute('repo', 'qcqced123/qcqced123.github.io');
    script.setAttribute('issue-term', 'pathname');
    
    script.setAttribute('theme', 'github-light');
    script.setAttribute('crossorigin', 'anonymous');

    commentContainer.appendChild(script);
  })();
</script>

  




<script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML">
</script>

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"] ],
    displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
    processEscapes: true
    },
    "HTML-CSS": { availableFonts: ["TeX"] }
});
</script>

  </body>
</html>
