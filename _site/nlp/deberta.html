<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.24.0 by Michael Rose
  Copyright 2013-2020 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->
<html lang="en" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>🪢 [DeBERTa] DeBERTa: Decoding-Enhanced BERT with Disentangled-Attention - AI/Business Study Log</title>
<meta name="description" content="Transformer Official Paper Review with Pytorch Implementation">


  <meta name="author" content="qcqced">
  
  <meta property="article:author" content="qcqced">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="AI/Business Study Log">
<meta property="og:title" content="🪢 [DeBERTa] DeBERTa: Decoding-Enhanced BERT with Disentangled-Attention">
<meta property="og:url" content="http://localhost:4000/nlp/deberta">


  <meta property="og:description" content="Transformer Official Paper Review with Pytorch Implementation">







  <meta property="article:published_time" content="2023-08-04T00:00:00+09:00">



  <meta property="article:modified_time" content="2023-08-05T02:00:00+09:00">



  

  


<link rel="canonical" href="http://localhost:4000/nlp/deberta">




<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    
      "@type": "Person",
      "name": "qcqced",
      "url": "http://localhost:4000/"
    
  }
</script>







<!-- end _includes/seo.html -->



  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="AI/Business Study Log Feed">


<!-- https://t.co/dKP3o1e -->
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css"></noscript>



<!-- Latex -->
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->
<link rel="apple-touch-icon" sizes="180x180" href="/assets/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/assets/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/assets/favicon-16x16.png">
<link rel="manifest" href="/assets/site.webmanifest">
<link rel="mask-icon" href="/assets/safari-pinned-tab.svg" color="#5bbad5">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="theme-color" content="#ffffff">
<!-- end custom head snippets -->

    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      TeX: {
        equationNumbers: {
          autoNumber: "AMS"
        }
      },
      tex2jax: {
      inlineMath: [ ['$', '$'] ],
      displayMath: [ ['$$', '$$'] ],
      processEscapes: true
    }
  });
  MathJax.Hub.Register.MessageHook("Math Processing Error",function (message) {
      alert("Math Processing Error: "+message[1]);
    });
  MathJax.Hub.Register.MessageHook("TeX Jax - parse error",function (message) {
      alert("Math Processing Error: "+message[1]);
    });
  </script>
  <script type="text/javascript" async
    src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>
  </head>

  <body class="layout--single">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
          AI/Business Study Log
          <span class="site-subtitle">NLP, Marketing</span>
        </a>
        
        
        <ul class="visible-links">
              
              
                  <li class="masthead__menu-item">
                      <a href="https://qcqced123.github.io/">Home</a>
                  </li>
              
          
              
              
                  <li class="dropdown ">
                      <a class="dropdown-toggle" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">CS/AI  <i class="fa fa-caret-down fa-sm" aria-hidden="true"></i><span class="caret"></span></a>
       
                      <ul class="dropdown-content" >
                          
                              <a class = "dropdown-item" href="/nlp/">    Natural Language Process</a>
                          
                              <a class = "dropdown-item" href="/multi-modal/">    Multi Modal</a>
                          
                              <a class = "dropdown-item" href="/cv/">    Computer Vision</a>
                          
                              <a class = "dropdown-item" href="/ml/">    Machine Learning</a>
                          
                              <a class = "dropdown-item" href="/framework-library/">    Framework & Library</a>
                          
                              <a class = "dropdown-item" href="/python/">    Python</a>
                          
                              <a class = "dropdown-item" href="/algorithm/">    Data Structure & Algorithm</a>
                          
                              <a class = "dropdown-item" href="/ps/">    Problem Solving</a>
                          
                      </ul>
                  </li>
              
          
              
              
                  <li class="dropdown ">
                      <a class="dropdown-toggle" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Math  <i class="fa fa-caret-down fa-sm" aria-hidden="true"></i><span class="caret"></span></a>
       
                      <ul class="dropdown-content" >
                          
                              <a class = "dropdown-item" href="/linear-algebra/">    Linear Algebra</a>
                          
                              <a class = "dropdown-item" href="/optimization-theory/">    Optimization Theory/Calculus</a>
                          
                              <a class = "dropdown-item" href="/signal-system/">    Signal & System</a>
                          
                      </ul>
                  </li>
              
          
              
              
                  <li class="dropdown ">
                      <a class="dropdown-toggle" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Business/Marketing  <i class="fa fa-caret-down fa-sm" aria-hidden="true"></i><span class="caret"></span></a>
       
                      <ul class="dropdown-content" >
                          
                              <a class = "dropdown-item" href="/device/">    Device</a>
                          
                              <a class = "dropdown-item" href="/semi-conductor/">    Semi-Conductor</a>
                          
                              <a class = "dropdown-item" href="/ai/">    AI</a>
                          
                      </ul>
                  </li>
              
          
              
              
                  <li class="masthead__menu-item">
                      <a href="/categories/">Category</a>
                  </li>
              
          
              
              
                  <li class="masthead__menu-item">
                      <a href="/about/">About</a>
                  </li>
              
          
       </ul>
       
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      





<div id="main" role="main">
  
  <div class="sidebar sticky">
  


<div itemscope itemtype="https://schema.org/Person" class="h-card">

  
    <div class="author__avatar">
      <a href="http://localhost:4000/">
        <img src="/assets/images/huggingface_emoji.png" alt="qcqced" itemprop="image" class="u-photo">
      </a>
    </div>
  

  <div class="author__content">
    <h3 class="author__name p-name" itemprop="name">
      <a class="u-url" rel="me" href="http://localhost:4000/" itemprop="url">qcqced</a>
    </h3>
    
      <div class="author__bio p-note" itemprop="description">
        <p>Interested in NLP, Marketing</p>

      </div>
    
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">Follow</button>
    <ul class="author__urls social-icons">
      
        <li itemprop="homeLocation" itemscope itemtype="https://schema.org/Place">
          <i class="fas fa-fw fa-map-marker-alt" aria-hidden="true"></i> <span itemprop="name" class="p-locality">Seoul, South Korea</span>
        </li>
      

      
        
          
            <li><a href="https://qcqced123.github.io" rel="nofollow noopener noreferrer me" itemprop="sameAs"><i class="fas fa-fw fa-link" aria-hidden="true"></i><span class="label">Website</span></a></li>
          
        
          
            <li><a href="https://github.com/qcqced123" rel="nofollow noopener noreferrer me" itemprop="sameAs"><i class="fab fa-fw fa-github" aria-hidden="true"></i><span class="label">GitHub</span></a></li>
          
        
          
            <li><a href="https://www.kaggle.com/qcqced" rel="nofollow noopener noreferrer me" itemprop="sameAs"><i class="fab fa-fw fa-kaggle" aria-hidden="true"></i><span class="label">Kaggle</span></a></li>
          
        
      

      

      
        <li>
          <a href="mailto:qcqced123@gmail.com" rel="me" class="u-email">
            <meta itemprop="email" content="qcqced123@gmail.com" />
            <i class="fas fa-fw fa-envelope-square" aria-hidden="true"></i><span class="label">Email</span>
          </a>
        </li>
      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      <!--
  <li>
    <a href="http://link-to-whatever-social-network.com/user/" itemprop="sameAs" rel="nofollow noopener noreferrer me">
      <i class="fas fa-fw" aria-hidden="true"></i> Custom Social Profile Link
    </a>
  </li>
-->
    </ul>
  </div>
</div>

  
  </div>



  <article class="page h-entry" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="🪢 [DeBERTa] DeBERTa: Decoding-Enhanced BERT with Disentangled-Attention">
    <meta itemprop="description" content="Transformer Official Paper Review with Pytorch Implementation">
    <meta itemprop="datePublished" content="2023-08-04T00:00:00+09:00">
    <meta itemprop="dateModified" content="2023-08-05T02:00:00+09:00">

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title p-name" itemprop="headline">
            <a href="http://localhost:4000/nlp/deberta" class="u-url" itemprop="url">🪢 [DeBERTa] DeBERTa: Decoding-Enhanced BERT with Disentangled-Attention
</a>
          </h1>
          <p class="page__date">
            <a href="https://hits.seeyoufarm.com/localhost:4000/nlp/deberta"target="_blank">
              <img src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https://localhost:4000/nlp/deberta&count_bg=%23399DE2&title_bg=%236D6D6D&icon=pytorch.svg&icon_color=%23E7E7E7&title=Views&edge_flat=false"/>
            </a>
            <i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> Updated: <time datetime="2023-08-04T00:00:00+09:00">August 04, 2023</time>
            <!-- <div style="text-align: left;"> -->
            <!-- </div> -->
          </p>
          
          
        </header>
      

      <section class="page__content e-content" itemprop="text">
        
          <aside class="sidebar__right sticky">
            <nav class="toc">
              <header><h4 class="nav__title"><i class="fas fa-file-alt"></i> On this page</h4></header>
              <ul class="toc__menu"><li><a href="#overview">🔭 Overview</a></li><li><a href="#inducitve-bias-in-deberta">🪢 Inducitve Bias in DeBERTa</a><ul><li><a href="#-types-of-embedding">📚 Types of Embedding</a></li><li><a href="#-relative-position-embedding">🔢 Relative Position Embedding</a></li><li><a href="#-word-context-vs-relative-position-vs-absolute-position">🤔 Word Context vs Relative Position vs Absolute Position</a></li><li><a href="#-deberta-inductive-bias">🤔 DeBERTa Inductive Bias</a></li></ul></li><li><a href="#modeling">🌟 Modeling</a><ul><li><a href="#disentangled-self-attention">🪢 Disentangled Self-Attention</a></li><li><a href="#enhanced-mask-decoder">😷 Enhanced Mask Decoder</a></li><li><a href="#multi-head-attention">👩‍👩‍👧‍👦 Multi-Head Attention</a></li><li><a href="#feed-forward-network">🔬 Feed Forward Network</a></li><li><a href="#debertaencoderlayer">📘 DeBERTaEncoderLayer</a></li><li><a href="#debertaencoder">📚 DeBERTaEncoder</a></li><li><a href="#deberta">🤖 DeBERTa</a></li></ul></li></ul>

            </nav>
          </aside>
        
        <h3 id="overview"><code class="language-plaintext highlighter-rouge">🔭 Overview</code></h3>

<p><code class="language-plaintext highlighter-rouge">DeBERTa</code>는 2020년 <code class="language-plaintext highlighter-rouge">Microsoft</code>가 <code class="language-plaintext highlighter-rouge">ICLR</code>에서 발표한 자연어 처리용 신경망 모델이다. <code class="language-plaintext highlighter-rouge">Disentangled Self-Attention</code>, <code class="language-plaintext highlighter-rouge">Enhanced Mask Decoder</code>라는 두가지 새로운 테크닉을 <code class="language-plaintext highlighter-rouge">BERT</code>, <code class="language-plaintext highlighter-rouge">RoBERTa</code>에 적용해 당시 <code class="language-plaintext highlighter-rouge">SOTA</code>를 달성했으며, 특히 영어처럼 문장에서 자리하는 위치에 따라 단어의 의미, 형태가 결정되는 굴절어 계열에 대한 성능이 좋아 꾸준히 사랑받고 있는 모델이다. 또한 인코딩 가능한 최대 시퀀스 길이가 <code class="language-plaintext highlighter-rouge">4096</code>으로 매우 긴 편 (<code class="language-plaintext highlighter-rouge">DeBERTa-V3-Large</code>) 에 속해, <code class="language-plaintext highlighter-rouge">Kaggle Competition</code>에서 자주 활용된다. 출시된지 2년이 넘도록 <code class="language-plaintext highlighter-rouge">SuperGLUE</code> 대시보드에서 꾸준히 상위권을 유지하고 있다는 점도 <code class="language-plaintext highlighter-rouge">DeBERTa</code>가 얼마나 잘 설계된 모델인지 알 수 있는 대목이다.</p>

<p>한편, <code class="language-plaintext highlighter-rouge">DeBERTa</code>의 설계 철학은 <code class="language-plaintext highlighter-rouge">Inductive Bias</code> 다. 간단하게 <code class="language-plaintext highlighter-rouge">Inductive Bias</code>란, 주어진 데이터로부터 일반화 성능을 높이기 위해 <code class="language-plaintext highlighter-rouge">"입력되는 데이터는 ~ 할 것이다"</code>, <code class="language-plaintext highlighter-rouge">"이런 특징을 갖고 있을 것이다"</code>와 같은 가정, 가중치, 가설 등을 기계학습 알고리즘에 적용하는 것을 말한다. <strong><a href="https://qcqced123.github.io/cv/vit"><code class="language-plaintext highlighter-rouge">ViT</code> 논문 리뷰</a></strong>에서도 밝혔듯, 퓨어한 <code class="language-plaintext highlighter-rouge">Self-Attention</code> 의 <code class="language-plaintext highlighter-rouge">Inductive Bias</code> 는 사실상 없으며, 전체 <code class="language-plaintext highlighter-rouge">Transformer</code> 구조 레벨에서 봐도 <code class="language-plaintext highlighter-rouge">Absolute Position Embedding</code>을 사용해 토큰의 위치 정보를 모델에 주입해주는 것이 그나마 약한 <code class="language-plaintext highlighter-rouge">Iniductive Bias</code>라고 볼 수 있다. 다른 포스팅에서는 분명 <code class="language-plaintext highlighter-rouge">Inductive Bias</code> 가 적기 때문에 자연어 처리에서 <code class="language-plaintext highlighter-rouge">Transformer</code> 가 성공을 거둘 수 있다고 해놓고 이게 지금 와서 말을 뒤집는다고 생각할 수 있다. 하지만 <code class="language-plaintext highlighter-rouge">Self-Attention</code>과 <code class="language-plaintext highlighter-rouge">Absolute Position Embedding</code>의 의미를 다시 한 번 상기해보면, <code class="language-plaintext highlighter-rouge">Inductive Bias</code> 추가를 주장하는 저자들의 생각이 꽤나 합리적이었음을 알 수 있게 된다. 구체적인 모델 구조를 파악하기 전에 먼저 <code class="language-plaintext highlighter-rouge">Inductive Bias</code> 추가가 왜 필요하며, 어떠한 가정이 필요한지 알아보자.</p>

<h3 id="inducitve-bias-in-deberta"><code class="language-plaintext highlighter-rouge">🪢 Inducitve Bias in DeBERTa</code></h3>
<ul>
  <li><strong><code class="language-plaintext highlighter-rouge">Absolute Position + Relative Position</code>을 모두 활용해 풍부하고 깊은 임베딩 추출</strong></li>
  <li><strong><code class="language-plaintext highlighter-rouge">단어의 발생 순서</code> 임베딩과 <code class="language-plaintext highlighter-rouge">단어 분포 가설</code> 임베딩을 모두 추출하는 것을 목적으로 설계</strong></li>
</ul>

<p>본 논문 초록에는 다음과 같은 문장이 서술되어 있다.</p>

<p><code class="language-plaintext highlighter-rouge">motivated by the observation that the attention weight of a word pair depends on not only their contents but their relative positions. For example, the dependency between the words “deep” and “learning” is much stronger when they occur next to each other than when they occur in different sentences.</code></p>

<p>위의 두 문장이 <code class="language-plaintext highlighter-rouge">DeBERTa</code>의 <code class="language-plaintext highlighter-rouge">Inducitve Bias</code> 를 가장 잘 설명하고 있다고 생각한다. 저자가 추가를 주장하는 <code class="language-plaintext highlighter-rouge">Inductive Bias</code>란, <code class="language-plaintext highlighter-rouge">relative position</code> 정보라는 것과 기존 모델링으로는 <code class="language-plaintext highlighter-rouge">relative position</code>이 주는 문맥 정보 포착이 불가능하다는 사실을 알 수 있다.</p>

<p>그렇다면 <code class="language-plaintext highlighter-rouge">relative position</code> 가 제공하는 문맥 정보가 도대체 뭐길래 기존의 방식으로는 포착이 불가능하다는 것일까?? 자연어에서 포착 가능한 문맥들의 종류와 기존의 모델링 방식에 대한 정리부터 해보자. 여기서 말하는 기존 방식이란, 퓨어한 <code class="language-plaintext highlighter-rouge">Self-Attention</code>과 <code class="language-plaintext highlighter-rouge">Absolute Position Embedding</code> 을 사용하는 <code class="language-plaintext highlighter-rouge">Transformer-Encoder-Base</code>  모델(<code class="language-plaintext highlighter-rouge">BERT</code>, <code class="language-plaintext highlighter-rouge">RoBERTa</code>)을 뜻한다. 이번 포스팅에서는 <code class="language-plaintext highlighter-rouge">BERT</code>를 기준으로 설명하겠다.</p>

<h4 id="-types-of-embedding"><strong><code class="language-plaintext highlighter-rouge">📚 Types of Embedding</code></strong></h4>
<p><strong>먼저 현존하는 모든 임베딩(<code class="language-plaintext highlighter-rouge">벡터에 문맥을 주입하는</code>)기법들을 정리해보자. 다음과 같이 3가지 카테고리로 분류가 가능하다.</strong></p>

<ul>
  <li><strong>1) 단어의 빈도수:  시퀀스에서 사용된 토큰들의 빈도수를 측정(<code class="language-plaintext highlighter-rouge">Bag of words</code>)</strong></li>
  <li><strong>2) 단어의 발생 순서: <code class="language-plaintext highlighter-rouge">corpus</code> 내부의 특정 <code class="language-plaintext highlighter-rouge">sequence</code> 등장 빈도를 카운트(<code class="language-plaintext highlighter-rouge">N-Gram</code>), 주어진 시퀀스를 가지고 다음 시점에 등장할 토큰을 맞추는 방식(<code class="language-plaintext highlighter-rouge">LM</code>)</strong></li>
  <li><strong>3) 단어 분포 가설 :  단어의 의미는 주변 문맥에 의해 결정된다는 가정, 어떤 단어 쌍이 자주 같이 등장하는지 카운트해 <code class="language-plaintext highlighter-rouge">PMI</code>를 측정하는 방식(<code class="language-plaintext highlighter-rouge">Word2Vec</code>)</strong></li>
</ul>

<p>기존의 모델링 방식은 어디에 포함될까?? <code class="language-plaintext highlighter-rouge">BERT</code> 는 대분류 상 신경망에 포함되고, <code class="language-plaintext highlighter-rouge">Language Modeling</code>을 통해 시퀀스를 학습한다는 점 그리고 <code class="language-plaintext highlighter-rouge">Self-Attention</code>과 <code class="language-plaintext highlighter-rouge">Absolute Position Embedding</code> 을 사용한다는 점에서 2번, <code class="language-plaintext highlighter-rouge">단어의 발생 순서</code> 에 포함된다고 볼 수 있다. <code class="language-plaintext highlighter-rouge">Absolute Position Embedding</code> 과 <code class="language-plaintext highlighter-rouge">Self-Attention</code>의 사용이 퓨어한 <code class="language-plaintext highlighter-rouge">BERT</code>가 분류상 2번이라는 사실을 뒷받침하는 증거라는 점에서 의아할 수 있다. 하지만 잘 생각해보자.</p>

<p><code class="language-plaintext highlighter-rouge">Absolute Position Embedding</code>은 주어진 시퀀스의 길이를 측정한 뒤, 나열된 순서 그대로 <code class="language-plaintext highlighter-rouge">forward</code>하게 <code class="language-plaintext highlighter-rouge">0</code>부터 <code class="language-plaintext highlighter-rouge">길이-1</code>의 번호를 개별 토큰에 할당한다. 다시 말해, 단어가 시퀀스에서 발생한 순서를 수학적으로 표현해 모델에 주입한다는 의미가 된다. <code class="language-plaintext highlighter-rouge">Self-Attention</code>은 <code class="language-plaintext highlighter-rouge">Absolute Position Embedding</code> 정보가 주입된 시퀀스 전체를 한 번에 병렬 처리한다. 따라서 충분히 <code class="language-plaintext highlighter-rouge">BERT</code> 같은 <code class="language-plaintext highlighter-rouge">Self-Attention</code>, <code class="language-plaintext highlighter-rouge">Absolute Position Embedding</code> 기반 모델을 2번에 분류할 수 있겠다.</p>

<p>한편, 혹자는 <code class="language-plaintext highlighter-rouge">"BERT는 MLM 을 사용하는데 Language Modeling을 한다고 하는게 맞나요"</code>라고 말할 수 있다. 하지만 <code class="language-plaintext highlighter-rouge">MLM</code> 역시 대분류 상 <code class="language-plaintext highlighter-rouge">Language Modeling</code> 기법에 속한다. <strong>다만, <code class="language-plaintext highlighter-rouge">Bi-Directional</code>하게 문맥을 파악하고 <code class="language-plaintext highlighter-rouge">LM</code>을 하니까 정말 엄밀히 따지면 3번의 속성도 조금은 있다고 보는게 무리는 아니라 생각한다.</strong> <code class="language-plaintext highlighter-rouge">MLM</code> 사용으로 더 많은 정보를 포착해 임베딩을 만들기 때문에 초기 <code class="language-plaintext highlighter-rouge">BERT</code>가 <code class="language-plaintext highlighter-rouge">GPT</code>보다 <code class="language-plaintext highlighter-rouge">NLU</code>에서 상대적으로 강점을 가졌던 것 아닐까 싶다.</p>

<h4 id="-relative-position-embedding"><strong><code class="language-plaintext highlighter-rouge">🔢 Relative Position Embedding</code></strong></h4>
<p>이제 <code class="language-plaintext highlighter-rouge">Relative Position Embedding</code>이 무엇이고, 도대체 어떤 문맥 정보를 포착한다는 것인지 알아보자. <code class="language-plaintext highlighter-rouge">Relative Position Embedding</code> 이란, 시퀀스 내부 토큰 사이의 위치 관계 표현을 통해 토큰 사이의 <code class="language-plaintext highlighter-rouge">relation</code>을 <code class="language-plaintext highlighter-rouge">pairwise</code>하게 학습하는 위치 임베딩 기법을 말한다. 일반적으로 상대 위치 관계는 서로 다른 두 토큰의 시퀀스 인덱스 값의 차를 이용해 나타낸다. 포착하는 문맥 정보는 예시와 함깨 설명하겠다. 딥러닝이라는 단어는 영어로 <code class="language-plaintext highlighter-rouge">Deep Learning</code> 이다. 두 단어를 합쳐놓고 보면 <code class="language-plaintext highlighter-rouge">신경망을 사용하는 머신러닝 기법의 한 종류</code>라는 의미를 갖겠지만, 따로 따로 보면 <code class="language-plaintext highlighter-rouge">깊은</code>, <code class="language-plaintext highlighter-rouge">배움</code>이라는 개별적인 의미로 나뉜다.</p>

<ul>
  <li><strong><code class="language-plaintext highlighter-rouge">1) The Deep Learning is the Best Technique in Computer Science</code></strong></li>
  <li><strong><code class="language-plaintext highlighter-rouge">2) I’m learning how to swim in the deep ocean</code></strong></li>
</ul>

<p><code class="language-plaintext highlighter-rouge">Deep</code>과 <code class="language-plaintext highlighter-rouge">Learning</code>의 상대적인 거리에 주목하면서 두 문장을 해석해보자. 첫 번째 문장에서 두 단어는 이웃하게 위치해 <code class="language-plaintext highlighter-rouge">신경망을 사용하는 머신러닝 기법의 한 종류</code> 라는 의미를 만들어내고 있다. 한편 두 번째 문장에서 두 단어는 띄어쓰기 기준 5개의 토큰만큼 떨어져 위치해 각각 <code class="language-plaintext highlighter-rouge">배움</code>, <code class="language-plaintext highlighter-rouge">깊은</code> 이라는 의미를 만들어 내고 있다. 이처럼 개별 토큰 사이의 위치 관계에 따라서 파생되는 문맥적 정보를 포착하려는 의도로 설계된 기법이 바로 <code class="language-plaintext highlighter-rouge">Relative Position Embedding</code> 이다.</p>

<p><code class="language-plaintext highlighter-rouge">pairwise</code> 하게 <code class="language-plaintext highlighter-rouge">relation</code> 을 포착한다는 점으로 보아 <code class="language-plaintext highlighter-rouge">skip-gram</code>의 <code class="language-plaintext highlighter-rouge">negative sampling</code>과 매우 유사한 느낌의 정보를 포착할 것이라고 예상되며 카테고리 분류상 <strong>3번, <code class="language-plaintext highlighter-rouge">단어 분포 가설</code></strong>에 포함시킬 수 있을 것 같다. (필자의 개인적인 의견이니 이 부분에 대한 다른 의견이 있다면 꼭 댓글에 적어주시면 감사하겠습니당🥰).</p>

<p>위 예시만으로는 상대 위치 임베딩 개념이 와닿지 않을 수 있다. 그렇다면 옆에 링크를 먼저 읽고 오자. (<a href="https://qcqced123.github.io/nlp/deberta#-word-context-vs-relative-position-vs-absolute-position">링크1</a>)</p>

<p><code class="language-plaintext highlighter-rouge">Relative Position Embedding</code> 을 실제 어떻게 코드로 구현하는지, 본 논문에서는 위치 관계를 어떻게 정의했는지 <code class="language-plaintext highlighter-rouge">Absolute Position Embedding</code>와 비교를 통해 알아보자. 다음과 같은 두 개의 문장이 있을 때, 개별 위치 임베딩 방식이 문장의 위치 정보를 인코딩하는 과정을 파이썬 코드로 작성해봤다. 함께 살펴보자.</p>

<ul>
  <li><strong><code class="language-plaintext highlighter-rouge">A) I love studying deep learning so much</code></strong></li>
  <li><strong><code class="language-plaintext highlighter-rouge">B) I love deep cheeze burguer so much</code></strong></li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Absolute Position Embedding
</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">max_length</span> <span class="o">=</span> <span class="mi">7</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">position_embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Embedding</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">512</span><span class="p">)</span> <span class="c1"># [max_seq, dim_model]
</span><span class="o">&gt;&gt;&gt;</span> <span class="n">pos_x</span> <span class="o">=</span> <span class="n">position_embedding</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="n">max_length</span><span class="p">))</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">pos_x</span><span class="p">,</span> <span class="n">pos_x</span><span class="p">.</span><span class="n">shape</span>
<span class="p">(</span><span class="n">tensor</span><span class="p">([[</span> <span class="mf">0.4027</span><span class="p">,</span>  <span class="mf">0.9331</span><span class="p">,</span>  <span class="mf">1.0556</span><span class="p">,</span>  <span class="p">...,</span> <span class="o">-</span><span class="mf">1.7370</span><span class="p">,</span>  <span class="mf">0.7799</span><span class="p">,</span>  <span class="mf">1.9851</span><span class="p">],</span>  <span class="c1"># A,B의 0번 토큰: I
</span>         <span class="p">[</span><span class="o">-</span><span class="mf">0.2206</span><span class="p">,</span>  <span class="mf">2.1024</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6055</span><span class="p">,</span>  <span class="p">...,</span> <span class="o">-</span><span class="mf">1.1342</span><span class="p">,</span>  <span class="mf">1.3956</span><span class="p">,</span>  <span class="mf">0.9017</span><span class="p">],</span>  <span class="c1"># A,B의 1번 토큰: love
</span>         <span class="p">[</span><span class="o">-</span><span class="mf">0.9560</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0426</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.8587</span><span class="p">,</span>  <span class="p">...,</span> <span class="o">-</span><span class="mf">0.9406</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1467</span><span class="p">,</span>  <span class="mf">0.1762</span><span class="p">],</span>  <span class="c1"># A,B의 2번 토큰: studying, deep
</span>         <span class="p">...,</span>                                                           <span class="c1"># A,B의 3번 토큰: deep, cheeze
</span>         <span class="p">[</span> <span class="mf">0.5999</span><span class="p">,</span>  <span class="mf">0.5235</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3445</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mf">1.9020</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.5003</span><span class="p">,</span>  <span class="mf">0.7535</span><span class="p">],</span>  <span class="c1"># A,B의 4번 토큰: learning, burger
</span>         <span class="p">[</span> <span class="mf">0.0688</span><span class="p">,</span>  <span class="mf">0.5867</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0340</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mf">0.8547</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.9196</span><span class="p">,</span>  <span class="mf">1.1193</span><span class="p">],</span>  <span class="c1"># A,B의 5번 토큰: so
</span>         <span class="p">[</span><span class="o">-</span><span class="mf">0.0751</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4133</span><span class="p">,</span>  <span class="mf">0.0256</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mf">0.0788</span><span class="p">,</span>  <span class="mf">1.4665</span><span class="p">,</span>  <span class="mf">0.8196</span><span class="p">]],</span> <span class="c1"># A,B의 6번 토큰: much
</span>        <span class="n">grad_fn</span><span class="o">=&lt;</span><span class="n">EmbeddingBackward0</span><span class="o">&gt;</span><span class="p">),</span>
 <span class="n">torch</span><span class="p">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">7</span><span class="p">,</span> <span class="mi">512</span><span class="p">]))</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">Absolute Position Embedding</code>은 주위 문맥에 상관없이 같은 위치의 토큰이라면 같은 포지션 값으로 인코딩하기 때문에 <code class="language-plaintext highlighter-rouge">512</code>개의 원소로 구성된 행벡터들의 인덱스를 실제 문장에서 토큰의 등장 순서에 맵핑해주는 방식으로 위치 정보를 표현한다. 예를 들면, 문장에서 가장 먼저 등장하는 <code class="language-plaintext highlighter-rouge">0</code>번 토큰에 <code class="language-plaintext highlighter-rouge">0</code>번째 <code class="language-plaintext highlighter-rouge">행벡터</code>를 배정하고 가장 마지막에 등장하는 <code class="language-plaintext highlighter-rouge">N-1</code> 번째 토큰은 <code class="language-plaintext highlighter-rouge">N-1</code>번째 <code class="language-plaintext highlighter-rouge">행벡터</code>를 위치 정보값으로 갖는 방식이다. 전체 시퀀스 관점에서 개별 토큰에 번호를 부여하기 때문에 <code class="language-plaintext highlighter-rouge">syntactical</code>한 정보를 모델링 해주기 적합하다는 장점이 있다.</p>

<p><code class="language-plaintext highlighter-rouge">Absolute Position Embedding</code> 은 일반적으로 <code class="language-plaintext highlighter-rouge">Input Embedding</code>과 행렬합 연산을 통해 <code class="language-plaintext highlighter-rouge">Word Embedding</code> 으로 만들어 인코더의 입력으로 사용한다.</p>

<p>아래 코드는 저자가 논문에서 제시한 <code class="language-plaintext highlighter-rouge">DeBERTa</code>의 <code class="language-plaintext highlighter-rouge">Relative Position Embedding</code> 구현을 파이토치로 옮긴 것이다. <code class="language-plaintext highlighter-rouge">Relative Position Embedding</code> 은 절대 위치에 비해 꽤나 복잡한 과정을 거쳐야 하기 때문에 코드 역시 긴 편이다. 하나 하나 천천히 살펴보자.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Relative Position Embedding
</span><span class="o">&gt;&gt;&gt;</span> <span class="n">position_embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Embedding</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">max_length</span><span class="p">,</span> <span class="n">dim_model</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">x</span><span class="p">,</span> <span class="n">p_x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="n">max_length</span><span class="p">,</span> <span class="n">dim_model</span><span class="p">),</span> <span class="n">position_embedding</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">max_length</span><span class="p">))</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">fc_q</span><span class="p">,</span> <span class="n">fc_kr</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">dim_model</span><span class="p">,</span> <span class="n">dim_head</span><span class="p">),</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">dim_model</span><span class="p">,</span> <span class="n">dim_head</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">q</span><span class="p">,</span> <span class="n">kr</span> <span class="o">=</span> <span class="n">fc_q</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">fc_kr</span><span class="p">(</span><span class="n">p_x</span><span class="p">)</span> <span class="c1"># [batch, max_length, dim_head], [batch, 2*max_length, dim_head]
</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">tmp_c2p</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">kr</span><span class="p">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">))</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">tmp_c2p</span><span class="p">,</span> <span class="n">tmp_c2p</span><span class="p">.</span><span class="n">shape</span>
<span class="p">(</span><span class="n">tensor</span><span class="p">([[</span> <span class="mf">2.8118</span><span class="p">,</span>  <span class="mf">0.8449</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6240</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6516</span><span class="p">,</span>  <span class="mf">3.4009</span><span class="p">,</span>  <span class="mf">1.8296</span><span class="p">,</span>  <span class="mf">0.8304</span><span class="p">,</span>  <span class="mf">1.0164</span><span class="p">,</span>
           <span class="mf">3.5664</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.4208</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.0821</span><span class="p">,</span>  <span class="mf">1.5752</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.9469</span><span class="p">,</span> <span class="o">-</span><span class="mf">7.1767</span><span class="p">],</span>
         <span class="p">[</span><span class="o">-</span><span class="mf">2.1907</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.2801</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.0628</span><span class="p">,</span>  <span class="mf">0.4443</span><span class="p">,</span>  <span class="mf">2.2272</span><span class="p">,</span> <span class="o">-</span><span class="mf">5.6653</span><span class="p">,</span> <span class="o">-</span><span class="mf">4.6036</span><span class="p">,</span>  <span class="mf">1.4134</span><span class="p">,</span>
          <span class="o">-</span><span class="mf">1.1742</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3361</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4586</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.1827</span><span class="p">,</span>  <span class="mf">1.0878</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.5657</span><span class="p">],</span>
         <span class="p">[</span><span class="o">-</span><span class="mf">4.8952</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.5330</span><span class="p">,</span>  <span class="mf">0.0251</span><span class="p">,</span>  <span class="mf">3.5001</span><span class="p">,</span>  <span class="mf">4.1619</span><span class="p">,</span>  <span class="mf">1.7408</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5100</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.4616</span><span class="p">,</span>
          <span class="o">-</span><span class="mf">1.6101</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.8741</span><span class="p">,</span>  <span class="mf">1.1404</span><span class="p">,</span>  <span class="mf">4.9860</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.5350</span><span class="p">,</span>  <span class="mf">1.0999</span><span class="p">],</span>
         <span class="p">[</span><span class="o">-</span><span class="mf">3.3437</span><span class="p">,</span>  <span class="mf">4.2276</span><span class="p">,</span>  <span class="mf">0.4509</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.8911</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.1069</span><span class="p">,</span>  <span class="mf">0.9540</span><span class="p">,</span>  <span class="mf">1.2045</span><span class="p">,</span>  <span class="mf">2.2194</span><span class="p">,</span>
          <span class="o">-</span><span class="mf">2.6509</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.4076</span><span class="p">,</span>  <span class="mf">5.1599</span><span class="p">,</span>  <span class="mf">1.6591</span><span class="p">,</span>  <span class="mf">3.8764</span><span class="p">,</span>  <span class="mf">2.5126</span><span class="p">],</span>
         <span class="p">[</span> <span class="mf">0.8164</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.9171</span><span class="p">,</span>  <span class="mf">0.8217</span><span class="p">,</span>  <span class="mf">1.3953</span><span class="p">,</span>  <span class="mf">1.6260</span><span class="p">,</span>  <span class="mf">3.8104</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0303</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.1631</span><span class="p">,</span>
           <span class="mf">3.9008</span><span class="p">,</span>  <span class="mf">0.5856</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.6212</span><span class="p">,</span>  <span class="mf">1.7220</span><span class="p">,</span>  <span class="mf">2.7997</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.8802</span><span class="p">],</span>
         <span class="p">[</span> <span class="mf">3.4473</span><span class="p">,</span>  <span class="mf">0.9721</span><span class="p">,</span>  <span class="mf">3.9137</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.2055</span><span class="p">,</span>  <span class="mf">0.6963</span><span class="p">,</span>  <span class="mf">1.2761</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2266</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.7274</span><span class="p">,</span>
          <span class="o">-</span><span class="mf">1.4928</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.9257</span><span class="p">,</span> <span class="o">-</span><span class="mf">5.4422</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.8544</span><span class="p">,</span>  <span class="mf">1.8749</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.4923</span><span class="p">],</span>
         <span class="p">[</span> <span class="mf">2.6639</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.4392</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.8818</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.4120</span><span class="p">,</span>  <span class="mf">1.7542</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.8774</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.0795</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2156</span><span class="p">,</span>
          <span class="o">-</span><span class="mf">1.0852</span><span class="p">,</span>  <span class="mf">3.7825</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.5581</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.6989</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.6705</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2262</span><span class="p">]],</span>
        <span class="n">grad_fn</span><span class="o">=&lt;</span><span class="n">MmBackward0</span><span class="o">&gt;</span><span class="p">),</span>
 <span class="n">torch</span><span class="p">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">7</span><span class="p">,</span> <span class="mi">14</span><span class="p">]))</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">max_seq</span><span class="p">,</span> <span class="n">max_pos</span> <span class="o">=</span> <span class="mi">7</span><span class="p">,</span> <span class="n">max_seq</span> <span class="o">*</span> <span class="mi">2</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">q_index</span><span class="p">,</span> <span class="n">k_index</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="n">max_seq</span><span class="p">),</span> <span class="n">torch</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="n">max_seq</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">q_index</span><span class="p">,</span> <span class="n">k_index</span>
<span class="p">(</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">]),</span> <span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">]))</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">tmp_pos</span> <span class="o">=</span> <span class="n">q_index</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="o">-</span> <span class="n">k_index</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">rel_pos_matrix</span> <span class="o">=</span> <span class="n">tmp_pos</span> <span class="o">+</span> <span class="n">max_relative_position</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">rel_pos_matrix</span>
<span class="n">tensor</span><span class="p">([[</span> <span class="mi">7</span><span class="p">,</span>  <span class="mi">6</span><span class="p">,</span>  <span class="mi">5</span><span class="p">,</span>  <span class="mi">4</span><span class="p">,</span>  <span class="mi">3</span><span class="p">,</span>  <span class="mi">2</span><span class="p">,</span>  <span class="mi">1</span><span class="p">],</span>
        <span class="p">[</span> <span class="mi">8</span><span class="p">,</span>  <span class="mi">7</span><span class="p">,</span>  <span class="mi">6</span><span class="p">,</span>  <span class="mi">5</span><span class="p">,</span>  <span class="mi">4</span><span class="p">,</span>  <span class="mi">3</span><span class="p">,</span>  <span class="mi">2</span><span class="p">],</span>
        <span class="p">[</span> <span class="mi">9</span><span class="p">,</span>  <span class="mi">8</span><span class="p">,</span>  <span class="mi">7</span><span class="p">,</span>  <span class="mi">6</span><span class="p">,</span>  <span class="mi">5</span><span class="p">,</span>  <span class="mi">4</span><span class="p">,</span>  <span class="mi">3</span><span class="p">],</span>
        <span class="p">[</span><span class="mi">10</span><span class="p">,</span>  <span class="mi">9</span><span class="p">,</span>  <span class="mi">8</span><span class="p">,</span>  <span class="mi">7</span><span class="p">,</span>  <span class="mi">6</span><span class="p">,</span>  <span class="mi">5</span><span class="p">,</span>  <span class="mi">4</span><span class="p">],</span>
        <span class="p">[</span><span class="mi">11</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span>  <span class="mi">9</span><span class="p">,</span>  <span class="mi">8</span><span class="p">,</span>  <span class="mi">7</span><span class="p">,</span>  <span class="mi">6</span><span class="p">,</span>  <span class="mi">5</span><span class="p">],</span>
        <span class="p">[</span><span class="mi">12</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span>  <span class="mi">9</span><span class="p">,</span>  <span class="mi">8</span><span class="p">,</span>  <span class="mi">7</span><span class="p">,</span>  <span class="mi">6</span><span class="p">],</span>
        <span class="p">[</span><span class="mi">13</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span>  <span class="mi">9</span><span class="p">,</span>  <span class="mi">8</span><span class="p">,</span>  <span class="mi">7</span><span class="p">]])</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">rel_pos_matrix</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">rel_pos_matrix</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">max_pos</span> <span class="o">-</span> <span class="mi">1</span><span class="p">).</span><span class="n">repeat</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">tmp_c2p</span> <span class="o">=</span> <span class="n">tmp_c2p</span><span class="p">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">rel_pos_matrix</span><span class="p">,</span> <span class="n">rel_pos_matrix</span><span class="p">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">tmp_c2p</span><span class="p">.</span><span class="n">shape</span> 
<span class="p">(</span><span class="n">tensor</span><span class="p">([[[</span> <span class="mi">7</span><span class="p">,</span>  <span class="mi">6</span><span class="p">,</span>  <span class="mi">5</span><span class="p">,</span>  <span class="mi">4</span><span class="p">,</span>  <span class="mi">3</span><span class="p">,</span>  <span class="mi">2</span><span class="p">,</span>  <span class="mi">1</span><span class="p">],</span>
          <span class="p">[</span> <span class="mi">8</span><span class="p">,</span>  <span class="mi">7</span><span class="p">,</span>  <span class="mi">6</span><span class="p">,</span>  <span class="mi">5</span><span class="p">,</span>  <span class="mi">4</span><span class="p">,</span>  <span class="mi">3</span><span class="p">,</span>  <span class="mi">2</span><span class="p">],</span>
          <span class="p">[</span> <span class="mi">9</span><span class="p">,</span>  <span class="mi">8</span><span class="p">,</span>  <span class="mi">7</span><span class="p">,</span>  <span class="mi">6</span><span class="p">,</span>  <span class="mi">5</span><span class="p">,</span>  <span class="mi">4</span><span class="p">,</span>  <span class="mi">3</span><span class="p">],</span>
          <span class="p">[</span><span class="mi">10</span><span class="p">,</span>  <span class="mi">9</span><span class="p">,</span>  <span class="mi">8</span><span class="p">,</span>  <span class="mi">7</span><span class="p">,</span>  <span class="mi">6</span><span class="p">,</span>  <span class="mi">5</span><span class="p">,</span>  <span class="mi">4</span><span class="p">],</span>
          <span class="p">[</span><span class="mi">11</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span>  <span class="mi">9</span><span class="p">,</span>  <span class="mi">8</span><span class="p">,</span>  <span class="mi">7</span><span class="p">,</span>  <span class="mi">6</span><span class="p">,</span>  <span class="mi">5</span><span class="p">],</span>
          <span class="p">[</span><span class="mi">12</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span>  <span class="mi">9</span><span class="p">,</span>  <span class="mi">8</span><span class="p">,</span>  <span class="mi">7</span><span class="p">,</span>  <span class="mi">6</span><span class="p">],</span>
          <span class="p">[</span><span class="mi">13</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span>  <span class="mi">9</span><span class="p">,</span>  <span class="mi">8</span><span class="p">,</span>  <span class="mi">7</span><span class="p">]],</span>
<span class="n">torch</span><span class="p">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">10</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">14</span><span class="p">]),</span>
<span class="n">torch</span><span class="p">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">10</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">14</span><span class="p">]))</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">outputs</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">gather</span><span class="p">(</span><span class="n">tmp_c2p</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">rel_pos_matrix</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">.</span><span class="n">shape</span>
<span class="p">(</span><span class="n">tensor</span><span class="p">([[[</span> <span class="mf">1.0164</span><span class="p">,</span>  <span class="mf">0.8304</span><span class="p">,</span>  <span class="mf">1.8296</span><span class="p">,</span>  <span class="mf">3.4009</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6516</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6240</span><span class="p">,</span>  <span class="mf">0.8449</span><span class="p">],</span>
          <span class="p">[</span><span class="o">-</span><span class="mf">1.1742</span><span class="p">,</span>  <span class="mf">1.4134</span><span class="p">,</span> <span class="o">-</span><span class="mf">4.6036</span><span class="p">,</span> <span class="o">-</span><span class="mf">5.6653</span><span class="p">,</span>  <span class="mf">2.2272</span><span class="p">,</span>  <span class="mf">0.4443</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.0628</span><span class="p">],</span>
          <span class="p">[</span><span class="o">-</span><span class="mf">1.8741</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.6101</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.4616</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5100</span><span class="p">,</span>  <span class="mf">1.7408</span><span class="p">,</span>  <span class="mf">4.1619</span><span class="p">,</span>  <span class="mf">3.5001</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">5.1599</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.4076</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.6509</span><span class="p">,</span>  <span class="mf">2.2194</span><span class="p">,</span>  <span class="mf">1.2045</span><span class="p">,</span>  <span class="mf">0.9540</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.1069</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">1.7220</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.6212</span><span class="p">,</span>  <span class="mf">0.5856</span><span class="p">,</span>  <span class="mf">3.9008</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.1631</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0303</span><span class="p">,</span>  <span class="mf">3.8104</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">1.8749</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.8544</span><span class="p">,</span> <span class="o">-</span><span class="mf">5.4422</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.9257</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.4928</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.7274</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2266</span><span class="p">],</span>
          <span class="p">[</span><span class="o">-</span><span class="mf">1.2262</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.6705</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.6989</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.5581</span><span class="p">,</span>  <span class="mf">3.7825</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0852</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2156</span><span class="p">]],</span>
          <span class="p">.....</span>
          <span class="p">[[</span> <span class="mf">1.0164</span><span class="p">,</span>  <span class="mf">0.8304</span><span class="p">,</span>  <span class="mf">1.8296</span><span class="p">,</span>  <span class="mf">3.4009</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6516</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6240</span><span class="p">,</span>  <span class="mf">0.8449</span><span class="p">],</span>
          <span class="p">[</span><span class="o">-</span><span class="mf">1.1742</span><span class="p">,</span>  <span class="mf">1.4134</span><span class="p">,</span> <span class="o">-</span><span class="mf">4.6036</span><span class="p">,</span> <span class="o">-</span><span class="mf">5.6653</span><span class="p">,</span>  <span class="mf">2.2272</span><span class="p">,</span>  <span class="mf">0.4443</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.0628</span><span class="p">],</span>
          <span class="p">[</span><span class="o">-</span><span class="mf">1.8741</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.6101</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.4616</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5100</span><span class="p">,</span>  <span class="mf">1.7408</span><span class="p">,</span>  <span class="mf">4.1619</span><span class="p">,</span>  <span class="mf">3.5001</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">5.1599</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.4076</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.6509</span><span class="p">,</span>  <span class="mf">2.2194</span><span class="p">,</span>  <span class="mf">1.2045</span><span class="p">,</span>  <span class="mf">0.9540</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.1069</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">1.7220</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.6212</span><span class="p">,</span>  <span class="mf">0.5856</span><span class="p">,</span>  <span class="mf">3.9008</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.1631</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0303</span><span class="p">,</span>  <span class="mf">3.8104</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">1.8749</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.8544</span><span class="p">,</span> <span class="o">-</span><span class="mf">5.4422</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.9257</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.4928</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.7274</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2266</span><span class="p">],</span>
          <span class="p">[</span><span class="o">-</span><span class="mf">1.2262</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.6705</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.6989</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.5581</span><span class="p">,</span>  <span class="mf">3.7825</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0852</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2156</span><span class="p">]]],</span>
        <span class="n">grad_fn</span><span class="o">=&lt;</span><span class="n">GatherBackward0</span><span class="o">&gt;</span><span class="p">),</span>
 <span class="n">torch</span><span class="p">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">10</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">7</span><span class="p">]))</span>
</code></pre></div></div>

<p>일단 절대 위치와 동일하게 <code class="language-plaintext highlighter-rouge">nn.Embedding</code>을 사용해 임베딩 룩업 테이블(레이어)를 정의하지만, 입력 차원이 다르다. 절대 위치 임베딩은 <code class="language-plaintext highlighter-rouge">forward</code>하게 위치값을 맵핑해야 하는 반면에 상대 위치 임베딩 방식은 <code class="language-plaintext highlighter-rouge">Bi-Directional</code>한 맵핑을 해야 해서, 기존 <code class="language-plaintext highlighter-rouge">max_length</code> 값의 두 배를 입력 차원(<code class="language-plaintext highlighter-rouge">max_pos</code>)으로 사용했다. 예를 들어 <code class="language-plaintext highlighter-rouge">0</code>번 토큰과 나머지 토큰 사이의 위치 관계를 표현해야 하는 상황이다. 그렇다면 우리는 <code class="language-plaintext highlighter-rouge">0</code>번 토큰과 나머지 토큰과의 위치 관계를 <code class="language-plaintext highlighter-rouge">[0, -1, -2, -3, -4, -5, -6]</code> 으로 인코딩할 수 있다.</p>

<p>반대로 마지막 <code class="language-plaintext highlighter-rouge">6</code>번 토큰과 나머지 토큰 사이의 위치 관계를 표현하는 경우라면 어떻게 될까?? <code class="language-plaintext highlighter-rouge">[6, 5, 4, 3, 2, 1, 0]</code> 으로 인코딩 될 것이다. 다시 말해, 위치 임베딩 원소 값은 <code class="language-plaintext highlighter-rouge">[-max_seq:max_seq]</code> 사이에서 정의된다는 것이다. 그러나 원소값의 범위를 그대로 사용할 수는 없다. 이유는 파이썬의 리스트, 텐서 같은 배열형 자료구조는 음이 아닌 정수를 인덱스로 활용해야 <code class="language-plaintext highlighter-rouge">forward</code> 하게 원소에 접근할 수 있기 때문이다. 일반적으로 배열 형태의 자료형은 모두 인덱스 <code class="language-plaintext highlighter-rouge">0</code>부터 <code class="language-plaintext highlighter-rouge">N-1</code>까지 순차적으로 맵핑된다. 그래서 의도한대로 토큰에 접근하려면 역시 토큰의 인덱스를 <code class="language-plaintext highlighter-rouge">forward</code> 한 형태로 만들어줘야 한다.</p>

<p>따라서 기존 <code class="language-plaintext highlighter-rouge">[-max_seq:max_seq]</code> 에  <code class="language-plaintext highlighter-rouge">max_seq</code>를 더해준 <code class="language-plaintext highlighter-rouge">[0:2*max_seq]</code> (<code class="language-plaintext highlighter-rouge">2 * max_seq</code>)을 원소 값의 범위로 사용하게 된다. 여기까지가 통상적으로 말하는 <code class="language-plaintext highlighter-rouge">Relative Position Embedding</code> 에 해당한다. 위 코드상으로는 <code class="language-plaintext highlighter-rouge">rel_pos_matrix</code> 를 만든 부분에 해당한다.</p>

\[∂(i,j)=
\begin{cases}
\ 0 &amp; {(i - j ≤ k)} \\ 
\ 2k-1 &amp; {(i - j ≥ k)} \\
\ i - j + k &amp; {(others)} \\
\end{cases}\]

<p>이제부터 저자가 주장하는 위치 관계 표현 방식에 대해 알아보자. 일반적인 <code class="language-plaintext highlighter-rouge">Relative Position Embedding</code>과 거의 유사하지만, <code class="language-plaintext highlighter-rouge">rel_pos_matrix</code> 내부 원소 값이 음수가 되거나 <code class="language-plaintext highlighter-rouge">max_pos</code> 을 초과하는 경우를 처리 해주기 위해 후처리 과정을 도입해 사용했다. 예외 상황은 <strong><code class="language-plaintext highlighter-rouge">max_seq &gt; 1/2 * max_pos(==k)</code></strong> 일 때 발생한다. <code class="language-plaintext highlighter-rouge">official repo</code> 의 코드를 보면 <code class="language-plaintext highlighter-rouge">max_seq</code>와 <code class="language-plaintext highlighter-rouge">k</code>를 일치시켜 모델링 하기 때문에 파인튜닝 하는 상황이라면 이것을 몰라도 상관없겠지만, 하나 하나 모델을 직접 만드는 입장이라면 예외 상황을 반드시 기억하자.</p>

<p>한편, 이러한 인코딩 방식은 <code class="language-plaintext highlighter-rouge">word2vec</code> 의 <code class="language-plaintext highlighter-rouge">window size</code> 도입과 비슷한 원리(<code class="language-plaintext highlighter-rouge">의미는 주변 문맥에 의해 결정</code>)라고 생각하면 되는데, 윈도우 사이즈 범위에서 벗어난 토큰들은 주변 문맥으로 인정 하지 않겠다는(<code class="language-plaintext highlighter-rouge">negative sample</code>) 의도를 갖고 있다. 실제 구현은 텐서 내부 원소값의 범위를 사용자 지정 범위로 제한할 수 있는 <code class="language-plaintext highlighter-rouge">torch.clamp</code> 를 사용하면 <code class="language-plaintext highlighter-rouge">1</code>줄로 깔끔하게 만들 수 있으니 참고하자.</p>

<p><code class="language-plaintext highlighter-rouge">torch.clamp</code> 까지 적용하고 난 최종 결과를 살펴보자. 행백터, 열벡터 모두 <code class="language-plaintext highlighter-rouge">[0:2*max_seq]</code> 사이에서 정의되고 있으며, 개별 방향 벡터 원소의 최대값과 최소값의 차이가 항상 <code class="language-plaintext highlighter-rouge">k</code> 로 유지 된다. 의도대로 정확히 윈도우 사이즈만큼의 주변 맥락을 반영해 임베딩을 형성하고 있음을 알 수 있다.</p>

<p>정리하면, <code class="language-plaintext highlighter-rouge">Relative Position Embedding</code> 란 절대 위치 방식처럼 임베딩 룩업 테이블을 만들되, 사용자가 지정한 윈도우 사이즈에 해당하는 토큰의 임베딩 값만 추출해 새로운 행벡터를 여러 개 만들어 내는 기법이라고 할 수 있다. <strong>이 때 행벡터는 대상 토큰과 그 나머지 토큰 사이의 위치 변화에 따라 발생하는 파생적인 맥락 정보를 담고 있다.</strong></p>

<h4 id="-word-context-vs-relative-position-vs-absolute-position"><strong><code class="language-plaintext highlighter-rouge">🤔 Word Context vs Relative Position vs Absolute Position</code></strong></h4>

<p align="center">
<img src="/assets/images/deberta/line_people.png" alt="줄 서있는 사람들" class="align-center image-caption" width="40%&quot;, height=&quot;50%" />
<strong><em><a href="https://kr.freepik.com/premium-photo/people-standing-in-line-during-airport-check-in_8754408.htm">줄 서있는 사람들</a></em></strong>
</p>

<p>지금까지 <code class="language-plaintext highlighter-rouge">Relative Position Embedding</code>이 무엇이고, 도대체 어떤 문맥 정보를 포착한다는 것인지 알아봤다. 필자의 설명이 매끄럽지 못하기도 하고 예시를 텍스트로 들고 있어서 직관적으로 <code class="language-plaintext highlighter-rouge">word context</code>는 무엇인지, <code class="language-plaintext highlighter-rouge">Position</code> 정보와는 뭐가 다른지, 두 가지 <code class="language-plaintext highlighter-rouge">Position</code> 정보는 뭐가 어떻게 다른지 와닿지 않는 분들이 많으실 것 같다. 그래서 최대한 직관적인 예시를 통해 세가지 정보의 차이점을 설명해보려 한다. (필자 본인이 햇갈려서 쓰는 건 비밀이다)</p>

<p>사람 5명이 공항 체크인을 위해 서 있다. 모두 왼쪽을 보고 있는 것을 보아 왼쪽에 키가 제일 작은 여자가 가장 앞줄이라고 볼 수 있겠다. 우리는 줄 서있는 순서대로 5명의 사람에게 번호를 부여할 것이다. 편의상 0번부터 시작해 4번까지 번호를 주겠다. 1번에 해당하는 사람은 누구인가??  바로 줄의 2번째에 서있는 여자다. 그럼 2번에 해당하는 사람은 누구인가?? 사진 속 줄의 가장 중간에 있는 남자가 2번이다. 이렇게 그룹 단위(전체 줄)에서 개개인에 일련의 번호를 부여해 위치를 표현하는 방법이 바로 <code class="language-plaintext highlighter-rouge">Absolute Position Embedding</code>이다.</p>

<p>한편, 다시 2번 사람에게 주목해보자. 우리는 2번 남자를 전체 줄에서 가운데 위치한 사람이 아니라, 검정색 양복과 구두를 신고 손에 쥔 무언가를 응시하고 있는 사람이라고 표현할 수도 있다. 이것이 바로 토큰의 의미 정보를 담은 <code class="language-plaintext highlighter-rouge">word context</code>에 해당한다.</p>

<p>마지막으로 <code class="language-plaintext highlighter-rouge">Relative Position Embedding</code> 방식으로 2번 남자를 표현해보자. 오른손으로는 커피를 들고 다른 손으로는 캐리어를 잡고 있으며 검정색 하이힐과 베이지색 바지를 입은 <strong>1번 여자의 뒤에 있는 사람</strong>, 회색 양복과 검은 뿔테 안경을 쓰고 한 손에는 캐리어를 잡고 있는 <strong>4번 여자의 앞에 있는 사람</strong>, 검정색 자켓과 청바지를 입고 한 손에는 회색 코트를 들고 있는 줄의 <strong>맨 앞 여자로부터 2번째 뒤에 서있는 사람</strong>, 턱수염이 길고 머리가 긴 편이며 파란색 가디건을 입고 초록색과 검정색이 혼합된 가방을 왼쪽으로 메고 있는 <strong>남자로부터 2번째 앞에 있는 사람.</strong></p>

<p>이처럼 표현하는게 바로 <code class="language-plaintext highlighter-rouge">Relative Position Embedding</code>에 대응된다고 볼 수 있다. 이제 위 예시를 자연어 처리에 그대로 대입시켜보면 이해가 한결 수월할 것이다.</p>

<h4 id="-deberta-inductive-bias"><strong><code class="language-plaintext highlighter-rouge">🤔 DeBERTa Inductive Bias</code></strong></h4>
<p><strong>결국</strong> <code class="language-plaintext highlighter-rouge">DeBERTa</code><strong>는 두가지 위치 정보 포착 방식을 적절히 섞어서 모델이 더욱 풍부한 임베딩을 갖도록 하려는 의도로 설계 되었다.</strong> 또한 우리는 이미 모델이 다양한 맥락 정보를 포착할수록 <code class="language-plaintext highlighter-rouge">NLU Task</code> 에서 더 나은 성능을 기록한다는 사실을 <code class="language-plaintext highlighter-rouge">BERT</code>와 <code class="language-plaintext highlighter-rouge">GPT</code> 사례에서 알 수 있었다. 따라서 <code class="language-plaintext highlighter-rouge">Relative Position Embedding</code> 을 추가하여 <code class="language-plaintext highlighter-rouge">단어의 발생 순서</code> 를 포착하는 모델에 <code class="language-plaintext highlighter-rouge">단어 분포 가설</code> 적인 특징을 더해주려는 저자의 아이디어는 매우 타당하다고 볼 수 있겠다.</p>

<p>이제 관건은 <strong><code class="language-plaintext highlighter-rouge">“두가지 위치 정보를 어떤 방식으로 추출하고 섞어줄 것인가”</code></strong> 하는 물음에 답하는 것이다. 저자는 물음에 답하기 위해 <code class="language-plaintext highlighter-rouge">Disentangled Self-Attention</code> 과  <code class="language-plaintext highlighter-rouge">Enhanced Mask Decoder</code> 라는 새로운 기법 두가지를 제시한다. 전자는 <code class="language-plaintext highlighter-rouge">단어 분포 가설</code> 에 해당되는 맥락 정보를 추출하기 위한 기법이고, 후자는 <code class="language-plaintext highlighter-rouge">단어 발생 순서</code> 에 포함되는 임베딩을 모델에 주입하기 위해 설계되었다. 모델링 파트에서는 두가지 새로운 기법에 대해서 자세히 살펴본 뒤, 모델을 코드로 빌드하는 과정을 설명하려 한다.<br />
코드는 논문의 내용과 microsoft의 공식 git repo를 참고해 만들었음을 밝힌다. 다만, 논문에서 모델 구현과 관련해 세부적인 내용은 상당수 생략하고 있으며, repo에 공개된 코드는 hard coding되어 그 의도를 정확하게 파악하는데 많은 어려움이 있었다. 그래서 어느 정도는 필자의 주관적인 생각이 반영된 코드라는 점을 미리 밝힌다.</p>

<h3 id="modeling"><code class="language-plaintext highlighter-rouge">🌟 Modeling</code></h3>

<p align="center">
<img src="/assets/images/deberta/deberta_overview.png" alt="DeBERTa Model Structure" class="align-center image-caption" width="60%&quot;, height=&quot;50%" />
<strong><em><a href="https://www.youtube.com/watch?v=gcMyKUXbY8s&amp;t=838s&amp;ab_channel=%EA%B3%A0%EB%A0%A4%EB%8C%80%ED%95%99%EA%B5%90%EC%82%B0%EC%97%85%EA%B2%BD%EC%98%81%EA%B3%B5%ED%95%99%EB%B6%80DSBA%EC%97%B0%EA%B5%AC%EC%8B%A4">DeBERTa Model Structure</a></em></strong>
</p>

<ul>
  <li><strong>1) Disentangled Self-Attention Encoder Block for <code class="language-plaintext highlighter-rouge">Relative Position Embedding</code></strong></li>
  <li><strong>2) Enhanced Mask Decoder for <code class="language-plaintext highlighter-rouge">Absolute Position Embedding</code></strong></li>
</ul>

<p><code class="language-plaintext highlighter-rouge">DeBERTa</code> 의 전반적인 구조는 일반적인 <code class="language-plaintext highlighter-rouge">BERT</code>, <code class="language-plaintext highlighter-rouge">RoBERTa</code>와 크게 다른 점이 없다. 다만, 모델의 초반부 <code class="language-plaintext highlighter-rouge">Input Embedding</code> 에서 <code class="language-plaintext highlighter-rouge">Absolute Position</code> 정보를 추가하는 부분이 후반부 <code class="language-plaintext highlighter-rouge">Enhanced Mask Decoder</code>라 부르는 인코더 블록으로 옮겨간 것과 <code class="language-plaintext highlighter-rouge">Disentangled Self-Attention</code> 을 위해 개별 인코더 블록마다 상대 위치 정보를 출처로 하는 <code class="language-plaintext highlighter-rouge">linear projection</code> 레이어가 추가되었음을 명심하자. 또한, <code class="language-plaintext highlighter-rouge">DeBERTa</code>의 <code class="language-plaintext highlighter-rouge">pre-train</code> 은 <code class="language-plaintext highlighter-rouge">RoBERTa</code>처럼 <code class="language-plaintext highlighter-rouge">NSP</code>를 삭제하고 <code class="language-plaintext highlighter-rouge">MLM</code>만 사용한 점도 기억하자.</p>

<p align="center">
<img src="/assets/images/deberta/deberta_class_diagram.png" alt="DeBERTa Class Diagram" class="align-center image-caption" width="60%&quot;, height=&quot;50%" />
<strong><em>DeBERTa Class Diagram</em></strong>
</p>

<p>위 자료는 필자가 구현한 <code class="language-plaintext highlighter-rouge">DeBERTa</code>의 구조를 표현한 그림이다. 코드 리뷰에 참고하시면 좋을 것 같다 첨부했다. 가장 중요한 <code class="language-plaintext highlighter-rouge">Disentangled-Attention</code>과 <code class="language-plaintext highlighter-rouge">EMD</code>부터 살펴본 뒤, 나머지 객체에 대해서 살펴보자.</p>

<h4 id="disentangled-self-attention"><strong><code class="language-plaintext highlighter-rouge">🪢 Disentangled Self-Attention</code></strong></h4>

\[\tilde{A_{ij}} = Q_i^c•K_j^{cT} + Q_i^c•K_{∂(i,j)}^{rT} + K_j^c•Q_{∂(i,j)}^{rT} \\
Attention(Q_c,K_c,V_c,Q_r,K_r) = softmax(\frac{\tilde{A}}{\sqrt{3d_h}})*V_c\]

<p><code class="language-plaintext highlighter-rouge">Disentangled Self-Attention</code>은 저자가 퓨어한 <code class="language-plaintext highlighter-rouge">Input Embedding</code> 정보와 <code class="language-plaintext highlighter-rouge">Relative Position</code> 정보를 통합시키기 위해 고안한 변형 <code class="language-plaintext highlighter-rouge">Self-Attention</code> 기법이다. 기존의 <code class="language-plaintext highlighter-rouge">Self-Attention</code>과 다르게 <code class="language-plaintext highlighter-rouge">Position Embedding</code>을 <code class="language-plaintext highlighter-rouge">Input Embedding</code>와 더하지 않고 따로 사용한다. <strong>즉, 같은</strong> $d_h$ <strong>공간에 <code class="language-plaintext highlighter-rouge">Input Embedding</code>과 <code class="language-plaintext highlighter-rouge">Relative Position</code>이라는 서로 다른 두 벡터를 맵핑하고 그 관계성을 파악해보겠다는 뜻이다.</strong></p>

<p><code class="language-plaintext highlighter-rouge">Input</code>과 <code class="language-plaintext highlighter-rouge">Position</code> 정보를 서로 주체적인 입장에서 한 번씩 내적한다고 해서 <code class="language-plaintext highlighter-rouge">Disentangled</code>라는 이름이 붙게 되었다. <code class="language-plaintext highlighter-rouge">Transformer-XL</code>, <code class="language-plaintext highlighter-rouge">XLNet</code>에 제시된 <code class="language-plaintext highlighter-rouge">Cross-Attention</code>과 매우 유사한 개념이다. 첫번째 수식에서 가장 마지막 항을 제외하면 <code class="language-plaintext highlighter-rouge">Cross-Attention</code>과 포착하는 정보가 동일하다고 저자 역시 밝히고 있으니 참고하자.</p>

<p><code class="language-plaintext highlighter-rouge">Disentangled Self-Attention</code> 은 총 5가지 <code class="language-plaintext highlighter-rouge">linear projection matrix</code>를 사용한다. <code class="language-plaintext highlighter-rouge">Input Embedding</code> 을 출처로 하는 $Q^c, k^c, V^c$, 그리고 <code class="language-plaintext highlighter-rouge">Position Embedding</code>을 출처로 하는 $Q^r, K^r$이다. 첨자 $c,r$은 각각 <code class="language-plaintext highlighter-rouge">content</code>, <code class="language-plaintext highlighter-rouge">relative</code> 의 약자로 행렬의 출처를 뜻한다. 한편 행렬 아래 첨자에 써있는 $i,j$는 각각 현재 어텐션 대상 토큰의 인덱스와 그 나머지 토큰의 인덱스를 가리킨다. 그래서 $\tilde{A_{ij}}$는 <code class="language-plaintext highlighter-rouge">[NxN]</code> 크기 행렬(<code class="language-plaintext highlighter-rouge">기존 어텐션에서 쿼리와 키의 내적 결과에 해당</code>)의 $i$번째 행백터의 $j$번째 원소의 값을 의미한다. <code class="language-plaintext highlighter-rouge">Input Embedding</code> 정보와 <code class="language-plaintext highlighter-rouge">Relative Position</code> 정보를 따로 따로 관리하기 때문에 우리가 기존에 알고 있던 <code class="language-plaintext highlighter-rouge">Self-Attention</code>과는 사뭇 다른 수식이다. 이제부터 수식의 항 하나하나의 의미를 구체적인 예시와 함꼐 파헤쳐보자.</p>

<p><strong><code class="language-plaintext highlighter-rouge">☺️ c2c matrix</code></strong><br />
<code class="language-plaintext highlighter-rouge">content2content</code>의 약자로 첫번째 수식 우변의 첫번째 항을 가리키는 말이다. 이름의 의미는 내적에 사용하는 두 행렬의 출처가 모두 <code class="language-plaintext highlighter-rouge">Input Embedding</code> 이라는 사실을 내포하고 있다. 기존에 알고 있던 <code class="language-plaintext highlighter-rouge">Self-Attention</code> 의 두번째 단계인 $Q•K^T$와 거의 동일한 의미를 담고 있는 항이라고 생각하면 될 것 같다. 완전히 같다고 할 수 없는 이유는 <code class="language-plaintext highlighter-rouge">Absolute Position</code> 정보가 빠진채로 내적했기 때문이다.</p>

<p>따라서 연산의 의미 역시 우리가 기존에 알고 있던 바와 동일하다. 혹시 행렬 $Q,K,V$와 <code class="language-plaintext highlighter-rouge">Self-Attention</code>가 내포하는 의미에 대해 자세히 궁금하신 분이라면 필자가 작성한 <a href="https://qcqced123.github.io/nlp/transformer">Transformer논문 리뷰</a>를 보고 오시길 바란다. 그래도 어차피 뒤에 남은 두개의 항을 설명하려면 어차피 예시를 들어야 하기 때문에 <code class="language-plaintext highlighter-rouge">c2c</code>항에서부터 시작해보려 한다.</p>

<p>당신은 오늘 저녁 밥으로 <strong>차돌박이 된장 찌개, 삼겹살 그리고 후식으로 구운 달걀을 먹고 싶다.</strong> 집에 재료가 하나도 없지만 마트에 가기 귀찮으니 <strong>필요한 식자재를 남편에게 사오라고 시킬 생각이다.</strong> 당신은 그래서 필요한 재료 리스트를 적고 있다. <strong>그렇다면 필요한 재료를 어떤 식으로 표현해서 적어줘야 남편이 가장 빠르고 정확하게 필요한 모든 식자재를 사올 수 있을까??</strong></p>

<p>이것을 고민하는게 바로 행렬 $Q^c$와 <code class="language-plaintext highlighter-rouge">linear projector</code> 인 $W_{Q^c}$의 역할이다.예를 들어 같은 앞다리살이라도 구이용이 있고 찌개용이 있다. 달걀도 구운 달걀이 있고 날달걀이 있다. 정확히 용도를 적어주는게 남편 입장에서는 아내의 의도대로 정확하게 장을 보기 훨씬 편할 것이다.</p>

<p>한편, 내적은 본래 파라미터가 필요한 연산은 아니라서 실제 손실함수 오차 역전을 통해 최적화(학습)되는 대상은 바로 $W_{Q^c}$가 된다. 남편이 장을 빠르고 정확하게 보는데 과연 당신이 적어준 리스트만 영향을 미칠까??</p>

<p>아니다. 당신이 어떤 음식을 위해 어떤 재료가 필요한지 그 의도를 잘 적어주는 것도 중요하지만 실제 마트에 적혀 있는 상품명과 상품설명 역시 중요하다. 좀 억지스러운 예시처럼 보이긴 하지만 달걀의 경우 육안으로만 보면 이것이 구운 달걀인지 날달걀인지 구분할 수 없다. 그런데 마트에 별다른 설명없이 상품명으로 <code class="language-plaintext highlighter-rouge">“달걀”</code> 이라고만 적혀있다 생각해보자.</p>

<p>아무리 당신이 좋은 행렬 $Q^c$를 표현해줘도 남편이 날달걀을 사올 확률이 꽤나 높을 것이다. 이렇게 마트에 적혀있는 상품명과 상품설명이 바로 행렬 $K^c$에 대응된다. 그리고 물건을 사기 위해 당신이 적어준 식자재 리스트와 매장에 적힌 상품명과 상품설명을 대조하며 이것이 의도에 맞는 상품인지 따져보는 작업이 바로 $Q_i^c•K_j^{cT}$, <code class="language-plaintext highlighter-rouge">c2c matrix</code>가 된다.</p>

<p>다만, 전역 어텐션을 사용하기 때문에 달걀을 사기 위해 매장에 있는 모든 상품과 대조를 한다고 생각하면 된다. 특히 기존 전역 어텐션의 $Q_i^c•K_j^{cT}$의 경우 모든 상품과 대조하는 과정에서 대조군이 매장에 전시된 위치, 카테고리 분류상 어느 코너에 속하는지 등의 위치 정보를 한꺼번에 고려하지만, 우리의(<code class="language-plaintext highlighter-rouge">c2c matrix</code>) 경우 여기서 이런 위치 정보를 전혀 고려하지 않고 뒤에 두 개의 항에서 따로 고려한다.</p>

<p>정리하면, <code class="language-plaintext highlighter-rouge">c2c</code>는 매장에 진열된 식자재의 상품명 및 설명만 가지고 내가 사야 하는 식재료인지 아닌지 판단하는 작업을 수학적으로 모델링 했다고 볼 수 있겠다. 자연어 처리 맥락에서 바라보면, 특정 토큰의 의미를 알기 위해서 <code class="language-plaintext highlighter-rouge">syntactical</code>한 정보없이 순수하게 나머지 다른 토큰들의 의미를 가중합으로 반영하는 행위에 대응된다.</p>

<p><strong><code class="language-plaintext highlighter-rouge">🗂️ c2p matrix</code></strong></p>

\[c2p = Q_i^c•K_{∂(i,j)}^{rT}\]

<p><code class="language-plaintext highlighter-rouge">content2position</code>의 약자로 수식 우변의 두번째 항, $Q_i^c•K_{∂(i,j)}^{rT}$를 가리킨다. <code class="language-plaintext highlighter-rouge">c2c</code>때와는 다르게 서로 출처가 다른 두 행렬을 사용해 <code class="language-plaintext highlighter-rouge">c2p</code>라는 이름을 붙였다. 내적 대상의 쿼리는 <code class="language-plaintext highlighter-rouge">Input Embedding</code>으로부터 만든 행렬 $Q_i^c$, 키는 <code class="language-plaintext highlighter-rouge">Position Embedding</code>으로부터 만든 행렬 $K_{∂(i,j)}^{rT}$ 을 사용했다. <code class="language-plaintext highlighter-rouge">word context</code>와 <code class="language-plaintext highlighter-rouge">relative position</code>을 서로 대조한다는 것이 무슨 의미를 갖는지 직관적으로 알기 힘드니 장보기 예시를 통해 이해해보자.</p>

<p>구운 달걀과 날달걀의 예시를 들면서 상품명과 설명이 장보기에 중요한 영향을 미친다고 언급했다. 하지만 상품명과 설명이 여전히 단순 <code class="language-plaintext highlighter-rouge">“달걀”</code>으로 적혀 있어도 우리는 이것을 구분해 낼 방법이 있다. 바로 주변에 진열된 상품이 무엇인지 살펴보는 것이다. <code class="language-plaintext highlighter-rouge">“달걀”</code> 바로 옆에 우유, 치즈, 생선, 정육과 같은 신선식품류가 배치되어 있다고 가정해보자. 우리는 우리 눈 앞에 있는 <code class="language-plaintext highlighter-rouge">“달걀”</code>이 날달걀이라고 기대해 봄직하다. 만약 <code class="language-plaintext highlighter-rouge">“달걀”</code> 옆에 쥐포, 말린 오징어, 육포, 과자 같은 간식류 상품들이 배치되어 있다면 어떨까?? 그럼 이 <code class="language-plaintext highlighter-rouge">“달걀”</code>은 충분히 구운 달걀이라고 해석해볼 수 있다. 이처럼 주위에 어떤 다른 상품들이 배치 되어 있는가를 통해 우리가 사려는 물건이 맞는지 대조해보는 행위가 바로 <code class="language-plaintext highlighter-rouge">c2p</code> 에 대응된다. 그렇다면 주위에 어떤 다른 상품들이 배치 되어 있는가 정보를 모아 놓은 것이 바로 $K_{∂(i,j)}^{rT}$가 된다.</p>

<p><strong><code class="language-plaintext highlighter-rouge">🔬 p2c matrix</code></strong></p>

\[p2c = K_j^c•Q_{∂(i,j)}^{rT}\]

<p><code class="language-plaintext highlighter-rouge">Disentangled Self-Attention</code>이 여타 다른 어텐션 기법들과 가장 차별화되는 부분이다. 저자가 논문에서 가장 강조하는 부분이기도 하다. 사실 그런 것치고는 논문 속 설명이 상당히 불친절해 이해하기 참 난해한 개념이다. 이거 설명하고 싶어서 장보기 예시를 생각해내게 되었다. 다시 남편에게 줄 장보기 리스트를 작성하던 시점으로 돌아가보자.</p>

<p>오늘 저녁 메뉴는 차돌박이 된장찌개와 구운 삼겹살이다. 먼저 차돌박이 된장찌개를 만들려면 어떤 재료가 필요할까?? 차돌박이, 된장, 청양고추, 양파, 다진 마늘, 호박과 같은 식자재가 필요할 것이다. 그리고 삼겹살에 필요한 재료를 생각해보자. 생삼겹살과 잡내를 없애는데 필요한 후추와 소금 그리고 구워 먹을 통마늘이  필요하다고 당신은 생각했다. 그럼 이제 이것을 바탕으로 리스트를 작성할 것이다. 어떤 식으로 리스트를 작성하는게 가장 최적일까??</p>

<p><code class="language-plaintext highlighter-rouge">c2c</code>, <code class="language-plaintext highlighter-rouge">c2p</code> 예시와 함께 생각해보면 알 수 있다. <code class="language-plaintext highlighter-rouge">c2c</code>에서는 같은 재료라도 그 용도에 따라서 사야할 품목이 달라진다고 언급한 바있다. <code class="language-plaintext highlighter-rouge">c2p</code> 에서는 정확한 설명이 없어도 주변에 나열된 품목들을 보면서 어떤 상품인지 유추가 가능하다고 했다. 이것을 합쳐보자. 만약 당신이 아래와 같은 순서로 리스트를 적었다고 가정해보겠다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 장보기 리스트 예시1
</span>
<span class="n">차돌박이</span><span class="p">,</span> <span class="n">된장</span><span class="p">,</span> <span class="n">마늘</span><span class="p">,</span> <span class="n">청양고추</span><span class="p">,</span> <span class="n">양파</span><span class="p">,</span> <span class="n">호박</span><span class="p">,</span> <span class="n">삼겹살</span><span class="p">,</span> <span class="n">후추</span><span class="p">,</span> <span class="n">소금</span>
</code></pre></div></div>

<p>아까 필요한 품목을 나열했을 때 분명히 다진 마늘과 통마늘을 동시에 생각했었다. 근데 위처럼 리스트를 작성해서 남편에게 줬다면 남편은 어떤 마늘을 사올까?? 당연히 차돌박이와 된장 그리고 양파 사이에 마늘이 위치한 것을 보고 남편은 국물용 마늘이 필요하구나 싶어서 다진 마늘을 사올 것이다.</p>

<p>그렇다면 반대로 당신이 아래처럼 리스트를 작성했다고 생각해보자.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 장보기 리스트 예시2
</span>
<span class="n">차돌박이</span><span class="p">,</span> <span class="n">된장</span><span class="p">,</span> <span class="n">청양고추</span><span class="p">,</span> <span class="n">양파</span><span class="p">,</span> <span class="n">호박</span><span class="p">,</span> <span class="n">삼겹살</span><span class="p">,</span> <span class="n">마늘</span><span class="p">,</span> <span class="n">후추</span><span class="p">,</span> <span class="n">소금</span>
</code></pre></div></div>

<p>이번에는 삼겹살 구울 때, 같이 구워먹을 통마늘이 필요하구나를 남편이 느낄 수 있을 것이다. 한편 아래와 같은 상황이라면 어떨까??</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 장보기 리스트 예시3
</span>
<span class="n">차돌박이</span><span class="p">,</span> <span class="n">된장</span><span class="p">,</span> <span class="n">마늘</span><span class="p">,</span> <span class="n">청양고추</span><span class="p">,</span> <span class="n">양파</span><span class="p">,</span> <span class="n">호박</span><span class="p">,</span> <span class="n">삼겹살</span><span class="p">,</span> <span class="n">마늘</span><span class="p">,</span> <span class="n">후추</span><span class="p">,</span> <span class="n">소금</span>
</code></pre></div></div>

<p>조금 센스가 있는 남편이라면 된장찌개 국물용 다진마늘과 삼겹살 구이용 통마늘이 동시에 필요하구나라고 유추하고 매장에서 다진마늘, 통마늘이라 써있는 품목을 찾아서 둘 다 사올 것이다. 물론 센스있는 아내라면 애초에 저렇게 애매하게 <code class="language-plaintext highlighter-rouge">마늘</code>이라고 2번 안적고 <code class="language-plaintext highlighter-rouge">다진마늘</code>, <code class="language-plaintext highlighter-rouge">통마늘</code>이라고 용도를 함께 적어줬겠지만 말이다.</p>

<p>이러한 일련의 상황이 바로 <code class="language-plaintext highlighter-rouge">p2c</code>에 대응된다. 그렇다면 아내가 적어준 리스트에서 주변에 위치한 품목들에 따라서 포착되는 대상 품목의 용도나 쓰임새, 의미 등이 바로 행렬 $Q_{∂(i,j)}^{rT}$가 된다.</p>

<p><strong><code class="language-plaintext highlighter-rouge">⚒️ DeBERTa Scale Factor</code></strong><br />
처음에 나열한 수식을 다시 보면 <code class="language-plaintext highlighter-rouge">DeBERTa</code>의 <code class="language-plaintext highlighter-rouge">scale factor</code>는 기존 <code class="language-plaintext highlighter-rouge">Self-Attention</code> 과 다르게 $\sqrt{3d_h}$를 사용한다. 이유가 뭘까?? 기존 방식은 <code class="language-plaintext highlighter-rouge">softmax layer</code>에 전달하는 행렬의 종류가 $Q•K^T$ 한 개다. <code class="language-plaintext highlighter-rouge">DeBERTa</code>의 경우는 3개를 전달하게 된다. 그래서 $d_h$앞에 3을 곱해준 것이다. official repo의 코드를 확인해보면 확실히 알 수 있는데, 어텐션에 사용하는 행렬 종류의 개수를 $d_h$앞에 곱해준다. 아래는 <code class="language-plaintext highlighter-rouge">repo</code>에 올라와 있는 코드의 일부를 발췌한 것이다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># official Disentangled Self-Attention by microsoft from official repo
</span>
<span class="p">...</span><span class="n">중략</span><span class="p">...</span>
<span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">,</span> <span class="n">return_att</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">query_states</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">relative_pos</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">rel_embeddings</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">query_states</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">query_states</span> <span class="o">=</span> <span class="n">hidden_states</span>
    <span class="n">query_layer</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">transpose_for_scores</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">query_proj</span><span class="p">(</span><span class="n">query_states</span><span class="p">),</span> <span class="bp">self</span><span class="p">.</span><span class="n">num_attention_heads</span><span class="p">).</span><span class="nb">float</span><span class="p">()</span>
    <span class="n">key_layer</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">transpose_for_scores</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">key_proj</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">),</span> <span class="bp">self</span><span class="p">.</span><span class="n">num_attention_heads</span><span class="p">).</span><span class="nb">float</span><span class="p">()</span>
    <span class="n">value_layer</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">transpose_for_scores</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">value_proj</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">),</span> <span class="bp">self</span><span class="p">.</span><span class="n">num_attention_heads</span><span class="p">)</span>
    
    <span class="n">rel_att</span> <span class="o">=</span> <span class="bp">None</span>
    <span class="c1"># Take the dot product between "query" and "key" to get the raw attention scores.
</span>    <span class="n">scale_factor</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="k">if</span> <span class="s">'c2p'</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">pos_att_type</span><span class="p">:</span>
        <span class="n">scale_factor</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">if</span> <span class="s">'p2c'</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">pos_att_type</span><span class="p">:</span>
        <span class="n">scale_factor</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">if</span> <span class="s">'p2p'</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">pos_att_type</span><span class="p">:</span>
        <span class="n">scale_factor</span> <span class="o">+=</span> <span class="mi">1</span>
</code></pre></div></div>

<p><strong><code class="language-plaintext highlighter-rouge">👩‍💻 Implementation</code></strong><br />
이렇게 <code class="language-plaintext highlighter-rouge">Disentangled Self-Attention</code>에 대한 모든 내용을 살펴봤다. 실제 구현은 어떻게 해야 하는지 필자가 작성한 파이토치 코드와 함께 알아보자.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Pytorch Implementation of DeBERTa Disentangled Self-Attention
</span>
<span class="k">def</span> <span class="nf">build_relative_position</span><span class="p">(</span><span class="n">x_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
    <span class="s">"""
    Build Relative Position Matrix for Disentangled Self-Attention in DeBERTa
    Args:
        x_size: sequence length of query matrix
    Reference:
        https://github.com/microsoft/DeBERTa/blob/master/DeBERTa/deberta/da_utils.py#L29
        https://arxiv.org/abs/2006.03654
    """</span>
    <span class="n">x_index</span><span class="p">,</span> <span class="n">y_index</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="n">x_size</span><span class="p">),</span> <span class="n">torch</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="n">x_size</span><span class="p">)</span>  <span class="c1"># same as rel_pos in official repo
</span>    <span class="n">rel_pos</span> <span class="o">=</span> <span class="n">x_index</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="o">-</span> <span class="n">y_index</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">rel_pos</span>

<span class="k">def</span> <span class="nf">disentangled_attention</span><span class="p">(</span><span class="n">q</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">k</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">v</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">qr</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">kr</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">dropout</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Dropout</span><span class="p">,</span> <span class="n">mask</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="bp">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
    <span class="s">"""
    Disentangled Self-Attention for DeBERTa, same role as Module "DisentangledSelfAttention" in official Repo
    Args:
        q: content query matrix, shape (batch_size, seq_len, dim_head)
        k: content key matrix, shape (batch_size, seq_len, dim_head)
        v: content value matrix, shape (batch_size, seq_len, dim_head)
        qr: position query matrix, shape (batch_size, 2*max_relative_position, dim_head), r means relative position
        kr: position key matrix, shape (batch_size, 2*max_relative_position, dim_head), r means relative position
        dropout: dropout for attention matrix, default rate is 0.1 from official paper
        mask: mask for attention matrix, shape (batch_size, seq_len, seq_len), apply before softmax layer
    Math:
        c2c = torch.matmul(q, k.transpose(-1, -2))  # A_c2c
        c2p = torch.gather(torch.matmul(q, kr.transpose(-1, -2)), dim=-1, index=c2p_pos)
        p2c = torch.gather(torch.matmul(qr, k.transpose(-1, -2)), dim=-2, index=c2p_pos)
        Attention Matrix = c2c + c2p + p2c
        A = softmax(Attention Matrix/sqrt(3*D_h)), SA(z) = Av
    Notes:
        dot_scale(range 1 ~ 3): scale factor for Q•K^T result, sqrt(3*dim_head) from official paper by microsoft,
        3 means that use full attention matrix(c2c, c2p, p2c), same as number of using what kind of matrix
        default 1, c2c is always used and c2p &amp; p2c is optional
    References:
        https://github.com/microsoft/DeBERTa/blob/master/DeBERTa/deberta/disentangled_attention.py
        https://arxiv.org/pdf/1803.02155.pdf
        https://arxiv.org/abs/2006.03654
        https://arxiv.org/abs/2111.09543
        https://arxiv.org/abs/1901.02860
        https://arxiv.org/abs/1906.08237
    """</span>
    <span class="n">scale_factor</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">c2c</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">))</span>  <span class="c1"># A_c2c
</span>
    <span class="n">c2p_att</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">kr</span><span class="p">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">))</span>
    <span class="n">c2p_pos</span> <span class="o">=</span> <span class="n">build_relative_position</span><span class="p">(</span><span class="n">q</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="o">+</span> <span class="n">kr</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">/</span> <span class="mi">2</span>  <span class="c1"># same as rel_pos in official repo
</span>    <span class="n">c2p_pos</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">c2p_pos</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">kr</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span><span class="p">).</span><span class="n">repeat</span><span class="p">(</span><span class="n">q</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">c2p</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">gather</span><span class="p">(</span><span class="n">c2p_att</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">c2p_pos</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">c2p</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">scale_factor</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="n">p2c_att</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">qr</span><span class="p">,</span> <span class="n">k</span><span class="p">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">))</span>
    <span class="n">p2c</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">gather</span><span class="p">(</span><span class="n">p2c_att</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">2</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">c2p_pos</span><span class="p">)</span>  <span class="c1"># same as torch.gather(k•qr^t, dim=-1, index=c2p_pos)
</span>    <span class="k">if</span> <span class="n">p2c</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">scale_factor</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="n">dot_scale</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">scale_factor</span> <span class="o">*</span> <span class="n">q</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]))</span>  <span class="c1"># from official paper by microsoft
</span>    <span class="n">attention_matrix</span> <span class="o">=</span> <span class="p">(</span><span class="n">c2c</span> <span class="o">+</span> <span class="n">c2p</span> <span class="o">+</span> <span class="n">p2c</span><span class="p">)</span> <span class="o">/</span> <span class="n">dot_scale</span>  <span class="c1"># Attention Matrix = A_c2c + A_c2r + A_r2c
</span>    <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">attention_matrix</span> <span class="o">=</span> <span class="n">attention_matrix</span><span class="p">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">mask</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="s">'-inf'</span><span class="p">))</span>  <span class="c1"># Padding Token Masking
</span>    <span class="n">attention_dist</span> <span class="o">=</span> <span class="n">dropout</span><span class="p">(</span>
        <span class="n">F</span><span class="p">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">attention_matrix</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="p">)</span>
    <span class="n">attention_matrix</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">attention_dist</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">attention_matrix</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">p2c</code> 를 구하는 과정의 코드라인에 주목해보자. 논문에 기재된 수식($K_j^c•Q_{∂(i,j)}^{rT}$)과 다르게, 쿼리와 키의 순서를 뒤집었다. 그래서 <code class="language-plaintext highlighter-rouge">torch.gather</code>의 차원 매개변수 <code class="language-plaintext highlighter-rouge">dim</code>를 <code class="language-plaintext highlighter-rouge">c2p</code>의 상황과 다르게 -<code class="language-plaintext highlighter-rouge">2</code>로 초기화하게 되었다. 내적하는 항의 순서를 뒤집은 것으로 인해 우리가 추출하고 싶은 대상 값인 상대 위치 임베딩이 <code class="language-plaintext highlighter-rouge">-2</code>번째 차원에 위치 하게 되기 때문이다.</p>

<h4 id="enhanced-mask-decoder"><strong><code class="language-plaintext highlighter-rouge">😷 Enhanced Mask Decoder</code></strong></h4>
<p><code class="language-plaintext highlighter-rouge">DeBERTa</code>의 설계 목적은 2가지 위치 정보를 적절히 섞어서 최대한 풍부한 임베딩을 만드는 것이라고 했다. 상대 위치 임베딩은 <code class="language-plaintext highlighter-rouge">Disentangled Self-Attention</code>을 통해 포착한다는 것을 이제 알았다. 그럼 절대 위치 임베딩은 어떤 식으로 모델링해줘야 할까?? 그 물음에 답은 바로 <code class="language-plaintext highlighter-rouge">EMD</code>라 불리는 <code class="language-plaintext highlighter-rouge">Enhanced Mask Decoder</code>에 있다. <code class="language-plaintext highlighter-rouge">EMD</code>의 원리에 대해 공부하기 전에 왜 절대 위치 임베딩이 <code class="language-plaintext highlighter-rouge">NLU</code>에 필요한지 짚고 넘어가자.</p>

<p><strong><center><i>a new <u>"store"</u> opened beside the new <u>"mall”</u></i></center></strong></p>

<p>위 문장은 저자가 논문에서 <code class="language-plaintext highlighter-rouge">Absolute Position Embedding</code>의 필요성을 역설할 때 사용한 예시 문장이다. 과연 상대 위치 임베딩만 사용해서 <strong><code class="language-plaintext highlighter-rouge">store</code></strong>와 <strong><code class="language-plaintext highlighter-rouge">mall</code></strong>의 차이를 잘 구별할 수 있을까 생각해보자. 앞서 우리는 상대 위치 임베딩을 <strong>대상 토큰과 그 나머지 토큰 사이의 위치 변화에 따라 발생하는 파생적인 맥락 정보를 담은 행렬</strong>이라고 정의한 바 있다. 다시 말해, 대상 토큰의 의미를 주변에 어떤 <code class="language-plaintext highlighter-rouge">context</code>가 있는지 파악해 통해 이해해보겠다는 것이다.</p>

<p>예시 문장을 다시 보자. 두 대상 단어 모두 주위에 비슷한 의미를 갖는 단어들이 위치해 있다. 이런 경우 상대 위치 임베딩만으로는 시퀀스 내부에서 <strong><code class="language-plaintext highlighter-rouge">store</code></strong>와 <strong><code class="language-plaintext highlighter-rouge">mall</code></strong>의 의미 차이를 모델이 명확하게 이해하기 매우 어려울 것이다. 현재 상황에서 두 단어의 뉘앙스 차이는 결국 문장의 주어냐 목적어냐 하는 <code class="language-plaintext highlighter-rouge">syntactical</code>한 정보에 의해서 결정된다. <code class="language-plaintext highlighter-rouge">syntactical</code>한 정보의 필요성은 바로 절대 위치 임베딩이 <code class="language-plaintext highlighter-rouge">NLU</code>에 꼭 필요한 이유에 대응된다.</p>

<p align="center">
<img src="/assets/images/deberta/emd_overview.png" alt="Enhanced Mask Decoder Overview" class="align-center image-caption" width="60%&quot;, height=&quot;50%" />
<strong><em><a href="https://arxiv.org/abs/2006.03654">Enhanced Mask Decoder Overview</a></em></strong>
</p>

<p><strong><code class="language-plaintext highlighter-rouge">🤔 why named decoder</code></strong><br />
필자는 처음 논문을 읽었을 때 <code class="language-plaintext highlighter-rouge">Decoder</code>라는 이름을 보면서 참 의아했다. 분명 <code class="language-plaintext highlighter-rouge">Only-Encoder</code> 모델로 알고 있는데 어찌하여 이름에 디코더가 붙는 모듈이 있는 것인가. 그렇다고 이름을 저렇게 붙인 의도를 설명하는 것도 아니다. 그래서 필자가 스스로 추측해봤다.</p>

<p><code class="language-plaintext highlighter-rouge">DeBERTa</code>는 <code class="language-plaintext highlighter-rouge">pre-train task</code> 로 <code class="language-plaintext highlighter-rouge">MLM</code>을 사용했다. <code class="language-plaintext highlighter-rouge">MLM</code>이 무엇인가?? 바로 마스킹된 자리에 적절한 토큰을 찾는 빈칸 채우기 문제다. 영미권에서는 이것을 <code class="language-plaintext highlighter-rouge">denoising</code>한다고 표현하기도 하는데, <code class="language-plaintext highlighter-rouge">Absolute Position Embedding</code>이 바로 이 <code class="language-plaintext highlighter-rouge">denoising</code>에 지대한 영향력을 미친다는 언급을 논문에서 찾아볼 수 있다. 따라서 <code class="language-plaintext highlighter-rouge">denoising</code> 성능에 큰 영향을 주는 <code class="language-plaintext highlighter-rouge">Absolute Position Embedding</code>을 활용한다고 해서 이름에 <code class="language-plaintext highlighter-rouge">decoder</code>를 붙였지 않았나 예상해본다.</p>

<p>논문에 같이 실린 그림을 통해서도 추측이 가능하다. <code class="language-plaintext highlighter-rouge">EMD</code> 의 구조를 설명하면서 옆에 BERT의 모식도도 함께 제공하는데, <code class="language-plaintext highlighter-rouge">BERT</code>에는 <code class="language-plaintext highlighter-rouge">Decoder</code>가 전혀 없다. 그런데도 이름을 <code class="language-plaintext highlighter-rouge">BERT decoding layer</code>라고 부르는 것보면 필자의 추측에 좀 더 정당성을 부여하는 것 같다.</p>

<p>(+ 추가) offical repo code에서도 <code class="language-plaintext highlighter-rouge">EMD</code>가 우리가 아는 그 <code class="language-plaintext highlighter-rouge">Encoder</code>를 사용한다는 사실을 확인할 수 있다.</p>

<p><strong><code class="language-plaintext highlighter-rouge">🤷‍♂️ How to add Absolute Position</code></strong></p>

<p align="center">
<img src="/assets/images/deberta/deberta_overview.png" alt="DeBERTa Model Structure" class="align-center image-caption" width="45%&quot;, height=&quot;50%" />
<strong><em><a href="https://www.youtube.com/watch?v=gcMyKUXbY8s&amp;t=838s&amp;ab_channel=%EA%B3%A0%EB%A0%A4%EB%8C%80%ED%95%99%EA%B5%90%EC%82%B0%EC%97%85%EA%B2%BD%EC%98%81%EA%B3%B5%ED%95%99%EB%B6%80DSBA%EC%97%B0%EA%B5%AC%EC%8B%A4">DeBERTa Model Structure</a></em></strong>
</p>

<p>이제 <code class="language-plaintext highlighter-rouge">EMD</code>가 무엇이며, <code class="language-plaintext highlighter-rouge">Absolute Position</code>을 어떻게 모델에 추가하는지 알아보자. <code class="language-plaintext highlighter-rouge">EMD</code>는 <code class="language-plaintext highlighter-rouge">MLM</code> 성능을 높이기 위해 고안된 구조다. 그래서 토큰 예측을 위한 <code class="language-plaintext highlighter-rouge">feedforward &amp; softmax</code> 레이어 직전에 쌓는다. 몇개의 <code class="language-plaintext highlighter-rouge">EMD</code>를 쌓을 것인지는 하이퍼파리미터이며, 저자의 실험 결과 <code class="language-plaintext highlighter-rouge">2</code>개 사용하는게 가장 효율적이라고 한다. 새롭게 인코더 블럭을 쌓지 않고 <code class="language-plaintext highlighter-rouge">Disentangled-Attention</code> 레이어의 가장 마지막 인코더 블럭과 가중치를 공유하는 형태로 구현한다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># EMD Implementation Example
</span>
<span class="k">class</span> <span class="nc">EnhancedMaskDecoder</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">encoder</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">nn</span><span class="p">.</span><span class="n">ModuleList</span><span class="p">],</span> <span class="n">dim_model</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1024</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">EnhancedMaskDecoder</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">emd_layers</span> <span class="o">=</span> <span class="n">encoder</span>

<span class="k">class</span> <span class="nc">DeBERTa</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">max_seq</span><span class="p">:</span> <span class="mi">512</span><span class="p">,</span> <span class="n">N</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">24</span><span class="p">,</span> <span class="n">N_EMD</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="n">dim_model</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1024</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">16</span><span class="p">,</span> <span class="n">dim_ffn</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">4096</span><span class="p">,</span> <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
			<span class="c1"># Init Sub-Blocks &amp; Modules
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">DeBERTaEncoder</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">max_seq</span><span class="p">,</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">N</span><span class="p">,</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span><span class="p">,</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">num_heads</span><span class="p">,</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">dim_ffn</span><span class="p">,</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">dropout_prob</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">emd_layers</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="p">.</span><span class="n">encoder</span><span class="p">.</span><span class="n">encoder_layers</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">N_EMD</span><span class="p">)].</span> <span class="c1"># weight share
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">emd_encoder</span> <span class="o">=</span> <span class="n">EnhancedMaskDecoder</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">emd_layers</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span><span class="p">)</span>
</code></pre></div></div>

<p>따라서 <code class="language-plaintext highlighter-rouge">N_EMD=2</code> 로 설정한다는 것은 결국, <code class="language-plaintext highlighter-rouge">Disentangled-Attention</code> 레이어의 가장 마지막 인코더 블럭을 2개 더 쌓는 것과 동치다. 대신 인코더의 <code class="language-plaintext highlighter-rouge">linear projection</code> 레이어의 입력값이 다르다. <code class="language-plaintext highlighter-rouge">Disentangled-Attention</code> 의 행렬 $Q^c, K^c, V^c$는 이전 블럭의 <code class="language-plaintext highlighter-rouge">hidden_states</code> 값인 행렬 $H$를 입력으로, 행렬 $Q^r, K^r$은 레이어의 위치에 상관없이 모두 같은 값의 <code class="language-plaintext highlighter-rouge">Relative Position Embedding</code> 을 입력으로 사용한다.</p>

<p>반면, <code class="language-plaintext highlighter-rouge">EMD</code> 맨 처음 인코더 블럭의 행렬 $Q^c$는 바로 직전 블럭의 <code class="language-plaintext highlighter-rouge">hidden_states</code> 에 <code class="language-plaintext highlighter-rouge">Absolute Position Embedding</code>을 더한 값을 입력으로 사용한다. 이후 나머지 블럭에는 <code class="language-plaintext highlighter-rouge">Disentangled-Attention</code> 와 마찬가지로 이전 블럭의 <code class="language-plaintext highlighter-rouge">hidden_states</code> 를 사용한다. 행렬 $K^c, V^c$는 블럭 순서에 상관없이 이전 블럭의 <code class="language-plaintext highlighter-rouge">hidden_states</code> 만 가지고 <code class="language-plaintext highlighter-rouge">linear projection</code>을 수행한다. 그리고 행렬 $Q^r, K^r$ 역시 같은 값의 <code class="language-plaintext highlighter-rouge">Relative Position Embedding</code> 을 입력으로 사용한다.</p>

<p>사실 필자는 논문만 읽었을 때 <code class="language-plaintext highlighter-rouge">EMD</code>도 <code class="language-plaintext highlighter-rouge">Relative Position</code> 정보를 주입해 <code class="language-plaintext highlighter-rouge">Disengtanled-Attention</code>을 수행한다고 전혀 생각하지 못했다. 이는 논문의 설명이 상당히 불친절한 덕분인데, 논문에 이와 관련해서 자세한 설명도 없고 <code class="language-plaintext highlighter-rouge">Absolute Position Embedding</code>을 사용하는 레이어라서 당연히 일반적인 <code class="language-plaintext highlighter-rouge">Self-Attention</code>을 사용할 것이라고 생각했던 것이다.</p>

<p>필자는 여기서 <code class="language-plaintext highlighter-rouge">Absolute Position</code>이 왜 필요한지도 알겠고 그래서 행렬합으로 더해서 어텐션을 수행하는 것도 잘 알겠는데 왜 굳이 가장 마지막 레이어에서 이걸 할까?? 하는 의문이 들었다. 일반 <code class="language-plaintext highlighter-rouge">Self-Attention</code>처럼 맨 처음에 더하고 시작하면 안될까??</p>

<p>저자의 실험에 따르면 <code class="language-plaintext highlighter-rouge">Absolute Position</code> 을 처음에 추가하는 것보다 <code class="language-plaintext highlighter-rouge">EMD</code>처럼 가장 마지막에 더해주는게 성능이 더 좋았다고 한다. 그 이유로 <code class="language-plaintext highlighter-rouge">Absolute Position</code>를 초반에 추가하면 모델이 <code class="language-plaintext highlighter-rouge">Relative Position</code>을 학습하는데 방해가 되는 것 같다는 추측을 함께 서술하고 있다. <strong>그렇다면 왜 방해가 되는 것일까??</strong></p>

<p>필자의 뇌피셜이지만 이것 역시 <code class="language-plaintext highlighter-rouge">blessing of dimensionality</code> 에서 파생된 문제라고 생각한다. 일단 용어의 뜻부터 알아보자. <code class="language-plaintext highlighter-rouge">blessing of dimensionality</code> 란, 고차원 공간에서 무작위로 서로 다른 벡터 두개를 선택하면 두 벡터는 거의 대부분 <code class="language-plaintext highlighter-rouge">approximate orthogonality</code>를 갖는 현상을 설명하는 용어다. 무조건 성립하는 성질은 아니고 확률론적인 접근이라는 것을 명심하자. 아무튼 직교하는 두 벡터는 내적값이 0에 수렴한다. 즉, 두 벡터는 서로에게 영향을 미치지 못한다는 것이다.</p>

<p>이것은 <code class="language-plaintext highlighter-rouge">Transformer</code>에서 <code class="language-plaintext highlighter-rouge">Input Embedding</code>과 <code class="language-plaintext highlighter-rouge">Absolute Position Embedding</code>을 행렬합으로 더해도 좋은 학습 결과를 얻을 수 있는 이유가 된다. 다시 말해서, <code class="language-plaintext highlighter-rouge">hidden states space</code> 에서 <code class="language-plaintext highlighter-rouge">Input Embedding</code> 과 <code class="language-plaintext highlighter-rouge">Absolute Position Embedding</code> 역시 개별 벡터가 <code class="language-plaintext highlighter-rouge">span</code> 하는 부분 공간 끼리는 서로 직교할 가능성이 매우 높다는 것을 의미한다. 따라서 서로 다른 출처를 통해 만들어진 두 행렬을 더해도 서로에게 영향을 미치지 못할 것이고 그로 인해 모델이 <code class="language-plaintext highlighter-rouge">Input</code>과 <code class="language-plaintext highlighter-rouge">Position</code> 정보를 따로 잘 학습할 수 있을 것이라 기대해볼 수 있다.</p>

<p align="center">
<img src="/assets/images/deberta/latent_space.png" alt="hidden states vector space example" class="align-center image-caption" width="60%&quot;, height=&quot;50%" />
<strong><em>hidden states vector space example</em></strong>
</p>

<p>이제 다시 <code class="language-plaintext highlighter-rouge">DeBERTa</code> 경우로 돌아와보자. 위 그림의 파란색 직선을 <code class="language-plaintext highlighter-rouge">Input Embedding</code>, 빨간색 직선을 <code class="language-plaintext highlighter-rouge">Absolute Position Embedding</code>, 왼쪽의 보라색 직선을 <code class="language-plaintext highlighter-rouge">Relative Position Embedding</code>이라고 가정하자. <code class="language-plaintext highlighter-rouge">blessing of dimensionality</code>에 의해 <code class="language-plaintext highlighter-rouge">word con text</code> 정보(파란 직선)와 <code class="language-plaintext highlighter-rouge">position</code> 정보(빨강, 보라 직선)는 그림처럼 서로 근사 직교할 가능성이 매우 높다. 여기부터 필자의 뇌피셜이 들어가는데, 보라색 직선과 빨강색 직선은 성격이 좀 다르지만 결국 둘 다 시퀀스의 <code class="language-plaintext highlighter-rouge">position</code> 정보를 나타낸다는 점에서 뿌리는 같다고 볼 수 있다. 따라서 실제 <code class="language-plaintext highlighter-rouge">hidden states</code> 공간에서 어떤 식으로 맵핑될지는 잘 모르겠지만, 서로 직교하는 형태는 아닐 것이라 추측할 수 있다.</p>

<p>그렇다면 <code class="language-plaintext highlighter-rouge">Absolute Position</code>을 모델 극초반에 더해준다고 생각해보자. 인코더에 들어가는 행렬은 결국 위 그림의 초록색 직선으로 표현될 것이다. 파란색 직선과 빨간색 직선이 근사 직교한다는 가정하에 두 백터의 합은 두 벡터의 45도 정도 되는 곳에 위치하게(초록색 직선) 될 것이다. 그렇다면 보라색 직선과 초록색 직선의 관계 역시 근사 직교에서 서로 간섭하는 형태로 변화한다. 따서 <code class="language-plaintext highlighter-rouge">EMD</code>를 극초반에 사용하면 간섭이 발생해 모델이 <code class="language-plaintext highlighter-rouge">Relative Position</code> 정보를 제대로 학습하지 못할 것이다.</p>

<p><strong><code class="language-plaintext highlighter-rouge">👩‍💻 Implementation</code></strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Pytorch Implementation of DeBERTa Enhanced Mask Decoder
</span>
<span class="k">class</span> <span class="nc">EnhancedMaskDecoder</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="s">"""
    Class for Enhanced Mask Decoder module in DeBERTa, which is used for Masked Language Model (Pretrain Task)
    Word 'Decoder' means that denoise masked token by predicting masked token
    In official paper &amp; repo, they might use 2 EMD layers for MLM Task
    And this layer's key &amp; value input is output from last disentangled self-attention encoder layer,
    Also, all of them can share parameters and this layer also do disentangled self-attention
    In official repo, they implement this layer so hard coding that we can't understand directly &amp; easily
    So, we implement this layer with our own style, as closely as possible to paper statement
    Notes:
        Also we temporarily implement only extract token embedding, not calculating logit, loss for MLM Task yet
        MLM Task will be implemented ASAP
    Args:
        encoder: list of nn.ModuleList, which is (N_EMD * last encoder layer) from DeBERTaEncoder
    References:
        https://arxiv.org/abs/2006.03654
        https://arxiv.org/abs/2111.09543
        https://github.com/microsoft/DeBERTa/blob/master/DeBERTa/apps/models/masked_language_model.py
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">encoder</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">nn</span><span class="p">.</span><span class="n">ModuleList</span><span class="p">],</span> <span class="n">dim_model</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1024</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">EnhancedMaskDecoder</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">emd_layers</span> <span class="o">=</span> <span class="n">encoder</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">layer_norm</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">dim_model</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">emd_context_layer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">,</span> <span class="n">abs_pos_emb</span><span class="p">,</span> <span class="n">rel_pos_emb</span><span class="p">,</span> <span class="n">mask</span><span class="p">):</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">query_states</span> <span class="o">=</span> <span class="n">hidden_states</span> <span class="o">+</span> <span class="n">abs_pos_emb</span>  <span class="c1"># "I" in official paper,
</span>        <span class="k">for</span> <span class="n">emd_layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">emd_layers</span><span class="p">:</span>
            <span class="n">query_states</span> <span class="o">=</span> <span class="n">emd_layer</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">pos_x</span><span class="o">=</span><span class="n">rel_pos_emb</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="n">mask</span><span class="p">,</span> <span class="n">emd</span><span class="o">=</span><span class="n">query_states</span><span class="p">)</span>
            <span class="n">outputs</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">query_states</span><span class="p">)</span>
        <span class="n">last_hidden_state</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">layer_norm</span><span class="p">(</span><span class="n">query_states</span><span class="p">)</span>  <span class="c1"># because of applying pre-layer norm
</span>        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">stack</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">).</span><span class="n">to</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">last_hidden_state</span><span class="p">,</span> <span class="n">hidden_states</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">abs_pos_emb</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">rel_pos_emb</span><span class="p">,</span> <span class="n">mask</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
        <span class="s">"""
        hidden_states: output from last disentangled self-attention encoder layer
        abs_pos_emb: absolute position embedding
        rel_pos_emb: relative position embedding
        """</span>
        <span class="n">last_hidden_state</span><span class="p">,</span> <span class="n">emd_hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">emd_context_layer</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">abs_pos_emb</span><span class="p">,</span> <span class="n">rel_pos_emb</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">last_hidden_state</span><span class="p">,</span> <span class="n">emd_hidden_states</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">emd_context_layer</code> 메서드에서 <code class="language-plaintext highlighter-rouge">Absolute Position</code> 정보를 추가해주는 부분을 제외하면 일반 <code class="language-plaintext highlighter-rouge">Encoder</code> 객체의 동작과 동일하다. 또한 DeBERTa는 모든 레이어가 같은 시점의 forward pass 때, 동일한 가중치의 <code class="language-plaintext highlighter-rouge">Relative Position Embedding</code>을 사용해야 하는데, <code class="language-plaintext highlighter-rouge">EMD</code> 역시 예외는 아니기 때문에 반드시 최상위 객체에서 초기화한 <code class="language-plaintext highlighter-rouge">Relative Position Embedding</code>을 똑같이 매개변수로 전달해줘야 한다.</p>

<p>그리고 마지막으로 객체에서 사용하는 <code class="language-plaintext highlighter-rouge">emd_layers</code> 는 모두 <code class="language-plaintext highlighter-rouge">Disentangled-Attention</code> 레이어의 가장 마지막 인코더라는 사실을 잊지 말자.</p>

<h4 id="multi-head-attention"><strong><code class="language-plaintext highlighter-rouge">👩‍👩‍👧‍👦 Multi-Head Attention</code></strong></h4>

<p>이제 나머지 블럭들에 대해서 살펴보겠다. 원리나 의미는 이미 <code class="language-plaintext highlighter-rouge">Transformer</code> 리뷰에서 모두 살펴봤기 때문에 생략하고, 구현상 특이점에 대해서만 언급하려고 한다. 먼저 <code class="language-plaintext highlighter-rouge">Single-Head Atttention</code> 코드를 보자.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Pytorch Implementation of Single Attention Head
</span>
<span class="k">class</span> <span class="nc">AttentionHead</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="s">"""
    In this class, we implement workflow of single attention head in DeBERTa-Large
    This class has same role as Module "BertAttention" in official Repo (bert.py)
    Args:
        dim_model: dimension of model's latent vector space, default 1024 from official paper
        dim_head: dimension of each attention head, default 64 from official paper (1024 / 16)
        dropout: dropout rate for attention matrix, default 0.1 from official paper
    Math:
        Attention Matrix = c2c + c2p + p2c
        A = softmax(Attention Matrix/sqrt(3*D_h)), SA(z) = Av
    Reference:
        https://arxiv.org/abs/1706.03762
        https://arxiv.org/abs/2006.03654
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim_model</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1024</span><span class="p">,</span> <span class="n">dim_head</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">64</span><span class="p">,</span> <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">AttentionHead</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span> <span class="o">=</span> <span class="n">dim_model</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dim_head</span> <span class="o">=</span> <span class="n">dim_head</span>  <span class="c1"># 1024 / 16 = 64
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">dot_scale</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">dim_head</span><span class="p">))</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">fc_q</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">dim_head</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">fc_k</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">dim_head</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">fc_v</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">dim_head</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">fc_qr</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">dim_head</span><span class="p">)</span>  <span class="c1"># projector for Relative Position Query matrix
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">fc_kr</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">dim_head</span><span class="p">)</span>  <span class="c1"># projector for Relative Position Key matrix
</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">pos_x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">mask</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">emd</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="bp">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">qr</span><span class="p">,</span> <span class="n">kr</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">fc_q</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="bp">self</span><span class="p">.</span><span class="n">fc_k</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="bp">self</span><span class="p">.</span><span class="n">fc_v</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="bp">self</span><span class="p">.</span><span class="n">fc_qr</span><span class="p">(</span><span class="n">pos_x</span><span class="p">),</span> <span class="bp">self</span><span class="p">.</span><span class="n">fc_kr</span><span class="p">(</span><span class="n">pos_x</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">emd</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">q</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">fc_q</span><span class="p">(</span><span class="n">emd</span><span class="p">)</span>
        <span class="n">attention_matrix</span> <span class="o">=</span> <span class="n">disentangled_attention</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">qr</span><span class="p">,</span> <span class="n">kr</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">dropout</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">attention_matrix</span>
</code></pre></div></div>

<p>동작 자체는 동일하지만, 상대 위치 정보에 대한 <code class="language-plaintext highlighter-rouge">linear projection</code> 레이어가 추가 되었다. 그리고 <code class="language-plaintext highlighter-rouge">Enhanced Mask Decoder</code> 를 위해 <code class="language-plaintext highlighter-rouge">forward</code> 메서드에 조건문을 활용하여 <code class="language-plaintext highlighter-rouge">Decoding</code>하는 시점에는 <code class="language-plaintext highlighter-rouge">hidden_states + absolute position embedding</code> 으로 행렬 $Q^c$를 표현하게 구현했다. 이렇게 구현하면 <code class="language-plaintext highlighter-rouge">EMD</code> 를 위해 따로 <code class="language-plaintext highlighter-rouge">AttentionHead</code>를 구현할 필요가 없어서 코드 간소화가 된다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Pytorch Implementation of Multi-Head Attention
</span>
<span class="k">class</span> <span class="nc">MultiHeadAttention</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="s">"""
    In this class, we implement workflow of Multi-Head Self-Attention for DeBERTa-Large
    This class has same role as Module "BertAttention" in official Repo (bert.py)
    In official repo, they use post-layer norm, but we use pre-layer norm which is more stable &amp; efficient for training
    Args:
        dim_model: dimension of model's latent vector space, default 1024 from official paper
        num_heads: number of heads in MHSA, default 16 from official paper for Transformer
        dim_head: dimension of each attention head, default 64 from official paper (1024 / 16)
        dropout: dropout rate, default 0.1
    Math:
        Attention Matrix = c2c + c2p + p2c
        A = softmax(Attention Matrix/sqrt(3*D_h)), SA(z) = Av
    Reference:
        https://arxiv.org/abs/1706.03762
        https://arxiv.org/abs/2006.03654
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim_model</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1024</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">16</span><span class="p">,</span> <span class="n">dim_head</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">64</span><span class="p">,</span> <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MultiHeadAttention</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span> <span class="o">=</span> <span class="n">dim_model</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dim_head</span> <span class="o">=</span> <span class="n">dim_head</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">dropout</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">attention_heads</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">ModuleList</span><span class="p">(</span>
            <span class="p">[</span><span class="n">AttentionHead</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">dim_head</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">dropout</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">num_heads</span><span class="p">)]</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">fc_concat</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">rel_pos_emb</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">mask</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">emd</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="bp">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="s">""" x is already passed nn.Layernorm """</span>
        <span class="k">assert</span> <span class="n">x</span><span class="p">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">3</span><span class="p">,</span> <span class="sa">f</span><span class="s">'Expected (batch, seq, hidden) got </span><span class="si">{</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="s">'</span>
        <span class="n">attention_output</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">fc_concat</span><span class="p">(</span>
            <span class="n">torch</span><span class="p">.</span><span class="n">cat</span><span class="p">([</span><span class="n">head</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">rel_pos_emb</span><span class="p">,</span> <span class="n">mask</span><span class="p">,</span> <span class="n">emd</span><span class="p">)</span> <span class="k">for</span> <span class="n">head</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">attention_heads</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">attention_output</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">MultiHeadAttention</code> 객체는 단일 <code class="language-plaintext highlighter-rouge">AttentionHead</code> 객체를 호출할 때 <code class="language-plaintext highlighter-rouge">rel_pos_emb</code> 를 매개변수로 전달해야 한다는 점만 기억하면 된다.</p>

<h4 id="feed-forward-network"><strong><code class="language-plaintext highlighter-rouge">🔬 Feed Forward Network</code></strong></h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Pytorch Implementation of FeedForward Network
</span>
<span class="k">class</span> <span class="nc">FeedForward</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="s">"""
    Class for Feed-Forward Network module in transformer
    In official paper, they use ReLU activation function, but GELU is better for now
    We change ReLU to GELU &amp; add dropout layer
    Args:
        dim_model: dimension of model's latent vector space, default 512
        dim_ffn: dimension of FFN's hidden layer, default 2048 from official paper
        dropout: dropout rate, default 0.1
    Math:
        FeedForward(x) = FeedForward(LN(x))+x
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim_model</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">512</span><span class="p">,</span> <span class="n">dim_ffn</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2048</span><span class="p">,</span> <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">FeedForward</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">ffn</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">dim_model</span><span class="p">,</span> <span class="n">dim_ffn</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">GELU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">dropout</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">dim_ffn</span><span class="p">,</span> <span class="n">dim_model</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">dropout</span><span class="p">),</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">ffn</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</code></pre></div></div>

<p>역시 기존 <code class="language-plaintext highlighter-rouge">Transformer</code>, <code class="language-plaintext highlighter-rouge">BERT</code>와 다른게 없다.</p>

<h4 id="debertaencoderlayer"><strong><code class="language-plaintext highlighter-rouge">📘 DeBERTaEncoderLayer</code></strong></h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Pytorch Implementation of DeBERTaEncoderLayer(single Disentangled-Attention Encoder Block)
</span>
<span class="k">class</span> <span class="nc">DeBERTaEncoderLayer</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="s">"""
    Class for encoder model module in DeBERTa-Large
    In this class, we stack each encoder_model module (Multi-Head Attention, Residual-Connection, LayerNorm, FFN)
    This class has same role as Module "BertEncoder" in official Repo (bert.py)
    In official repo, they use post-layer norm, but we use pre-layer norm which is more stable &amp; efficient for training
    References:
        https://github.com/microsoft/DeBERTa/blob/master/DeBERTa/deberta/bert.py
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim_model</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1024</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">16</span><span class="p">,</span> <span class="n">dim_ffn</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">4096</span><span class="p">,</span> <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">DeBERTaEncoderLayer</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">self_attention</span> <span class="o">=</span> <span class="n">MultiHeadAttention</span><span class="p">(</span>
            <span class="n">dim_model</span><span class="p">,</span>
            <span class="n">num_heads</span><span class="p">,</span>
            <span class="nb">int</span><span class="p">(</span><span class="n">dim_model</span> <span class="o">/</span> <span class="n">num_heads</span><span class="p">),</span>
            <span class="n">dropout</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">layer_norm1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">dim_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">layer_norm2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">dim_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">dropout</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">ffn</span> <span class="o">=</span> <span class="n">FeedForward</span><span class="p">(</span>
            <span class="n">dim_model</span><span class="p">,</span>
            <span class="n">dim_ffn</span><span class="p">,</span>
            <span class="n">dropout</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">pos_x</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Embedding</span><span class="p">,</span> <span class="n">mask</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">emd</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="bp">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="s">""" rel_pos_emb is fixed for all layer in same forward pass time """</span>
        <span class="n">ln_x</span><span class="p">,</span> <span class="n">ln_pos_x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">layer_norm1</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="bp">self</span><span class="p">.</span><span class="n">layer_norm1</span><span class="p">(</span><span class="n">pos_x</span><span class="p">)</span>  <span class="c1"># pre-layer norm, weight share
</span>        <span class="n">residual_x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">dropout</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">self_attention</span><span class="p">(</span><span class="n">ln_x</span><span class="p">,</span> <span class="n">ln_pos_x</span><span class="p">,</span> <span class="n">mask</span><span class="p">,</span> <span class="n">emd</span><span class="p">))</span> <span class="o">+</span> <span class="n">x</span>

        <span class="n">ln_x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">layer_norm2</span><span class="p">(</span><span class="n">residual_x</span><span class="p">)</span>
        <span class="n">fx</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">ffn</span><span class="p">(</span><span class="n">ln_x</span><span class="p">)</span> <span class="o">+</span> <span class="n">residual_x</span>
        <span class="k">return</span> <span class="n">fx</span>
</code></pre></div></div>

<p>official code와 다르게 <code class="language-plaintext highlighter-rouge">pre-layernorm</code> 을 사용해 구현했다. <code class="language-plaintext highlighter-rouge">pre-layernorm</code>에 대해 궁금하다면 <a href="https://qcqced123.github.io/nlp/transformer#encoderlayer"><strong><em>여기</em></strong></a>를 클릭해 확인해보자.</p>

<h4 id="debertaencoder"><strong><code class="language-plaintext highlighter-rouge">📚 DeBERTaEncoder</code></strong></h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Pytorch Implementation of DeBERTaEncoderr(N stacked DeBERTaEncoderLayer)
</span>
<span class="k">class</span> <span class="nc">DeBERTaEncoder</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="s">"""
    In this class, 1) encode input sequence, 2) make relative position embedding, 3) stack N DeBERTaEncoderLayer
    This class's forward output is not integrated with EMD Layer's output
    Output have ONLY result of disentangled self-attention
    All of ops order is from official paper &amp; repo by microsoft, but ops operating is slightly different,
    Because they use custom ops, e.g. XDropout, XSoftmax, ..., we just apply pure pytorch ops
    Args:
        max_seq: maximum sequence length, named "max_position_embedding" in official repo, default 512
                 in official paper, this value is called 'k'
        N: number of EncoderLayer, default 24 for large model
    Notes:
        self.rel_pos_emb: P in paper, this matrix is fixed during forward pass in same time,
                          all layer &amp; all module must share this layer from official paper
    References:
        https://github.com/microsoft/DeBERTa/blob/master/DeBERTa/deberta/ops.py
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">max_seq</span><span class="p">:</span> <span class="mi">512</span><span class="p">,</span> <span class="n">N</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">24</span><span class="p">,</span> <span class="n">dim_model</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1024</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">16</span><span class="p">,</span> <span class="n">dim_ffn</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">4096</span><span class="p">,</span> <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">DeBERTaEncoder</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">max_seq</span> <span class="o">=</span> <span class="n">max_seq</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">num_layers</span> <span class="o">=</span> <span class="n">N</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span> <span class="o">=</span> <span class="n">dim_model</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dim_ffn</span> <span class="o">=</span> <span class="n">dim_ffn</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">dropout</span><span class="p">)</span>  <span class="c1"># dropout is not learnable
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">encoder_layers</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">ModuleList</span><span class="p">(</span>
            <span class="p">[</span><span class="n">DeBERTaEncoderLayer</span><span class="p">(</span><span class="n">dim_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">dim_ffn</span><span class="p">,</span> <span class="n">dropout</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">num_layers</span><span class="p">)]</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">layer_norm</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">dim_model</span><span class="p">)</span>  <span class="c1"># for final-Encoder output
</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">rel_pos_emb</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">mask</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
        <span class="s">"""
        inputs: embedding from input sequence
        rel_pos_emb: relative position embedding
        mask: mask for Encoder padded token for speeding up to calculate attention score
        """</span>
        <span class="n">layer_output</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">pos_x</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">rel_pos_emb</span>  <span class="c1"># x is same as word_embeddings or embeddings in official repo
</span>        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">encoder_layers</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">pos_x</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span>
            <span class="n">layer_output</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">last_hidden_state</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">layer_norm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># because of applying pre-layer norm
</span>        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">stack</span><span class="p">(</span><span class="n">layer_output</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">).</span><span class="n">to</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>  <span class="c1"># shape: [N, BS, SEQ_LEN, DIM_Model]
</span>        <span class="k">return</span> <span class="n">last_hidden_state</span><span class="p">,</span> <span class="n">hidden_states</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">EMD</code>와 마찬가지로 레이어의 위치에 상관없이 같은 시점에는 모두 동일한 <code class="language-plaintext highlighter-rouge">Relative Position Embedding</code> 을 사용해 <code class="language-plaintext highlighter-rouge">linear projection</code> 하도록 구현해주는 것이 중요 포인트다. <code class="language-plaintext highlighter-rouge">forward</code> 메서드를 확인하자!</p>

<h4 id="deberta"><strong><code class="language-plaintext highlighter-rouge">🤖 DeBERTa</code></strong></h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Pytorch Implementation of DeBERTa
</span>
<span class="k">class</span> <span class="nc">DeBERTa</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="s">"""
    Main class for DeBERTa, having all of sub-blocks &amp; modules such as Disentangled Self-Attention, DeBERTaEncoder, EMD
    Init Scale of DeBERTa Hyper-Parameters, Embedding Layer, Encoder Blocks, EMD Blocks
    And then make 3-types of Embedding Layer, Word Embedding, Absolute Position Embedding, Relative Position Embedding
    Args:
        max_seq: maximum sequence length
        N: number of Disentangled-Encoder layers
        N_EMD: number of EMD layers
        dim_model: dimension of model
        num_heads: number of heads in multi-head attention
        dim_ffn: dimension of feed-forward network, same as intermediate size in official repo
        dropout: dropout rate
    Notes:
        MLM Task is not implemented yet, will be implemented ASAP, but you can get token encode output (embedding)
    References:
        https://arxiv.org/abs/2006.03654
        https://arxiv.org/abs/2111.09543
        https://github.com/microsoft/DeBERTa/blob/master/experiments/language_model/deberta_xxlarge.json
        https://github.com/microsoft/DeBERTa/blob/master/DeBERTa/deberta/config.py
        https://github.com/microsoft/DeBERTa/blob/master/DeBERTa/deberta/deberta.py
        https://github.com/microsoft/DeBERTa/blob/master/DeBERTa/deberta/bert.py
        https://github.com/microsoft/DeBERTa/blob/master/DeBERTa/deberta/disentangled_attention.py
        https://github.com/microsoft/DeBERTa/blob/master/DeBERTa/apps/models/masked_language_model.py
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span>
            <span class="n">vocab_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
            <span class="n">max_seq</span><span class="p">:</span> <span class="mi">512</span><span class="p">,</span>
            <span class="n">N</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">24</span><span class="p">,</span>
            <span class="n">N_EMD</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span>
            <span class="n">dim_model</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1024</span><span class="p">,</span>
            <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">16</span><span class="p">,</span>
            <span class="n">dim_ffn</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">4096</span><span class="p">,</span>
            <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">DeBERTa</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="c1"># Init Scale of DeBERTa
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">max_seq</span> <span class="o">=</span> <span class="n">max_seq</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">max_rel_pos</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">max_seq</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">N</span> <span class="o">=</span> <span class="n">N</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">N_EMD</span> <span class="o">=</span> <span class="n">N_EMD</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span> <span class="o">=</span> <span class="n">dim_model</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dim_ffn</span> <span class="o">=</span> <span class="n">dim_ffn</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dropout_prob</span> <span class="o">=</span> <span class="n">dropout</span>

        <span class="c1"># Init Embedding Layer
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">word_embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">dim_model</span><span class="p">)</span>  <span class="c1"># Word Embedding which is not add Absolute Position
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">rel_pos_emb</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Embedding</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">max_rel_pos</span><span class="p">,</span> <span class="n">dim_model</span><span class="p">)</span>  <span class="c1"># Relative Position Embedding
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">abs_pos_emb</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">max_seq</span><span class="p">,</span> <span class="n">dim_model</span><span class="p">)</span>  <span class="c1"># Absolute Position Embedding for EMD Layer
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">layer_norm1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">dim_model</span><span class="p">)</span>  <span class="c1"># for word embedding
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">layer_norm2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">dim_model</span><span class="p">)</span>  <span class="c1"># for rel_pos_emb
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">dropout_prob</span><span class="p">)</span>

        <span class="c1"># Init Sub-Blocks &amp; Modules
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">DeBERTaEncoder</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">max_seq</span><span class="p">,</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">N</span><span class="p">,</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span><span class="p">,</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">num_heads</span><span class="p">,</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">dim_ffn</span><span class="p">,</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">dropout_prob</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">emd_layers</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="p">.</span><span class="n">encoder</span><span class="p">.</span><span class="n">encoder_layers</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">N_EMD</span><span class="p">)]</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">emd_encoder</span> <span class="o">=</span> <span class="n">EnhancedMaskDecoder</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">emd_layers</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">mask</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">):</span>
        <span class="k">assert</span> <span class="n">inputs</span><span class="p">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">3</span><span class="p">,</span> <span class="sa">f</span><span class="s">'Expected (batch, sequence, vocab_size) got </span><span class="si">{</span><span class="n">inputs</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="s">'</span>
        <span class="c1"># Embedding Layer
</span>        <span class="n">word_embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">dropout</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">layer_norm1</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">word_embedding</span><span class="p">(</span><span class="n">inputs</span><span class="p">))</span>
        <span class="p">)</span>
        <span class="n">rel_pos_emb</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">dropout</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">layer_norm2</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">rel_pos_emb</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">max_rel_pos</span><span class="p">).</span><span class="n">repeat</span><span class="p">(</span><span class="n">inputs</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]).</span><span class="n">to</span><span class="p">(</span><span class="n">inputs</span><span class="p">)))</span>
        <span class="p">)</span>
        <span class="n">abs_pos_emb</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">abs_pos_emb</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">max_seq</span><span class="p">).</span><span class="n">repeat</span><span class="p">(</span><span class="n">inputs</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]).</span><span class="n">to</span><span class="p">(</span><span class="n">inputs</span><span class="p">))</span>  <span class="c1"># "I" in paper
</span>
        <span class="c1"># Disentangled Self-Attention Encoder Layer
</span>        <span class="n">last_hidden_state</span><span class="p">,</span> <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">word_embeddings</span><span class="p">,</span> <span class="n">rel_pos_emb</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span>

        <span class="c1"># Enhanced Mask Decoder Layer
</span>        <span class="n">emd_hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span>
        <span class="n">emd_last_hidden_state</span><span class="p">,</span> <span class="n">emd_hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">emd_encoder</span><span class="p">(</span><span class="n">emd_hidden_states</span><span class="p">,</span> <span class="n">abs_pos_emb</span><span class="p">,</span> <span class="n">rel_pos_emb</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">last_hidden_state</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">,</span> <span class="n">emd_last_hidden_state</span><span class="p">,</span> <span class="n">emd_hidden_states</span>
</code></pre></div></div>

        
      </section>

      <footer class="page__meta">
        
        
  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      <a href="/tags/#bert" class="page__taxonomy-item p-category" rel="tag">BERT</a><span class="sep">, </span>
    
      <a href="/tags/#deberta" class="page__taxonomy-item p-category" rel="tag">DeBERTa</a><span class="sep">, </span>
    
      <a href="/tags/#disentangled-attention" class="page__taxonomy-item p-category" rel="tag">Disentangled-Attention</a><span class="sep">, </span>
    
      <a href="/tags/#emd" class="page__taxonomy-item p-category" rel="tag">EMD</a><span class="sep">, </span>
    
      <a href="/tags/#encoder" class="page__taxonomy-item p-category" rel="tag">Encoder</a><span class="sep">, </span>
    
      <a href="/tags/#natural-language-process" class="page__taxonomy-item p-category" rel="tag">Natural Language Process</a><span class="sep">, </span>
    
      <a href="/tags/#relative-position-embedding" class="page__taxonomy-item p-category" rel="tag">Relative Position Embedding</a><span class="sep">, </span>
    
      <a href="/tags/#roberta" class="page__taxonomy-item p-category" rel="tag">RoBERTa</a><span class="sep">, </span>
    
      <a href="/tags/#self-attention" class="page__taxonomy-item p-category" rel="tag">Self-Attention</a><span class="sep">, </span>
    
      <a href="/tags/#transformer" class="page__taxonomy-item p-category" rel="tag">Transformer</a>
    
    </span>
  </p>




  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> Categories: </strong>
    <span itemprop="keywords">
    
      <a href="/categories/#nlp" class="page__taxonomy-item p-category" rel="tag">NLP</a>
    
    </span>
  </p>


        

  <p class="page__date"><strong><i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> Updated:</strong> <time class="dt-published" datetime="2023-08-04">August 4, 2023</time></p>

      </footer>

      <section class="page__share">
  
    <h4 class="page__share-title">Share on</h4>
  

  <a href="https://twitter.com/intent/tweet?text=%F0%9F%AA%A2%C2%A0%5BDeBERTa%5D+DeBERTa%3A+Decoding-Enhanced+BERT+with+Disentangled-Attention%20http%3A%2F%2Flocalhost%3A4000%2Fnlp%2Fdeberta" class="btn btn--twitter" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Twitter"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=http%3A%2F%2Flocalhost%3A4000%2Fnlp%2Fdeberta" class="btn btn--facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Facebook"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=http%3A%2F%2Flocalhost%3A4000%2Fnlp%2Fdeberta" class="btn btn--linkedin" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on LinkedIn"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>


      
  <nav class="pagination">
    
      <a href="/cv/vit" class="pagination--pager" title="🌆 [ViT] An Image Is Worth 16x16 Words: Transformers For Image Recognition At Scale
">Previous</a>
    
    
      <a href="/nlp/transformer" class="pagination--pager" title="🤖 [Transformer] Attention Is All You Need
">Next</a>
    
  </nav>

    </div>

    
  </article>

  
  
    <div class="page__related">
      <h2 class="page__related-title">You may also enjoy</h2>
      <div class="grid__wrapper">
        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
      <div class="archive__item-teaser">
        <img src="/assets/images/huggingface_emoji.png" alt="">
      </div>
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/nlp/linear_attention" rel="permalink">🌆 [Linear Attention] Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention
</a>
      
    </h2>
    <!-- 

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          8 minute read
        
      </span>
    
  </p>

 -->
    
      <p class="page__meta"><i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i> March 14 2024</p>
    
    <p class="archive__item-excerpt" itemprop="description">Linear Attention Official Paper Review with Pytorch Implementation
</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
      <div class="archive__item-teaser">
        <img src="/assets/images/huggingface_emoji.png" alt="">
      </div>
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/nlp/spanbert" rel="permalink">🗂️[SpanBERT] SpanBERT: Improving Pre-training by Representing and Predicting Spans
</a>
      
    </h2>
    <!-- 

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          6 minute read
        
      </span>
    
  </p>

 -->
    
      <p class="page__meta"><i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i> March 11 2024</p>
    
    <p class="archive__item-excerpt" itemprop="description">SpanBERT Official Paper Review with Pytorch Implementation
</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
      <div class="archive__item-teaser">
        <img src="/assets/images/huggingface_emoji.png" alt="">
      </div>
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/nlp/roformer" rel="permalink">🎡 [Roformer] RoFormer: Enhanced Transformer with Rotary Position Embedding
</a>
      
    </h2>
    <!-- 

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          8 minute read
        
      </span>
    
  </p>

 -->
    
      <p class="page__meta"><i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i> March 11 2024</p>
    
    <p class="archive__item-excerpt" itemprop="description">Roformer Official Paper Review with Pytorch Implementation
</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
      <div class="archive__item-teaser">
        <img src="/assets/images/huggingface_emoji.png" alt="">
      </div>
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/nlp/electra" rel="permalink">👮 [ELECTRA] Pre-training Text Encoders as Discriminators Rather Than Generators
</a>
      
    </h2>
    <!-- 

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          7 minute read
        
      </span>
    
  </p>

 -->
    
      <p class="page__meta"><i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i> March 11 2024</p>
    
    <p class="archive__item-excerpt" itemprop="description">ELECTRA Official Paper Review with Pytorch Implementation
</p>
  </article>
</div>

        
      </div>
    </div>
  
  
</div>

    </div>

    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>Follow:</strong></li>
    

    
      
        
      
        
      
        
      
        
      
        
      
        
      
    

    
      <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
    
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2024 qcqced. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>







    <script>
  'use strict';

  (function() {
    var commentContainer = document.querySelector('#utterances-comments');

    if (!commentContainer) {
      return;
    }

    var script = document.createElement('script');
    script.setAttribute('src', 'https://utteranc.es/client.js');
    script.setAttribute('repo', 'qcqced123/qcqced123.github.io');
    script.setAttribute('issue-term', 'pathname');
    
    script.setAttribute('theme', 'github-light');
    script.setAttribute('crossorigin', 'anonymous');

    commentContainer.appendChild(script);
  })();
</script>

  




<script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML">
</script>

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"] ],
    displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
    processEscapes: true
    },
    "HTML-CSS": { availableFonts: ["TeX"] }
});
</script>

  </body>
</html>
