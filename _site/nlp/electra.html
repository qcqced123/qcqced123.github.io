<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.24.0 by Michael Rose
  Copyright 2013-2020 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->
<html lang="en" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>ğŸ‘® [ELECTRA] Pre-training Text Encoders as Discriminators Rather Than Generators - AI/Business Study Log</title>
<meta name="description" content="ELECTRA Official Paper Review with Pytorch Implementation">


  <meta name="author" content="qcqced">
  
  <meta property="article:author" content="qcqced">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="AI/Business Study Log">
<meta property="og:title" content="ğŸ‘® [ELECTRA] Pre-training Text Encoders as Discriminators Rather Than Generators">
<meta property="og:url" content="http://localhost:4000/nlp/electra">


  <meta property="og:description" content="ELECTRA Official Paper Review with Pytorch Implementation">







  <meta property="article:published_time" content="2024-03-11T00:00:00+09:00">



  <meta property="article:modified_time" content="2024-03-12T02:00:00+09:00">



  

  


<link rel="canonical" href="http://localhost:4000/nlp/electra">




<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    
      "@type": "Person",
      "name": "qcqced",
      "url": "http://localhost:4000/"
    
  }
</script>







<!-- end _includes/seo.html -->



  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="AI/Business Study Log Feed">


<!-- https://t.co/dKP3o1e -->
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css"></noscript>



<!-- Latex -->
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->
<link rel="apple-touch-icon" sizes="180x180" href="/assets/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/assets/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/assets/favicon-16x16.png">
<link rel="manifest" href="/assets/site.webmanifest">
<link rel="mask-icon" href="/assets/safari-pinned-tab.svg" color="#5bbad5">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="theme-color" content="#ffffff">
<!-- end custom head snippets -->

    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      TeX: {
        equationNumbers: {
          autoNumber: "AMS"
        }
      },
      tex2jax: {
      inlineMath: [ ['$', '$'] ],
      displayMath: [ ['$$', '$$'] ],
      processEscapes: true
    }
  });
  MathJax.Hub.Register.MessageHook("Math Processing Error",function (message) {
      alert("Math Processing Error: "+message[1]);
    });
  MathJax.Hub.Register.MessageHook("TeX Jax - parse error",function (message) {
      alert("Math Processing Error: "+message[1]);
    });
  </script>
  <script type="text/javascript" async
    src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>
  </head>

  <body class="layout--single">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
          AI/Business Study Log
          <span class="site-subtitle">NLP, Marketing</span>
        </a>
        
        
        <ul class="visible-links">
              
              
                  <li class="masthead__menu-item">
                      <a href="https://qcqced123.github.io/">Home</a>
                  </li>
              
          
              
              
                  <li class="dropdown ">
                      <a class="dropdown-toggle" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">CS/AI  <i class="fa fa-caret-down fa-sm" aria-hidden="true"></i><span class="caret"></span></a>
       
                      <ul class="dropdown-content" >
                          
                              <a class = "dropdown-item" href="/nlp/">    Natural Language Process</a>
                          
                              <a class = "dropdown-item" href="/multi-modal/">    Multi Modal</a>
                          
                              <a class = "dropdown-item" href="/cv/">    Computer Vision</a>
                          
                              <a class = "dropdown-item" href="/ml/">    Machine Learning</a>
                          
                              <a class = "dropdown-item" href="/framework-library/">    Framework & Library</a>
                          
                              <a class = "dropdown-item" href="/python/">    Python</a>
                          
                              <a class = "dropdown-item" href="/algorithm/">    Data Structure & Algorithm</a>
                          
                              <a class = "dropdown-item" href="/ps/">    Problem Solving</a>
                          
                      </ul>
                  </li>
              
          
              
              
                  <li class="dropdown ">
                      <a class="dropdown-toggle" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Math  <i class="fa fa-caret-down fa-sm" aria-hidden="true"></i><span class="caret"></span></a>
       
                      <ul class="dropdown-content" >
                          
                              <a class = "dropdown-item" href="/linear-algebra/">    Linear Algebra</a>
                          
                              <a class = "dropdown-item" href="/optimization-theory/">    Optimization Theory/Calculus</a>
                          
                              <a class = "dropdown-item" href="/signal-system/">    Signal & System</a>
                          
                      </ul>
                  </li>
              
          
              
              
                  <li class="dropdown ">
                      <a class="dropdown-toggle" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Business/Marketing  <i class="fa fa-caret-down fa-sm" aria-hidden="true"></i><span class="caret"></span></a>
       
                      <ul class="dropdown-content" >
                          
                              <a class = "dropdown-item" href="/device/">    Device</a>
                          
                              <a class = "dropdown-item" href="/semi-conductor/">    Semi-Conductor</a>
                          
                              <a class = "dropdown-item" href="/ai/">    AI</a>
                          
                      </ul>
                  </li>
              
          
              
              
                  <li class="masthead__menu-item">
                      <a href="/categories/">Category</a>
                  </li>
              
          
              
              
                  <li class="masthead__menu-item">
                      <a href="/about/">About</a>
                  </li>
              
          
       </ul>
       
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      





<div id="main" role="main">
  
  <div class="sidebar sticky">
  


<div itemscope itemtype="https://schema.org/Person" class="h-card">

  
    <div class="author__avatar">
      <a href="http://localhost:4000/">
        <img src="/assets/images/huggingface_emoji.png" alt="qcqced" itemprop="image" class="u-photo">
      </a>
    </div>
  

  <div class="author__content">
    <h3 class="author__name p-name" itemprop="name">
      <a class="u-url" rel="me" href="http://localhost:4000/" itemprop="url">qcqced</a>
    </h3>
    
      <div class="author__bio p-note" itemprop="description">
        <p>Interested in NLP, Marketing</p>

      </div>
    
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">Follow</button>
    <ul class="author__urls social-icons">
      
        <li itemprop="homeLocation" itemscope itemtype="https://schema.org/Place">
          <i class="fas fa-fw fa-map-marker-alt" aria-hidden="true"></i> <span itemprop="name" class="p-locality">Seoul, South Korea</span>
        </li>
      

      
        
          
            <li><a href="https://qcqced123.github.io" rel="nofollow noopener noreferrer me" itemprop="sameAs"><i class="fas fa-fw fa-link" aria-hidden="true"></i><span class="label">Website</span></a></li>
          
        
          
            <li><a href="https://github.com/qcqced123" rel="nofollow noopener noreferrer me" itemprop="sameAs"><i class="fab fa-fw fa-github" aria-hidden="true"></i><span class="label">GitHub</span></a></li>
          
        
          
            <li><a href="https://www.kaggle.com/qcqced" rel="nofollow noopener noreferrer me" itemprop="sameAs"><i class="fab fa-fw fa-kaggle" aria-hidden="true"></i><span class="label">Kaggle</span></a></li>
          
        
      

      

      
        <li>
          <a href="mailto:qcqced123@gmail.com" rel="me" class="u-email">
            <meta itemprop="email" content="qcqced123@gmail.com" />
            <i class="fas fa-fw fa-envelope-square" aria-hidden="true"></i><span class="label">Email</span>
          </a>
        </li>
      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      <!--
  <li>
    <a href="http://link-to-whatever-social-network.com/user/" itemprop="sameAs" rel="nofollow noopener noreferrer me">
      <i class="fas fa-fw" aria-hidden="true"></i> Custom Social Profile Link
    </a>
  </li>
-->
    </ul>
  </div>
</div>

  
  </div>



  <article class="page h-entry" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="ğŸ‘® [ELECTRA] Pre-training Text Encoders as Discriminators Rather Than Generators">
    <meta itemprop="description" content="ELECTRA Official Paper Review with Pytorch Implementation">
    <meta itemprop="datePublished" content="2024-03-11T00:00:00+09:00">
    <meta itemprop="dateModified" content="2024-03-12T02:00:00+09:00">

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title p-name" itemprop="headline">
            <a href="http://localhost:4000/nlp/electra" class="u-url" itemprop="url">ğŸ‘® [ELECTRA] Pre-training Text Encoders as Discriminators Rather Than Generators
</a>
          </h1>
          <p class="page__date">
            <a href="https://hits.seeyoufarm.com/localhost:4000/nlp/electra"target="_blank">
              <img src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https://localhost:4000/nlp/electra&count_bg=%23399DE2&title_bg=%236D6D6D&icon=pytorch.svg&icon_color=%23E7E7E7&title=Views&edge_flat=false"/>
            </a>
            <i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> Updated: <time datetime="2024-03-11T00:00:00+09:00">March 11, 2024</time>
            <!-- <div style="text-align: left;"> -->
            <!-- </div> -->
          </p>
          
          
        </header>
      

      <section class="page__content e-content" itemprop="text">
        
          <aside class="sidebar__right sticky">
            <nav class="toc">
              <header><h4 class="nav__title"><i class="fas fa-file-alt"></i> On this page</h4></header>
              <ul class="toc__menu"><li><a href="#overview">ğŸ”­Â Overview</a></li><li><a href="#rtd-new-pre-train-task">ğŸ‘®Â RTD: New Pre-train Task</a></li><li><a href="#architecture">ğŸŒŸÂ Architecture</a></li><li><a href="#implementation-by-pytorch">ğŸ‘©â€ğŸ’»Â Implementation by Pytorch</a><ul><li><a href="#-rtd-trainer-method">ğŸŒ† RTD trainer method</a></li><li><a href="#-electra-module">ğŸŒ† ELECTRA Module</a></li><li><a href="#-rtd-input-making">ğŸŒ† RTD Input Making</a></li></ul></li><li><a href="#-future-work-ì½ê³ -êµ¬í˜„í•˜ë©´ì„œ-ëŠë‚€ì --ê°œì„ ë°©í–¥">ğŸŒŸ Future Work (ì½ê³  êµ¬í˜„í•˜ë©´ì„œ ëŠë‚€ì  &amp; ê°œì„ ë°©í–¥)</a></li></ul>

            </nav>
          </aside>
        
        <h3 id="overview"><code class="language-plaintext highlighter-rouge">ğŸ”­Â Overview</code></h3>

<p><code class="language-plaintext highlighter-rouge">ELECTRA</code>ëŠ” 2020ë…„ Googleì—ì„œ ì²˜ìŒ ë°œí‘œí•œ ëª¨ë¸ë¡œ, GAN(Generative Adversarial Networks) Style ì•„í‚¤í…ì²˜ë¥¼ NLPì— ì ìš©í•œ ê²ƒì´ íŠ¹ì§•ì´ë‹¤. ìƒˆë¡œìš´ êµ¬ì¡° ì°¨ìš©ì— ë§ì¶°ì„œ <code class="language-plaintext highlighter-rouge">RTD(Replace Token Dection)</code> Taskë¥¼ ê³ ì•ˆì— ì‚¬ì „ í•™ìŠµìœ¼ë¡œ ì‚¬ìš©í–ˆë‹¤. ëª¨ë“  ì•„ì´ë””ì–´ëŠ” ê¸°ì¡´ MLM(Masked Language Model)ì„ ì‚¬ì „í•™ìŠµ ë°©ë²•ë¡ ìœ¼ë¡œ ì‚¬ìš©í•˜ëŠ” ì¸ì½”ë” ì–¸ì–´ ëª¨ë¸(BERT ê³„ì—´)ì˜ ë‹¨ì ìœ¼ë¡œë¶€í„° ì¶œë°œí•œë‹¤.</p>

<p><strong>[MLM ë‹¨ì ]</strong></p>
<ul>
  <li>1) ì‚¬ì „í•™ìŠµê³¼ íŒŒì¸íŠœë‹ ì‚¬ì´ ë¶ˆì¼ì¹˜
    <ul>
      <li>íŒŒì¸íŠœë‹ ë•Œ Masking Taskê°€ ì—†ìŒ</li>
    </ul>
  </li>
  <li>2) ì—°ì‚°ëŸ‰ ëŒ€ë¹„ í•™ìŠµëŸ‰ì€ ì ì€í¸
    <ul>
      <li>ì „ì²´ ì‹œí€€ìŠ¤ì˜ 15%ë§Œ ë§ˆìŠ¤í‚¹ í™œìš©(15%ë§Œ í•™ìŠµ)</li>
      <li>ì „ì—­ ì–´í…ì…˜ì˜ ì‹œê³µê°„ ë³µì¡ë„ ê³ ë ¤í•˜ë©´ ìƒë‹¹íˆ ë¹„íš¨ìœ¨ì ì¸ ìˆ˜ì¹˜
        <ul>
          <li>ì‹œí€€ìŠ¤ê¸¸ì´ ** 2ì˜ ë³µì¡ë„</li>
          <li>Vocab Sizeë§Œí¼ì˜ ì°¨ì›ì„ ê°–ëŠ” ì†Œí”„íŠ¸ë§¥ìŠ¤ ê³„ì‚° ë°˜ë³µ</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p>ê·¸ë˜ì„œ MLMì€ í™œìš©í•˜ë˜, íŒŒì¸íŠœë‹ê³¼ ê´´ë¦¬ëŠ” í¬ì§€ ì•Šì€ ëª©ì í•¨ìˆ˜ë¥¼ ì„¤ê³„í•¨ìœ¼ë¡œì„œ ì…ë ¥ëœ ì „ì²´ ì‹œí€€ìŠ¤ì— ëŒ€í•´ì„œ ëª¨ë¸ì´ í•™ìŠµí•˜ì—¬ ì—°ì‚°ëŸ‰ ëŒ€ë¹„ í•™ìŠµëŸ‰ì„ ëŠ˜ë¦¬ê³ ì í–ˆë˜ê²Œ ë°”ë¡œ ELECTRA ëª¨ë¸ì´ë‹¤.</p>

<p>ì •ë¦¬í•˜ìë©´, ELECTRA ëª¨ë¸ì€ ê¸°ì¡´ BERTì˜ êµ¬ì¡°ì  ì¸¡ë©´ ê°œì„ ì´ ì•„ë‹Œ, ì‚¬ì „í•™ìŠµ ë°©ë²•ì— ëŒ€í•œ ê°œì„  ì‹œë„ë¼ê³  ë³¼ ìˆ˜ ìˆë‹¤. ë”°ë¼ì„œ ì–´ë–¤ ëª¨ë¸ì´ë”ë¼ë„, ì¸ì½”ë” ì–¸ì–´ ëª¨ë¸ì´ë¼ë©´ ëª¨ë‘ ELECTRA êµ¬ì¡°ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìœ¼ë©°, ê¸°ì¡´ ë…¼ë¬¸ì—ì„œëŠ” ì›ë³¸ BERT êµ¬ì¡°ë¥¼ ì‚¬ìš©í–ˆë‹¤. ê·¸ë˜ì„œ ë³¸ í¬ìŠ¤íŒ…ì—ì„œë„ BERTì— ëŒ€í•œ ì„¤ëª… ì—†ì´ RTDì— ëŒ€í•´ì„œë§Œ ë‹¤ë£¨ë ¤ê³  í•œë‹¤.</p>

<h3 id="rtd-new-pre-train-task"><code class="language-plaintext highlighter-rouge">ğŸ‘®Â RTD: New Pre-train Task</code></h3>

<p align="center">
<img src="/assets/images/electra/electra.png" alt="RTD Task" class="align-center image-caption" width="100%&quot;, height=&quot;100%" />
<strong><em><a href="https://arxiv.org/abs/2003.10555">RTD Task</a></em></strong>
</p>

<p>RTDì˜ ì•„ì´ë””ì–´ëŠ” ê°„ë‹¨í•˜ë‹¤. ìƒì„±ì(Generator)ê°€ ì¶œë ¥ìœ¼ë¡œ ë‚´ë†“ì€ í† í° ì‹œí€€ìŠ¤ì— ëŒ€í•´ì„œ íŒë³„ì(Discriminator)ê°€ ê°œë³„ í† í°ë“¤ì´ ì›ë³¸ì¸ì§€ ì•„ë‹Œì§€ë¥¼ íŒì •(ì´ì§„ ë¶„ë¥˜)í•˜ë„ë¡ ë§Œë“ ë‹¤. ìƒì„±ìëŠ” ê¸°ì¡´ì˜ MLMì„ ê·¸ëŒ€ë¡œ ìˆ˜í–‰í•˜ê³ , íŒë³„ìëŠ” ìƒì„±ìì˜ ì˜ˆì¸¡ì— ëŒ€í•´ ì§„ì§œì¸ì§€ ê°€ì§œì¸ì§€ ë¶„ë¥˜í•˜ëŠ” ì‹ì´ë‹¤.</p>

<p>ìœ„ ê·¸ë¦¼ì„ ì˜ˆì‹œë¡œ ì‚´í´ë³´ì. ëª¨ë¸ì— ì…ë ¥ìœ¼ë¡œ <code class="language-plaintext highlighter-rouge">the chef cooked the meal</code>ë¼ëŠ” ì‹œí€€ìŠ¤ ì¤€ë‹¤. ê·¸ëŸ¬ë©´ MLM ê·œì¹™ì— ë”°ë¼ì„œ 15%ì˜ í† í°ì´ ë¬´ì‘ìœ„ë¡œ ì„ íƒëœë‹¤. ê·¸ë˜ì„œ <code class="language-plaintext highlighter-rouge">the</code>, <code class="language-plaintext highlighter-rouge">cooked</code>ê°€ ë§ˆìŠ¤í‚¹ ë˜ì—ˆë‹¤. ì´ì œ ìƒì„±ìëŠ” ë§ˆìŠ¤í‚¹ í† í°ì— ëŒ€í•´ <code class="language-plaintext highlighter-rouge">the</code>, <code class="language-plaintext highlighter-rouge">ate</code>ë¼ëŠ” ê²°ê³¼ë¥¼ ë‚´ë†“ëŠ”ë‹¤. ê·¸ë˜ì„œ ìµœì¢…ì ìœ¼ë¡œ ìƒì„±ìê°€ ë°˜í™˜í•˜ëŠ” ì‹œí€€ìŠ¤ëŠ” <code class="language-plaintext highlighter-rouge">the chef ate the meal</code>ì´ ëœë‹¤. ì´ì œ ìƒì„±ìê°€ ë°˜í™˜í•œ ì‹œí€€ìŠ¤ë¥¼ íŒë³„ìì— ì…ë ¥ìœ¼ë¡œ ëŒ€ì…í•œë‹¤. íŒë³„ìëŠ” ê°œë³„ í† í°ë“¤ì´ ì›ë³¸ì¸ì§€ ì•„ë‹Œì§€ë¥¼ íŒì •í•´ ê²°ê³¼ë¥¼ ì¶œë ¥í•œë‹¤.</p>

<p>ì´ëŸ¬í•œ êµ¬ì¡° ë° ì‚¬ì „í•™ìŠµ ë°©ì‹ì˜ ì¥ì ì€ íŒë³„ìê°€ MLM í•™ìŠµì— ë”°ë¥¸ ì§€ì‹ì„ ìƒì„±ìë¡œë¶€í„° ì „ìˆ˜ ë°›ëŠ” ë™ì‹œì— ì „ì²´ ì‹œí€€ìŠ¤ì— ëŒ€í•´ì„œ í•™ìŠµí•  ê¸°íšŒê°€ ìƒê¸´ë‹¤ëŠ” ê²ƒì´ë‹¤. ì‹œí€€ìŠ¤ ë‚´ë¶€ ëª¨ë“  í† í°ì— ëŒ€í•´ì„œ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•˜ê³  ì†ì‹¤ì„ ê³„ì‚°í•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì— ê°™ì€ í¬ê¸°ì˜ ì‹œí€€ìŠ¤ë¥¼ ì‚¬ìš©í•´ë„ ê¸°ì¡´ MLM ëŒ€ë¹„ ë” í’ë¶€í•œ ë¬¸ë§¥ ì •ë³´ë¥¼ ëª¨ë¸ì´ í¬ì°©í•  ìˆ˜ ìˆê²Œ ëœë‹¤. ë˜í•œ íŒë³„ìë¥¼ íŒŒì¸íŠœë‹ì˜ BackBoneìœ¼ë¡œ ì‚¬ìš©í•˜ë©´, íŒë³„ìì˜ ì‚¬ì „í•™ìŠµì€ ê²°êµ­ ë§ˆìŠ¤í‚¹ ì—†ì´ ëª¨ë“  ì‹œí€€ìŠ¤ë¥¼ í™œìš©í•œ ì´ì§„ ë¶„ë¥˜ë¼ê³  ë³¼ ìˆ˜ ìˆê¸° ë•Œë¬¸ì—, ì‚¬ì „í•™ìŠµê³¼ íŒŒì¸íŠœë‹ ì‚¬ì´ì˜ ê´´ë¦¬ë„ ìƒë‹¹íˆ ë§ì´ ì¤„ì–´ë“¤ê²Œ ëœë‹¤.</p>

<h3 id="architecture"><code class="language-plaintext highlighter-rouge">ğŸŒŸÂ Architecture</code></h3>

<p align="center">
<img src="/assets/images/electra/electra_experiment.png" alt="Model Architecture" class="align-center image-caption" width="100%&quot;, height=&quot;100%" />
<strong><em><a href="https://arxiv.org/abs/2003.10555">Model Architecture</a></em></strong>
</p>

<p>ì €ìëŠ” ìœ„ì™€ ê°™ì€ ì‹¤í—˜ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ìƒì„±ìì˜ width (ì€ë‹‰ì¸µ) í¬ê¸°ê°€ íŒë³„ìë³´ë‹¤ ì‘ë„ë¡ ëª¨ë¸ í¬ê¸°ë¥¼ ì„¸íŒ…í•˜ëŠ”ê²Œ ê°€ì¥ íš¨ìœ¨ì ì´ë¼ê³  ì£¼ì¥í•œë‹¤. ì œì‹œëœ ê·¸ë˜í”„ëŠ” ìƒì„±ìì™€ íŒë³„ìì˜ í¬ê¸° ë³€í™” ëŒ€ë¹„ íŒŒì¸íŠœë‹ ì„±ëŠ¥ì˜ ì¶”ì´ë¥¼ ë‚˜íƒ€ë‚¸ë‹¤. ìƒì„±ìì˜ width í¬ê¸°ê°€ 256, íŒë³„ìì˜ width í¬ê¸°ê°€ 768ì¼ ë•Œ ê°€ì¥ ì ìˆ˜ê°€ ë†’ë‹¤. depth(ë ˆì´ì–´ ê°œìˆ˜)ì— ëŒ€í•œ ì–¸ê¸‰ì€ ë”°ë¡œ ì—†ì§€ë§Œ, ì €ìì— ì˜í•´ ê³µê°œëœ Hyper-Param í…Œì´ë¸”ì„ ë³´ë©´ ì€ë‹‰ì¸µì˜ í¬ê¸°ë§Œ ì¤„ì´ê³ , ë ˆì´ì–´ ê°œìˆ˜ëŠ” ìƒì„±ìì™€ íŒë³„ìê°€ ê°™ì€ ê²ƒìœ¼ë¡œ ì¶”ì •ëœë‹¤.</p>

<p>ì¶”ê°€ë¡œ, ìƒì„±ìì™€ íŒë³„ìê°€ ì„ë² ë”© ì¸µì„ ì„œë¡œ ê³µìœ í•˜ëŠ”ê²Œ ê°€ì¥ ë†’ì€ ì„±ëŠ¥ì„ ë‚¸ë‹¤ê³  ì£¼ì¥í•œë‹¤. ì˜¤ë¥¸ìª½ ê·¸ë˜í”„ ì¶”ì´ë¥¼ ë³´ë©´ ê°™ì€ ì—°ì‚°ëŸ‰ì´ë¼ë©´, ì„ë² ë”© ê³µìœ (íŒŒë€ìƒ‰ ì‹¤ì„ ) ë°©ì‹ì´ ê°€ì¥ ë†’ì€ íŒŒì¸íŠœë‹ ì„±ëŠ¥ì„ ë³´ì—¬ì¤€ë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤. ë”°ë¼ì„œ ë‹¨ì–´ ì„ë² ë”©, ì ˆëŒ€ ìœ„ì¹˜ ì„ë² ë”©ì„ ì„œë¡œ ê³µìœ í•˜ë„ë¡ ì„¤ê³„í•œë‹¤. ëŒ€ì‹  ìƒì„±ì ì€ë‹‰ì¸µì˜ í¬ê¸°ê°€ ë” ì‘ì€ê²Œ ìœ ë¦¬í•˜ë‹¤ê³  ì–¸ê¸‰í–ˆê¸° ë•Œë¬¸ì—, ì´ê²ƒì„ ì‹¤ì œë¡œ êµ¬í˜„í•˜ë ¤ë©´ ì„ë² ë”© ì¸µìœ¼ë¡œë¶€í„° ë‚˜ì˜¨ ê²°ê³¼ê°’ì„ ìƒì„±ìì˜ ì€ë‹‰ì¸µ ì°¨ì›ìœ¼ë¡œ ì„ í˜• íˆ¬ì˜í•´ì¤˜ì•¼ í•œë‹¤. ê·¸ë˜ì„œ ìƒì„±ìì˜ ì„ë² ë”© ì¸µê³¼ ì¸ì½”ë” ì‚¬ì´ì— linear layerê°€ ì¶”ê°€ë˜ì–´ì•¼ í•œë‹¤.</p>

\[\min_{\theta_G, \theta_D}\sum_{x \in X} \mathcal{L}_{\text{MLM}}(x, \theta_G) + \lambda \mathcal{L}_{\text{Disc}}(x, \theta_D)\]

<p>ë”°ë¼ì„œ, ì§€ê¸ˆê¹Œì§€ ì‚´í´ë³¸ ëª¨ë“  ë‚´ìš©ì„ ì¢…í•©í•´ë³´ë©´ ELECTRAì˜ ëª©ì í•¨ìˆ˜ëŠ” ë‹¤ìŒ ìˆ˜ì‹ê³¼ ê°™ë‹¤. ìƒì„±ìì˜ MLM ì†ì‹¤ê³¼ íŒë³„ìì˜ ì´ì§„ ë¶„ë¥˜ ì†ì‹¤ì„ ë”í•´ì„œ ëª¨ë¸ì— ì˜¤ì°¨ ì—­ì „í•´ì£¼ë©´ ë˜ëŠ”ë°, íŠ¹ì´í•œ ì ì€ íŒë³„ìì˜ ì†ì‹¤ì— ìƒìˆ˜ê°’ì¸ ëŒë‹¤ê°€ ê³±í•´ì§„ë‹¤ëŠ” ê²ƒì´ë‹¤. ì‹¤ì œ ëª¨ë¸ì„ êµ¬í˜„í•˜ê³  ì‚¬ì „í•™ìŠµì„ í•´ë³´ë©´, ë°ì´í„°ì˜ ì–‘ì´ë‚˜ ëª¨ë¸ í¬ê¸° í˜¹ì€ ì¢…ë¥˜ì— ë”°ë¼ ë‹¬ë¼ì§€ê² ì§€ë§Œ ë‘ ì†ì‹¤ ì‚¬ì´ì˜ ìŠ¤ì¼€ì¼ì˜ ì°¨ì´ê°€ 10ë°°ì •ë„ ì°¨ì´ ë‚˜ê²Œ ëœë‹¤. ë‘ ì†ì‹¤ì˜ ìŠ¤ì¼€ì¼ì„ ë§ì¶°ì£¼ëŠ” ë™ì‹œì—, ì„ë² ë”©ì¸µì˜ í•™ìŠµì´ íŒë³„ìì˜ ì†ì‹¤ì„ ì¤„ì´ëŠ”ë° ë” ì§‘ì¤‘í•˜ë„ë¡ ë§Œë“¤ê¸° ìœ„í•´ ë„ì…í•œ ê²ƒìœ¼ë¡œ ì¶”ì •ëœë‹¤. ë…¼ë¬¸ê³¼ ì½”ë“œë¥¼ ë³´ë©´ ì €ìëŠ” $\lambda=50$ ìœ¼ë¡œ ë‘ê³  í•™ìŠµí•˜ê³  ìˆë‹¤.</p>

<h3 id="implementation-by-pytorch"><code class="language-plaintext highlighter-rouge">ğŸ‘©â€ğŸ’»Â Implementation by Pytorch</code></h3>

<p>ë…¼ë¬¸ì˜ ë‚´ìš©ê³¼ ì €ìê°€ ì§ì ‘ ê³µê°œí•œ ì½”ë“œë¥¼ ì¢…í•©í•˜ì—¬ íŒŒì´í† ì¹˜ë¡œ ELECTRAë¥¼ êµ¬í˜„í•´ë´¤ë‹¤. ë‘ ê°œì˜ ì„œë¡œ ë‹¤ë¥¸ ëª¨ë¸ì„ ê°™ì€ ìŠ¤íƒ­ì—ì„œ í•™ìŠµì‹œì¼œì•¼ í•˜ê¸° ë•Œë¬¸ì—, ì œì‹œëœ ë‚´ìš©ì— ë¹„í•´ ì‹¤ì œ êµ¬í˜„ì€ ë§¤ìš° ê¹Œë‹¤ë¡œìš´ í¸ì´ì—ˆë‹¤. ë³¸ í¬ìŠ¤íŒ…ì—ì„œëŠ” ELECTRA ëª¨ë¸ êµ¬ì¡°ë¥¼ ë¹„ë¡¯í•´ RTD í•™ìŠµ íŒŒì´í”„ë¼ì¸ êµ¬ì„±ì— í•„ìˆ˜ì ì¸ ìš”ì†Œ ëª‡ ê°€ì§€ì— ëŒ€í•´ì„œë§Œ ì„¤ëª…í•˜ë ¤ í•œë‹¤. ì „ì²´ êµ¬ì¡°ì— ëŒ€í•œ ì½”ë“œëŠ” <strong><a href="https://github.com/qcqced123/model_study">ì—¬ê¸° ë§í¬</a></strong>ë¥¼ í†µí•´ ì°¸ê³  ë¶€íƒë“œë¦°ë‹¤.</p>

<p>ELECTRAì˜ ì‚¬ì „ í•™ìŠµì¸ RTDì˜ í•™ìŠµ íŒŒì´í”„ë¼ì¸ì„ êµ¬í˜„í•œ ì½”ë“œë¥¼ ë³¸ ë’¤, ì„¸ë¶€ êµ¬ì„± ìš”ì†Œë“¤ì— ëŒ€í•´ì„œ ì‚´í´ë³´ì.</p>

<h4 id="-rtd-trainer-method"><strong><code class="language-plaintext highlighter-rouge">ğŸŒ† RTD trainer method</code></strong></h4>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">train_val_fn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">loader_train</span><span class="p">,</span> <span class="n">model</span><span class="p">:</span> <span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">criterion</span><span class="p">:</span> <span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">scheduler</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]:</span>
  <span class="n">scaler</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">amp</span><span class="p">.</span><span class="n">GradScaler</span><span class="p">(</span><span class="n">enabled</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">amp_scaler</span><span class="p">)</span>
  <span class="n">model</span><span class="p">.</span><span class="n">train</span><span class="p">()</span>
  <span class="k">for</span> <span class="n">step</span><span class="p">,</span> <span class="n">batch</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">tqdm</span><span class="p">(</span><span class="n">loader_train</span><span class="p">)):</span>
      <span class="n">optimizer</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">(</span><span class="n">set_to_none</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
      <span class="n">inputs</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="s">'input_ids'</span><span class="p">].</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">device</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
      <span class="n">labels</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="s">'labels'</span><span class="p">].</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">device</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>  
      <span class="n">padding_mask</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="s">'padding_mask'</span><span class="p">].</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">device</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>  

      <span class="n">mask_labels</span> <span class="o">=</span> <span class="bp">None</span>
      <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">rtd_masking</span> <span class="o">==</span> <span class="s">'SpanBoundaryObjective'</span><span class="p">:</span>
          <span class="n">mask_labels</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="s">'mask_labels'</span><span class="p">].</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">device</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

      <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">amp</span><span class="p">.</span><span class="n">autocast</span><span class="p">(</span><span class="n">enabled</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">amp_scaler</span><span class="p">):</span>
          <span class="n">g_logit</span><span class="p">,</span> <span class="n">d_inputs</span><span class="p">,</span> <span class="n">d_labels</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">generator_fw</span><span class="p">(</span>
              <span class="n">inputs</span><span class="p">,</span>
              <span class="n">labels</span><span class="p">,</span>
              <span class="n">padding_mask</span><span class="p">,</span>
              <span class="n">mask_labels</span>
          <span class="p">)</span>
          <span class="n">d_logit</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">discriminator_fw</span><span class="p">(</span>
              <span class="n">d_inputs</span><span class="p">,</span>
              <span class="n">padding_mask</span>
          <span class="p">)</span>
          <span class="n">g_loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">g_logit</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">vocab_size</span><span class="p">),</span> <span class="n">labels</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>
          <span class="n">d_loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">d_logit</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">d_labels</span><span class="p">)</span>
          <span class="n">loss</span> <span class="o">=</span> <span class="n">g_loss</span> <span class="o">+</span> <span class="n">d_loss</span><span class="o">*</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">discriminator_lambda</span>

      <span class="n">scaler</span><span class="p">.</span><span class="n">scale</span><span class="p">(</span><span class="n">loss</span><span class="p">).</span><span class="n">backward</span><span class="p">()</span>
      <span class="n">scaler</span><span class="p">.</span><span class="n">step</span><span class="p">(</span><span class="n">optimizer</span><span class="p">)</span>
      <span class="n">scaler</span><span class="p">.</span><span class="n">update</span><span class="p">()</span>
      <span class="n">scheduler</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>
</code></pre></div></div>
<p>ë°ì´í„°ë¡œë”ë¡œë¶€í„° ë°›ì€ ì…ë ¥ë“¤ì„ ìƒì„±ìì— ë„£ê³  MLM ì˜ˆì¸¡ ê²°ê³¼, RTD ìˆ˜í–‰ì„ ìœ„í•´ í•„ìš”í•œ ìƒˆë¡œìš´ ë¼ë²¨ê°’ì„ ë°˜í™˜ ë°›ëŠ”ë‹¤. ê·¸ë¦¬ê³  ì´ê²ƒì„ ë‹¤ì‹œ íŒë³„ìì˜ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©í•˜ê³ , íŒë³„ìì˜ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ë°˜í™˜ë°›ì•„ ì„œë¡œ ë‹¤ë¥¸ ë‘ ëª¨ë¸ì— ëŒ€í•œ ê°€ì¤‘ ì†ì‹¤í•©ì‚°ì„ êµ¬í•œ ë’¤, ì˜µí‹°ë§ˆì´ì €ì— ë³´ë‚´ê³  ìµœì í™”ë¥¼ ìˆ˜í–‰í•œë‹¤. ì´ ë•Œ, ì²˜ìŒì— ë°ì´í„°ë¡œë”ê°€ ë°˜í™˜í•˜ëŠ” ì…ë ¥ ì‹œí€€ìŠ¤ì™€ ë¼ë²¨ì€ MLMì˜ ê·¸ê²ƒê³¼ ë™ì¼í•˜ë‹¤,</p>

<p>êµ¬í˜„í•˜ë©´ì„œ ê°€ì¥ ì–´ë ¤ì› ë˜ê²Œ, ì˜µí‹°ë§ˆì´ì € ë° ìŠ¤ì¼€ì¤„ëŸ¬ì˜ êµ¬ì„±ì´ì—ˆë‹¤. ë‘ ê°œì˜ ëª¨ë¸ì„ ê°™ì€ ìŠ¤íƒ­ì—ì„œ í•™ìŠµì‹œí‚¤ëŠ” ê²½í—˜ì´ ì²˜ìŒì´ë¼ì„œ ì²˜ìŒì— ëª¨ë¸ ê°œìˆ˜ë§Œí¼ ì˜µí‹°ë§ˆì´ì €ì™€ ìŠ¤ì¼€ì¤„ëŸ¬ ê°ì²´ë¥¼ ë§Œë“¤ì–´ì¤˜ì•¼ í•œë‹¤ê³  ìƒê°í–ˆë‹¤. íŠ¹íˆ ë‘ ëª¨ë¸ì˜ ìŠ¤ì¼€ì¼ì´ ë‹¤ë¥´ê¸° ë•Œë¬¸ì— ì„œë¡œ ë‹¤ë¥¸ ì˜µí‹°ë§ˆì´ì €, ìŠ¤ì¼€ì¤„ëŸ¬ ë„ì…ìœ¼ë¡œ ê°ê¸° ë‹¤ë¥¸ í•™ìŠµë¥ ì„ ì ìš©í•˜ëŠ”ê²Œ ì •í™•í•  ê²ƒì´ë¼ ìƒê°í–ˆë‹¤.</p>

<p>í•˜ì§€ë§Œ, ì˜µí‹°ë§ˆì´ì €ë¥¼ ë‘ ê°œ ì‚¬ìš©í•˜ëŠ” ê²ƒì€ ë§¤ìš° ë§ì€ ë©”ëª¨ë¦¬ë¥¼ ì°¨ì§€í•  ë¿ë”ëŸ¬ ë…¼ë¬¸ì—ì„œ ê³µê°œí•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° í…Œì´ë¸”ì„ ë³´ë©´ ë‘ ëª¨ë¸ì— ê°™ì€ í•™ìŠµë¥ ì„ ì ìš©í•˜ê³  ìˆëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆì—ˆë‹¤. ë”°ë¼ì„œ ê·¸ì— ë§ê²Œ ê°™ì€ ì˜µí‹°ë§ˆì´ì €, ìŠ¤ì¼€ì¤„ëŸ¬ë¥¼ ì‚¬ìš©í•´ ë™ì‹œì— ë‘ ëª¨ë¸ì´ í•™ìŠµë˜ë„ë¡ íŒŒì´í”„ë¼ì¸ì„ ë§Œë“¤ê²Œ ë˜ì—ˆë‹¤.</p>

<p>ì¶”ê°€ë¡œ, ê³µê°œëœ ì˜¤í”¼ì…œ ì½”ë“œ ì—­ì‹œ ë‹¨ì¼ ì˜µí‹°ë§ˆì´ì € ë° ìŠ¤ì¼€ì¤„ëŸ¬ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì„ í™•ì¸í–ˆë‹¤.</p>

<h4 id="-electra-module"><strong><code class="language-plaintext highlighter-rouge">ğŸŒ† ELECTRA Module</code></strong></h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="nn">experiment.models.abstract_model</span> <span class="kn">import</span> <span class="n">AbstractModel</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">Tensor</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">Callable</span>
<span class="kn">from</span> <span class="nn">einops.layers.torch</span> <span class="kn">import</span> <span class="n">Rearrange</span>
<span class="kn">from</span> <span class="nn">experiment.tuner.mlm</span> <span class="kn">import</span> <span class="n">MLMHead</span>
<span class="kn">from</span> <span class="nn">experiment.tuner.sbo</span> <span class="kn">import</span> <span class="n">SBOHead</span>
<span class="kn">from</span> <span class="nn">experiment.tuner.rtd</span> <span class="kn">import</span> <span class="n">get_discriminator_input</span><span class="p">,</span> <span class="n">RTDHead</span>
<span class="kn">from</span> <span class="nn">configuration</span> <span class="kn">import</span> <span class="n">CFG</span>

<span class="k">class</span> <span class="nc">ELECTRA</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">AbstractModel</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">cfg</span><span class="p">:</span> <span class="n">CFG</span><span class="p">,</span> <span class="n">model_func</span><span class="p">:</span> <span class="n">Callable</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">ELECTRA</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span> <span class="o">=</span> <span class="n">cfg</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">generator</span> <span class="o">=</span> <span class="n">model_func</span><span class="p">(</span><span class="n">cfg</span><span class="p">.</span><span class="n">generator_num_layers</span><span class="p">)</span>  <span class="c1"># init generator
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">mlm_head</span> <span class="o">=</span> <span class="n">MLMHead</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">rtd_masking</span> <span class="o">==</span> <span class="s">'SpanBoundaryObjective'</span><span class="p">:</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">mlm_head</span> <span class="o">=</span> <span class="n">SBOHead</span><span class="p">(</span>
                <span class="n">cfg</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">,</span>
                <span class="n">is_concatenate</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">is_concatenate</span><span class="p">,</span>
                <span class="n">max_span_length</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">max_span_length</span>
            <span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">discriminator</span> <span class="o">=</span> <span class="n">model_func</span><span class="p">(</span><span class="n">cfg</span><span class="p">.</span><span class="n">discriminator_num_layers</span><span class="p">)</span>  <span class="c1"># init generator
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">rtd_head</span> <span class="o">=</span> <span class="n">RTDHead</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">share_embed_method</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">share_embed_method</span>  <span class="c1"># instance, es, gdes
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">share_embedding</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">share_embedding</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="k">def</span> <span class="nf">discriminator_hook</span><span class="p">(</span><span class="n">module</span><span class="p">:</span> <span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">,</span> <span class="o">*</span><span class="n">inputs</span><span class="p">):</span>
            <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">share_embed_method</span> <span class="o">==</span> <span class="s">'instance'</span><span class="p">:</span>  <span class="c1"># Instance Sharing
</span>                <span class="bp">self</span><span class="p">.</span><span class="n">discriminator</span><span class="p">.</span><span class="n">embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">generator</span><span class="p">.</span><span class="n">embeddings</span>

            <span class="k">elif</span> <span class="bp">self</span><span class="p">.</span><span class="n">share_embed_method</span> <span class="o">==</span> <span class="s">'ES'</span><span class="p">:</span>  <span class="c1"># ES (Embedding Sharing)
</span>                <span class="bp">self</span><span class="p">.</span><span class="n">discriminator</span><span class="p">.</span><span class="n">embeddings</span><span class="p">.</span><span class="n">word_embedding</span><span class="p">.</span><span class="n">weight</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">generator</span><span class="p">.</span><span class="n">embeddings</span><span class="p">.</span><span class="n">word_embedding</span><span class="p">.</span><span class="n">weight</span>
                <span class="bp">self</span><span class="p">.</span><span class="n">discriminator</span><span class="p">.</span><span class="n">embeddings</span><span class="p">.</span><span class="n">abs_pos_emb</span><span class="p">.</span><span class="n">weight</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">generator</span><span class="p">.</span><span class="n">embeddings</span><span class="p">.</span><span class="n">abs_pos_emb</span><span class="p">.</span><span class="n">weight</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">discriminator</span><span class="p">.</span><span class="n">register_forward_pre_hook</span><span class="p">(</span><span class="n">discriminator_hook</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">generator_fw</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span>
            <span class="n">inputs</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
            <span class="n">labels</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
            <span class="n">padding_mask</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
            <span class="n">mask_labels</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
            <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="bp">None</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
        <span class="n">g_last_hidden_states</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">generator</span><span class="p">(</span>
            <span class="n">inputs</span><span class="p">,</span>
            <span class="n">padding_mask</span><span class="p">,</span>
            <span class="n">attention_mask</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">rtd_masking</span> <span class="o">==</span> <span class="s">'MaskedLanguageModel'</span><span class="p">:</span>
            <span class="n">g_logit</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">mlm_head</span><span class="p">(</span>
                <span class="n">g_last_hidden_states</span>
            <span class="p">)</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">rtd_masking</span> <span class="o">==</span> <span class="s">'SpanBoundaryObjective'</span><span class="p">:</span>
            <span class="n">g_logit</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">mlm_head</span><span class="p">(</span>
                <span class="n">g_last_hidden_states</span><span class="p">,</span>
                <span class="n">mask_labels</span>
            <span class="p">)</span>
        <span class="n">pred</span> <span class="o">=</span> <span class="n">g_logit</span><span class="p">.</span><span class="n">clone</span><span class="p">().</span><span class="n">detach</span><span class="p">()</span>
        <span class="n">d_inputs</span><span class="p">,</span> <span class="n">d_labels</span> <span class="o">=</span> <span class="n">get_discriminator_input</span><span class="p">(</span>
            <span class="n">inputs</span><span class="p">,</span>
            <span class="n">labels</span><span class="p">,</span>
            <span class="n">pred</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">g_logit</span><span class="p">,</span> <span class="n">d_inputs</span><span class="p">,</span> <span class="n">d_labels</span>

    <span class="k">def</span> <span class="nf">discriminator_fw</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span>
            <span class="n">inputs</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
            <span class="n">padding_mask</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
            <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="bp">None</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="n">d_last_hidden_states</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">discriminator</span><span class="p">(</span>
            <span class="n">inputs</span><span class="p">,</span>
            <span class="n">padding_mask</span><span class="p">,</span>
            <span class="n">attention_mask</span>
        <span class="p">)</span>
        <span class="n">d_logit</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">rtd_head</span><span class="p">(</span>
            <span class="n">d_last_hidden_states</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">d_logit</span>

</code></pre></div></div>
<p>ELECTRA ëª¨ë¸ ê°ì²´ëŠ” í¬ê²Œ ì„ë°°ë”© ë ˆì´ì–´ ê³µìœ , ìƒì„±ì í¬ì›Œë“œ, íŒë³„ì í¬ì›Œë“œ íŒŒíŠ¸ë¡œ ë‚˜ë‰œë‹¤. ë¨¼ì € ì„ë² ë”© ë ˆì´ì–´ ê³µìœ ëŠ” í¬ê²Œ ë‘ ê°€ì§€ ë°©ì‹ìœ¼ë¡œ êµ¬í˜„ ê°€ëŠ¥í•˜ë‹¤. í•˜ë‚˜ëŠ” ì„ë² ë”© ë ˆì´ì–´ ì¸ìŠ¤í„´ìŠ¤ ìì²´ë¥¼ ê³µìœ í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ, ìƒì„±ìì™€ íŒë³„ìì˜ ìŠ¤ì¼€ì¼ì´ ë™ì¼í•  ë•Œ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤. ë‚˜ë¨¸ì§€ëŠ” ë‹¨ì–´ ì„ë² ë”©, í¬ì§€ì…˜ ì„ë² ë”©ì˜ ê°€ì¤‘ì¹˜ í–‰ë ¬ë§Œ ê³µìœ í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ, ì„œë¡œ ìŠ¤ì¼€ì¼ì´ ë‹¬ë¼ë„ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤. ë…¼ë¬¸ì—ì„œ ì œì‹œí•˜ëŠ” ê°€ì¥ íš¨ìœ¨ì ì¸ ë°©ë²•ì€ í›„ìì´ë©°, íŒë³„ìì˜ ì„ë² ë”© í–‰ë ¬ì´ ìƒì„±ìì˜ ì„ë² ë”© í–‰ë ¬ì˜ ì£¼ì†Œë¥¼ ê°€ë¦¬í‚¤ë„ë¡ í•¨ìœ¼ë¡œì„œ êµ¬í˜„ ê°€ëŠ¥í•˜ë‹¤.</p>

<h4 id="-rtd-input-making"><strong><code class="language-plaintext highlighter-rouge">ğŸŒ† RTD Input Making</code></strong></h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">Tensor</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Tuple</span>
<span class="kn">from</span> <span class="nn">configuration</span> <span class="kn">import</span> <span class="n">CFG</span>

<span class="k">def</span> <span class="nf">get_discriminator_input</span><span class="p">(</span><span class="n">inputs</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">labels</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">pred</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
    <span class="s">""" Post Processing for Replaced Token Detection Task
    1) get index of the highest probability of [MASK] token in pred tensor
    2) convert [MASK] token to prediction token
    3) make label for Discriminator

    Args:
        inputs: pure inputs from tokenizing by tokenizer
        labels: labels for masked language modeling
        pred: prediction tensor from Generator

    returns:
        d_inputs: torch.Tensor, shape of [Batch, Sequence], for Discriminator inputs
        d_labels: torch.Tensor, shape of [Sequence], for Discriminator labels
    """</span>
    <span class="c1"># 1) flatten pred to 2D Tensor
</span>    <span class="n">d_inputs</span><span class="p">,</span> <span class="n">d_labels</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">.</span><span class="n">clone</span><span class="p">().</span><span class="n">detach</span><span class="p">().</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="bp">None</span>  <span class="c1"># detach to prevent back-propagation
</span>    <span class="n">flat_pred</span><span class="p">,</span> <span class="n">flat_label</span> <span class="o">=</span> <span class="n">pred</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">pred</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)),</span> <span class="n">labels</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># (batch * sequence, vocab_size)
</span>
    <span class="c1"># 2) get index of the highest probability of [MASK] token
</span>    <span class="n">pred_token_idx</span><span class="p">,</span> <span class="n">mlm_mask_idx</span> <span class="o">=</span> <span class="n">flat_pred</span><span class="p">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">),</span> <span class="n">torch</span><span class="p">.</span><span class="n">where</span><span class="p">(</span><span class="n">flat_label</span> <span class="o">!=</span> <span class="o">-</span><span class="mi">100</span><span class="p">)</span>
    <span class="n">pred_tokens</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">index_select</span><span class="p">(</span><span class="n">pred_token_idx</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">mlm_mask_idx</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

    <span class="c1"># 3) convert [MASK] token to prediction token
</span>    <span class="n">d_inputs</span><span class="p">[</span><span class="n">mlm_mask_idx</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span> <span class="o">=</span> <span class="n">pred_tokens</span>

    <span class="c1"># 4) make label for Discriminator
</span>    <span class="n">original_tokens</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">.</span><span class="n">clone</span><span class="p">().</span><span class="n">detach</span><span class="p">().</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">original_tokens</span><span class="p">[</span><span class="n">mlm_mask_idx</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span> <span class="o">=</span> <span class="n">flat_label</span><span class="p">[</span><span class="n">mlm_mask_idx</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span>
    <span class="n">d_labels</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">eq</span><span class="p">(</span><span class="n">original_tokens</span><span class="p">,</span> <span class="n">d_inputs</span><span class="p">).</span><span class="nb">long</span><span class="p">()</span>
    <span class="n">d_inputs</span> <span class="o">=</span> <span class="n">d_inputs</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="n">pred</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># covert to [batch, sequence]
</span>    <span class="k">return</span> <span class="n">d_inputs</span><span class="p">,</span> <span class="n">d_labels</span>
</code></pre></div></div>
<p>ì´ì œ ë§ˆì§€ë§‰ìœ¼ë¡œ íŒë³„ìì˜ ì…ë ¥ì„ ë§Œë“œëŠ” ì•Œê³ ë¦¬ì¦˜ì— ëŒ€í•œ ì½”ë“œë¥¼ ë³´ì. ì•Œê³ ë¦¬ì¦˜ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.</p>
<ul>
  <li>1) ê°œë³„ ë§ˆìŠ¤í‚¹ í† í°ì— ëŒ€í•œ ì˜ˆì¸¡ í† í° êµ¬í•˜ê¸°
    <ul>
      <li>ë¡œì§“ì„ ì‹¤ì œ í† í° ì¸ë±ìŠ¤ë¡œ ë³€í™˜</li>
    </ul>
  </li>
  <li>2) ëª¨ë“  ë§ˆìŠ¤í‚¹ ë¶€ë¶„ì— ì˜ˆì¸¡ í† í°ë“¤ë¡œ ëŒ€ì²´</li>
  <li>3) ê¸°ì¡´ ì…ë ¥ê³¼ 2ë²ˆìœ¼ë¡œ ë§Œë“¤ì–´ì§„ ì‹œí€€ìŠ¤ ë¹„êµí•´ ë¼ë²¨ ìƒì„±
    <ul>
      <li>ì„œë¡œ ê°™ìœ¼ë©´ 0</li>
      <li>ì„œë¡œ ë‹¤ë¥´ë©´ 1
ì´ë ‡ê²Œ ë§Œë“¤ì–´ì§„ ìƒˆë¡œìš´ ì…ë ¥ ì‹œí€€ìŠ¤ì™€ ë¼ë²¨ì„ ELECTRA ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ì˜ íŒë³„ì í¬ì›Œë“œ ë©”ì„œë“œì— ì¸ìë¡œ ì „ë‹¬í•˜ë©´ ëœë‹¤.</li>
    </ul>
  </li>
</ul>

<h3 id="-future-work-ì½ê³ -êµ¬í˜„í•˜ë©´ì„œ-ëŠë‚€ì --ê°œì„ ë°©í–¥"><code class="language-plaintext highlighter-rouge">ğŸŒŸ Future Work (ì½ê³  êµ¬í˜„í•˜ë©´ì„œ ëŠë‚€ì  &amp; ê°œì„ ë°©í–¥)</code></h3>

<p>ì´ë ‡ê²Œ ELECTRA ëª¨ë¸ì— ëŒ€í•œ êµ¬í˜„ê¹Œì§€ ì‚´í´ë´¤ë‹¤. ë…¼ë¬¸ì„ ì½ê³  êµ¬í˜„í•˜ë©´ì„œ ê°€ì¥ ì˜ë¬¸ìŠ¤ëŸ¬ì› ë˜ ë¶€ë¶„ì€ ì„ë² ë”© ê³µìœ  ë°©ë²•ì´ì—ˆë‹¤. ìˆ˜í•™ì ìœ¼ë¡œ ì—„ë°€í•˜ê²Œ ê³„ì‚°í•˜ê³  ë”°ì ¸ë³´ì§€ ëª»í–ˆì§€ë§Œ, ì§ê´€ì ìœ¼ë¡œë„ ìƒì„±ìì˜ MLMê³¼ íŒë³„ìì˜ RTDëŠ” ì„œë¡œ ì„±ê²©ì´ ìƒë‹¹íˆ ë‹¤ë¥¸ ì‚¬ì „ í•™ìŠµ ë°©ë²•ë¡ ì´ë¼ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤. ê·¸ë ‡ë‹¤ë©´ ë‹¨ìˆœíˆ ë‹¨ì–´, í¬ì§€ì…˜ ì„ë² ë”©ì„ ê³µìœ í•˜ëŠ” ê²½ìš° í•™ìŠµ ë°©í–¥ì„±ì´ ë‹¬ë¼ì„œ ê°„ì„­ì´ ë°œìƒí•˜ê³  ëª¨ë¸ì´ ìˆ˜ë ´í•˜ì§€ ëª»í•  ì—¬ì§€ê°€ ìƒê¸´ë‹¤. ì´ëŸ¬í•œ <code class="language-plaintext highlighter-rouge">ì¤„ë‹¤ë¦¬ê¸° í˜„ìƒ(tag-of-war)</code>ì„ ì–´ë–»ê²Œ í•´ê²°í•  ìˆ˜ ìˆì„ê¹Œì— ëŒ€í•œ ê³ ë¯¼ì´ ë” í•„ìš”í•˜ë‹¤ê³  ìƒê°í•œë‹¤.</p>

<p>ê·¸ë˜ì„œ ë‹¤ìŒ í¬ìŠ¤íŒ…ì—ì„œëŠ” ì´ëŸ¬í•œ ì¤„ë‹¤ë¦¬ê¸° í˜„ìƒì„ í•´ê²°í•˜ê³ ìí•œ ë…¼ë¬¸ì¸ <strong><a href="https://arxiv.org/abs/2111.09543">&lt;DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing&gt;</a></strong>ì„ ë¦¬ë·°í•´ë³´ê³ ì í•œë‹¤.</p>

        
      </section>

      <footer class="page__meta">
        
        
  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      <a href="/tags/#bert" class="page__taxonomy-item p-category" rel="tag">BERT</a><span class="sep">, </span>
    
      <a href="/tags/#electra" class="page__taxonomy-item p-category" rel="tag">ELECTRA</a><span class="sep">, </span>
    
      <a href="/tags/#gan" class="page__taxonomy-item p-category" rel="tag">GAN</a><span class="sep">, </span>
    
      <a href="/tags/#natural-language-process" class="page__taxonomy-item p-category" rel="tag">Natural Language Process</a><span class="sep">, </span>
    
      <a href="/tags/#pytorch" class="page__taxonomy-item p-category" rel="tag">Pytorch</a><span class="sep">, </span>
    
      <a href="/tags/#self-attention" class="page__taxonomy-item p-category" rel="tag">Self-Attention</a><span class="sep">, </span>
    
      <a href="/tags/#transformer" class="page__taxonomy-item p-category" rel="tag">Transformer</a>
    
    </span>
  </p>




  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> Categories: </strong>
    <span itemprop="keywords">
    
      <a href="/categories/#nlp" class="page__taxonomy-item p-category" rel="tag">NLP</a>
    
    </span>
  </p>


        

  <p class="page__date"><strong><i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> Updated:</strong> <time class="dt-published" datetime="2024-03-11">March 11, 2024</time></p>

      </footer>

      <section class="page__share">
  
    <h4 class="page__share-title">Share on</h4>
  

  <a href="https://twitter.com/intent/tweet?text=%F0%9F%91%AE+%5BELECTRA%5D+Pre-training+Text+Encoders+as+Discriminators+Rather+Than+Generators%20http%3A%2F%2Flocalhost%3A4000%2Fnlp%2Felectra" class="btn btn--twitter" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Twitter"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=http%3A%2F%2Flocalhost%3A4000%2Fnlp%2Felectra" class="btn btn--facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Facebook"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=http%3A%2F%2Flocalhost%3A4000%2Fnlp%2Felectra" class="btn btn--linkedin" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on LinkedIn"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>


      
  <nav class="pagination">
    
      <a href="/nlp/distilbert" class="pagination--pager" title="ğŸ§‘â€ğŸ« [DistilBERT] DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter
">Previous</a>
    
    
      <a href="/nlp/roformer" class="pagination--pager" title="ğŸ¡ [Roformer] RoFormer: Enhanced Transformer with Rotary Position Embedding
">Next</a>
    
  </nav>

    </div>

    
  </article>

  
  
    <div class="page__related">
      <h2 class="page__related-title">You may also enjoy</h2>
      <div class="grid__wrapper">
        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
      <div class="archive__item-teaser">
        <img src="/assets/images/huggingface_emoji.png" alt="">
      </div>
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/nlp/linear_attention" rel="permalink">ğŸŒ† [Linear Attention] Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention
</a>
      
    </h2>
    <!-- 

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          8 minute read
        
      </span>
    
  </p>

 -->
    
      <p class="page__meta"><i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i> March 14 2024</p>
    
    <p class="archive__item-excerpt" itemprop="description">Linear Attention Official Paper Review with Pytorch Implementation
</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
      <div class="archive__item-teaser">
        <img src="/assets/images/huggingface_emoji.png" alt="">
      </div>
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/nlp/spanbert" rel="permalink">ğŸ—‚ï¸[SpanBERT] SpanBERT: Improving Pre-training by Representing and Predicting Spans
</a>
      
    </h2>
    <!-- 

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          6 minute read
        
      </span>
    
  </p>

 -->
    
      <p class="page__meta"><i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i> March 11 2024</p>
    
    <p class="archive__item-excerpt" itemprop="description">SpanBERT Official Paper Review with Pytorch Implementation
</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
      <div class="archive__item-teaser">
        <img src="/assets/images/huggingface_emoji.png" alt="">
      </div>
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/nlp/roformer" rel="permalink">ğŸ¡ [Roformer] RoFormer: Enhanced Transformer with Rotary Position Embedding
</a>
      
    </h2>
    <!-- 

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          12 minute read
        
      </span>
    
  </p>

 -->
    
      <p class="page__meta"><i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i> March 11 2024</p>
    
    <p class="archive__item-excerpt" itemprop="description">Roformer Official Paper Review with Pytorch Implementation
</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
      <div class="archive__item-teaser">
        <img src="/assets/images/huggingface_emoji.png" alt="">
      </div>
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/nlp/distilbert" rel="permalink">ğŸ§‘â€ğŸ« [DistilBERT] DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter
</a>
      
    </h2>
    <!-- 

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          8 minute read
        
      </span>
    
  </p>

 -->
    
      <p class="page__meta"><i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i> March 11 2024</p>
    
    <p class="archive__item-excerpt" itemprop="description">DistilBERT Official Paper Review with Pytorch Implementation
</p>
  </article>
</div>

        
      </div>
    </div>
  
  
</div>

    </div>

    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>Follow:</strong></li>
    

    
      
        
      
        
      
        
      
        
      
        
      
        
      
    

    
      <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
    
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2024 qcqced. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>







    <script>
  'use strict';

  (function() {
    var commentContainer = document.querySelector('#utterances-comments');

    if (!commentContainer) {
      return;
    }

    var script = document.createElement('script');
    script.setAttribute('src', 'https://utteranc.es/client.js');
    script.setAttribute('repo', 'qcqced123/qcqced123.github.io');
    script.setAttribute('issue-term', 'pathname');
    
    script.setAttribute('theme', 'github-light');
    script.setAttribute('crossorigin', 'anonymous');

    commentContainer.appendChild(script);
  })();
</script>

  




<script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML">
</script>

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"] ],
    displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
    processEscapes: true
    },
    "HTML-CSS": { availableFonts: ["TeX"] }
});
</script>

  </body>
</html>
