<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.24.0 by Michael Rose
  Copyright 2013-2020 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->
<html lang="en" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>ğŸ—‚ï¸[SpanBERT] SpanBERT: Improving Pre-training by Representing and Predicting Spans - AI/Business Study Log</title>
<meta name="description" content="SpanBERT Official Paper Review with Pytorch Implementation">


  <meta name="author" content="qcqced">
  
  <meta property="article:author" content="qcqced">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="AI/Business Study Log">
<meta property="og:title" content="ğŸ—‚ï¸[SpanBERT] SpanBERT: Improving Pre-training by Representing and Predicting Spans">
<meta property="og:url" content="http://localhost:4000/nlp/spanbert">


  <meta property="og:description" content="SpanBERT Official Paper Review with Pytorch Implementation">







  <meta property="article:published_time" content="2024-03-11T00:00:00+09:00">



  <meta property="article:modified_time" content="2024-03-12T02:00:00+09:00">



  

  


<link rel="canonical" href="http://localhost:4000/nlp/spanbert">




<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    
      "@type": "Person",
      "name": "qcqced",
      "url": "http://localhost:4000/"
    
  }
</script>







<!-- end _includes/seo.html -->



  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="AI/Business Study Log Feed">


<!-- https://t.co/dKP3o1e -->
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css"></noscript>



<!-- Latex -->
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->
<link rel="apple-touch-icon" sizes="180x180" href="/assets/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/assets/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/assets/favicon-16x16.png">
<link rel="manifest" href="/assets/site.webmanifest">
<link rel="mask-icon" href="/assets/safari-pinned-tab.svg" color="#5bbad5">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="theme-color" content="#ffffff">
<!-- end custom head snippets -->

    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      TeX: {
        equationNumbers: {
          autoNumber: "AMS"
        }
      },
      tex2jax: {
      inlineMath: [ ['$', '$'] ],
      displayMath: [ ['$$', '$$'] ],
      processEscapes: true
    }
  });
  MathJax.Hub.Register.MessageHook("Math Processing Error",function (message) {
      alert("Math Processing Error: "+message[1]);
    });
  MathJax.Hub.Register.MessageHook("TeX Jax - parse error",function (message) {
      alert("Math Processing Error: "+message[1]);
    });
  </script>
  <script type="text/javascript" async
    src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>
  </head>

  <body class="layout--single">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
          AI/Business Study Log
          <span class="site-subtitle">NLP, Marketing</span>
        </a>
        
        
        <ul class="visible-links">
              
              
                  <li class="masthead__menu-item">
                      <a href="https://qcqced123.github.io/">Home</a>
                  </li>
              
          
              
              
                  <li class="dropdown ">
                      <a class="dropdown-toggle" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">CS/AI  <i class="fa fa-caret-down fa-sm" aria-hidden="true"></i><span class="caret"></span></a>
       
                      <ul class="dropdown-content" >
                          
                              <a class = "dropdown-item" href="/nlp/">    Natural Language Process</a>
                          
                              <a class = "dropdown-item" href="/multi-modal/">    Multi Modal</a>
                          
                              <a class = "dropdown-item" href="/cv/">    Computer Vision</a>
                          
                              <a class = "dropdown-item" href="/ml/">    Machine Learning</a>
                          
                              <a class = "dropdown-item" href="/framework-library/">    Framework & Library</a>
                          
                              <a class = "dropdown-item" href="/python/">    Python</a>
                          
                              <a class = "dropdown-item" href="/algorithm/">    Data Structure & Algorithm</a>
                          
                              <a class = "dropdown-item" href="/ps/">    Problem Solving</a>
                          
                      </ul>
                  </li>
              
          
              
              
                  <li class="dropdown ">
                      <a class="dropdown-toggle" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Math  <i class="fa fa-caret-down fa-sm" aria-hidden="true"></i><span class="caret"></span></a>
       
                      <ul class="dropdown-content" >
                          
                              <a class = "dropdown-item" href="/linear-algebra/">    Linear Algebra</a>
                          
                              <a class = "dropdown-item" href="/optimization-theory/">    Optimization Theory/Calculus</a>
                          
                              <a class = "dropdown-item" href="/signal-system/">    Signal & System</a>
                          
                      </ul>
                  </li>
              
          
              
              
                  <li class="dropdown ">
                      <a class="dropdown-toggle" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Business/Marketing  <i class="fa fa-caret-down fa-sm" aria-hidden="true"></i><span class="caret"></span></a>
       
                      <ul class="dropdown-content" >
                          
                              <a class = "dropdown-item" href="/device/">    Device</a>
                          
                              <a class = "dropdown-item" href="/semi-conductor/">    Semi-Conductor</a>
                          
                              <a class = "dropdown-item" href="/ai/">    AI</a>
                          
                      </ul>
                  </li>
              
          
              
              
                  <li class="masthead__menu-item">
                      <a href="/categories/">Category</a>
                  </li>
              
          
              
              
                  <li class="masthead__menu-item">
                      <a href="/about/">About</a>
                  </li>
              
          
       </ul>
       
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      





<div id="main" role="main">
  
  <div class="sidebar sticky">
  


<div itemscope itemtype="https://schema.org/Person" class="h-card">

  
    <div class="author__avatar">
      <a href="http://localhost:4000/">
        <img src="/assets/images/huggingface_emoji.png" alt="qcqced" itemprop="image" class="u-photo">
      </a>
    </div>
  

  <div class="author__content">
    <h3 class="author__name p-name" itemprop="name">
      <a class="u-url" rel="me" href="http://localhost:4000/" itemprop="url">qcqced</a>
    </h3>
    
      <div class="author__bio p-note" itemprop="description">
        <p>Interested in NLP, Marketing</p>

      </div>
    
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">Follow</button>
    <ul class="author__urls social-icons">
      
        <li itemprop="homeLocation" itemscope itemtype="https://schema.org/Place">
          <i class="fas fa-fw fa-map-marker-alt" aria-hidden="true"></i> <span itemprop="name" class="p-locality">Seoul, South Korea</span>
        </li>
      

      
        
          
            <li><a href="https://qcqced123.github.io" rel="nofollow noopener noreferrer me" itemprop="sameAs"><i class="fas fa-fw fa-link" aria-hidden="true"></i><span class="label">Website</span></a></li>
          
        
          
            <li><a href="https://github.com/qcqced123" rel="nofollow noopener noreferrer me" itemprop="sameAs"><i class="fab fa-fw fa-github" aria-hidden="true"></i><span class="label">GitHub</span></a></li>
          
        
          
            <li><a href="https://www.kaggle.com/qcqced" rel="nofollow noopener noreferrer me" itemprop="sameAs"><i class="fab fa-fw fa-kaggle" aria-hidden="true"></i><span class="label">Kaggle</span></a></li>
          
        
      

      

      
        <li>
          <a href="mailto:qcqced123@gmail.com" rel="me" class="u-email">
            <meta itemprop="email" content="qcqced123@gmail.com" />
            <i class="fas fa-fw fa-envelope-square" aria-hidden="true"></i><span class="label">Email</span>
          </a>
        </li>
      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      <!--
  <li>
    <a href="http://link-to-whatever-social-network.com/user/" itemprop="sameAs" rel="nofollow noopener noreferrer me">
      <i class="fas fa-fw" aria-hidden="true"></i> Custom Social Profile Link
    </a>
  </li>
-->
    </ul>
  </div>
</div>

  
  </div>



  <article class="page h-entry" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="ğŸ—‚ï¸[SpanBERT] SpanBERT: Improving Pre-training by Representing and Predicting Spans">
    <meta itemprop="description" content="SpanBERT Official Paper Review with Pytorch Implementation">
    <meta itemprop="datePublished" content="2024-03-11T00:00:00+09:00">
    <meta itemprop="dateModified" content="2024-03-12T02:00:00+09:00">

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title p-name" itemprop="headline">
            <a href="http://localhost:4000/nlp/spanbert" class="u-url" itemprop="url">ğŸ—‚ï¸[SpanBERT] SpanBERT: Improving Pre-training by Representing and Predicting Spans
</a>
          </h1>
          <p class="page__date">
            <a href="https://hits.seeyoufarm.com/localhost:4000/nlp/spanbert"target="_blank">
              <img src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https://localhost:4000/nlp/spanbert&count_bg=%23399DE2&title_bg=%236D6D6D&icon=pytorch.svg&icon_color=%23E7E7E7&title=Views&edge_flat=false"/>
            </a>
            <i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> Updated: <time datetime="2024-03-11T00:00:00+09:00">March 11, 2024</time>
            <!-- <div style="text-align: left;"> -->
            <!-- </div> -->
          </p>
          
          
        </header>
      

      <section class="page__content e-content" itemprop="text">
        
          <aside class="sidebar__right sticky">
            <nav class="toc">
              <header><h4 class="nav__title"><i class="fas fa-file-alt"></i> On this page</h4></header>
              <ul class="toc__menu"><li><a href="#overview">ğŸ”­Â Overview</a></li><li><a href="#sbo-span-boundary-objective">ğŸ“šÂ SBO: Span Boundary Objective</a></li><li><a href="#implementation-by-pytorch">ğŸ‘©â€ğŸ’»Â Implementation by Pytorch</a><ul><li><a href="#span-masking-algoritm">ğŸ‘©â€ğŸ’»Â Span Masking Algoritm</a></li><li><a href="#sbo-head">ğŸ‘©â€ğŸ’»Â SBO Head</a></li></ul></li></ul>

            </nav>
          </aside>
        
        <h3 id="overview"><code class="language-plaintext highlighter-rouge">ğŸ”­Â Overview</code></h3>

<p><code class="language-plaintext highlighter-rouge">SpanBERT</code>ëŠ” 2020ë…„ í˜ì´ìŠ¤ë¶ì—ì„œ ë°œí‘œí•œ BERT ê³„ì—´ ëª¨ë¸ë¡œ, ìƒˆë¡œìš´ ë°©ë²•ë¡ ì¸ <code class="language-plaintext highlighter-rouge">SBO(Span Boundary Objective)</code>ë¥¼ ê³ ì•ˆí•´ ì‚¬ì „í•™ìŠµì„ í•˜ì—¬ ê¸°ì¡´ ëŒ€ë¹„ ë†’ì€ ì„±ëŠ¥ì„ ê¸°ë¡í–ˆë‹¤. ê¸°ì¡´ <code class="language-plaintext highlighter-rouge">MLM</code>, <code class="language-plaintext highlighter-rouge">CLM</code>ì€ ë‹¨ì¼ í† í°ì„ ì˜ˆì¸¡í•˜ëŠ” ë°©ì‹ì„ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì— Word-Level Taskì— ì•„ì£¼ ì í•©í•˜ì§€ë§Œ ìƒëŒ€ì ìœ¼ë¡œ QA, Sentence-Similarity ê°™ì€ ë¬¸ì¥ ë‹¨ìœ„ í…ŒìŠ¤í¬ì— ê·¸ëŒ€ë¡œ í™œìš©í•˜ê¸°ì—ëŠ” ë¶€ì¡±í•œ ì ì´ ìˆì—ˆë‹¤. ì´ëŸ¬í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ê³ ì•ˆëœ ë°©ë²•ë¡ ì´ ë°”ë¡œ <code class="language-plaintext highlighter-rouge">SBO</code>ë‹¤. <code class="language-plaintext highlighter-rouge">SBO</code>ë€, MLMê³¼ ë¹„ìŠ·í•˜ì§€ë§Œ, Span(ì ˆâ€¢êµ¬ë¬¸) ë‹¨ìœ„ë¡œ ë§ˆìŠ¤í‚¹í•˜ê³  ë‹¤ì‹œ Denoisingì„ í•˜ê¸° ë•Œë¬¸ì—, Sentence-Level Taskì— ì†í•˜ëŠ” Down-Stream Taskë¥¼ ìœ„í•œ ëª¨ë¸ì˜ ì‚¬ì „ í›ˆë ¨ìœ¼ë¡œ ì í•©í•˜ë‹¤.</p>

<p>ì •ë¦¬í•˜ìë©´, <code class="language-plaintext highlighter-rouge">SpanBERT</code> ëª¨ë¸ì€ ê¸°ì¡´ BERTì˜ êµ¬ì¡°ì  ì¸¡ë©´ ê°œì„ ì´ ì•„ë‹Œ, ì‚¬ì „í•™ìŠµ ë°©ë²•ì— ëŒ€í•œ ê°œì„  ì‹œë„ë¼ê³  ë³¼ ìˆ˜ ìˆë‹¤. ë”°ë¼ì„œ ì–´ë–¤ ëª¨ë¸ì´ë”ë¼ë„, ì¸ì½”ë” ì–¸ì–´ ëª¨ë¸ì´ë¼ë©´ ëª¨ë‘ <code class="language-plaintext highlighter-rouge">SpanBERT</code> êµ¬ì¡°ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìœ¼ë©°, ê¸°ì¡´ ë…¼ë¬¸ì—ì„œëŠ” ì›ë³¸ BERT êµ¬ì¡°ë¥¼ ì‚¬ìš©í–ˆë‹¤. ê·¸ë˜ì„œ ë³¸ í¬ìŠ¤íŒ…ì—ì„œë„ BERTì— ëŒ€í•œ ì„¤ëª… ì—†ì´ SBOì— ëŒ€í•´ì„œë§Œ ë‹¤ë£¨ë ¤ê³  í•œë‹¤.</p>

<h3 id="sbo-span-boundary-objective"><code class="language-plaintext highlighter-rouge">ğŸ“šÂ SBO: Span Boundary Objective</code></h3>

<p align="center">
<img src="/assets/images/spanbert/sbo.png" alt="SBO Task" class="align-center image-caption" width="100%&quot;, height=&quot;100%" />
<strong><em><a href="https://arxiv.org/abs/1907.10529">SBO Task</a></em></strong>
</p>

<p><strong>[SBO Algorithm Summary]</strong></p>
<ul>
  <li><strong>1) ì—°ì†ëœ ë²”ìœ„ì˜ Span ìƒì„±</strong>
    <ul>
      <li><strong>ë¬´ì‘ìœ„ë¡œ Spanì˜ ì–‘ìª½ ë í† í° ì§€ì • ($x_{4}, x_{9}$)</strong>
        <ul>
          <li><strong>$x_{5}$ to $x_{8}$ ì€ ìŠ¤íŒ¬ ë‚´ë¶€ í† í°</strong></li>
        </ul>
      </li>
      <li><strong>ë§ˆìŠ¤í‚¹ ì˜ˆì‚° ê³„ì‚°</strong>
        <ul>
          <li><strong>ë¬¸ì¥ ë‹¹ ë§ˆìŠ¤í‚¹ ì˜ˆì‚°(í•©ì‚° Span ê¸¸ì´)ì€ ë¬¸ì¥ ê¸¸ì´ì˜ 15%</strong></li>
          <li><strong>ì˜ˆì‹œ ì‹œí€€ìŠ¤ ê¸¸ì´: 512</strong></li>
          <li><strong>ë§ˆìŠ¤í‚¹ ì˜ˆì‚°: ëŒ€ëµ 75 = 512*0.15</strong></li>
        </ul>
      </li>
      <li><strong>ê¸°í•˜ ë¶„í¬ ì‚¬ìš©í•´ì„œ ê°œë³„ ìŠ¤íŒ¬ ê¸¸ì´ ì§€ì •</strong>
        <ul>
          <li><strong>ê°œë³„ ìŠ¤íŒ¬ë‹¹ ìµœëŒ€ ê¸¸ì´ ì§€ì •, ìµœëŒ€ 10ì´ ë„˜ì§€ ì•Šë„ë¡ ì„¤ì •</strong></li>
          <li><strong>ìµœëŒ€ ìŠ¤íŒ¬ í•©ì‚° ê¸¸ì´ ë„ë‹¬ê¹Œì§€ ë§ˆìŠ¤í‚¹ ë°˜ë³µ</strong>
            <ul>
              <li><strong>ë‚¨ì€ ë§ˆìŠ¤í‚¹ ì˜ˆì‚° &lt; í˜„ì¬ ìŠ¤íŒ¬ ê¸¸ì´</strong>
                <ul>
                  <li><strong>ë‚¨ì€ ë§ˆìŠ¤í‚¹ ì˜ˆì‚°ì„ í˜„ì¬ ìŠ¤íŒ¬ ê¸¸ì´ë¡œ ì„¤ì •</strong></li>
                </ul>
              </li>
            </ul>
          </li>
        </ul>
      </li>
      <li><strong>ë”°ë¼ì„œ Subword Tokenizingì´ ì•„ë‹ˆë¼ Whole Word Masking ë‹¨ìœ„ ì‘ì—…ì´ í•„ìš”</strong></li>
    </ul>
  </li>
  <li><strong>2) ì‹œì‘ í† í° ê¸°ì¤€, ìƒëŒ€ ìœ„ì¹˜ ê³„ì‚°</strong>
    <ul>
      <li><strong>ìŠ¤íŒ¬ ë‚´ë¶€ í† í°ì˜ ìƒëŒ€ ìœ„ì¹˜ ì„ë² ë”© ìƒì„± ë° ê³„ì‚°</strong></li>
      <li><strong>ì‹œì‘í† í°, ë§ˆì§€ë§‰í† í°, ìŠ¤íŒ¬ ë‚´ë¶€ í† í°ì˜ ìƒëŒ€ ìœ„ì¹˜ ì„ë² ë”©ì„ concat, ì€ë‹‰ ë²¡í„° ìƒì„±</strong></li>
      <li><strong>SpanHeadì— ì€ë‹‰ ë²¡í„° í†µê³¼ì‹œí‚¤ê¸°</strong></li>
    </ul>
  </li>
  <li><strong>3) SpanHead ì¶œë ¥ê°’ì„ ë§ˆìŠ¤í‚¹ì— ëŒ€í•œ ì˜ˆì¸¡ í‘œí˜„ìœ¼ë¡œ ì‚¬ìš©</strong></li>
</ul>

<p>SBOì˜ ì•„ì´ë””ì–´ ìì²´ëŠ” ìƒë‹¹íˆ ê°„ë‹¨í•˜ë‹¤. ê¸°ì¡´ MLMì²˜ëŸ¼ ë¬´ì‘ìœ„ë¡œ ì‹œí€€ìŠ¤ì—ì„œ ì•„ë¬´ í† í°ì´ë‚˜ ì„ íƒí•˜ëŠ”ê²Œ ì•„ë‹ˆë¼, ì£¼ì–´ì§„ ë¬¸ì¥ì—ì„œ ì¼ì • ê¸¸ì´ì˜ ì—°ì†ëœ í† í°ë“¤ì„ í•œë²ˆì— ì„ íƒí•´ ë§ˆìŠ¤í‚¹ ì²˜ë¦¬í•˜ì—¬ í•™ìŠµí•˜ê² ë‹¤ëŠ” ê²ƒì´ë‹¤. ë…¼ë¬¸ì—ì„œ ì œì‹œí•œ SBO ì•Œê³ ë¦¬ì¦˜ì„ ì •ë¦¬í•˜ë©´ ì•„ë˜ì™€ ê°™ë‹¤.</p>

\[\begin{align*}
h_0 &amp;= [x_{s-1}; x_{e+1}; p_{i-s+1}] \\
h_1 &amp;= \text{LayerNorm}(\text{GeLU}(W_1 h_0)) \\
y_i &amp;= \text{LayerNorm}(\text{GeLU}(W_2 h_1))
\end{align*}\]

<p>ìœ„ ê·¸ë¦¼ì„ ì˜ˆì‹œë¡œ ì•Œê³ ë¦¬ì¦˜ì„ ì‚´í´ë³´ì. ë¨¼ì € ì£¼ì–´ì§„ ìŠ¤íŒ¬ ê¸¸ì´ì— ë§ê²Œ, ìŠ¤íŒ¬ì˜ ì‹œì‘ê³¼ ë ì§€ì ì´ ë˜ëŠ” í† í°ì„ ë¬´ì‘ìœ„ë¡œ ì„ íƒí•œë‹¤. ê·¸ë‹¤ìŒ ì‹œì‘ ìœ„ì¹˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ, ìŠ¤íŒ¬ ë‚´ë¶€ì— ì†í•˜ëŠ” í† í°ë“¤ì˜ ìƒëŒ€ ìœ„ì¹˜ ì¸ë±ìŠ¤ë¥¼ ê³„ì‚°í•´ì¤€ë‹¤. ê·¸ë¦¼ ì† $x_{7}$ í† í°ì˜ ìƒëŒ€ ìœ„ì¹˜ ë²ˆí˜¸ëŠ” 3ì´ ëœë‹¤. ë¯¸ë¦¬ ì •ì˜í•œ ìƒëŒ€ ìœ„ì¹˜ ì„ë² ë”©ì—ì„œ í–‰ ì¸ë±ìŠ¤ê°€ 3ì¸ í–‰ë²¡í„°ë¥¼ ê°€ì ¸ì˜¨ë‹¤. ê·¸ ë‹¤ìŒ ì–‘ìª½ ë ë²¡í„°ì™€ concatì„ ìˆ˜í–‰í•˜ì—¬ $h_{0}$ ì„ ë§Œë“ ë‹¤. ê·¸ë¦¬ê³  ë¯¸ë¦¬ ì •ì˜ëœ <code class="language-plaintext highlighter-rouge">SBOHead</code>ì— í†µê³¼ì‹œí‚¨ë‹¤. <code class="language-plaintext highlighter-rouge">SBOHead</code>ì—ê²Œ ë°˜í™˜ ë°›ì€ ì€ë‹‰ ë²¡í„°ê°’ì€ í•´ë‹¹ ìœ„ì¹˜ì˜ ë§ˆìŠ¤í‚¹ì— ëŒ€í•œ ì˜ˆì¸¡ê°’($y_{i}$)ìœ¼ë¡œ ì‚¬ìš©í•˜ê³  ì´ë¥¼ ì´ìš©í•´ SBO ì†ì‹¤ì„ êµ¬í•œë‹¤. ì§€ê¸ˆê¹Œì§€ ë‚´ìš©ì„ ì •ë¦¬í•´ ìˆ˜ì‹ìœ¼ë¡œ í‘œí˜„í•˜ë©´ ìœ„ì™€ ê°™ë‹¤.</p>

\[L(x_i) = L_{MLM}(x_i) + L_{SBO}(x_i)\]

<p><code class="language-plaintext highlighter-rouge">SpanBERT</code>ì˜ ëª©ì í•¨ìˆ˜ëŠ” SBO ì†ì‹¤ ë¿ë§Œ ì•„ë‹ˆë¼ ê¸°ì¡´ MLM ì†ì‹¤ë„ í•¨ê¼ í¬í•¨ë˜ì–´ ìˆë‹¤. ë‹¤ë§Œ MLM ì†ì‹¤ì„ êµ¬í•˜ê¸° ìœ„í•´ ì£¼ì–´ì§„ ì‹œí€€ìŠ¤ì— ëŒ€í•´ ë”°ë¡œ ë§ˆìŠ¤í‚¹ì„ í•˜ëŠ” ê²ƒì€ ì•„ë‹ˆê³ , SBOë¥¼ ìœ„í•´ ì ìš©í–ˆë˜ Span Maskingì„ ê·¸ëŒ€ë¡œ í™œìš©í•œë‹¤. ëŒ€ì‹  ìœ„ì˜ SBO ìˆ˜ì‹ì˜ $h_{0}$ ì´ ì•„ë‹ˆë¼, $x_{i-s+1}$ ($i-s+1$ ë²ˆì§¸ í† í°ì˜ ì¸ì½”ë” ì¶œë ¥ê°’)ì„ ê·¸ëŒ€ë¡œ MLM ì†ì‹¤ì„ êµ¬í•˜ëŠ”ë° ì‚¬ìš©í•œë‹¤. ì •ë¦¬í•˜ë©´, <code class="language-plaintext highlighter-rouge">SpanBERT</code>ì˜ ìµœì¢… ì†ì‹¤ì€ ìœ„ ìˆ˜ì‹ê³¼ ê°™ë‹¤. í•œí¸, <code class="language-plaintext highlighter-rouge">ELECTRA</code> ë•Œì™€ëŠ” ë‹¤ë¥´ê²Œ ë‘ ì†ì‹¤ì˜ ìŠ¤ì¼€ì¼ ì°¨ì´ê°€ ê±°ì˜ ì—†ì–´ ë”°ë¡œ ìŠ¤ì¼€ì¼ ìƒìˆ˜ë¥¼ ê³±í•´ì£¼ì§€ëŠ” ì•ŠëŠ” ê²ƒ ê°™ë‹¤.</p>

<h3 id="implementation-by-pytorch"><code class="language-plaintext highlighter-rouge">ğŸ‘©â€ğŸ’»Â Implementation by Pytorch</code></h3>

<p>ë…¼ë¬¸ì˜ ë‚´ìš© ì¢…í•©í•˜ì—¬ íŒŒì´í† ì¹˜ë¡œ <code class="language-plaintext highlighter-rouge">SpanBERT</code>ë¥¼ êµ¬í˜„í•´ë´¤ë‹¤. ë…¼ë¬¸ì— í¬í•¨ëœ ì•„ì´ë””ì–´ë¥¼ ì´í•´í•˜ëŠ”ë°ëŠ” ì–´ë µì§€ ì•Šì•˜ì§€ë§Œ, ì œí•œëœ ì¡°ê±´ì— ë§ëŠ” ìŠ¤íŒ¬ì„ ì°¾ê³ , ë§ˆìŠ¤í‚¹í•˜ëŠ” ê³¼ì •ì„ ì‹¤ì œ êµ¬í˜„í•˜ëŠ” ê²ƒì€ ë§¤ìš° ê¹Œë‹¤ë¡œìš´ í¸ì´ì—ˆë‹¤.
ë³¸ í¬ìŠ¤íŒ…ì—ì„œëŠ” <code class="language-plaintext highlighter-rouge">SpanBERT</code>ì˜ SBO í•™ìŠµì„ ìœ„í•œ ì…ë ¥ ë§Œë“¤ê¸°, SBOHeadì— ëŒ€í•´ì„œë§Œ ì„¤ëª…í•˜ë ¤ê³  í•œë‹¤. <code class="language-plaintext highlighter-rouge">BERT</code>, <code class="language-plaintext highlighter-rouge">Whole World Masking</code>ì— ëŒ€í•´ ê¶ê¸ˆí•˜ë‹¤ë©´ ì´ì „ í¬ìŠ¤íŒ…ì„, ì „ì²´ ëª¨ë¸ êµ¬ì¡° ëŒ€í•œ ì½”ë“œëŠ” <strong><a href="https://github.com/qcqced123/model_study">ì—¬ê¸° ë§í¬</a></strong>ë¥¼ í†µí•´ ì°¸ê³ ë°”ë€ë‹¤.</p>

<p>ê³µê°œí•  ì½”ë“œëŠ” ì•„ì§ ì™„ë²½í•˜ê²Œ ë²¡í„°í™”ë¥¼ ì ìš©í•˜ì§€ ëª»í•´, GPU ë³‘ë ¬ ì—°ì‚°ì— ìµœì í™” ë˜ì§€ ëª»í•œ ì  ì–‘í•´ ë¶€íƒí•œë‹¤. ë¹ ë¥¸ ì‹œì¼ ì´ë‚´ì— ë²¡í„°í™”ë¥¼ ì ìš©í•´ì„œ ë‹¤ì‹œ ìˆ˜ì •ëœ ì½”ë“œë¥¼ ì˜¬ë¦¬ê² ë‹¤.</p>

<h4 id="span-masking-algoritm"><code class="language-plaintext highlighter-rouge">ğŸ‘©â€ğŸ’»Â Span Masking Algoritm</code></h4>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">random</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="nn">torch.nn.utils.rnn</span> <span class="kn">import</span> <span class="n">pad_sequence</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">Tensor</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">List</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">Any</span>
<span class="kn">from</span> <span class="nn">..tuner.mlm</span> <span class="kn">import</span> <span class="n">WholeWordMaskingCollator</span>
<span class="kn">from</span> <span class="nn">configuration</span> <span class="kn">import</span> <span class="n">CFG</span>

<span class="n">BPE</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s">'RobertaTokenizerFast'</span><span class="p">,</span>
    <span class="s">'GPT2TokenizerFast'</span><span class="p">,</span>
<span class="p">]</span>

<span class="n">SPM</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s">'DebertaV2TokenizerFast'</span><span class="p">,</span>
    <span class="s">'DebertaTokenizerFast'</span><span class="p">,</span>
    <span class="s">'XLMRobertaTokenizerFast'</span><span class="p">,</span>
<span class="p">]</span>

<span class="n">WORDPIECE</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s">'BertTokenizerFast'</span><span class="p">,</span>
    <span class="s">'ElectraTokenizerFast'</span><span class="p">,</span>
<span class="p">]</span>

<span class="k">def</span> <span class="nf">random_non_negative_integer</span><span class="p">(</span><span class="n">max_value</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">random</span><span class="p">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">max_value</span><span class="p">)</span>

<span class="k">class</span> <span class="nc">SpanCollator</span><span class="p">(</span><span class="n">WholeWordMaskingCollator</span><span class="p">):</span>
    <span class="s">""" Custom Collator for Span Boundary Objective Task, which is used for span masking algorithm
    Span Masking is similar to Whole Word Masking, but it has some differences:
        1) Span Masking does not use 10% of selected token left &amp; 10% of selected token replaced other vocab token
            - just replace all selected token to [MASK] token
    Algorithm:
    1) Select 2 random tokens from input tokens for spanning
    2) Calculate relative position embedding for each token with 2 random tokens froms step 1.
    3) Calculate span boundary objective with 2 random tokens from step 1 &amp; pos embedding from step 2.
    Args:
        cfg: configuration.CFG
        masking_budget: masking budget for Span Masking
                        (default: 0.15 =&gt; Recommended by original paper)
        span_probability: probability of span length for Geometric Distribution
                         (default: 0.2 =&gt; Recommended by original paper)
        max_span_length: maximum span length of each span in one batch sequence
                         (default: 10 =&gt; Recommended by original paper)
    References:
        https://arxiv.org/pdf/1907.10529.pdf
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">cfg</span><span class="p">:</span> <span class="n">CFG</span><span class="p">,</span>
        <span class="n">masking_budget</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.15</span><span class="p">,</span>
        <span class="n">span_probability</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.2</span><span class="p">,</span>
        <span class="n">max_span_length</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">SpanCollator</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">(</span><span class="n">cfg</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span> <span class="o">=</span> <span class="n">cfg</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">tokenizer</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">tokenizer</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">masking_budget</span> <span class="o">=</span> <span class="n">masking_budget</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">span_probability</span> <span class="o">=</span> <span class="n">span_probability</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">max_span_length</span> <span class="o">=</span> <span class="n">max_span_length</span>

    <span class="k">def</span> <span class="nf">_whole_word_mask</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">input_tokens</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span>
        <span class="n">max_predictions</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">CFG</span><span class="p">.</span><span class="n">max_seq</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]:</span>
        <span class="s">"""
        0) apply Whole Word Masking Algorithm for make gathering original token index in natural language
        1) calculate number of convert into masking tokens with masking budget*len(input_tokens)
        2) define span length of this iteration
            - span length follow geometric distribution
            - span length is limited by max_span_length
        """</span>
        <span class="n">cand_indexes</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">token</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">input_tokens</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">token</span> <span class="o">==</span> <span class="s">"[CLS]"</span> <span class="ow">or</span> <span class="n">token</span> <span class="o">==</span> <span class="s">"[SEP]"</span><span class="p">:</span>
                <span class="k">continue</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">cand_indexes</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="mi">1</span> <span class="ow">and</span> <span class="bp">self</span><span class="p">.</span><span class="n">select_post_string</span><span class="p">(</span><span class="n">token</span><span class="p">):</span>  <span class="c1"># method from WholeWordMaskingCollator
</span>                <span class="n">cand_indexes</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">].</span><span class="n">append</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
            <span class="k">elif</span> <span class="bp">self</span><span class="p">.</span><span class="n">select_src_string</span><span class="p">(</span><span class="n">token</span><span class="p">):</span>  <span class="c1"># method from WholeWordMaskingCollator
</span>                <span class="n">cand_indexes</span><span class="p">.</span><span class="n">append</span><span class="p">([</span><span class="n">i</span><span class="p">])</span>

        <span class="n">l</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">input_tokens</span><span class="p">)</span>
        <span class="n">src_l</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">cand_indexes</span><span class="p">)</span>
        <span class="n">num_convert_tokens</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">masking_budget</span> <span class="o">*</span> <span class="n">l</span><span class="p">)</span>
        <span class="n">budget</span> <span class="o">=</span> <span class="n">num_convert_tokens</span>  <span class="c1"># int is immutable object, so do not copy manually
</span>        <span class="n">masked_lms</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">covered_indexes</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
        <span class="k">while</span> <span class="n">budget</span><span class="p">:</span>
            <span class="n">span_length</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">min</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">distributions</span><span class="p">.</span><span class="n">Geometric</span><span class="p">(</span><span class="n">probs</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">span_probability</span><span class="p">).</span><span class="n">sample</span><span class="p">())))</span>
            <span class="n">src_index</span> <span class="o">=</span> <span class="n">random_non_negative_integer</span><span class="p">(</span><span class="n">src_l</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">span_length</span> <span class="o">&gt;</span> <span class="n">budget</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">budget</span> <span class="o">&lt;</span> <span class="mi">5</span><span class="p">:</span>  <span class="c1"># Set the span length to budget to avoid a large number of iter if the remaining budget is too small
</span>                    <span class="n">span_length</span> <span class="o">=</span> <span class="n">budget</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">continue</span>
            <span class="k">if</span> <span class="n">cand_indexes</span><span class="p">[</span><span class="n">src_index</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">span_length</span> <span class="o">&gt;</span> <span class="n">l</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>  <span class="c1"># If the index of the last token in the span is outside the full sequence range
</span>                <span class="k">continue</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">cand_indexes</span><span class="p">[</span><span class="n">src_index</span><span class="p">])</span> <span class="o">&gt;</span> <span class="n">span_length</span><span class="p">:</span>  <span class="c1"># handling bad case: violating WWM algorithm at start
</span>                <span class="k">continue</span>
            <span class="n">span_token_index</span> <span class="o">=</span> <span class="n">cand_indexes</span><span class="p">[</span><span class="n">src_index</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>  <span class="c1"># init span token index: src token
</span>            <span class="k">while</span> <span class="n">span_length</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">span_length</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="k">break</span>
                <span class="k">if</span> <span class="n">span_token_index</span> <span class="ow">in</span> <span class="n">covered_indexes</span><span class="p">:</span> <span class="c1"># If it encounters an index that is already masked, it ends, and starts the next iteration
</span>                    <span class="k">break</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">covered_indexes</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">span_token_index</span><span class="p">)</span>
                    <span class="n">masked_lms</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">span_token_index</span><span class="p">)</span>
                    <span class="n">span_length</span> <span class="o">-=</span> <span class="mi">1</span>
                    <span class="n">budget</span> <span class="o">-=</span> <span class="mi">1</span>
                    <span class="n">span_token_index</span> <span class="o">+=</span> <span class="mi">1</span>
                    <span class="k">continue</span>

        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">covered_indexes</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">masked_lms</span><span class="p">):</span>
            <span class="k">raise</span> <span class="nb">ValueError</span><span class="p">(</span><span class="s">"Length of covered_indexes is not equal to length of masked_lms."</span><span class="p">)</span>
        <span class="n">mask_labels</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span> <span class="k">if</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">covered_indexes</span> <span class="k">else</span> <span class="mi">0</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">input_tokens</span><span class="p">))]</span>
        <span class="k">return</span> <span class="n">mask_labels</span>

    <span class="k">def</span> <span class="nf">get_mask_tokens</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">inputs</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">mask_labels</span><span class="p">:</span> <span class="n">Tensor</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
        <span class="s">""" Prepare masked tokens inputs/labels for Span Boundary Objective with MLM (15%),
        All of masked tokens (15%) are replaced by [MASK] token,
        Unlike BERT MLM which is replaced by random token or stay original token left
        """</span>
        <span class="n">labels</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">.</span><span class="n">clone</span><span class="p">()</span>
        <span class="n">probability_matrix</span> <span class="o">=</span> <span class="n">mask_labels</span>

        <span class="n">special_tokens_mask</span> <span class="o">=</span> <span class="p">[</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">tokenizer</span><span class="p">.</span><span class="n">get_special_tokens_mask</span><span class="p">(</span><span class="n">val</span><span class="p">,</span> <span class="n">already_has_special_tokens</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span> <span class="k">for</span> <span class="n">val</span> <span class="ow">in</span> <span class="n">labels</span><span class="p">.</span><span class="n">tolist</span><span class="p">()</span>
        <span class="p">]</span>
        <span class="n">probability_matrix</span><span class="p">.</span><span class="n">masked_fill_</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">special_tokens_mask</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">bool</span><span class="p">),</span> <span class="n">value</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">tokenizer</span><span class="p">.</span><span class="n">pad_token</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">padding_mask</span> <span class="o">=</span> <span class="n">labels</span><span class="p">.</span><span class="n">eq</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">tokenizer</span><span class="p">.</span><span class="n">pad_token_id</span><span class="p">)</span>
            <span class="n">probability_matrix</span><span class="p">.</span><span class="n">masked_fill_</span><span class="p">(</span><span class="n">padding_mask</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)</span>

        <span class="n">masked_indices</span> <span class="o">=</span> <span class="n">probability_matrix</span><span class="p">.</span><span class="nb">bool</span><span class="p">()</span>
        <span class="n">labels</span><span class="p">[</span><span class="o">~</span><span class="n">masked_indices</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mi">100</span>  <span class="c1"># We only compute loss on masked tokens
</span>        <span class="n">inputs</span><span class="p">[</span><span class="n">masked_indices</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">tokenizer</span><span class="p">.</span><span class="n">convert_tokens_to_ids</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">tokenizer</span><span class="p">.</span><span class="n">mask_token</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">labels</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batched</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]])</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">:</span>
        <span class="s">""" Abstract Method for Collator, you must implement this method in child class """</span>
        <span class="n">input_ids</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="s">"input_ids"</span><span class="p">])</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">batched</span><span class="p">]</span>
        <span class="n">padding_mask</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="p">.</span><span class="n">get_padding_mask</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">input_ids</span><span class="p">]</span>

        <span class="n">padding_mask</span> <span class="o">=</span> <span class="n">pad_sequence</span><span class="p">(</span><span class="n">padding_mask</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">padding_value</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="n">input_ids</span> <span class="o">=</span> <span class="n">pad_sequence</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">padding_value</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

        <span class="n">mask_labels</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">batched</span><span class="p">:</span>
            <span class="n">ref_tokens</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">input_id</span> <span class="ow">in</span> <span class="n">x</span><span class="p">[</span><span class="s">"input_ids"</span><span class="p">]:</span>
                <span class="n">token</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">tokenizer</span><span class="p">.</span><span class="n">_convert_id_to_token</span><span class="p">(</span><span class="n">input_id</span><span class="p">)</span>
                <span class="n">ref_tokens</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">token</span><span class="p">)</span>
            <span class="n">mask_labels</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">_whole_word_mask</span><span class="p">(</span><span class="n">ref_tokens</span><span class="p">))</span>

        <span class="n">mask_labels</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">mask_labels</span><span class="p">]</span>
        <span class="n">mask_labels</span> <span class="o">=</span> <span class="n">pad_sequence</span><span class="p">(</span><span class="n">mask_labels</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">padding_value</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">input_ids</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">get_mask_tokens</span><span class="p">(</span>
            <span class="n">input_ids</span><span class="p">,</span>
            <span class="n">mask_labels</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="p">{</span>
            <span class="s">"input_ids"</span><span class="p">:</span> <span class="n">input_ids</span><span class="p">,</span>
            <span class="s">"labels"</span><span class="p">:</span> <span class="n">labels</span><span class="p">,</span>
            <span class="s">"padding_mask"</span><span class="p">:</span> <span class="n">padding_mask</span><span class="p">,</span>
            <span class="s">"mask_labels"</span><span class="p">:</span> <span class="n">mask_labels</span>
        <span class="p">}</span>
</code></pre></div></div>

<h4 id="sbo-head"><code class="language-plaintext highlighter-rouge">ğŸ‘©â€ğŸ’»Â SBO Head</code></h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">SBOHead</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="s">""" Custom Head for Span Boundary Objective Task, this module return logit value for each token
    we use z for class logit, each Fully Connected Layer doesn't have bias term in original paper
    so we don't use bias term in this module =&gt; nn.Linear(bias=False)

    You must select option for matrix sum or concatenate with x_s-1, x_e+1, p_i-s+1
    If you select concatenate option, you must pass is_concatenate=True to cfg.is_concatenate, default is True
    
    Math:
        h_0 = [x_s-1;x_e+1;p_i-s+1]
        h_t = LayerNorm(GELU(W_0â€¢h_0))
        z = LayerNorm(GELU(W_1â€¢h_t))

    Args:
        cfg: configuration.CFG
        is_concatenate: option for matrix sum or concatenate with x_s-1, x_e+1, p_i-s+1, default True
        max_span_length: maximum span length of each span in one batch sequence
                         (default: 10 =&gt; Recommended by original paper)
    References:
        https://arxiv.org/pdf/1907.10529.pdf
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">cfg</span><span class="p">:</span> <span class="n">CFG</span><span class="p">,</span>
        <span class="n">is_concatenate</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="bp">True</span><span class="p">,</span>
        <span class="n">max_span_length</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">SBOHead</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span> <span class="o">=</span> <span class="n">cfg</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">is_concatenate</span> <span class="o">=</span> <span class="n">is_concatenate</span>  <span class="c1"># for matrix sum or concatenate with x_s-1, x_e+1, p_i-s+1
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">projector</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">dim_model</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">dim_model</span><span class="o">*</span><span class="mi">3</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>  <span class="c1"># for concatenate x_s-1, x_e+1, p_i-s+1
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">span_pos_emb</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">max_span_length</span><span class="o">*</span><span class="mi">3</span><span class="p">,</span> <span class="n">cfg</span><span class="p">.</span><span class="n">dim_model</span><span class="p">)</span>  <span class="c1"># size of dim_model is research topic
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">head</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">dim_model</span><span class="o">*</span><span class="mi">3</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">dim_ffn</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">GELU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">dim_ffn</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">dim_ffn</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">dim_model</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">GELU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">dim_model</span><span class="p">),</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">classifier</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">dim_model</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">cfg</span><span class="p">.</span><span class="n">vocab_size</span><span class="p">))</span>  <span class="c1"># for matching vocab size
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">classifier</span><span class="p">.</span><span class="n">bias</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">bias</span>

    <span class="o">@</span><span class="nb">staticmethod</span>
    <span class="k">def</span> <span class="nf">find_consecutive_groups</span><span class="p">(</span><span class="n">mask_labels</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">target_value</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">Dict</span><span class="p">]]:</span>
        <span class="s">""" Get the start and end positions of consecutive groups in tensor for the target value
        This method is used for SBO Objective Function, this version is not best performance to make span groups

        Args:
            mask_labels: masking tensor for span
            target_value: target value for finding consecutive groups
        """</span>
        <span class="n">all_consecutive_groups</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">mask_label</span> <span class="ow">in</span> <span class="n">mask_labels</span><span class="p">:</span>
            <span class="n">consecutive_groups</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="n">current_group</span> <span class="o">=</span> <span class="bp">None</span>
            <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">mask_label</span><span class="p">):</span>
                <span class="k">if</span> <span class="n">value</span> <span class="o">==</span> <span class="n">target_value</span><span class="p">:</span>
                    <span class="k">if</span> <span class="n">current_group</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
                        <span class="n">current_group</span> <span class="o">=</span> <span class="p">{</span><span class="s">"start"</span><span class="p">:</span> <span class="n">i</span><span class="p">,</span> <span class="s">"end"</span><span class="p">:</span> <span class="n">i</span><span class="p">}</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">current_group</span><span class="p">[</span><span class="s">"end"</span><span class="p">]</span> <span class="o">=</span> <span class="n">i</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">if</span> <span class="n">current_group</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
                        <span class="n">consecutive_groups</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">current_group</span><span class="p">)</span>
                        <span class="n">current_group</span> <span class="o">=</span> <span class="bp">None</span>
            <span class="k">if</span> <span class="n">current_group</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
                <span class="n">consecutive_groups</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">current_group</span><span class="p">)</span>
            <span class="n">all_consecutive_groups</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">consecutive_groups</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">all_consecutive_groups</span>

    <span class="k">def</span> <span class="nf">cal_span_emb</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">h</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">consecutive_groups</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">Dict</span><span class="p">]])</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="s">""" Calculate span embedding for each span in one batch sequence

        Args:
            h: hidden states, already passed through projection layer (dim*3)
            hidden_states: hidden states from encoder
            consecutive_groups: consecutive groups for each batch sequence
        """</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">batch</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">consecutive_groups</span><span class="p">):</span>  <span class="c1"># batch level
</span>            <span class="k">for</span> <span class="n">j</span><span class="p">,</span> <span class="n">span</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">batch</span><span class="p">):</span>  <span class="c1"># span level
</span>                <span class="n">start</span><span class="p">,</span> <span class="n">end</span> <span class="o">=</span> <span class="n">span</span><span class="p">[</span><span class="s">"start"</span><span class="p">],</span> <span class="n">span</span><span class="p">[</span><span class="s">"end"</span><span class="p">]</span>
                <span class="n">length</span> <span class="o">=</span> <span class="n">end</span> <span class="o">-</span> <span class="n">start</span> <span class="o">+</span> <span class="mi">1</span>
                <span class="n">idx</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="n">length</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>   <span class="c1"># .to(self.cfg.device)
</span>                <span class="n">context_s</span><span class="p">,</span> <span class="n">context_e</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">start</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="p">:],</span> <span class="n">hidden_states</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">end</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="p">:]</span>
                <span class="n">span_pos_emb</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">span_pos_emb</span><span class="p">(</span><span class="n">idx</span><span class="p">).</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">length</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                    <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">p_h</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">span_pos_emb</span><span class="p">):</span>  <span class="c1"># length of span_pos_emb == length of span of this iterations
</span>                        <span class="n">h</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">start</span><span class="o">+</span><span class="n">k</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cat</span><span class="p">([</span><span class="n">context_s</span><span class="p">,</span> <span class="n">p_h</span><span class="p">,</span> <span class="n">context_e</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">h</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">start</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cat</span><span class="p">([</span><span class="n">context_s</span><span class="p">,</span> <span class="n">span_pos_emb</span><span class="p">,</span> <span class="n">context_e</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">h</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">mask_labels</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="n">consecutive_groups</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">find_consecutive_groups</span><span class="p">(</span><span class="n">mask_labels</span><span class="p">)</span>  <span class="c1"># [batch, num_consecutive_groups]
</span>        <span class="n">h</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">projector</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>  <span class="c1"># [batch, seq, dim_model*3]
</span>        <span class="n">h_t</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">cal_span_emb</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">,</span> <span class="n">consecutive_groups</span><span class="p">)</span>
        <span class="n">z</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">head</span><span class="p">(</span><span class="n">h_t</span><span class="p">)</span>
        <span class="n">logit</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">classifier</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">logit</span>

</code></pre></div></div>

        
      </section>

      <footer class="page__meta">
        
        
  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      <a href="/tags/#bert" class="page__taxonomy-item p-category" rel="tag">BERT</a><span class="sep">, </span>
    
      <a href="/tags/#natural-language-process" class="page__taxonomy-item p-category" rel="tag">Natural Language Process</a><span class="sep">, </span>
    
      <a href="/tags/#pytorch" class="page__taxonomy-item p-category" rel="tag">Pytorch</a><span class="sep">, </span>
    
      <a href="/tags/#self-attention" class="page__taxonomy-item p-category" rel="tag">Self-Attention</a><span class="sep">, </span>
    
      <a href="/tags/#spanbert" class="page__taxonomy-item p-category" rel="tag">SpanBERT</a>
    
    </span>
  </p>




  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> Categories: </strong>
    <span itemprop="keywords">
    
      <a href="/categories/#nlp" class="page__taxonomy-item p-category" rel="tag">NLP</a>
    
    </span>
  </p>


        

  <p class="page__date"><strong><i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> Updated:</strong> <time class="dt-published" datetime="2024-03-11">March 11, 2024</time></p>

      </footer>

      <section class="page__share">
  
    <h4 class="page__share-title">Share on</h4>
  

  <a href="https://twitter.com/intent/tweet?text=%F0%9F%97%82%EF%B8%8F%5BSpanBERT%5D+SpanBERT%3A+Improving+Pre-training+by+Representing+and+Predicting+Spans%20http%3A%2F%2Flocalhost%3A4000%2Fnlp%2Fspanbert" class="btn btn--twitter" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Twitter"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=http%3A%2F%2Flocalhost%3A4000%2Fnlp%2Fspanbert" class="btn btn--facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Facebook"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=http%3A%2F%2Flocalhost%3A4000%2Fnlp%2Fspanbert" class="btn btn--linkedin" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on LinkedIn"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>


      
  <nav class="pagination">
    
      <a href="/nlp/roformer" class="pagination--pager" title="ğŸ¡ [Roformer] RoFormer: Enhanced Transformer with Rotary Position Embedding
">Previous</a>
    
    
      <a href="/nlp/linear_attention" class="pagination--pager" title="ğŸŒ† [Linear Attention] Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention
">Next</a>
    
  </nav>

    </div>

    
  </article>

  
  
    <div class="page__related">
      <h2 class="page__related-title">You may also enjoy</h2>
      <div class="grid__wrapper">
        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
      <div class="archive__item-teaser">
        <img src="/assets/images/huggingface_emoji.png" alt="">
      </div>
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/nlp/linear_attention" rel="permalink">ğŸŒ† [Linear Attention] Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention
</a>
      
    </h2>
    <!-- 

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          8 minute read
        
      </span>
    
  </p>

 -->
    
      <p class="page__meta"><i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i> March 14 2024</p>
    
    <p class="archive__item-excerpt" itemprop="description">Linear Attention Official Paper Review with Pytorch Implementation
</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
      <div class="archive__item-teaser">
        <img src="/assets/images/huggingface_emoji.png" alt="">
      </div>
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/nlp/roformer" rel="permalink">ğŸ¡ [Roformer] RoFormer: Enhanced Transformer with Rotary Position Embedding
</a>
      
    </h2>
    <!-- 

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          8 minute read
        
      </span>
    
  </p>

 -->
    
      <p class="page__meta"><i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i> March 11 2024</p>
    
    <p class="archive__item-excerpt" itemprop="description">Roformer Official Paper Review with Pytorch Implementation
</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
      <div class="archive__item-teaser">
        <img src="/assets/images/huggingface_emoji.png" alt="">
      </div>
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/nlp/electra" rel="permalink">ğŸ‘® [ELECTRA] Pre-training Text Encoders as Discriminators Rather Than Generators
</a>
      
    </h2>
    <!-- 

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          7 minute read
        
      </span>
    
  </p>

 -->
    
      <p class="page__meta"><i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i> March 11 2024</p>
    
    <p class="archive__item-excerpt" itemprop="description">ELECTRA Official Paper Review with Pytorch Implementation
</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
      <div class="archive__item-teaser">
        <img src="/assets/images/huggingface_emoji.png" alt="">
      </div>
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/nlp/distilbert" rel="permalink">ğŸ§‘â€ğŸ« [DistilBERT] DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter
</a>
      
    </h2>
    <!-- 

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          8 minute read
        
      </span>
    
  </p>

 -->
    
      <p class="page__meta"><i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i> March 11 2024</p>
    
    <p class="archive__item-excerpt" itemprop="description">DistilBERT Official Paper Review with Pytorch Implementation
</p>
  </article>
</div>

        
      </div>
    </div>
  
  
</div>

    </div>

    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>Follow:</strong></li>
    

    
      
        
      
        
      
        
      
        
      
        
      
        
      
    

    
      <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
    
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2024 qcqced. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>







    <script>
  'use strict';

  (function() {
    var commentContainer = document.querySelector('#utterances-comments');

    if (!commentContainer) {
      return;
    }

    var script = document.createElement('script');
    script.setAttribute('src', 'https://utteranc.es/client.js');
    script.setAttribute('repo', 'qcqced123/qcqced123.github.io');
    script.setAttribute('issue-term', 'pathname');
    
    script.setAttribute('theme', 'github-light');
    script.setAttribute('crossorigin', 'anonymous');

    commentContainer.appendChild(script);
  })();
</script>

  




<script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML">
</script>

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"] ],
    displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
    processEscapes: true
    },
    "HTML-CSS": { availableFonts: ["TeX"] }
});
</script>

  </body>
</html>
