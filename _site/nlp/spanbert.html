<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.24.0 by Michael Rose
  Copyright 2013-2020 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->
<html lang="en" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>üóÇÔ∏è[SpanBERT] SpanBERT: Improving Pre-training by Representing and Predicting Spans - AI/Business Study Log</title>
<meta name="description" content="SpanBERT Official Paper Review with Pytorch Implementation">


  <meta name="author" content="qcqced">
  
  <meta property="article:author" content="qcqced">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="AI/Business Study Log">
<meta property="og:title" content="üóÇÔ∏è[SpanBERT] SpanBERT: Improving Pre-training by Representing and Predicting Spans">
<meta property="og:url" content="http://localhost:4000/nlp/spanbert">


  <meta property="og:description" content="SpanBERT Official Paper Review with Pytorch Implementation">







  <meta property="article:published_time" content="2024-03-11T00:00:00+09:00">



  <meta property="article:modified_time" content="2024-03-12T02:00:00+09:00">



  

  


<link rel="canonical" href="http://localhost:4000/nlp/spanbert">




<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    
      "@type": "Person",
      "name": "qcqced",
      "url": "http://localhost:4000/"
    
  }
</script>







<!-- end _includes/seo.html -->



  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="AI/Business Study Log Feed">


<!-- https://t.co/dKP3o1e -->
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css"></noscript>



<!-- Latex -->
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->
<link rel="apple-touch-icon" sizes="180x180" href="/assets/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/assets/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/assets/favicon-16x16.png">
<link rel="manifest" href="/assets/site.webmanifest">
<link rel="mask-icon" href="/assets/safari-pinned-tab.svg" color="#5bbad5">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="theme-color" content="#ffffff">
<!-- end custom head snippets -->

    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      TeX: {
        equationNumbers: {
          autoNumber: "AMS"
        }
      },
      tex2jax: {
      inlineMath: [ ['$', '$'] ],
      displayMath: [ ['$$', '$$'] ],
      processEscapes: true
    }
  });
  MathJax.Hub.Register.MessageHook("Math Processing Error",function (message) {
      alert("Math Processing Error: "+message[1]);
    });
  MathJax.Hub.Register.MessageHook("TeX Jax - parse error",function (message) {
      alert("Math Processing Error: "+message[1]);
    });
  </script>
  <script type="text/javascript" async
    src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>
  </head>

  <body class="layout--single">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
          AI/Business Study Log
          <span class="site-subtitle">NLP, Marketing</span>
        </a>
        
        
        <ul class="visible-links">
              
              
                  <li class="masthead__menu-item">
                      <a href="https://qcqced123.github.io/">Home</a>
                  </li>
              
          
              
              
                  <li class="dropdown ">
                      <a class="dropdown-toggle" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">CS/AI  <i class="fa fa-caret-down fa-sm" aria-hidden="true"></i><span class="caret"></span></a>
       
                      <ul class="dropdown-content" >
                          
                              <a class = "dropdown-item" href="/nlp/">    Natural Language Process</a>
                          
                              <a class = "dropdown-item" href="/multi-modal/">    Multi Modal</a>
                          
                              <a class = "dropdown-item" href="/cv/">    Computer Vision</a>
                          
                              <a class = "dropdown-item" href="/ml/">    Machine Learning</a>
                          
                              <a class = "dropdown-item" href="/framework-library/">    Framework & Library</a>
                          
                              <a class = "dropdown-item" href="/python/">    Python</a>
                          
                              <a class = "dropdown-item" href="/algorithm/">    Data Structure & Algorithm</a>
                          
                              <a class = "dropdown-item" href="/ps/">    Problem Solving</a>
                          
                      </ul>
                  </li>
              
          
              
              
                  <li class="dropdown ">
                      <a class="dropdown-toggle" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Math  <i class="fa fa-caret-down fa-sm" aria-hidden="true"></i><span class="caret"></span></a>
       
                      <ul class="dropdown-content" >
                          
                              <a class = "dropdown-item" href="/linear-algebra/">    Linear Algebra</a>
                          
                              <a class = "dropdown-item" href="/optimization-theory/">    Optimization Theory/Calculus</a>
                          
                              <a class = "dropdown-item" href="/signal-system/">    Signal & System</a>
                          
                      </ul>
                  </li>
              
          
              
              
                  <li class="dropdown ">
                      <a class="dropdown-toggle" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Business/Marketing  <i class="fa fa-caret-down fa-sm" aria-hidden="true"></i><span class="caret"></span></a>
       
                      <ul class="dropdown-content" >
                          
                              <a class = "dropdown-item" href="/device/">    Device</a>
                          
                              <a class = "dropdown-item" href="/semi-conductor/">    Semi-Conductor</a>
                          
                              <a class = "dropdown-item" href="/ai/">    AI</a>
                          
                      </ul>
                  </li>
              
          
              
              
                  <li class="masthead__menu-item">
                      <a href="/categories/">Category</a>
                  </li>
              
          
              
              
                  <li class="masthead__menu-item">
                      <a href="/about/">About</a>
                  </li>
              
          
       </ul>
       
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      





<div id="main" role="main">
  
  <div class="sidebar sticky">
  


<div itemscope itemtype="https://schema.org/Person" class="h-card">

  
    <div class="author__avatar">
      <a href="http://localhost:4000/">
        <img src="/assets/images/huggingface_emoji.png" alt="qcqced" itemprop="image" class="u-photo">
      </a>
    </div>
  

  <div class="author__content">
    <h3 class="author__name p-name" itemprop="name">
      <a class="u-url" rel="me" href="http://localhost:4000/" itemprop="url">qcqced</a>
    </h3>
    
      <div class="author__bio p-note" itemprop="description">
        <p>Interested in NLP, Marketing</p>

      </div>
    
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">Follow</button>
    <ul class="author__urls social-icons">
      
        <li itemprop="homeLocation" itemscope itemtype="https://schema.org/Place">
          <i class="fas fa-fw fa-map-marker-alt" aria-hidden="true"></i> <span itemprop="name" class="p-locality">Seoul, South Korea</span>
        </li>
      

      
        
          
            <li><a href="https://qcqced123.github.io" rel="nofollow noopener noreferrer me" itemprop="sameAs"><i class="fas fa-fw fa-link" aria-hidden="true"></i><span class="label">Website</span></a></li>
          
        
          
            <li><a href="https://github.com/qcqced123" rel="nofollow noopener noreferrer me" itemprop="sameAs"><i class="fab fa-fw fa-github" aria-hidden="true"></i><span class="label">GitHub</span></a></li>
          
        
          
            <li><a href="https://www.kaggle.com/qcqced" rel="nofollow noopener noreferrer me" itemprop="sameAs"><i class="fab fa-fw fa-kaggle" aria-hidden="true"></i><span class="label">Kaggle</span></a></li>
          
        
      

      

      
        <li>
          <a href="mailto:qcqced123@gmail.com" rel="me" class="u-email">
            <meta itemprop="email" content="qcqced123@gmail.com" />
            <i class="fas fa-fw fa-envelope-square" aria-hidden="true"></i><span class="label">Email</span>
          </a>
        </li>
      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      <!--
  <li>
    <a href="http://link-to-whatever-social-network.com/user/" itemprop="sameAs" rel="nofollow noopener noreferrer me">
      <i class="fas fa-fw" aria-hidden="true"></i> Custom Social Profile Link
    </a>
  </li>
-->
    </ul>
  </div>
</div>

  
  </div>



  <article class="page h-entry" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="üóÇÔ∏è[SpanBERT] SpanBERT: Improving Pre-training by Representing and Predicting Spans">
    <meta itemprop="description" content="SpanBERT Official Paper Review with Pytorch Implementation">
    <meta itemprop="datePublished" content="2024-03-11T00:00:00+09:00">
    <meta itemprop="dateModified" content="2024-03-12T02:00:00+09:00">

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title p-name" itemprop="headline">
            <a href="http://localhost:4000/nlp/spanbert" class="u-url" itemprop="url">üóÇÔ∏è[SpanBERT] SpanBERT: Improving Pre-training by Representing and Predicting Spans
</a>
          </h1>
          <p class="page__date">
            <a href="https://hits.seeyoufarm.com/localhost:4000/nlp/spanbert"target="_blank">
              <img src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https://localhost:4000/nlp/spanbert&count_bg=%23399DE2&title_bg=%236D6D6D&icon=pytorch.svg&icon_color=%23E7E7E7&title=Views&edge_flat=false"/>
            </a>
            <i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> Updated: <time datetime="2024-03-11T00:00:00+09:00">March 11, 2024</time>
            <!-- <div style="text-align: left;"> -->
            <!-- </div> -->
          </p>
          
          
        </header>
      

      <section class="page__content e-content" itemprop="text">
        
          <aside class="sidebar__right sticky">
            <nav class="toc">
              <header><h4 class="nav__title"><i class="fas fa-file-alt"></i> On this page</h4></header>
              <ul class="toc__menu"><li><a href="#overview">üî≠¬†Overview</a></li><li><a href="#sbo-span-boundary-objective">üìö¬†SBO: Span Boundary Objective</a></li><li><a href="#implementation-by-pytorch">üë©‚Äçüíª¬†Implementation by Pytorch</a><ul><li><a href="#span-masking-algoritm">üë©‚Äçüíª¬†Span Masking Algoritm</a></li><li><a href="#sbo-head">üë©‚Äçüíª¬†SBO Head</a></li></ul></li></ul>

            </nav>
          </aside>
        
        <h3 id="overview"><code class="language-plaintext highlighter-rouge">üî≠¬†Overview</code></h3>

<p><code class="language-plaintext highlighter-rouge">SpanBERT</code>Îäî 2020ÎÖÑ ÌéòÏù¥Ïä§Î∂ÅÏóêÏÑú Î∞úÌëúÌïú BERT Í≥ÑÏó¥ Î™®Îç∏Î°ú, ÏÉàÎ°úÏö¥ Î∞©Î≤ïÎ°†Ïù∏ <code class="language-plaintext highlighter-rouge">SBO(Span Boundary Objective)</code>Î•º Í≥†ÏïàÌï¥ ÏÇ¨Ï†ÑÌïôÏäµÏùÑ ÌïòÏó¨ Í∏∞Ï°¥ ÎåÄÎπÑ ÎÜíÏùÄ ÏÑ±Îä•ÏùÑ Í∏∞Î°ùÌñàÎã§. Í∏∞Ï°¥ <code class="language-plaintext highlighter-rouge">MLM</code>, <code class="language-plaintext highlighter-rouge">CLM</code>ÏùÄ Îã®Ïùº ÌÜ†ÌÅ∞ÏùÑ ÏòàÏ∏°ÌïòÎäî Î∞©ÏãùÏùÑ ÏÇ¨Ïö©ÌïòÍ∏∞ ÎïåÎ¨∏Ïóê Word-Level TaskÏóê ÏïÑÏ£º Ï†ÅÌï©ÌïòÏßÄÎßå ÏÉÅÎåÄÏ†ÅÏúºÎ°ú QA, Sentence-Similarity Í∞ôÏùÄ Î¨∏Ïû• Îã®ÏúÑ ÌÖåÏä§ÌÅ¨Ïóê Í∑∏ÎåÄÎ°ú ÌôúÏö©ÌïòÍ∏∞ÏóêÎäî Î∂ÄÏ°±Ìïú Ï†êÏù¥ ÏûàÏóàÎã§. Ïù¥Îü¨Ìïú Î¨∏Ï†úÎ•º Ìï¥Í≤∞ÌïòÍ∏∞ ÏúÑÌï¥ Í≥†ÏïàÎêú Î∞©Î≤ïÎ°†Ïù¥ Î∞îÎ°ú <code class="language-plaintext highlighter-rouge">SBO</code>Îã§. <code class="language-plaintext highlighter-rouge">SBO</code>ÎûÄ, MLMÍ≥º ÎπÑÏä∑ÌïòÏßÄÎßå, Span(Ï†à‚Ä¢Íµ¨Î¨∏) Îã®ÏúÑÎ°ú ÎßàÏä§ÌÇπÌïòÍ≥† Îã§Ïãú DenoisingÏùÑ ÌïòÍ∏∞ ÎïåÎ¨∏Ïóê, Sentence-Level TaskÏóê ÏÜçÌïòÎäî Down-Stream TaskÎ•º ÏúÑÌïú Î™®Îç∏Ïùò ÏÇ¨Ï†Ñ ÌõàÎ†®ÏúºÎ°ú Ï†ÅÌï©ÌïòÎã§.</p>

<p>Ï†ïÎ¶¨ÌïòÏûêÎ©¥, <code class="language-plaintext highlighter-rouge">SpanBERT</code> Î™®Îç∏ÏùÄ Í∏∞Ï°¥ BERTÏùò Íµ¨Ï°∞Ï†Å Ï∏°Î©¥ Í∞úÏÑ†Ïù¥ ÏïÑÎãå, ÏÇ¨Ï†ÑÌïôÏäµ Î∞©Î≤ïÏóê ÎåÄÌïú Í∞úÏÑ† ÏãúÎèÑÎùºÍ≥† Î≥º Ïàò ÏûàÎã§. Îî∞ÎùºÏÑú Ïñ¥Îñ§ Î™®Îç∏Ïù¥ÎçîÎùºÎèÑ, Ïù∏ÏΩîÎçî Ïñ∏Ïñ¥ Î™®Îç∏Ïù¥ÎùºÎ©¥ Î™®Îëê <code class="language-plaintext highlighter-rouge">SpanBERT</code> Íµ¨Ï°∞Î•º ÏÇ¨Ïö©Ìï† Ïàò ÏûàÏúºÎ©∞, Í∏∞Ï°¥ ÎÖºÎ¨∏ÏóêÏÑúÎäî ÏõêÎ≥∏ BERT Íµ¨Ï°∞Î•º ÏÇ¨Ïö©ÌñàÎã§. Í∑∏ÎûòÏÑú Î≥∏ Ìè¨Ïä§ÌåÖÏóêÏÑúÎèÑ BERTÏóê ÎåÄÌïú ÏÑ§Î™Ö ÏóÜÏù¥ SBOÏóê ÎåÄÌï¥ÏÑúÎßå Îã§Î£®Î†§Í≥† ÌïúÎã§.</p>

<h3 id="sbo-span-boundary-objective"><code class="language-plaintext highlighter-rouge">üìö¬†SBO: Span Boundary Objective</code></h3>

<p align="center">
<img src="/assets/images/spanbert/sbo.png" alt="SBO Task" class="align-center image-caption" width="100%&quot;, height=&quot;100%" />
<strong><em><a href="https://arxiv.org/abs/1907.10529">SBO Task</a></em></strong>
</p>

<p><strong>[SBO Algorithm Summary]</strong></p>
<ul>
  <li><strong>1) Ïó∞ÏÜçÎêú Î≤îÏúÑÏùò Span ÏÉùÏÑ±</strong>
    <ul>
      <li><strong>Î¨¥ÏûëÏúÑÎ°ú SpanÏùò ÏñëÏ™Ω ÎÅù ÌÜ†ÌÅ∞ ÏßÄÏ†ï ($x_{4}, x_{9}$)</strong>
        <ul>
          <li><strong>$x_{5}$ to $x_{8}$ ÏùÄ Ïä§Ìå¨ ÎÇ¥Î∂Ä ÌÜ†ÌÅ∞</strong></li>
        </ul>
      </li>
      <li><strong>ÎßàÏä§ÌÇπ ÏòàÏÇ∞ Í≥ÑÏÇ∞</strong>
        <ul>
          <li><strong>Î¨∏Ïû• Îãπ ÎßàÏä§ÌÇπ ÏòàÏÇ∞(Ìï©ÏÇ∞ Span Í∏∏Ïù¥)ÏùÄ Î¨∏Ïû• Í∏∏Ïù¥Ïùò 15%</strong></li>
          <li><strong>ÏòàÏãú ÏãúÌÄÄÏä§ Í∏∏Ïù¥: 512</strong></li>
          <li><strong>ÎßàÏä§ÌÇπ ÏòàÏÇ∞: ÎåÄÎûµ 75 = 512*0.15</strong></li>
        </ul>
      </li>
      <li><strong>Í∏∞Ìïò Î∂ÑÌè¨ ÏÇ¨Ïö©Ìï¥ÏÑú Í∞úÎ≥Ñ Ïä§Ìå¨ Í∏∏Ïù¥ ÏßÄÏ†ï</strong>
        <ul>
          <li><strong>Í∞úÎ≥Ñ Ïä§Ìå¨Îãπ ÏµúÎåÄ Í∏∏Ïù¥ ÏßÄÏ†ï, ÏµúÎåÄ 10Ïù¥ ÎÑòÏßÄ ÏïäÎèÑÎ°ù ÏÑ§Ï†ï</strong></li>
          <li><strong>ÏµúÎåÄ Ïä§Ìå¨ Ìï©ÏÇ∞ Í∏∏Ïù¥ ÎèÑÎã¨ÍπåÏßÄ ÎßàÏä§ÌÇπ Î∞òÎ≥µ</strong>
            <ul>
              <li><strong>ÎÇ®ÏùÄ ÎßàÏä§ÌÇπ ÏòàÏÇ∞ &lt; ÌòÑÏû¨ Ïä§Ìå¨ Í∏∏Ïù¥</strong>
                <ul>
                  <li><strong>ÎÇ®ÏùÄ ÎßàÏä§ÌÇπ ÏòàÏÇ∞ÏùÑ ÌòÑÏû¨ Ïä§Ìå¨ Í∏∏Ïù¥Î°ú ÏÑ§Ï†ï</strong></li>
                </ul>
              </li>
            </ul>
          </li>
        </ul>
      </li>
      <li><strong>Îî∞ÎùºÏÑú Subword TokenizingÏù¥ ÏïÑÎãàÎùº Whole Word Masking Îã®ÏúÑ ÏûëÏóÖÏù¥ ÌïÑÏöî</strong></li>
    </ul>
  </li>
  <li><strong>2) ÏãúÏûë ÌÜ†ÌÅ∞ Í∏∞Ï§Ä, ÏÉÅÎåÄ ÏúÑÏπò Í≥ÑÏÇ∞</strong>
    <ul>
      <li><strong>Ïä§Ìå¨ ÎÇ¥Î∂Ä ÌÜ†ÌÅ∞Ïùò ÏÉÅÎåÄ ÏúÑÏπò ÏûÑÎ≤†Îî© ÏÉùÏÑ± Î∞è Í≥ÑÏÇ∞</strong></li>
      <li><strong>ÏãúÏûëÌÜ†ÌÅ∞, ÎßàÏßÄÎßâÌÜ†ÌÅ∞, Ïä§Ìå¨ ÎÇ¥Î∂Ä ÌÜ†ÌÅ∞Ïùò ÏÉÅÎåÄ ÏúÑÏπò ÏûÑÎ≤†Îî©ÏùÑ concat, ÏùÄÎãâ Î≤°ÌÑ∞ ÏÉùÏÑ±</strong></li>
      <li><strong>SpanHeadÏóê ÏùÄÎãâ Î≤°ÌÑ∞ ÌÜµÍ≥ºÏãúÌÇ§Í∏∞</strong></li>
    </ul>
  </li>
  <li><strong>3) SpanHead Ï∂úÎ†•Í∞íÏùÑ ÎßàÏä§ÌÇπÏóê ÎåÄÌïú ÏòàÏ∏° ÌëúÌòÑÏúºÎ°ú ÏÇ¨Ïö©</strong></li>
</ul>

<p>SBOÏùò ÏïÑÏù¥ÎîîÏñ¥ ÏûêÏ≤¥Îäî ÏÉÅÎãπÌûà Í∞ÑÎã®ÌïòÎã§. Í∏∞Ï°¥ MLMÏ≤òÎüº Î¨¥ÏûëÏúÑÎ°ú ÏãúÌÄÄÏä§ÏóêÏÑú ÏïÑÎ¨¥ ÌÜ†ÌÅ∞Ïù¥ÎÇò ÏÑ†ÌÉùÌïòÎäîÍ≤å ÏïÑÎãàÎùº, Ï£ºÏñ¥ÏßÑ Î¨∏Ïû•ÏóêÏÑú ÏùºÏ†ï Í∏∏Ïù¥Ïùò Ïó∞ÏÜçÎêú ÌÜ†ÌÅ∞Îì§ÏùÑ ÌïúÎ≤àÏóê ÏÑ†ÌÉùÌï¥ ÎßàÏä§ÌÇπ Ï≤òÎ¶¨ÌïòÏó¨ ÌïôÏäµÌïòÍ≤†Îã§Îäî Í≤ÉÏù¥Îã§. ÎÖºÎ¨∏ÏóêÏÑú Ï†úÏãúÌïú SBO ÏïåÍ≥†Î¶¨Ï¶òÏùÑ Ï†ïÎ¶¨ÌïòÎ©¥ ÏïÑÎûòÏôÄ Í∞ôÎã§.</p>

\[\begin{align*}
h_0 &amp;= [x_{s-1}; x_{e+1}; p_{i-s+1}] \\
h_1 &amp;= \text{LayerNorm}(\text{GeLU}(W_1 h_0)) \\
y_i &amp;= \text{LayerNorm}(\text{GeLU}(W_2 h_1))
\end{align*}\]

<p>ÏúÑ Í∑∏Î¶ºÏùÑ ÏòàÏãúÎ°ú ÏïåÍ≥†Î¶¨Ï¶òÏùÑ ÏÇ¥Ìé¥Î≥¥Ïûê. Î®ºÏ†Ä Ï£ºÏñ¥ÏßÑ Ïä§Ìå¨ Í∏∏Ïù¥Ïóê ÎßûÍ≤å, Ïä§Ìå¨Ïùò ÏãúÏûëÍ≥º ÎÅù ÏßÄÏ†êÏù¥ ÎêòÎäî ÌÜ†ÌÅ∞ÏùÑ Î¨¥ÏûëÏúÑÎ°ú ÏÑ†ÌÉùÌïúÎã§. Í∑∏Îã§Ïùå ÏãúÏûë ÏúÑÏπòÎ•º Í∏∞Ï§ÄÏúºÎ°ú, Ïä§Ìå¨ ÎÇ¥Î∂ÄÏóê ÏÜçÌïòÎäî ÌÜ†ÌÅ∞Îì§Ïùò ÏÉÅÎåÄ ÏúÑÏπò Ïù∏Îç±Ïä§Î•º Í≥ÑÏÇ∞Ìï¥Ï§ÄÎã§. Í∑∏Î¶º ÏÜç $x_{7}$ ÌÜ†ÌÅ∞Ïùò ÏÉÅÎåÄ ÏúÑÏπò Î≤àÌò∏Îäî 3Ïù¥ ÎêúÎã§. ÎØ∏Î¶¨ Ï†ïÏùòÌïú ÏÉÅÎåÄ ÏúÑÏπò ÏûÑÎ≤†Îî©ÏóêÏÑú Ìñâ Ïù∏Îç±Ïä§Í∞Ä 3Ïù∏ ÌñâÎ≤°ÌÑ∞Î•º Í∞ÄÏ†∏Ïò®Îã§. Í∑∏ Îã§Ïùå ÏñëÏ™Ω ÎÅù Î≤°ÌÑ∞ÏôÄ concatÏùÑ ÏàòÌñâÌïòÏó¨ $h_{0}$ ÏùÑ ÎßåÎì†Îã§. Í∑∏Î¶¨Í≥† ÎØ∏Î¶¨ Ï†ïÏùòÎêú <code class="language-plaintext highlighter-rouge">SBOHead</code>Ïóê ÌÜµÍ≥ºÏãúÌÇ®Îã§. <code class="language-plaintext highlighter-rouge">SBOHead</code>ÏóêÍ≤å Î∞òÌôò Î∞õÏùÄ ÏùÄÎãâ Î≤°ÌÑ∞Í∞íÏùÄ Ìï¥Îãπ ÏúÑÏπòÏùò ÎßàÏä§ÌÇπÏóê ÎåÄÌïú ÏòàÏ∏°Í∞í($y_{i}$)ÏúºÎ°ú ÏÇ¨Ïö©ÌïòÍ≥† Ïù¥Î•º Ïù¥Ïö©Ìï¥ SBO ÏÜêÏã§ÏùÑ Íµ¨ÌïúÎã§. ÏßÄÍ∏àÍπåÏßÄ ÎÇ¥Ïö©ÏùÑ Ï†ïÎ¶¨Ìï¥ ÏàòÏãùÏúºÎ°ú ÌëúÌòÑÌïòÎ©¥ ÏúÑÏôÄ Í∞ôÎã§.</p>

\[L(x_i) = L_{MLM}(x_i) + L_{SBO}(x_i)\]

<p><code class="language-plaintext highlighter-rouge">SpanBERT</code>Ïùò Î™©Ï†ÅÌï®ÏàòÎäî SBO ÏÜêÏã§ ÎøêÎßå ÏïÑÎãàÎùº Í∏∞Ï°¥ MLM ÏÜêÏã§ÎèÑ Ìï®Íºê Ìè¨Ìï®ÎêòÏñ¥ ÏûàÎã§. Îã§Îßå MLM ÏÜêÏã§ÏùÑ Íµ¨ÌïòÍ∏∞ ÏúÑÌï¥ Ï£ºÏñ¥ÏßÑ ÏãúÌÄÄÏä§Ïóê ÎåÄÌï¥ Îî∞Î°ú ÎßàÏä§ÌÇπÏùÑ ÌïòÎäî Í≤ÉÏùÄ ÏïÑÎãàÍ≥†, SBOÎ•º ÏúÑÌï¥ Ï†ÅÏö©ÌñàÎçò Span MaskingÏùÑ Í∑∏ÎåÄÎ°ú ÌôúÏö©ÌïúÎã§. ÎåÄÏã† ÏúÑÏùò SBO ÏàòÏãùÏùò $h_{0}$ Ïù¥ ÏïÑÎãàÎùº, $x_{i-s+1}$ ($i-s+1$ Î≤àÏß∏ ÌÜ†ÌÅ∞Ïùò Ïù∏ÏΩîÎçî Ï∂úÎ†•Í∞í)ÏùÑ Í∑∏ÎåÄÎ°ú MLM ÏÜêÏã§ÏùÑ Íµ¨ÌïòÎäîÎç∞ ÏÇ¨Ïö©ÌïúÎã§. Ï†ïÎ¶¨ÌïòÎ©¥, <code class="language-plaintext highlighter-rouge">SpanBERT</code>Ïùò ÏµúÏ¢Ö ÏÜêÏã§ÏùÄ ÏúÑ ÏàòÏãùÍ≥º Í∞ôÎã§. ÌïúÌé∏, <code class="language-plaintext highlighter-rouge">ELECTRA</code> ÎïåÏôÄÎäî Îã§Î•¥Í≤å Îëê ÏÜêÏã§Ïùò Ïä§ÏºÄÏùº Ï∞®Ïù¥Í∞Ä Í±∞Ïùò ÏóÜÏñ¥ Îî∞Î°ú Ïä§ÏºÄÏùº ÏÉÅÏàòÎ•º Í≥±Ìï¥Ï£ºÏßÄÎäî ÏïäÎäî Í≤É Í∞ôÎã§.</p>

<h3 id="implementation-by-pytorch"><code class="language-plaintext highlighter-rouge">üë©‚Äçüíª¬†Implementation by Pytorch</code></h3>

<p>ÎÖºÎ¨∏Ïùò ÎÇ¥Ïö© Ï¢ÖÌï©ÌïòÏó¨ ÌååÏù¥ÌÜ†ÏπòÎ°ú <code class="language-plaintext highlighter-rouge">SpanBERT</code>Î•º Íµ¨ÌòÑÌï¥Î¥§Îã§. ÎÖºÎ¨∏Ïóê Ìè¨Ìï®Îêú ÏïÑÏù¥ÎîîÏñ¥Î•º Ïù¥Ìï¥ÌïòÎäîÎç∞Îäî Ïñ¥Î†µÏßÄ ÏïäÏïòÏßÄÎßå, Ï†úÌïúÎêú Ï°∞Í±¥Ïóê ÎßûÎäî Ïä§Ìå¨ÏùÑ Ï∞æÍ≥†, ÎßàÏä§ÌÇπÌïòÎäî Í≥ºÏ†ïÏùÑ Ïã§Ï†ú Íµ¨ÌòÑÌïòÎäî Í≤ÉÏùÄ Îß§Ïö∞ ÍπåÎã§Î°úÏö¥ Ìé∏Ïù¥ÏóàÎã§.
Î≥∏ Ìè¨Ïä§ÌåÖÏóêÏÑúÎäî <code class="language-plaintext highlighter-rouge">SpanBERT</code>Ïùò SBO ÌïôÏäµÏùÑ ÏúÑÌïú ÏûÖÎ†• ÎßåÎì§Í∏∞, SBOHeadÏóê ÎåÄÌï¥ÏÑúÎßå ÏÑ§Î™ÖÌïòÎ†§Í≥† ÌïúÎã§. <code class="language-plaintext highlighter-rouge">BERT</code>, <code class="language-plaintext highlighter-rouge">Whole World Masking</code>Ïóê ÎåÄÌï¥ Í∂ÅÍ∏àÌïòÎã§Î©¥ Ïù¥Ï†Ñ Ìè¨Ïä§ÌåÖÏùÑ, Ï†ÑÏ≤¥ Î™®Îç∏ Íµ¨Ï°∞ ÎåÄÌïú ÏΩîÎìúÎäî <strong><a href="https://github.com/qcqced123/model_study">Ïó¨Í∏∞ ÎßÅÌÅ¨</a></strong>Î•º ÌÜµÌï¥ Ï∞∏Í≥†Î∞îÎûÄÎã§.</p>

<p>Í≥µÍ∞úÌï† ÏΩîÎìúÎäî ÏïÑÏßÅ ÏôÑÎ≤ΩÌïòÍ≤å Î≤°ÌÑ∞ÌôîÎ•º Ï†ÅÏö©ÌïòÏßÄ Î™ªÌï¥, GPU Î≥ëÎ†¨ Ïó∞ÏÇ∞Ïóê ÏµúÏ†ÅÌôî ÎêòÏßÄ Î™ªÌïú Ï†ê ÏñëÌï¥ Î∂ÄÌÉÅÌïúÎã§. Îπ†Î•∏ ÏãúÏùº Ïù¥ÎÇ¥Ïóê Î≤°ÌÑ∞ÌôîÎ•º Ï†ÅÏö©Ìï¥ÏÑú Îã§Ïãú ÏàòÏ†ïÎêú ÏΩîÎìúÎ•º Ïò¨Î¶¨Í≤†Îã§.</p>

<h4 id="span-masking-algoritm"><code class="language-plaintext highlighter-rouge">üë©‚Äçüíª¬†Span Masking Algoritm</code></h4>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">random</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="nn">torch.nn.utils.rnn</span> <span class="kn">import</span> <span class="n">pad_sequence</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">Tensor</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">List</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">Any</span>
<span class="kn">from</span> <span class="nn">..tuner.mlm</span> <span class="kn">import</span> <span class="n">WholeWordMaskingCollator</span>
<span class="kn">from</span> <span class="nn">configuration</span> <span class="kn">import</span> <span class="n">CFG</span>

<span class="n">BPE</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s">'RobertaTokenizerFast'</span><span class="p">,</span>
    <span class="s">'GPT2TokenizerFast'</span><span class="p">,</span>
<span class="p">]</span>

<span class="n">SPM</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s">'DebertaV2TokenizerFast'</span><span class="p">,</span>
    <span class="s">'DebertaTokenizerFast'</span><span class="p">,</span>
    <span class="s">'XLMRobertaTokenizerFast'</span><span class="p">,</span>
<span class="p">]</span>

<span class="n">WORDPIECE</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s">'BertTokenizerFast'</span><span class="p">,</span>
    <span class="s">'ElectraTokenizerFast'</span><span class="p">,</span>
<span class="p">]</span>

<span class="k">def</span> <span class="nf">random_non_negative_integer</span><span class="p">(</span><span class="n">max_value</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">random</span><span class="p">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">max_value</span><span class="p">)</span>

<span class="k">class</span> <span class="nc">SpanCollator</span><span class="p">(</span><span class="n">WholeWordMaskingCollator</span><span class="p">):</span>
    <span class="s">""" Custom Collator for Span Boundary Objective Task, which is used for span masking algorithm
    Span Masking is similar to Whole Word Masking, but it has some differences:
        1) Span Masking does not use 10% of selected token left &amp; 10% of selected token replaced other vocab token
            - just replace all selected token to [MASK] token
    Algorithm:
    1) Select 2 random tokens from input tokens for spanning
    2) Calculate relative position embedding for each token with 2 random tokens froms step 1.
    3) Calculate span boundary objective with 2 random tokens from step 1 &amp; pos embedding from step 2.
    Args:
        cfg: configuration.CFG
        masking_budget: masking budget for Span Masking
                        (default: 0.15 =&gt; Recommended by original paper)
        span_probability: probability of span length for Geometric Distribution
                         (default: 0.2 =&gt; Recommended by original paper)
        max_span_length: maximum span length of each span in one batch sequence
                         (default: 10 =&gt; Recommended by original paper)
    References:
        https://arxiv.org/pdf/1907.10529.pdf
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">cfg</span><span class="p">:</span> <span class="n">CFG</span><span class="p">,</span>
        <span class="n">masking_budget</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.15</span><span class="p">,</span>
        <span class="n">span_probability</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.2</span><span class="p">,</span>
        <span class="n">max_span_length</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">SpanCollator</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">(</span><span class="n">cfg</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span> <span class="o">=</span> <span class="n">cfg</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">tokenizer</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">tokenizer</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">masking_budget</span> <span class="o">=</span> <span class="n">masking_budget</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">span_probability</span> <span class="o">=</span> <span class="n">span_probability</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">max_span_length</span> <span class="o">=</span> <span class="n">max_span_length</span>

    <span class="k">def</span> <span class="nf">_whole_word_mask</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">input_tokens</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span>
        <span class="n">max_predictions</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">CFG</span><span class="p">.</span><span class="n">max_seq</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]:</span>
        <span class="s">"""
        0) apply Whole Word Masking Algorithm for make gathering original token index in natural language
        1) calculate number of convert into masking tokens with masking budget*len(input_tokens)
        2) define span length of this iteration
            - span length follow geometric distribution
            - span length is limited by max_span_length
        """</span>
        <span class="n">cand_indexes</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">token</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">input_tokens</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">token</span> <span class="o">==</span> <span class="s">"[CLS]"</span> <span class="ow">or</span> <span class="n">token</span> <span class="o">==</span> <span class="s">"[SEP]"</span><span class="p">:</span>
                <span class="k">continue</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">cand_indexes</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="mi">1</span> <span class="ow">and</span> <span class="bp">self</span><span class="p">.</span><span class="n">select_post_string</span><span class="p">(</span><span class="n">token</span><span class="p">):</span>  <span class="c1"># method from WholeWordMaskingCollator
</span>                <span class="n">cand_indexes</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">].</span><span class="n">append</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
            <span class="k">elif</span> <span class="bp">self</span><span class="p">.</span><span class="n">select_src_string</span><span class="p">(</span><span class="n">token</span><span class="p">):</span>  <span class="c1"># method from WholeWordMaskingCollator
</span>                <span class="n">cand_indexes</span><span class="p">.</span><span class="n">append</span><span class="p">([</span><span class="n">i</span><span class="p">])</span>

        <span class="n">l</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">input_tokens</span><span class="p">)</span>
        <span class="n">src_l</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">cand_indexes</span><span class="p">)</span>
        <span class="n">num_convert_tokens</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">masking_budget</span> <span class="o">*</span> <span class="n">l</span><span class="p">)</span>
        <span class="n">budget</span> <span class="o">=</span> <span class="n">num_convert_tokens</span>  <span class="c1"># int is immutable object, so do not copy manually
</span>        <span class="n">masked_lms</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">covered_indexes</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
        <span class="k">while</span> <span class="n">budget</span><span class="p">:</span>
            <span class="n">span_length</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">min</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">distributions</span><span class="p">.</span><span class="n">Geometric</span><span class="p">(</span><span class="n">probs</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">span_probability</span><span class="p">).</span><span class="n">sample</span><span class="p">())))</span>
            <span class="n">src_index</span> <span class="o">=</span> <span class="n">random_non_negative_integer</span><span class="p">(</span><span class="n">src_l</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">span_length</span> <span class="o">&gt;</span> <span class="n">budget</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">budget</span> <span class="o">&lt;</span> <span class="mi">5</span><span class="p">:</span>  <span class="c1"># Set the span length to budget to avoid a large number of iter if the remaining budget is too small
</span>                    <span class="n">span_length</span> <span class="o">=</span> <span class="n">budget</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">continue</span>
            <span class="k">if</span> <span class="n">cand_indexes</span><span class="p">[</span><span class="n">src_index</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">span_length</span> <span class="o">&gt;</span> <span class="n">l</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>  <span class="c1"># If the index of the last token in the span is outside the full sequence range
</span>                <span class="k">continue</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">cand_indexes</span><span class="p">[</span><span class="n">src_index</span><span class="p">])</span> <span class="o">&gt;</span> <span class="n">span_length</span><span class="p">:</span>  <span class="c1"># handling bad case: violating WWM algorithm at start
</span>                <span class="k">continue</span>
            <span class="n">span_token_index</span> <span class="o">=</span> <span class="n">cand_indexes</span><span class="p">[</span><span class="n">src_index</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>  <span class="c1"># init span token index: src token
</span>            <span class="k">while</span> <span class="n">span_length</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">span_length</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="k">break</span>
                <span class="k">if</span> <span class="n">span_token_index</span> <span class="ow">in</span> <span class="n">covered_indexes</span><span class="p">:</span> <span class="c1"># If it encounters an index that is already masked, it ends, and starts the next iteration
</span>                    <span class="k">break</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">covered_indexes</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">span_token_index</span><span class="p">)</span>
                    <span class="n">masked_lms</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">span_token_index</span><span class="p">)</span>
                    <span class="n">span_length</span> <span class="o">-=</span> <span class="mi">1</span>
                    <span class="n">budget</span> <span class="o">-=</span> <span class="mi">1</span>
                    <span class="n">span_token_index</span> <span class="o">+=</span> <span class="mi">1</span>
                    <span class="k">continue</span>

        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">covered_indexes</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">masked_lms</span><span class="p">):</span>
            <span class="k">raise</span> <span class="nb">ValueError</span><span class="p">(</span><span class="s">"Length of covered_indexes is not equal to length of masked_lms."</span><span class="p">)</span>
        <span class="n">mask_labels</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span> <span class="k">if</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">covered_indexes</span> <span class="k">else</span> <span class="mi">0</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">input_tokens</span><span class="p">))]</span>
        <span class="k">return</span> <span class="n">mask_labels</span>

    <span class="k">def</span> <span class="nf">get_mask_tokens</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">inputs</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">mask_labels</span><span class="p">:</span> <span class="n">Tensor</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
        <span class="s">""" Prepare masked tokens inputs/labels for Span Boundary Objective with MLM (15%),
        All of masked tokens (15%) are replaced by [MASK] token,
        Unlike BERT MLM which is replaced by random token or stay original token left
        """</span>
        <span class="n">labels</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">.</span><span class="n">clone</span><span class="p">()</span>
        <span class="n">probability_matrix</span> <span class="o">=</span> <span class="n">mask_labels</span>

        <span class="n">special_tokens_mask</span> <span class="o">=</span> <span class="p">[</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">tokenizer</span><span class="p">.</span><span class="n">get_special_tokens_mask</span><span class="p">(</span><span class="n">val</span><span class="p">,</span> <span class="n">already_has_special_tokens</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span> <span class="k">for</span> <span class="n">val</span> <span class="ow">in</span> <span class="n">labels</span><span class="p">.</span><span class="n">tolist</span><span class="p">()</span>
        <span class="p">]</span>
        <span class="n">probability_matrix</span><span class="p">.</span><span class="n">masked_fill_</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">special_tokens_mask</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">bool</span><span class="p">),</span> <span class="n">value</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">tokenizer</span><span class="p">.</span><span class="n">pad_token</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">padding_mask</span> <span class="o">=</span> <span class="n">labels</span><span class="p">.</span><span class="n">eq</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">tokenizer</span><span class="p">.</span><span class="n">pad_token_id</span><span class="p">)</span>
            <span class="n">probability_matrix</span><span class="p">.</span><span class="n">masked_fill_</span><span class="p">(</span><span class="n">padding_mask</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)</span>

        <span class="n">masked_indices</span> <span class="o">=</span> <span class="n">probability_matrix</span><span class="p">.</span><span class="nb">bool</span><span class="p">()</span>
        <span class="n">labels</span><span class="p">[</span><span class="o">~</span><span class="n">masked_indices</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mi">100</span>  <span class="c1"># We only compute loss on masked tokens
</span>        <span class="n">inputs</span><span class="p">[</span><span class="n">masked_indices</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">tokenizer</span><span class="p">.</span><span class="n">convert_tokens_to_ids</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">tokenizer</span><span class="p">.</span><span class="n">mask_token</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">labels</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batched</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]])</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">:</span>
        <span class="s">""" Abstract Method for Collator, you must implement this method in child class """</span>
        <span class="n">input_ids</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="s">"input_ids"</span><span class="p">])</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">batched</span><span class="p">]</span>
        <span class="n">padding_mask</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="p">.</span><span class="n">get_padding_mask</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">input_ids</span><span class="p">]</span>

        <span class="n">padding_mask</span> <span class="o">=</span> <span class="n">pad_sequence</span><span class="p">(</span><span class="n">padding_mask</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">padding_value</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="n">input_ids</span> <span class="o">=</span> <span class="n">pad_sequence</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">padding_value</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

        <span class="n">mask_labels</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">batched</span><span class="p">:</span>
            <span class="n">ref_tokens</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">input_id</span> <span class="ow">in</span> <span class="n">x</span><span class="p">[</span><span class="s">"input_ids"</span><span class="p">]:</span>
                <span class="n">token</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">tokenizer</span><span class="p">.</span><span class="n">_convert_id_to_token</span><span class="p">(</span><span class="n">input_id</span><span class="p">)</span>
                <span class="n">ref_tokens</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">token</span><span class="p">)</span>
            <span class="n">mask_labels</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">_whole_word_mask</span><span class="p">(</span><span class="n">ref_tokens</span><span class="p">))</span>

        <span class="n">mask_labels</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">mask_labels</span><span class="p">]</span>
        <span class="n">mask_labels</span> <span class="o">=</span> <span class="n">pad_sequence</span><span class="p">(</span><span class="n">mask_labels</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">padding_value</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">input_ids</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">get_mask_tokens</span><span class="p">(</span>
            <span class="n">input_ids</span><span class="p">,</span>
            <span class="n">mask_labels</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="p">{</span>
            <span class="s">"input_ids"</span><span class="p">:</span> <span class="n">input_ids</span><span class="p">,</span>
            <span class="s">"labels"</span><span class="p">:</span> <span class="n">labels</span><span class="p">,</span>
            <span class="s">"padding_mask"</span><span class="p">:</span> <span class="n">padding_mask</span><span class="p">,</span>
            <span class="s">"mask_labels"</span><span class="p">:</span> <span class="n">mask_labels</span>
        <span class="p">}</span>
</code></pre></div></div>

<h4 id="sbo-head"><code class="language-plaintext highlighter-rouge">üë©‚Äçüíª¬†SBO Head</code></h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">SBOHead</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="s">""" Custom Head for Span Boundary Objective Task, this module return logit value for each token
    we use z for class logit, each Fully Connected Layer doesn't have bias term in original paper
    so we don't use bias term in this module =&gt; nn.Linear(bias=False)

    You must select option for matrix sum or concatenate with x_s-1, x_e+1, p_i-s+1
    If you select concatenate option, you must pass is_concatenate=True to cfg.is_concatenate, default is True
    
    Math:
        h_0 = [x_s-1;x_e+1;p_i-s+1]
        h_t = LayerNorm(GELU(W_0‚Ä¢h_0))
        z = LayerNorm(GELU(W_1‚Ä¢h_t))

    Args:
        cfg: configuration.CFG
        is_concatenate: option for matrix sum or concatenate with x_s-1, x_e+1, p_i-s+1, default True
        max_span_length: maximum span length of each span in one batch sequence
                         (default: 10 =&gt; Recommended by original paper)
    References:
        https://arxiv.org/pdf/1907.10529.pdf
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">cfg</span><span class="p">:</span> <span class="n">CFG</span><span class="p">,</span>
        <span class="n">is_concatenate</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="bp">True</span><span class="p">,</span>
        <span class="n">max_span_length</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">SBOHead</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span> <span class="o">=</span> <span class="n">cfg</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">is_concatenate</span> <span class="o">=</span> <span class="n">is_concatenate</span>  <span class="c1"># for matrix sum or concatenate with x_s-1, x_e+1, p_i-s+1
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">projector</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">dim_model</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">dim_model</span><span class="o">*</span><span class="mi">3</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>  <span class="c1"># for concatenate x_s-1, x_e+1, p_i-s+1
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">span_pos_emb</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">max_span_length</span><span class="o">*</span><span class="mi">3</span><span class="p">,</span> <span class="n">cfg</span><span class="p">.</span><span class="n">dim_model</span><span class="p">)</span>  <span class="c1"># size of dim_model is research topic
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">head</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">dim_model</span><span class="o">*</span><span class="mi">3</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">dim_ffn</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">GELU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">dim_ffn</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">dim_ffn</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">dim_model</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">GELU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">dim_model</span><span class="p">),</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">classifier</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">dim_model</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">cfg</span><span class="p">.</span><span class="n">vocab_size</span><span class="p">))</span>  <span class="c1"># for matching vocab size
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">classifier</span><span class="p">.</span><span class="n">bias</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">bias</span>

    <span class="o">@</span><span class="nb">staticmethod</span>
    <span class="k">def</span> <span class="nf">find_consecutive_groups</span><span class="p">(</span><span class="n">mask_labels</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">target_value</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">Dict</span><span class="p">]]:</span>
        <span class="s">""" Get the start and end positions of consecutive groups in tensor for the target value
        This method is used for SBO Objective Function, this version is not best performance to make span groups

        Args:
            mask_labels: masking tensor for span
            target_value: target value for finding consecutive groups
        """</span>
        <span class="n">all_consecutive_groups</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">mask_label</span> <span class="ow">in</span> <span class="n">mask_labels</span><span class="p">:</span>
            <span class="n">consecutive_groups</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="n">current_group</span> <span class="o">=</span> <span class="bp">None</span>
            <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">mask_label</span><span class="p">):</span>
                <span class="k">if</span> <span class="n">value</span> <span class="o">==</span> <span class="n">target_value</span><span class="p">:</span>
                    <span class="k">if</span> <span class="n">current_group</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
                        <span class="n">current_group</span> <span class="o">=</span> <span class="p">{</span><span class="s">"start"</span><span class="p">:</span> <span class="n">i</span><span class="p">,</span> <span class="s">"end"</span><span class="p">:</span> <span class="n">i</span><span class="p">}</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">current_group</span><span class="p">[</span><span class="s">"end"</span><span class="p">]</span> <span class="o">=</span> <span class="n">i</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">if</span> <span class="n">current_group</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
                        <span class="n">consecutive_groups</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">current_group</span><span class="p">)</span>
                        <span class="n">current_group</span> <span class="o">=</span> <span class="bp">None</span>
            <span class="k">if</span> <span class="n">current_group</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
                <span class="n">consecutive_groups</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">current_group</span><span class="p">)</span>
            <span class="n">all_consecutive_groups</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">consecutive_groups</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">all_consecutive_groups</span>

    <span class="k">def</span> <span class="nf">cal_span_emb</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">h</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">consecutive_groups</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">Dict</span><span class="p">]])</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="s">""" Calculate span embedding for each span in one batch sequence

        Args:
            h: hidden states, already passed through projection layer (dim*3)
            hidden_states: hidden states from encoder
            consecutive_groups: consecutive groups for each batch sequence
        """</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">batch</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">consecutive_groups</span><span class="p">):</span>  <span class="c1"># batch level
</span>            <span class="k">for</span> <span class="n">j</span><span class="p">,</span> <span class="n">span</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">batch</span><span class="p">):</span>  <span class="c1"># span level
</span>                <span class="n">start</span><span class="p">,</span> <span class="n">end</span> <span class="o">=</span> <span class="n">span</span><span class="p">[</span><span class="s">"start"</span><span class="p">],</span> <span class="n">span</span><span class="p">[</span><span class="s">"end"</span><span class="p">]</span>
                <span class="n">length</span> <span class="o">=</span> <span class="n">end</span> <span class="o">-</span> <span class="n">start</span> <span class="o">+</span> <span class="mi">1</span>
                <span class="n">idx</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="n">length</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>   <span class="c1"># .to(self.cfg.device)
</span>                <span class="n">context_s</span><span class="p">,</span> <span class="n">context_e</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">start</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="p">:],</span> <span class="n">hidden_states</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">end</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="p">:]</span>
                <span class="n">span_pos_emb</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">span_pos_emb</span><span class="p">(</span><span class="n">idx</span><span class="p">).</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">length</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                    <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">p_h</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">span_pos_emb</span><span class="p">):</span>  <span class="c1"># length of span_pos_emb == length of span of this iterations
</span>                        <span class="n">h</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">start</span><span class="o">+</span><span class="n">k</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cat</span><span class="p">([</span><span class="n">context_s</span><span class="p">,</span> <span class="n">p_h</span><span class="p">,</span> <span class="n">context_e</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">h</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">start</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cat</span><span class="p">([</span><span class="n">context_s</span><span class="p">,</span> <span class="n">span_pos_emb</span><span class="p">,</span> <span class="n">context_e</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">h</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">mask_labels</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="n">consecutive_groups</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">find_consecutive_groups</span><span class="p">(</span><span class="n">mask_labels</span><span class="p">)</span>  <span class="c1"># [batch, num_consecutive_groups]
</span>        <span class="n">h</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">projector</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>  <span class="c1"># [batch, seq, dim_model*3]
</span>        <span class="n">h_t</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">cal_span_emb</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">,</span> <span class="n">consecutive_groups</span><span class="p">)</span>
        <span class="n">z</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">head</span><span class="p">(</span><span class="n">h_t</span><span class="p">)</span>
        <span class="n">logit</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">classifier</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">logit</span>

</code></pre></div></div>

        
      </section>

      <footer class="page__meta">
        
        
  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      <a href="/tags/#bert" class="page__taxonomy-item p-category" rel="tag">BERT</a><span class="sep">, </span>
    
      <a href="/tags/#natural-language-process" class="page__taxonomy-item p-category" rel="tag">Natural Language Process</a><span class="sep">, </span>
    
      <a href="/tags/#pytorch" class="page__taxonomy-item p-category" rel="tag">Pytorch</a><span class="sep">, </span>
    
      <a href="/tags/#self-attention" class="page__taxonomy-item p-category" rel="tag">Self-Attention</a><span class="sep">, </span>
    
      <a href="/tags/#spanbert" class="page__taxonomy-item p-category" rel="tag">SpanBERT</a>
    
    </span>
  </p>




  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> Categories: </strong>
    <span itemprop="keywords">
    
      <a href="/categories/#nlp" class="page__taxonomy-item p-category" rel="tag">NLP</a>
    
    </span>
  </p>


        

  <p class="page__date"><strong><i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> Updated:</strong> <time class="dt-published" datetime="2024-03-11">March 11, 2024</time></p>

      </footer>

      <section class="page__share">
  
    <h4 class="page__share-title">Share on</h4>
  

  <a href="https://twitter.com/intent/tweet?text=%F0%9F%97%82%EF%B8%8F%5BSpanBERT%5D+SpanBERT%3A+Improving+Pre-training+by+Representing+and+Predicting+Spans%20http%3A%2F%2Flocalhost%3A4000%2Fnlp%2Fspanbert" class="btn btn--twitter" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Twitter"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=http%3A%2F%2Flocalhost%3A4000%2Fnlp%2Fspanbert" class="btn btn--facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Facebook"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=http%3A%2F%2Flocalhost%3A4000%2Fnlp%2Fspanbert" class="btn btn--linkedin" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on LinkedIn"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>


      
  <nav class="pagination">
    
      <a href="/nlp/roformer" class="pagination--pager" title="üé° [Roformer] RoFormer: Enhanced Transformer with Rotary Position Embedding
">Previous</a>
    
    
      <a href="/nlp/linear_attention" class="pagination--pager" title="üåÜ [Linear Attention] Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention
">Next</a>
    
  </nav>

    </div>

    
  </article>

  
  
    <div class="page__related">
      <h2 class="page__related-title">You may also enjoy</h2>
      <div class="grid__wrapper">
        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
      <div class="archive__item-teaser">
        <img src="/assets/images/huggingface_emoji.png" alt="">
      </div>
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/nlp/linear_attention" rel="permalink">üåÜ [Linear Attention] Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention
</a>
      
    </h2>
    <!-- 

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          8 minute read
        
      </span>
    
  </p>

 -->
    
      <p class="page__meta"><i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i> March 14 2024</p>
    
    <p class="archive__item-excerpt" itemprop="description">Linear Attention Official Paper Review with Pytorch Implementation
</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
      <div class="archive__item-teaser">
        <img src="/assets/images/huggingface_emoji.png" alt="">
      </div>
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/nlp/roformer" rel="permalink">üé° [Roformer] RoFormer: Enhanced Transformer with Rotary Position Embedding
</a>
      
    </h2>
    <!-- 

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          8 minute read
        
      </span>
    
  </p>

 -->
    
      <p class="page__meta"><i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i> March 11 2024</p>
    
    <p class="archive__item-excerpt" itemprop="description">Roformer Official Paper Review with Pytorch Implementation
</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
      <div class="archive__item-teaser">
        <img src="/assets/images/huggingface_emoji.png" alt="">
      </div>
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/nlp/electra" rel="permalink">üëÆ [ELECTRA] Pre-training Text Encoders as Discriminators Rather Than Generators
</a>
      
    </h2>
    <!-- 

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          7 minute read
        
      </span>
    
  </p>

 -->
    
      <p class="page__meta"><i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i> March 11 2024</p>
    
    <p class="archive__item-excerpt" itemprop="description">ELECTRA Official Paper Review with Pytorch Implementation
</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
      <div class="archive__item-teaser">
        <img src="/assets/images/huggingface_emoji.png" alt="">
      </div>
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/nlp/distilbert" rel="permalink">üßë‚Äçüè´ [DistilBERT] DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter
</a>
      
    </h2>
    <!-- 

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          8 minute read
        
      </span>
    
  </p>

 -->
    
      <p class="page__meta"><i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i> March 11 2024</p>
    
    <p class="archive__item-excerpt" itemprop="description">DistilBERT Official Paper Review with Pytorch Implementation
</p>
  </article>
</div>

        
      </div>
    </div>
  
  
</div>

    </div>

    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>Follow:</strong></li>
    

    
      
        
      
        
      
        
      
        
      
        
      
        
      
    

    
      <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
    
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2024 qcqced. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>







    <script>
  'use strict';

  (function() {
    var commentContainer = document.querySelector('#utterances-comments');

    if (!commentContainer) {
      return;
    }

    var script = document.createElement('script');
    script.setAttribute('src', 'https://utteranc.es/client.js');
    script.setAttribute('repo', 'qcqced123/qcqced123.github.io');
    script.setAttribute('issue-term', 'pathname');
    
    script.setAttribute('theme', 'github-light');
    script.setAttribute('crossorigin', 'anonymous');

    commentContainer.appendChild(script);
  })();
</script>

  




<script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML">
</script>

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"] ],
    displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
    processEscapes: true
    },
    "HTML-CSS": { availableFonts: ["TeX"] }
});
</script>

  </body>
</html>
