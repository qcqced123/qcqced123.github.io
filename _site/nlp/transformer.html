<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.24.0 by Michael Rose
  Copyright 2013-2020 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->
<html lang="en" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>🤖 [Transformer] Attention Is All You Need - AI/Business Study Log</title>
<meta name="description" content="Transformer Official Paper Review with Pytorch Implementation">


  <meta name="author" content="qcqced">
  
  <meta property="article:author" content="qcqced">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="AI/Business Study Log">
<meta property="og:title" content="🤖 [Transformer] Attention Is All You Need">
<meta property="og:url" content="http://localhost:4000/nlp/transformer">


  <meta property="og:description" content="Transformer Official Paper Review with Pytorch Implementation">







  <meta property="article:published_time" content="2023-08-04T00:00:00+09:00">



  <meta property="article:modified_time" content="2023-08-04T01:00:00+09:00">



  

  


<link rel="canonical" href="http://localhost:4000/nlp/transformer">




<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    
      "@type": "Person",
      "name": "qcqced",
      "url": "http://localhost:4000/"
    
  }
</script>







<!-- end _includes/seo.html -->



  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="AI/Business Study Log Feed">


<!-- https://t.co/dKP3o1e -->
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css"></noscript>



<!-- Latex -->
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->
<link rel="apple-touch-icon" sizes="180x180" href="/assets/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/assets/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/assets/favicon-16x16.png">
<link rel="manifest" href="/assets/site.webmanifest">
<link rel="mask-icon" href="/assets/safari-pinned-tab.svg" color="#5bbad5">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="theme-color" content="#ffffff">
<!-- end custom head snippets -->

    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      TeX: {
        equationNumbers: {
          autoNumber: "AMS"
        }
      },
      tex2jax: {
      inlineMath: [ ['$', '$'] ],
      displayMath: [ ['$$', '$$'] ],
      processEscapes: true
    }
  });
  MathJax.Hub.Register.MessageHook("Math Processing Error",function (message) {
      alert("Math Processing Error: "+message[1]);
    });
  MathJax.Hub.Register.MessageHook("TeX Jax - parse error",function (message) {
      alert("Math Processing Error: "+message[1]);
    });
  </script>
  <script type="text/javascript" async
    src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>
  </head>

  <body class="layout--single">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
          AI/Business Study Log
          <span class="site-subtitle">NLP, Marketing</span>
        </a>
        
        
        <ul class="visible-links">
              
              
                  <li class="masthead__menu-item">
                      <a href="https://qcqced123.github.io/">Home</a>
                  </li>
              
          
              
              
                  <li class="dropdown ">
                      <a class="dropdown-toggle" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">CS/AI  <i class="fa fa-caret-down fa-sm" aria-hidden="true"></i><span class="caret"></span></a>
       
                      <ul class="dropdown-content" >
                          
                              <a class = "dropdown-item" href="/nlp/">    Natural Language Process</a>
                          
                              <a class = "dropdown-item" href="/multi-modal/">    Multi Modal</a>
                          
                              <a class = "dropdown-item" href="/cv/">    Computer Vision</a>
                          
                              <a class = "dropdown-item" href="/ml/">    Machine Learning</a>
                          
                              <a class = "dropdown-item" href="/framework-library/">    Framework & Library</a>
                          
                              <a class = "dropdown-item" href="/python/">    Python</a>
                          
                              <a class = "dropdown-item" href="/algorithm/">    Data Structure & Algorithm</a>
                          
                              <a class = "dropdown-item" href="/ps/">    Problem Solving</a>
                          
                      </ul>
                  </li>
              
          
              
              
                  <li class="dropdown ">
                      <a class="dropdown-toggle" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Math  <i class="fa fa-caret-down fa-sm" aria-hidden="true"></i><span class="caret"></span></a>
       
                      <ul class="dropdown-content" >
                          
                              <a class = "dropdown-item" href="/linear-algebra/">    Linear Algebra</a>
                          
                              <a class = "dropdown-item" href="/optimization-theory/">    Optimization Theory/Calculus</a>
                          
                              <a class = "dropdown-item" href="/signal-system/">    Signal & System</a>
                          
                      </ul>
                  </li>
              
          
              
              
                  <li class="dropdown ">
                      <a class="dropdown-toggle" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Business/Marketing  <i class="fa fa-caret-down fa-sm" aria-hidden="true"></i><span class="caret"></span></a>
       
                      <ul class="dropdown-content" >
                          
                              <a class = "dropdown-item" href="/device/">    Device</a>
                          
                              <a class = "dropdown-item" href="/semi-conductor/">    Semi-Conductor</a>
                          
                              <a class = "dropdown-item" href="/ai/">    AI</a>
                          
                      </ul>
                  </li>
              
          
              
              
                  <li class="masthead__menu-item">
                      <a href="/categories/">Category</a>
                  </li>
              
          
              
              
                  <li class="masthead__menu-item">
                      <a href="/about/">About</a>
                  </li>
              
          
       </ul>
       
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      





<div id="main" role="main">
  
  <div class="sidebar sticky">
  


<div itemscope itemtype="https://schema.org/Person" class="h-card">

  
    <div class="author__avatar">
      <a href="http://localhost:4000/">
        <img src="/assets/images/huggingface_emoji.png" alt="qcqced" itemprop="image" class="u-photo">
      </a>
    </div>
  

  <div class="author__content">
    <h3 class="author__name p-name" itemprop="name">
      <a class="u-url" rel="me" href="http://localhost:4000/" itemprop="url">qcqced</a>
    </h3>
    
      <div class="author__bio p-note" itemprop="description">
        <p>Interested in NLP, Marketing</p>

      </div>
    
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">Follow</button>
    <ul class="author__urls social-icons">
      
        <li itemprop="homeLocation" itemscope itemtype="https://schema.org/Place">
          <i class="fas fa-fw fa-map-marker-alt" aria-hidden="true"></i> <span itemprop="name" class="p-locality">Seoul, South Korea</span>
        </li>
      

      
        
          
            <li><a href="https://qcqced123.github.io" rel="nofollow noopener noreferrer me" itemprop="sameAs"><i class="fas fa-fw fa-link" aria-hidden="true"></i><span class="label">Website</span></a></li>
          
        
          
            <li><a href="https://github.com/qcqced123" rel="nofollow noopener noreferrer me" itemprop="sameAs"><i class="fab fa-fw fa-github" aria-hidden="true"></i><span class="label">GitHub</span></a></li>
          
        
          
            <li><a href="https://www.kaggle.com/qcqced" rel="nofollow noopener noreferrer me" itemprop="sameAs"><i class="fab fa-fw fa-kaggle" aria-hidden="true"></i><span class="label">Kaggle</span></a></li>
          
        
      

      

      
        <li>
          <a href="mailto:qcqced123@gmail.com" rel="me" class="u-email">
            <meta itemprop="email" content="qcqced123@gmail.com" />
            <i class="fas fa-fw fa-envelope-square" aria-hidden="true"></i><span class="label">Email</span>
          </a>
        </li>
      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      <!--
  <li>
    <a href="http://link-to-whatever-social-network.com/user/" itemprop="sameAs" rel="nofollow noopener noreferrer me">
      <i class="fas fa-fw" aria-hidden="true"></i> Custom Social Profile Link
    </a>
  </li>
-->
    </ul>
  </div>
</div>

  
  </div>



  <article class="page h-entry" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="🤖 [Transformer] Attention Is All You Need">
    <meta itemprop="description" content="Transformer Official Paper Review with Pytorch Implementation">
    <meta itemprop="datePublished" content="2023-08-04T00:00:00+09:00">
    <meta itemprop="dateModified" content="2023-08-04T01:00:00+09:00">

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title p-name" itemprop="headline">
            <a href="http://localhost:4000/nlp/transformer" class="u-url" itemprop="url">🤖 [Transformer] Attention Is All You Need
</a>
          </h1>
          <p class="page__date">
            <a href="https://hits.seeyoufarm.com/localhost:4000/nlp/transformer"target="_blank">
              <img src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https://localhost:4000/nlp/transformer&count_bg=%23399DE2&title_bg=%236D6D6D&icon=pytorch.svg&icon_color=%23E7E7E7&title=Views&edge_flat=false"/>
            </a>
            <i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> Updated: <time datetime="2023-08-04T00:00:00+09:00">August 04, 2023</time>
            <!-- <div style="text-align: left;"> -->
            <!-- </div> -->
          </p>
          
          
        </header>
      

      <section class="page__content e-content" itemprop="text">
        
          <aside class="sidebar__right sticky">
            <nav class="toc">
              <header><h4 class="nav__title"><i class="fas fa-file-alt"></i> On this page</h4></header>
              <ul class="toc__menu"><li><a href="#overview">🔭 Overview</a></li><li><a href="#limitation-of-recurrent-structure">🤔 Limitation of Recurrent Structure</a></li><li><a href="#modeling">🌟 Modeling</a><ul><li><a href="#-input-embedding">🔬 Input Embedding</a></li><li><a href="#-self-attention-with-linear-projection">🚀 Self-Attention with linear projection</a></li><li><a href="#scaled-dot-product-attention">📐 Scaled Dot-Product Attention</a></li><li><a href="#multi-head-attention-block">👩‍👩‍👧‍👦 Multi-Head Attention Block</a></li><li><a href="#-feed-forward-network">🔬 Feed Forward Network</a></li><li><a href="#add--norm">➕ Add &amp; Norm</a></li><li><a href="#encoderlayer">📘 EncoderLayer</a></li><li><a href="#-encoder">📚 Encoder</a></li><li><a href="#decoderlayer">📘 DecoderLayer</a></li><li><a href="#decoder">📚 Decoder</a></li><li><a href="#-transformer">🦾 Transformer</a></li></ul></li></ul>

            </nav>
          </aside>
        
        <h3 id="overview"><code class="language-plaintext highlighter-rouge">🔭 Overview</code></h3>

<p><code class="language-plaintext highlighter-rouge">Transformer</code>는 2017년 Google이 NIPS에서 발표한 자연어 처리용 신경망으로 기존 <code class="language-plaintext highlighter-rouge">RNN</code> 계열(LSTM, GRU) 신경망이 가진 문제를 해결하고 최대한 인간의 자연어 이해 방식을 수학적으로 모델링 하려는 의도로 설계 되었다. 이 모델은 초기 <code class="language-plaintext highlighter-rouge">Encoder-Decoder</code> 를 모두 갖춘 <code class="language-plaintext highlighter-rouge">seq2seq</code> 형태로 고안 되었으며, 다양한 번역 테스크에서 <code class="language-plaintext highlighter-rouge">SOTA</code>를 달성해 주목을 받았다. 이후에는 여러분도 잘 아시는 것처럼  <code class="language-plaintext highlighter-rouge">BERT</code>, <code class="language-plaintext highlighter-rouge">GPT</code>, <code class="language-plaintext highlighter-rouge">ViT</code>의 베이스 라인으로 채택 되며, 현대 딥러닝 역사에 한 획을 그은 모델로 평가 받고 있다.</p>

<p>현대 딥러닝의 전성기를 열어준 <code class="language-plaintext highlighter-rouge">Transformer</code>는 어떤 아이디어로 기존 <code class="language-plaintext highlighter-rouge">Recurrent</code> 계열이 가졌던 문제들을 해결했을까?? 이것을 제대로 이해하려면 먼저 기존 순환 신경망 모델들이 가졌던 문제부터 짚고 넘어갈 필요가 있다.</p>

<h3 id="limitation-of-recurrent-structure"><strong><code class="language-plaintext highlighter-rouge">🤔 Limitation of Recurrent Structure</code></strong></h3>

<ul>
  <li><strong>1) 인간과 다른 메커니즘의 Vanishing Gradient 발생 (Activation Function with Backward)</strong></li>
  <li><strong>2) 점점 흐려지는 Inputs에 Attention (Activation Function with Forward)</strong></li>
  <li><strong>3) 디코더가 가장 마지막 단어만 열심히 보고 <code class="language-plaintext highlighter-rouge">denoising</code> 수행 (Seq2Seq with Bi-Directional RNN)</strong></li>
</ul>

<p><strong><code class="language-plaintext highlighter-rouge">📈 1) 인간과 다른 메커니즘의 Vanishing Gradient 발생 (Activation Function with Backward)</code></strong></p>

\[h(t) = tanh(x_tW_x + h_{t-1}W_h + b)\]

<p><code class="language-plaintext highlighter-rouge">RNN</code>의 활성 함수인 <code class="language-plaintext highlighter-rouge">Hyperbolic Tangent</code> 는 $y$값이 <code class="language-plaintext highlighter-rouge">[-1, 1]</code> 사이에서 정의되며 기울기의 최대값은 1이다. 따라서 이전 시점 정보는 시점이 지나면 지날수록 (더 많은 셀을 통과할수록) 그라디언트 값이 작아져 미래 시점의 학습에 매우 작은 영향력을 갖게 된다. 이것이 바로 그 유명한 <code class="language-plaintext highlighter-rouge">RNN</code>의 <code class="language-plaintext highlighter-rouge">Vanishing Gradient</code> 현상이다. 사실 현상의 발생 자체는 그렇게 큰 문제가 되지 않는다. <code class="language-plaintext highlighter-rouge">RNN</code>에서 발생하는 <code class="language-plaintext highlighter-rouge">Vanishing Gradient</code> 가 문제가 되는 이유는 바로 인간이 자연어를 이해하는 메커니즘과 다른 방식으로 현상이 발생하기 때문이다. 우리가 글을 읽는 과정을 잘 떠올려 보자. 어떤 단어의 의미를 알기 위해 가까운 주변 단어의 문맥을 활용할 때도 있지만, 저 멀리 떨어진 문단의 문맥을 활용할 때도 있다. 이처럼 단어 혹은 시퀀스를 구성하는 <code class="language-plaintext highlighter-rouge">원소 사이의 관계성</code>이나 <code class="language-plaintext highlighter-rouge">어떤 다른 의미론적인 이유</code>로 <code class="language-plaintext highlighter-rouge">불균형</code>하게 현재 시점의 학습에 영향력을 갖게 되는게 아니라, 단순 <code class="language-plaintext highlighter-rouge">입력 시점</code> 때문에 불균형이 발생하기 때문에 <code class="language-plaintext highlighter-rouge">RNN</code>의 <code class="language-plaintext highlighter-rouge">Vanishing Gradient</code>가 낮은 성능의 원인으로 지목되는 것이다.</p>

<p>다시 말해, 실제 자연어의 문맥을 파악해 그라디언트에 반영하는게 아니라 단순히 시점에 따라서 그 영향력을 반영하게 된다는 것이다. 멀리 떨어진 시퀀스의 문맥이 필요한 경우를 <code class="language-plaintext highlighter-rouge">Recurrent</code> 구조는 정확히 학습할 수 없다.</p>

<p>그렇다면 활성 함수를 <code class="language-plaintext highlighter-rouge">relu</code> 혹은 <code class="language-plaintext highlighter-rouge">gelu</code> 를 사용하면 위 문제를 해결할 수 있을까? <code class="language-plaintext highlighter-rouge">Vanishing Graident</code> 문제는 해결할 수도 있으나 <code class="language-plaintext highlighter-rouge">hidden_state</code> 값이 발산할 것이다. 그 이유는 두 활성 함수 모두 양수 구간에서 선형인데, 이전 정보를 누적해서 가중치와 곱하고 현재 입력값에 더하는 <code class="language-plaintext highlighter-rouge">RNN</code>의 구조를 생각해보면 넘어오는 이전 정보는 누적되면서 점점 커질 것이고 그러다 결국 발산하게 된다.</p>

<p>결론적으로 <code class="language-plaintext highlighter-rouge">Vanishing Gradient</code> 현상 자체가 문제는 아니지만 모델이 자연어의 문맥을 파악해 그라디언트에 반영하는게 아니라 단순히 시점에 따라서 불균형하게 발생하기 때문에 낮은 성능의 원인으로 지목 받는 것이다. 이것을 <code class="language-plaintext highlighter-rouge">long-term dependency</code>라고 부르기도 한다.</p>

<p><strong><code class="language-plaintext highlighter-rouge">✏️ 2) 점점 흐려지는 Inputs에 Attention (Activation Function with Forward)</code></strong></p>

<p align="center">
<img src="/assets/images/transformer/tanh.png" alt="tanh function" class="align-center image-caption" width="75%&quot;, height=&quot;50%" />
<strong><em>tanh function</em></strong>
</p>

<p><code class="language-plaintext highlighter-rouge">Hyperbolic Tangent</code> 은  $y$값이 <code class="language-plaintext highlighter-rouge">[-1, 1]</code> 사이에서 정의된다고 했다. 다시 말해 셀의 출력값이 항상 일정 범위값( <code class="language-plaintext highlighter-rouge">[-1,1]</code> )으로 제한(가중치, 편향 더하는 것은 일단 제외) 된다는 것이다. 따라서 한정된 좁은 범위에 출력값들이 맵핑되는데, 이는 결국 입력값의 정보는 대부분 소실된 채 일부 특징만 정제 되어 출력되고 다음 레이어로 <code class="language-plaintext highlighter-rouge">forward</code> 됨을 의미한다. 그래프를 한 번 살펴보자. 특히 <code class="language-plaintext highlighter-rouge">Inputs</code> 값이 2.5 이상인 경우부터는 출력값이 거의 1에 수렴해 그 차이를 직관적으로 파악하기 힘들다. 이러한 활성함수가 수십개, 수백개 쌓인다면 결국 원본 정보는 매우 흐려지고 뭉개져서 다른 인스턴스와 구별이 힘들어 질 것이다.</p>

<p><strong><code class="language-plaintext highlighter-rouge">🔬 3) 디코더가 가장 마지막 단어만 열심히 보고 denoising 수행 (Seq2Seq with Bi-Directional RNN)</code></strong><br />
<code class="language-plaintext highlighter-rouge">“쓰다”</code> ($t_7$)라는 단어의 뜻을 이해하려면 <code class="language-plaintext highlighter-rouge">“돈을”</code>, <code class="language-plaintext highlighter-rouge">“모자를”</code>, <code class="language-plaintext highlighter-rouge">“맛이”</code>, <code class="language-plaintext highlighter-rouge">“글을”</code>($t_1$)과 같이 멀리 있는 앞 단어를 봐야 알 수 있는데, $h_7$ 에는 $t_1$이 흐려진 채로 들어가 있어서 $t_7$의 제대로 된 의미를 포착하지 못한다. 심지어 언어가 영어라면 뒤를 봐야 정확한 문맥을 알 수 있는데 <code class="language-plaintext highlighter-rouge">Vanilla RNN</code>은 단방향으로만 학습을 하게 되어 문장의 뒷부분 문맥은 반영조차(뒤에 위치한 목적어에 따라서 쓰다라는 단어의 뉘앙스는 달라짐) 할 수 없다. 그래서 <code class="language-plaintext highlighter-rouge">Bi-directional RNN</code> 써야하는데, 이것도 역시도 여전히 <code class="language-plaintext highlighter-rouge">“거리”</code>에 영향 받는다는 건 변하지 않기 때문에 근본적인 해결책이라 볼 수 없다.</p>

<p>한편, 디코더의 <code class="language-plaintext highlighter-rouge">Next Token Prediction</code> 성능은 무조건 인코더로부터 받는 <code class="language-plaintext highlighter-rouge">Context Vector</code>의 품질에 따라 좌지우지 된다. 그러나 Recurrent 구조의 인코더로부터 나온 Context Vector는 앞서 서술한 것처럼 좋은 품질(뒤쪽 단어가 상대적으로 선명함)이 아니다. 따라서 디코더의 번역(다음 단어 예측) 성능 역시 좋을리가 없다.</p>

<p>결국 <code class="language-plaintext highlighter-rouge">Recurrent</code> 구조 자체에 명확한 한계가 존재하여 인간이 자연어를 사용하고 이해하는 맥락과 다른 방식으로 동작햐게 되었다. <code class="language-plaintext highlighter-rouge">LSTM</code>, <code class="language-plaintext highlighter-rouge">GRU</code>의 제안으로 어느 정도 문제를 완화 시켰으나, 앞에서 서술했듯이 태생이 <code class="language-plaintext highlighter-rouge">Recurrent Structure</code>을 가지기 때문에 근본적인 해결책이 되지는 못했다. 그렇다면 이제 <code class="language-plaintext highlighter-rouge">Transformer</code>가 어떻게 위에 서술한 3가지 문제를 해결하고 현재의 위상을 갖게 되었는지 알아보자.</p>

<h3 id="modeling"><strong><code class="language-plaintext highlighter-rouge">🌟 Modeling</code></strong></h3>

<p align="center">
<img src="/assets/images/transformer/transformer_overview.png" alt="Attention Is All You Need" class="align-center image-caption" width="50%&quot;, height=&quot;50%" />
<strong><em><a href="https://arxiv.org/abs/1706.03762">Attention Is All You Need</a></em></strong>
</p>

<p>앞서 <code class="language-plaintext highlighter-rouge">Recurrent</code> 구조의 <code class="language-plaintext highlighter-rouge">Vanishing Gradient</code> 을 설명하면서 시점에 따라 정보를 소실하게 되는 현상은 인간의 자연어 이해 방식이 아니라는 점을 언급한 적 있다. 따라서 <code class="language-plaintext highlighter-rouge">Transformer</code>는 최대한 인간의 자연어 이해 방식을 수학적으로 모델링 하는 것에 초점을 맞췄다. 우리가 쓰여진 글을 이해하기 위해 하는 행동들을 떠올려 보자. <strong><code class="language-plaintext highlighter-rouge">“Apple”</code><u>이란 단어가 사과를 말하는 것인지, 브랜드 애플을 지칭하는 것인지 파악하기 위해 같은 문장에 속한 주변 단어를 살피기도 하고 그래도 파악하기 힘들다면 앞뒤 문장, 나아가 문서 전체 레벨에서 맥락을 파악하기 위해 노력한다.</u></strong> <code class="language-plaintext highlighter-rouge">Transformer</code> 연구진은 바로 이 과정에 주목했으며 이것을 모델링하여 그 유명한 <code class="language-plaintext highlighter-rouge">Self-Attention</code>을 고안해낸다.</p>

<p align="center">
<img src="/assets/images/transformer/word_embedding.png" alt="Word Embedding Space" class="align-center image-caption" width="50%&quot;, height=&quot;50%" />
<strong><em><a href="https://www.researchgate.net/figure/Visualization-of-the-word-embedding-space_fig4_343595281/download">Word Embedding Space</a></em></strong>
</p>

<p>다시 말해 <code class="language-plaintext highlighter-rouge">Self-Attention</code>은 토큰의 의미를 이해하기 위해 <code class="language-plaintext highlighter-rouge">전체 입력 시퀀스</code> 중에서 어떤 단어에 주목해야할지를 수학적으로 표현한 것이라 볼 수 있다. <strong><u>좀 더 구체적으로는 시퀀스에 속한 여러 토큰 벡터(행백터)를 임베딩 공간 어디에 배치할 것인가에 대해 훈련하는 행위다.</u></strong></p>

<p align="center">
<img src="/assets/images/transformer/scaled_dot_attention.png" alt="Scaled Dot-Product Attention" class="align-center image-caption" width="75%&quot;, height=&quot;50%" />
<strong><em><a href="https://arxiv.org/abs/1706.03762">Scaled Dot-Product Attention</a></em></strong>
</p>

<p>그렇다면 이제부터 <code class="language-plaintext highlighter-rouge">Transformer</code> 가 어떤 아이데이션을 통해 기존 순환 신경망 모델의 단점을 해결하고 딥러닝계의 <code class="language-plaintext highlighter-rouge">G.O.A.T</code> 자리를 차지했는지 알아보자. 모델은 크게 인코더와 디코더 부분으로 나뉘는데, 하는 역할과 미세한 구조상의 차이만 있을뿐 두 모듈 모두 <code class="language-plaintext highlighter-rouge">Self-Attention</code>이 제일 중요하다는 본질은 변하지 않는다. 따라서 <code class="language-plaintext highlighter-rouge">Input Embedding</code>부터 차례대로 살펴보되,  <code class="language-plaintext highlighter-rouge">Self-Attention</code> 은 특별히 사용된 하위 블럭 단위를 빠짐 없이, 세세하게 살펴볼 것이다.</p>

<p align="center">
<img src="/assets/images/transformer/class_diagram.png" alt="Class Diagram" class="align-center image-caption" width="35%&quot;, height=&quot;50%" />
<strong><em>Class Diagram</em></strong>
</p>

<p>이렇게 하위 모듈에 대한 설명부터 쌓아 나가 마지막에는 실제 구현 코드와 함께 전체적인 구조 측면에서도 모델을 해석해볼 것이다. 끝까지 포스팅을 읽어주시길 바란다.</p>

<h4 id="-input-embedding"><code class="language-plaintext highlighter-rouge">🔬 Input Embedding</code></h4>

\[X_E \in R^{B * S_E * V_E} \\
X_D \in R^{B * S_D * V_D}\]

<p><code class="language-plaintext highlighter-rouge">Transformer</code>는 인코더와 디코더로 이뤄진 <code class="language-plaintext highlighter-rouge">seq2seq</code> 구조를 가지고 있다. 즉, 대상 언어를 타겟 언어로 번역하는데 목적을 두고 있기 때문에 입력으로 대상 언어 시퀀스와 타겟 언어 시퀀스 모두 필요하다. $X_E$는 <code class="language-plaintext highlighter-rouge">인코더</code>의 입력 행렬을 나타내고, $X_D$는 <code class="language-plaintext highlighter-rouge">디코더</code>의 입력 행렬을 의미한다. 이 때, $B$는 <code class="language-plaintext highlighter-rouge">batch size</code>, $S$는 <code class="language-plaintext highlighter-rouge">max_seq</code>, $V$는 개별 모듈이 가진 <code class="language-plaintext highlighter-rouge">Vocab</code>의 사이즈를 가리킨다. 위 수식은 사실 논문에 입력에 대한 수식이 따로 서술 되어 있지 않아, 필자가 직접 만든 것이다. 앞으로도 해당 기호를 이용해 수식을 표현할 예정이니 참고 바란다.</p>

\[W_E \in R^{V_E * d} \\
W_D \in R^{V_D * d} \\\]

<p>이렇게 정의된 입력값을 개별 모듈의 임베딩 레이어에 통과 시킨 결과물이 바로 <code class="language-plaintext highlighter-rouge">Input Embedding</code>이 된다. $d$는 <code class="language-plaintext highlighter-rouge">Transformer</code> 모델의 은닉층의 크기를 의미한다. 따라서 <code class="language-plaintext highlighter-rouge">Position Embedding</code> 과 더해지기 전, 임베딩 레이어를 통과한 <code class="language-plaintext highlighter-rouge">Input Embedding</code>의 모양은 아래 수식과 같다.</p>

\[X_E \in R^{B*S_E*d} \\
X_D \in R^{B*S_D*d} \\\]

<p>그렇다면 실제 구현은 어떻게 할까?? <code class="language-plaintext highlighter-rouge">Transformer</code> 의 <code class="language-plaintext highlighter-rouge">Input Embedding</code>은 <code class="language-plaintext highlighter-rouge">nn.Embedding</code>으로 레이어를 정의해 사용한다. <code class="language-plaintext highlighter-rouge">nn.Linear</code>도 있는데 왜 굳이 <code class="language-plaintext highlighter-rouge">nn.Embedding</code>을 사용하는 것일까??</p>

<p>자연어 처리에서 입력 임베딩을 만들때는 모델의 토크나이저에 의해 사전 정의된 <code class="language-plaintext highlighter-rouge">vocab</code>의 사이즈가 입력 시퀀스에 속한 토큰 개수보다 훨씬 크기 때문에 데이터 룩업 테이블 방식의 <code class="language-plaintext highlighter-rouge">nn.Embedding</code> 을 사용하게 된다. 이게 무슨 말이냐면, 토크나이저에 의해 사전에 정의된 <code class="language-plaintext highlighter-rouge">vocab</code> 전체가 <code class="language-plaintext highlighter-rouge">nn.Embedding(vocab_size, dim_model)</code>로 투영 되어 가로는 <code class="language-plaintext highlighter-rouge">vocab</code> 사이즈, 세로는 모델의 차원 크기에 해당하는 룩업 테이블이 생성되고, 내가 입력한 토큰들은 전체 <code class="language-plaintext highlighter-rouge">vocab</code>의 일부분일테니 전체 임베딩 룩업 테이블에서 내가 임베딩하고 싶은 토큰들의 인덱스만 알아낸다는 것이다. 그래서 <code class="language-plaintext highlighter-rouge">nn.Embedding</code> 은 레이어에 정의된 차원과 실제 입력 데이터의 차원이 맞지 않아도 함수가 동작하게 된다. <code class="language-plaintext highlighter-rouge">nn.Linear</code> 와 입력 차원에 대한 조건 빼고는 동일한 동작을 수행하기 때문에 사전 정의된 <code class="language-plaintext highlighter-rouge">vocab</code> 사이즈와 입력 시퀀스의 토큰 개수가 같다면 <code class="language-plaintext highlighter-rouge">nn.Linear</code>를 사용해도 무방하다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Input Embedding Example
</span>
<span class="k">class</span> <span class="nc">Transformer</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
	<span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">enc_vocab_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">dec_vocab_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">max_seq</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">512</span><span class="p">,</span>
        <span class="n">enc_N</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">6</span><span class="p">,</span>
        <span class="n">dec_N</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">6</span><span class="p">,</span>
        <span class="n">dim_model</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">512</span><span class="p">,</span> <span class="c1"># latent vector space
</span>        <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span>
        <span class="n">dim_ffn</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2048</span><span class="p">,</span>
        <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Transformer</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">enc_input_embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">enc_vocab_size</span><span class="p">,</span> <span class="n">dim_model</span><span class="p">)</span> <span class="c1"># Encoder Input Embedding Layer
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">dec_input_embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">dec_vocab_size</span><span class="p">,</span> <span class="n">dim_model</span><span class="p">)</span> <span class="c1"># Decoder Input Embedding Layer
</span>	
	<span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">enc_inputs</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">dec_inputs</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">enc_pad_index</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">dec_pad_index</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
            <span class="n">enc_x</span><span class="p">,</span> <span class="n">dec_x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">enc_input_embedding</span><span class="p">(</span><span class="n">enc_inputs</span><span class="p">),</span> <span class="bp">self</span><span class="p">.</span><span class="n">dec_input_embedding</span><span class="p">(</span><span class="n">dec_inputs</span><span class="p">)</span>
</code></pre></div></div>

<p>위의 예시 코드를 함께 살펴보자. <code class="language-plaintext highlighter-rouge">__init__</code> 의 <code class="language-plaintext highlighter-rouge">self.enc_input_embedding</code>, <code class="language-plaintext highlighter-rouge">self._dec_input_embedding</code>이 바로 $W_E, W_D$에 대응된다. 한편 <code class="language-plaintext highlighter-rouge">forward</code> 메서드에 정의된 <code class="language-plaintext highlighter-rouge">enc_x</code>, <code class="language-plaintext highlighter-rouge">dec_x</code> 는 임베딩 레이어를 거치고 나온 $X_E, X_D$에 해당된다.</p>

<p>한편, $X_E, X_D$은 각각 인코더, 디코더 모듈로 흘러 들어가 <code class="language-plaintext highlighter-rouge">Absolute Position Embedding</code>과 더해진(행렬 합) 뒤, 개별 모듈의 입력값으로 활용된다.</p>

<p><strong><code class="language-plaintext highlighter-rouge">🔢 Absolute Position Embedding(Encoding)</code></strong><br />
입력 시퀀스에 위치 정보를 맵핑해주는 역할을 한다. 필자는 개인적으로 <code class="language-plaintext highlighter-rouge">Transformer</code>에서 가장 중요한 요소를 뽑으라고 하면 세 손가락 안에 들어가는 파트라고 생각한다. 다음 파트에서 자세히 기술하겠지만, <code class="language-plaintext highlighter-rouge">Self-Attention(내적)</code>은 입력 시퀀스를 병렬로 한꺼번에 처리할 수 있다는 장점을 갖고 있지만, 그 자체로는 토큰의 위치 정보를 인코딩할 수 없다. 우리가 따로 위치 정보를 알려주지 않는 이상 쿼리 행렬의 2번째 행벡터가 입력 시퀀스에서 몇 번째 위치한 토큰인지 모델은 알 길이 없다.</p>

<p>그런데, 텍스트는 <code class="language-plaintext highlighter-rouge">Permutation Equivariant</code>한 <code class="language-plaintext highlighter-rouge">Bias</code> 가 있기 때문에 토큰의 위치 정보는 <code class="language-plaintext highlighter-rouge">NLP</code>에서 매우 중요한 요소로 꼽힌다. <strong>직관적으로도 토큰의 순서는 시퀀스가 내포하는 의미에 지대한 영향을 끼친다는 것을 알 수 있다.</strong> 예를 들어 <code class="language-plaintext highlighter-rouge">“철수는 영희를 좋아한다”</code>라는 문장과 <code class="language-plaintext highlighter-rouge">“영희는 철수를 좋아한다”</code>라는 문장의 의미가 같은가 생각해보자. 주어와 목적어 위치가 바뀌면서 정반대의 뜻이 되어버린다.</p>

<p align="center">
<img src="/assets/images/transformer/positional_encoding.png" alt="Positional Encoding Example" class="align-center image-caption" width="75%&quot;, height=&quot;50%" />
<strong><em><a href="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/56cf1596-c770-410c-8053-5876c3c66fff/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2022-10-09_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_5.13.48.png">Positional Encoding Example</a></em></strong>
</p>

<p>따라서 저자는 입력 입베딩에 위치 정보를 추가하고자 <code class="language-plaintext highlighter-rouge">Position Encoding</code> 을 제안한다. 사실 <code class="language-plaintext highlighter-rouge">Position Encoding</code> 은 여러 단점 때문에 후대 <code class="language-plaintext highlighter-rouge">Transformer</code>  파생 모델에서는 잘 사용되지 않는 추세다. 대신 모델이 학습을 통해 최적값을 찾아주는 <code class="language-plaintext highlighter-rouge">Position Embedding</code> 방식을 대부분 차용하고 있다. 필자 역시 <code class="language-plaintext highlighter-rouge">Position Embedding</code> 을 사용해 위치 임베딩을 구현했기 때문에 원리와 단점에 대해서만 간단히 소개하고 넘어가려 한다. 또한 저자 역시 논문에서 두 방식 중 어느 것을 써도 비슷한 성능을 보여준다고 언급하고 있다.</p>

\[P_E \in R^{B*S_E*D} \\
 P_D \in R^{B*S_D*D} \\
P(pos, 2i) = sin(pos/\overset{}
  {10000_{}^{2i/dmodel}}) \\
P(pos, 2i+1) = cos(pos/\overset{}
  {10000_{}^{2i/dmodel}})\]

<p><strong>원리는 매우 간단하다. 사인함수와 코사인 함수의 주기성을 이용해 개별 인덱스의 행벡터 값을 표현하는 것이다.</strong> 행벡터의 원소 중에서 짝수번째 인덱스에 위치한 원소는 (짝수번째 열벡터) \(sin(pos/\overset{}{10000_{}^{2i/dmodel}})\) 의 함숫값을 이용해 채워넣고, 홀수번째 원소는 \(cos(pos/\overset{}{10000_{}^{2i/dmodel}})\)를 이용해 채워넣는다.</p>

<p align="center">
<img src="/assets/images/transformer/sin_cos_graph.png" alt="periodic function graph" class="align-center image-caption" width="75%&quot;, height=&quot;50%" />
<strong><em>periodic function graph</em></strong>
</p>

<p>초록색 그래프는 \(sin(pos/\overset{}{10000_{}^{2i/dmodel}})\), 주황색 그래프는 \(cos(pos/\overset{}{10000_{}^{2i/dmodel}})\)를 시각화했다. 지면의 제한으로 <code class="language-plaintext highlighter-rouge">max_seq=512</code> 만큼의 변화량을 담지는 못했지만, x축이 커질수록 두 함수 모두 진동 주기가 조금씩 커지는 양상을 보여준다. 따라서 개별 인덱스(행벡터)를 중복되는 값 없이 표현하는 것이 가능하다고 저자는 주장한다.</p>

<p align="center">
<img src="/assets/images/transformer/positional_encoding_result.png" alt="Positional Encoding Result" class="align-center image-caption" width="75%&quot;, height=&quot;50%" />
<strong><em><a href="https://wikidocs.net/162099">Positional Encoding Result</a></em></strong>
</p>

<p>위 그림은 토큰 <code class="language-plaintext highlighter-rouge">50</code>개, 은닉층이 256차원으로 구성된 시퀀스에 대해 <code class="language-plaintext highlighter-rouge">Positional Encoding</code>한 결과를 시각화한 자료다. 그래프의 $x$축은 <code class="language-plaintext highlighter-rouge">행벡터의 원소</code>이자 <code class="language-plaintext highlighter-rouge">Transformer</code>의 은닉 벡터 차원을 가리키고, $y$축은 <code class="language-plaintext highlighter-rouge">시퀀스의 인덱스</code>(행벡터)를 의미한다. 육안으로 정확하게 차이를 인식하기 쉽지는 않지만, 행벡터가 모두 유니크하게 표현된다는 사실(직접 실수값을 확인해보면 정말 미세한 차이지만 개별 토큰의 희소성이 보장)을 알 수 있다. 작은 차이를 시각화 자료로 파악하기는 쉽지 않기 때문에 진짜 그런가 궁금하신 분들은 직접 실수값을 구해보는 것을 추천드린다.</p>

<p><strong>여기서 행벡터의 희소성이란 개별 행벡터 원소의 희소성을 말하는게 아니다.</strong> 0번 토큰, 4번 토큰, 9번 토큰의 행벡터 1번째 원소의 값은 같을 수 있다. 하지만 진동 주기가 갈수록 커지는 주기함수를 사용하기 때문에 다른 원소(차원)값은 다를 것이라 기대할 수 있는데, <strong>바로 이것을 행벡터의 희소성이라고 정의하는 것이다.</strong> 만약 1번 토큰과 2번 토큰의 모든 행벡터 원소값이 같다면 그것은 희소성 원칙에 위배되는 상황이다.</p>

<p align="center">
<img src="/assets/images/transformer/encoding.png" alt="Positional Encoding" class="align-center image-caption" width="75%&quot;, height=&quot;50%" />
</p>

<p align="center">
<img src="/assets/images/transformer/embedding.png" alt="Compare Performance between Encoding and Embedding" class="align-center image-caption" width="75%&quot;, height=&quot;50%" />
<strong><em><a href="https://arxiv.org/abs/1706.03762">Compare Performance between Encoding and Embedding</a></em></strong>
</p>

<p>비록 개별 행벡터의 희소성이 보장된다고 해도 <code class="language-plaintext highlighter-rouge">Position Encoding</code>은 <code class="language-plaintext highlighter-rouge">not trainable</code>해서 <code class="language-plaintext highlighter-rouge">static</code>하다는 단점이 있다. 모든 배치의 시퀀스가 동일한 위치 정보값을 갖게 된다는 것이다. <code class="language-plaintext highlighter-rouge">512</code>개의 토큰으로 구성된 시퀀스 A와 B가 있다고 가정해보자. 이 때 시퀀스 A는 문장 <code class="language-plaintext highlighter-rouge">5</code>개로 구성 되어 있고, B는 문장 <code class="language-plaintext highlighter-rouge">12</code>개로 만들어졌다. 두 시퀀스의 <code class="language-plaintext highlighter-rouge">11</code>번째 토큰의 문장 성분은 과연 같을까?? 아마도 대부분의 경우에 다를 것이다. 텍스트 데이터에서 순서 정보가 중요한 이유 중 하나는 바로 <code class="language-plaintext highlighter-rouge">syntactical</code> 한 정보를 포착하기 위함이다. <code class="language-plaintext highlighter-rouge">Position Encoding</code>은 <code class="language-plaintext highlighter-rouge">static</code> 하기 때문에 이러한 타입의 정보를 인코딩 하기 쉽지 않다. 그래서 좀 더 풍부한 표현을 담을 수 있는 <code class="language-plaintext highlighter-rouge">Position Embedding</code>을 사용하는 것이 최근 추세다.</p>

<p><strong><code class="language-plaintext highlighter-rouge">✏️ Position Embedding</code></strong></p>

<p>그렇다면 이제 <code class="language-plaintext highlighter-rouge">Position Embedding</code>에 대해 알아보자. <code class="language-plaintext highlighter-rouge">Position Embedding</code> 은 <code class="language-plaintext highlighter-rouge">Input Embedding</code>을 정의한 방식과 거의 유사하다. 먼저 입력값과 <code class="language-plaintext highlighter-rouge">weight</code> 의 모양부터 확인해보자.</p>

\[P_E \in R^{B*S_E*d} \\
P_D \in R^{B*S_d*d} \\
W_{P_E} \in R^{S_E * d} \\
W_{P_D} \in R^{S_D * d} \\\]

<p>$P_E, P_D$는 개별 모듈의 위치 임베딩 레이어 입력을 가리키며, $W_{P_E}, W_{P_D}$가 개별 모듈의 위치 임베딩 레이어가 된다. 이제 이것을 코드로 어떻게 구현하는지 살펴보자.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Absolute Position Embedding Example
</span>
<span class="k">class</span> <span class="nc">Encoder</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="s">"""
    In this class, encode input sequence and then we stack N EncoderLayer
    First, we define "positional embedding" and then add to input embedding for making "word embedding"
    Second, forward "word embedding" to N EncoderLayer and then get output embedding
    In official paper, they use positional encoding, which is base on sinusoidal function(fixed, not learnable)
    But we use "positional embedding" which is learnable from training
    Args:
        max_seq: maximum sequence length, default 512 from official paper
        N: number of EncoderLayer, default 6 for base model
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">max_seq</span><span class="p">:</span> <span class="mi">512</span><span class="p">,</span> <span class="n">N</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">6</span><span class="p">,</span> <span class="n">dim_model</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">512</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span> <span class="n">dim_ffn</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2048</span><span class="p">,</span> <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Encoder</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">max_seq</span> <span class="o">=</span> <span class="n">max_seq</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">scale</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">dim_model</span><span class="p">))</span>  <span class="c1"># scale factor for input embedding from official paper
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">positional_embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">max_seq</span><span class="p">,</span> <span class="n">dim_model</span><span class="p">)</span>  <span class="c1"># add 1 for cls token
</span>
		<span class="p">...</span> <span class="n">중략</span> <span class="p">...</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">mask</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
        <span class="s">"""
        inputs: embedding from input sequence, shape =&gt; [BS, SEQ_LEN, DIM_MODEL]
        mask: mask for Encoder padded token for speeding up to calculate attention score
        """</span>
        <span class="n">layer_output</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">pos_x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">max_seq</span><span class="p">).</span><span class="n">repeat</span><span class="p">(</span><span class="n">inputs</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]).</span><span class="n">to</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">dropout</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">scale</span> <span class="o">*</span> <span class="n">inputs</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">positional_embedding</span><span class="p">(</span><span class="n">pos_x</span><span class="p">)</span>  <span class="c1"># layernorm 적용하고
</span>        <span class="p">)</span>
		<span class="p">...</span> <span class="n">중략</span> <span class="p">...</span> 
</code></pre></div></div>

<p>위 코드는 <code class="language-plaintext highlighter-rouge">Transformer</code>의 인코더 모듈을 구현한 것이다. 그래서 <code class="language-plaintext highlighter-rouge">forward</code> 메서드의 <code class="language-plaintext highlighter-rouge">pos_x</code> 가 바로 $P_E$가 되며, <code class="language-plaintext highlighter-rouge">__init__</code>의 <code class="language-plaintext highlighter-rouge">self.positional_embedding</code>이 바로 $W_{P_E}$에 대응된다. 이렇게 정의한 <code class="language-plaintext highlighter-rouge">Position Embedding</code>은 <code class="language-plaintext highlighter-rouge">Input Embedding</code>과 더해서 <code class="language-plaintext highlighter-rouge">Word Embedding</code> 을 만든다. <code class="language-plaintext highlighter-rouge">Word Embedding</code> 은 다시 개별 모듈의 <code class="language-plaintext highlighter-rouge">linear projection</code> 레이어에 대한 입력 $X$로 사용 된다.</p>

<p><strong>한편,</strong> <code class="language-plaintext highlighter-rouge">Input Embedding</code> <strong>과</strong> <code class="language-plaintext highlighter-rouge">Position Embedding</code><strong>을 더한다는 것에 주목해보자. 필자는 본 논문을 보며 가장 의문이 들었던 부분이다. 도대체 왜 완전히 서로 다른 출처에서 만들어진 행렬 두개를</strong> <code class="language-plaintext highlighter-rouge">concat</code> <strong>하지 않고 더해서 사용했을까??</strong> <code class="language-plaintext highlighter-rouge">concat</code><strong>을 이용하면 <code class="language-plaintext highlighter-rouge">Input</code>과 <code class="language-plaintext highlighter-rouge">Position</code> 정보를 서로 다른 차원에 두고 학습하는게 가능했을텐데 말이다.</strong></p>

<p><strong><code class="language-plaintext highlighter-rouge">🤔 Why Sum instead of Concatenate</code></strong><br />
행렬합을 사용하는 이유에 대해 저자가 특별히 언급하지는 않아서 때문에 정확한 의도를 알 수 없지만, <strong>추측하건데 <code class="language-plaintext highlighter-rouge">blessing of dimensionality</code> 효과를 의도했지 않았나 싶다.</strong> <code class="language-plaintext highlighter-rouge">blessing of dimensionality</code> 란, 고차원 공간에서 무작위로 서로 다른 벡터 두개를 선택하면 두 벡터는 거의 대부분 <code class="language-plaintext highlighter-rouge">approximate orthogonality</code>를 갖는 현상을 설명하는 용어다. 무조건 성립하는 성질은 아니고 확률론적인 접근이라는 것을 명심하자. 아무튼 직교하는 두 벡터는 내적값이 0에 수렴한다. 즉, 두 벡터는 서로에게 영향을 미치지 못한다는 것이다. 이것은 전체 모델의 <code class="language-plaintext highlighter-rouge">hidden states space</code> 에서 <code class="language-plaintext highlighter-rouge">Input Embedding</code> 과 <code class="language-plaintext highlighter-rouge">Position Embedding</code> 역시 개별 벡터가 <code class="language-plaintext highlighter-rouge">span</code> 하는 부분 공간 끼리는 서로 직교할 가능성이 매우 높다는 것을 의미한다. 따라서 서로 다른 출처를 통해 만들어진 두 행렬을 더해도 서로에게 영향을 미치지 못할 것이고 그로 인해 모델이 <code class="language-plaintext highlighter-rouge">Input</code>과 <code class="language-plaintext highlighter-rouge">Position</code> 정보를 따로 잘 학습할 수 있을 것이라 기대해볼 수 있다. 가정대로만 된다면, <code class="language-plaintext highlighter-rouge">concat</code> 을 사용해 모델의 <code class="language-plaintext highlighter-rouge">hidden states space</code> 를 늘려 <code class="language-plaintext highlighter-rouge">Computational Overhead</code> 를 유발하는 것보다 훨씬 효율적이라고 볼 수 있겠다.</p>

<p>한편 <code class="language-plaintext highlighter-rouge">blessing of dimensionality</code>에 대한 설명과 증명은 꽤나 많은 내용이 필요해 여기서는 자세히 다루지 않고, 다른 포스트에서 따로 다루겠다. 관련하여 좋은 내용을 담고 있는 글의 링크를 같이 첨부했으니 읽어보실 것을 권한다(<a href="https://softwaredoug.com/blog/2022/12/26/surpries-at-hi-dimensions-orthoginality.html">링크1</a>, <a href="https://www.reddit.com/r/MachineLearning/comments/cttefo/comment/exs7d08/">링크2</a>).</p>

<h4 id="-self-attention-with-linear-projection"><code class="language-plaintext highlighter-rouge">🚀 Self-Attention with linear projection</code></h4>

<p>왜 이름이 <code class="language-plaintext highlighter-rouge">self-attention</code>일까 먼저 고민해보자. 사실 <code class="language-plaintext highlighter-rouge">attention</code> 개념은 본 논문이 발표되기 이전부터 사용되던 개념이다. <code class="language-plaintext highlighter-rouge">attention</code>은 <code class="language-plaintext highlighter-rouge">seq2seq</code> 구조에서 처음 나왔는데, <code class="language-plaintext highlighter-rouge">seq2seq</code> 은 번역 성능을 높이는 것을 목적으로 고안된 구조라서, 목표인 디코더의 <code class="language-plaintext highlighter-rouge">hidden_states</code> 값을 쿼리로, 인코더의 <code class="language-plaintext highlighter-rouge">hidden_states</code>를 키, 벨류의 출처로 사용했다. 즉, 서로 다른 출처에서 나온 <code class="language-plaintext highlighter-rouge">hidden_states</code> 을 사용해 내적 연산을 수행했던 것이다. 이런 개념에 이제 <code class="language-plaintext highlighter-rouge">“self"</code> 라는 이름이 붙었다. 결국 같은 출처에서 나온 <code class="language-plaintext highlighter-rouge">hidden_states</code> 를 내적하겠다는 의미를 내포하고 있는 것이다. 내적은 두 벡터의 <code class="language-plaintext highlighter-rouge">“닮은 정도”</code> 를 수학적으로 계산한다. 따라서 <code class="language-plaintext highlighter-rouge">self-attention</code> 이란 간단하게, 같은 출처에서 만들어진 $Q$(쿼리), $K$(키), $V$(벨류)가 <code class="language-plaintext highlighter-rouge">서로 얼마나 닮았는지</code> 계산해보겠다는 것이다.</p>

<p align="center">
<img src="/assets/images/transformer/linear_projection.png" alt="self-attention with linear projection" class="align-center image-caption" width="50%&quot;, height=&quot;50%" />
<strong><em><a href="https://jalammar.github.io/illustrated-transformer/">self-attention with linear projection</a></em></strong>
</p>

<p>그렇다면 이제 $Q$(쿼리), $K$(키), $V$(벨류)의 정체, 같은 출처에서 나왔다는 말의 의미 그리고 입력 행렬 $X$를 <code class="language-plaintext highlighter-rouge">linear projection</code> 하여 $Q$(쿼리), $K$(키), $V$(벨류) 행렬을 만드는 이유를 <strong>구체적인 예시를 통해 이해해보자.</strong> 추가로 $Q$(쿼리), $K$(키), $V$(벨류) 개념은 <code class="language-plaintext highlighter-rouge">Information Retrieval</code>에서 먼저 파생된 개념이라서 예시 역시 정보 검색과 관련된 것으로 준비했다.</p>

<p>당신이 만약 <code class="language-plaintext highlighter-rouge">“에어컨 필터 청소하는 방법”</code>이 궁금해 구글에 검색하는 상황이라고 가정해보겠다. <strong>목표는 가장 빠르고 정확하게 내가 원하는 필터 청소 방법에 대한 지식을 획득하는 것이다.</strong> <strong><code class="language-plaintext highlighter-rouge">그렇다면 당신은 뭐라고 구글 검색창에 검색할 것인가??</code></strong> <strong>이것이 바로</strong> $Q$<strong>(쿼리)에 해당한다.</strong> 당신은 검색창에 <code class="language-plaintext highlighter-rouge">“에어컨 필터 청소하는 방법”</code>을 입력해 검색 결과를 반환 받았다. <strong>반환 받은 결과물의 집합이 바로</strong> $K$<strong>(키)가 된다.</strong> 당신은 총 100개의 블로그 게시물을 키 값으로 받았다. 그래서 당신이 사용하는 삼성 무풍 에어컨의 필터 청소법이 정확히 적힌 게시물을 찾기 위해 하나 하나 링크를 타고 들어가 보았다. 하지만 정확하게 원하는 정보가 없어서 계속 찾다보니 결국 4페이지 쯤에서 원하던 정보가 담긴 게시물을 찾을 수 있었다. <strong>이렇게 내가 원하는 정보인지 아닌지 대조하는 과정이 바로</strong> $Q$<strong>(쿼리)와</strong> $K$<strong>(키) 행렬을</strong> <code class="language-plaintext highlighter-rouge">내적</code><strong>하는 행위가 된다.</strong> 곧바로 에어컨 청소를 하려고 보니, 방법을 까먹어서 매년 여름마다 검색을 해야할 것 같아 해당 게시물을 북마크에 저장해두었다. <strong>여기서 북마크가 바로</strong> $V$<strong>(벨류) 행렬이 된다.</strong></p>

<p>이 모든 과정에 10분이 걸렸다. 겨우 필터 청소 방법을 찾는데 10분이라니 당신은 자존심이 상했다. <code class="language-plaintext highlighter-rouge">더 빨리 원하는 정보(손실 함수 최적화)</code>를 찾을 수 있는 방법이 없을까 고민해보다가 <code class="language-plaintext highlighter-rouge">당신이 사용하는 에어컨 브랜드명(삼성 Bespoke 에어컨)을 검색어에 추가하기로 했다</code>. 그랬더니 1페이지 최하단에서 아까 4페이지에서 찾은 정보를 곧바로 찾을 수 있었다. 그 덕분에 시간을 <code class="language-plaintext highlighter-rouge">10분</code>에서 <code class="language-plaintext highlighter-rouge">1분 30초</code>로 단축시킬 수 있었다. <strong>이렇게 검색 시간을 단축(손실 줄이기)하기 위해 더 나은 검색 표현을 고민하고 수정하는 행위가 바로 입력</strong> $X$에 $W_{Q}$<strong>를 곱해 행렬</strong> $Q$ <strong>을 만드는 수식으로 표현된다.</strong></p>

<p>1년 뒤 여름, 당신은 브라우저를 바꾼 탓에 북마크가 초기화 되어 다시 한 번 검색을 해야 했다. 하지만 여전히 검색어는 기억하고 있어서, 1년전 최적의 결과를 얻었던 그대로 다시 검색을 했다. 분명 똑같이 검색을 했는데 같은 결과가 1페이지 최상단에서 반환되고 있었다. 당신은 이게 어떻게 된 일인지 궁금해 포스트를 천천히 보던 중, 제목에 1년전에는 없던 <code class="language-plaintext highlighter-rouge">삼성 Bespoke 에어컨</code> 이라는 키워드가 포함 되어 있었다. 게시물의 주인장이 <code class="language-plaintext highlighter-rouge">SEO 최적화</code>를 위해 추가했던 것이었다. 덕분에 당신은 소요 시간을 <code class="language-plaintext highlighter-rouge">1분 30초</code>에서 <code class="language-plaintext highlighter-rouge">20초</code>로 줄일 수 있었다. <strong>이런 상황이 바로 입력</strong> $X$에 $W_{K}$<strong>를 곱해 행렬</strong> $K$ <strong>를 만드는 수식에 대응된다.</strong></p>

<p>우리는 위 예시를 통해 원하는 정보를 빠르고 정확하게 찾는 행위란, 답변자가 이해하기 좋은 질문과 질문자의 질문 의도에 부합하는 좋은 답변으로 완성된다는 것을 알 수 있었다. 뿐만 아니라, 좋은 질문과 좋은 답변이라는 것은 처음부터 완성되는게 아니라 <strong>검색 시간을 단축하려는 끊임없는 노력</strong>을 통해 성취된다는 것 역시 깨우쳤다. 두가지 인사이트가 바로 <code class="language-plaintext highlighter-rouge">linear projection</code>으로 행렬 $Q, K,V$을 정의한 이유다. <strong>내가 원하는 정보인지 아닌지 대조하는 내적 연산은 수행하는데 가중치 행렬이 필요 없기 때문에 손실함수의 오차 역전을 활용한 수치 최적화를 수행할 수 없다.</strong> 그래서 손실함수 미분에 의한 최적화가 가능하도록  <code class="language-plaintext highlighter-rouge">linear projection matrix</code>를 활용해 행렬 $Q, K,V$를 정의해준 것이다. <strong>이렇게 하면 모델이 우리의 목적에 가장 적합한 질문과 답변을 알아서 표현 해줄 것이라 기대할 수 있게 된다.</strong> 한편, 같은 출처에서 나왔다는 말은 방금 예시에서 행렬 $Q, K,V$를 만드는데 동일하게 입력 $X$를 사용 것과 같은 상황을 의미한다.</p>

<p>이제 다시 자연어 처리 맥락으로 돌아와보자. <code class="language-plaintext highlighter-rouge">Transformer</code> 는 좋은 번역기를 만들기 위해 고안된 <code class="language-plaintext highlighter-rouge">seq2seq</code> 구조의 모델이다. 즉, 빠르고 정확하게 대상 언어에서 타겟 언어로 번역하는 것에 목표를 두고 만들어졌다는 것이다. 번역을 잘하기 위해서는 어떻게 해야 할까?? <strong>1) 대상 언어로 쓰인 시퀀스의 의미를 정확하게 파악해야 하고, 2) 파악한 의미와 가장 유사한 시퀀스를 타겟 언어로 만들어 내야 한다.</strong> <code class="language-plaintext highlighter-rouge">그래서 1번의 역할은 Encoder가 그리고 2번은 Decoder가 맡게 된다</code>. 인코더는 결국 (번역하는데 적합한 형태로) 대상 언어 시퀀스의 의미를 정확히 이해하는 방향(숫자로 표현, 임베딩 추출)으로 학습을 수행하게 되며, 디코더는 인코더의 학습 결과와 가장 유사한 문장을 타겟 언어로 생성해내는 과정을 배우게 된다. 따라서 인코더는 대상 언어를 출처로, 디코더는 타겟 언어를 출처로 행렬 $Q, K,V$를 만든다. 정확히 <code class="language-plaintext highlighter-rouge">self</code> 라는 단어를 이름에 갖다 붙인 의도와 일맥상통하는 모습이다.</p>

<p><strong>결국</strong> <code class="language-plaintext highlighter-rouge">Transformer</code> <strong>의 성능을 좌지우지 하는 것은 누가 얼마나 더</strong> <code class="language-plaintext highlighter-rouge">linear projection weight</code><strong>을 잘 최적화 하는가에 달렸다고 볼 수 있다.</strong></p>

<p><strong>한편 필자는 처음 이 논문을 읽었을 때</strong> <code class="language-plaintext highlighter-rouge">linear projection</code> <strong>자체의 필요성은 공감했으나, 굳이 3개의 행렬로 나눠서</strong> <code class="language-plaintext highlighter-rouge">train</code> <strong>시켜야 하는</strong> <code class="language-plaintext highlighter-rouge">param</code> <strong>숫자를 늘리는 것보다는</strong> <code class="language-plaintext highlighter-rouge">weight share</code> <strong>하는 형태로 만드는게 더 효율적일 것 같다는 추측을 했었다.</strong></p>

<p>그러나 이번 리뷰를 위해 다시 논문을 읽던 중, 좋은 질문을 하기 위한 노력과 좋은 답변을 하기 위한 노력, 그리고 필요한 정보를 정확히 추출해내는 행위를 각각 서로 다른 3개의 벡터로 표현했을 때 <strong>벡터들이 가지는 방향성이 서로 다를텐데</strong> 그것을 하나의 벡터로 표현하려면 모델이 학습을 하기 힘들 것 같다는 생각이 들었다. 방금 위에서 든 예시만 봐도 그렇다. 서로 다른 3개의 행위 사이의 최적 지점을 찾으라는 것과 마찬가진데 그런 스팟이 있다고 해도 언어 모델이 잘 찾을 수 있을까?? 인간도 찾기 힘든 것을 모델이 잘 찾을리가 없다.</p>

<h4 id="scaled-dot-product-attention"><strong><code class="language-plaintext highlighter-rouge">📐 Scaled Dot-Product Attention</code></strong></h4>

\[Attention(Q,K,V) = softmax(\frac{Q·K^T}{\sqrt{d_k}})V\]

<p>이번에는 <code class="language-plaintext highlighter-rouge">Self-Attention</code> 의 두 번째 하위 블럭인 <code class="language-plaintext highlighter-rouge">Scaled Dot-Product Attention</code> 차례다. 사실 우리는 <code class="language-plaintext highlighter-rouge">Linear Projection</code> 파트에서 이미 우리도 모르게 <code class="language-plaintext highlighter-rouge">Scaled Dot-Product Attention</code> 에 대해 공부했다. 예시를 다시 한 번 상기시켜보자. 질의를 통해 얻은 결과 리스트(키)에서 내가 원하는 정보를 찾기 위해 쿼리와 키를 대조한다고 했던 것 기억나는가?? 바로 그 대조하는 행위를 수학적으로 모델링한 것이 바로 <code class="language-plaintext highlighter-rouge">Scaled Dot-Product Attention</code> 에 해당한다.</p>

<p align="center">
<img src="/assets/images/transformer/dot_attention.png" alt="Attention is All You Need" class="align-center image-caption" width="50%&quot;, height=&quot;50%" />
<strong><em><a href="https://arxiv.org/abs/1706.03762">Attention is All You Need</a></em></strong>
</p>

<p><code class="language-plaintext highlighter-rouge">Scaled Dot-Product Attention</code> 은 총 5단계를 거쳐 완성된다. 단계마다 어떤 연산을 왜 하는지 그리고 무슨 인사이트가 담겨 있는지 알아보자. 이 중에서 마스킹 단계는 인코더와 디코더의 동작을 자세히 알아야하기 때문에 전체적인 구조 관점에서 모델을 바라볼 때 함께 설명하도록 하겠다.</p>

<p><strong><code class="language-plaintext highlighter-rouge">✖️ Stage 1. Q•K^T Dot-Product</code></strong></p>

\[Q•K^T\]

<p>인간은 문장이나 어떤 표현의 의미를 파악하는데 바로 주변 맥락을 참고하거나, 더 멀리 떨어진 곳의 단어•시퀀스를 이용하기도 한다. <strong>즉, 주어진 시퀀스 내부의 모든 맥락을 이용해 특정 부분의 의미를 이해한다는 것이다.</strong> 그렇다고 모든 정보가 동일하게 특정 표현의 의미에 영향을 미치는 것은 또 아닌데, 수능 영어에 킬러 문항으로 등장하는 빈칸 채우기 문제를 어떻게 풀었나 떠올려보자. 디테일한 풀이 방식에는 사람마다 차이가 있겠지만, 일반적으로 지문은 모두 훑어 보되 빈칸에 들어갈 정답의 근거가 되는 특정 문장 혹은 표현 1~2개를 찾아내어 비슷한 의미를 지닌 선지를 골라 내는 방식을 사용한다. <strong>다시 말해, 주어진 전체 단락에서 의미를 이해하는데 중요한 역할을 하는 표현이나 문장을 골라내어 <code class="language-plaintext highlighter-rouge">중요도</code> 만큼 <code class="language-plaintext highlighter-rouge">가중치</code> 를 주겠다는 것이다.</strong></p>

<p align="center">
<img src="/assets/images/transformer/attention_visualization.png" alt="Q•K^T Dot Product Visualization" class="align-center image-caption" width="50%&quot;, height=&quot;50%" />
<strong><em><a href="https://jalammar.github.io/illustrated-transformer/">Q•K^T Dot Product Visualization</a></em></strong>
</p>

<p>그렇다면 이것을 어떻게 수학적으로 모델링했을까?? 바로 행렬 $Q$와 $K^T$의 <code class="language-plaintext highlighter-rouge">내적</code>을 활용한다. 행렬 $Q$는 모델이 의미를 파악해야 하는 대상이 담겨 있고, 행렬 $K$에는 의미 파악에 필요한 단서들이 담겨있다. 내적은 두 벡터의 서로 <code class="language-plaintext highlighter-rouge">“닮은 정도”</code> 를 의미한다고 했다. <code class="language-plaintext highlighter-rouge">“닮은 정도”</code> 가 바로 <code class="language-plaintext highlighter-rouge">중요도•가중치</code>에 대응된다. 따라서 연산 결과는 전체 시퀀스에 속한 토큰들 사이의 <code class="language-plaintext highlighter-rouge">“닮은 정도”</code> 가 수치로 변환되어 행렬에 담긴다.</p>

<p>왜 <code class="language-plaintext highlighter-rouge">내적 결과</code>가 <code class="language-plaintext highlighter-rouge">중요도</code>와 같은 의미를 갖게 되는 것일까?? 아까 <code class="language-plaintext highlighter-rouge">Input Embedding</code>과 <code class="language-plaintext highlighter-rouge">Position Embedding</code>을 행렬합 하는 것에 대한 당위성을 설명하면서 고차원으로 갈수록 대부분의 벡터 쌍은 <code class="language-plaintext highlighter-rouge">직교성</code>을 갖게 된다고 언급한 바 있다. 그래서 두 벡터가 비슷한 방향성을 갖는다는 것 자체가 매우 드문일이다. 희귀하고 드문 사건은 그만큼 중요하다고 말할 수 있기 때문에 <code class="language-plaintext highlighter-rouge">내적 결과</code>를 <code class="language-plaintext highlighter-rouge">중요도</code>에 맵핑하는 것이다.</p>

<p>한편, 행렬 $Q,K$ 모두 차원이 <code class="language-plaintext highlighter-rouge">[Batch, Max_Seq, Dim_Head]</code> 인 텐서라서 내적한 결과의 모양은 <code class="language-plaintext highlighter-rouge">[Batch, Max_Seq, Max_Seq]</code> 이 될 것이다.</p>

<p><strong><code class="language-plaintext highlighter-rouge">🔭 Stage 2. Scale</code></strong></p>

\[Q•K^T = \begin{bmatrix}
56.8 &amp; 12.1 &amp; 43.5 \\
30.4 &amp; 100.8 &amp; 24.2 \\
11.11 &amp; 7.34 &amp; 20.23 \\
\end{bmatrix}\]

<p><code class="language-plaintext highlighter-rouge">“I am dog”</code> 라는 문장을 $Q•K^T$하면 위와 같은 <code class="language-plaintext highlighter-rouge">3x3</code> 짜리 행렬이 나올 것이다. 행렬을 행벡터로 바라보자. <strong>행 사이의 값의 분포가 고르지 못하다는 것을 알 수 있다.</strong> 이렇게 분산이 큰 상태로 <code class="language-plaintext highlighter-rouge">softmax</code> 에 통과시키게 되면 역전파 과정에서 <code class="language-plaintext highlighter-rouge">softmax</code> 의 미분값이 줄어 들어 학습 속도가 느려지고 나아가 <code class="language-plaintext highlighter-rouge">vanishing gradient</code> 현상이 발생할 수 있다. 따라서 행벡터 사이의 분산을 줄여주기 위해서 <code class="language-plaintext highlighter-rouge">Scale Factor</code> 를 정의하게 된다. 그렇다면 어떤 <code class="language-plaintext highlighter-rouge">Scale Factor</code> 를 써야할까??</p>

\[\frac{Q•K^T}{\sqrt{d_h}}\]

<p>애초에 <code class="language-plaintext highlighter-rouge">Dim Head</code> 차원에 속한 값들의 분산이 큰 것도 문제가 되지만 이것은 <code class="language-plaintext highlighter-rouge">Input Embedding</code>이나 <code class="language-plaintext highlighter-rouge">Position Embedding</code>에 <code class="language-plaintext highlighter-rouge">layernorm</code> 을 적용하면 해결할 수 있기 때문에 논의 대상이 아니다. 그것보다는 내적 과정에 주목해보자. 우리는 내적을 하다보면 <code class="language-plaintext highlighter-rouge">Dim Head</code>의 차원이 커질수록 더해줘야 하는 스칼라 값의 개수가 늘어나게 된다는 사실을 알 수 있다. 만약 위에서 예시로 든 수식의 <code class="language-plaintext highlighter-rouge">Dim Head</code>가 64라고 가정해보자. 그럼 우리는 1행 1열의 값을 얻기 위해 64개의 스칼라 값을 더해줘야 한다. 만약 <code class="language-plaintext highlighter-rouge">512</code>차원이라면 <code class="language-plaintext highlighter-rouge">512</code>개로 불어난다. <strong>더해줘야 하는 스칼라 값이 많아진다면 행벡터 끼리의 분산이 커질 우려가 있다.</strong> 따라서 차원 크기의 스케일에 따라 <code class="language-plaintext highlighter-rouge">softmax</code>의 미분값이 줄어드는 것을 방지하기 위해 $Q•K^T$결과에 $\sqrt{d_h}$를 나눠 준다.</p>

<p>여담으로 이러한 <code class="language-plaintext highlighter-rouge">scale factor</code> 의 존재 때문에 <code class="language-plaintext highlighter-rouge">Self-Attention</code>을 <code class="language-plaintext highlighter-rouge">Scaled Dot-Product Attention</code> 이라고 부르기도 한다.</p>

<p><strong><code class="language-plaintext highlighter-rouge">🎭 Stage 3. masking</code></strong><br />
마스킹은 인코더 <code class="language-plaintext highlighter-rouge">Input Padding</code>, 디코더 <code class="language-plaintext highlighter-rouge">Masked Multi-Head Attention</code>, 인코더-디코더 <code class="language-plaintext highlighter-rouge">Self-Attention</code> 을 위해 필요한 계층이다. 뒤에 두개는 디코더의 동작을 알아야 이해가 가능하기 때문에 여기서는 인코더의 마스킹에 대해서만 알아보자.</p>

<p align="center">
<img src="/assets/images/transformer/encoder_mask.png" alt="Encoder Padding Mask" class="align-center image-caption" width="50%&quot;, height=&quot;50%" />
<strong><em><a href="https://paul-hyun.github.io/transformer-02/">Encoder Padding Mask</a></em></strong>
</p>

<p>실제 텍스트 데이터는 배치된 시퀀스마다 그 길이가 제각각이다. 효율성을 위해 행렬을 사용하는 컴퓨터 연산 특성상 배치된 시퀀스의 길이가 모두 다르다면 연산을 진행할 수가 없다. 따라서 배치 내부의 모든 시퀀스의 길이를 통일해주는 작업을 하게 되는데, 이 때 기준 길이보다 짧은 시퀀스에 대해서는 <code class="language-plaintext highlighter-rouge">0</code>값을 채워넣는 <code class="language-plaintext highlighter-rouge">padding</code> 작업을 한다.  행렬 연산에는 꼭 필요했던 <code class="language-plaintext highlighter-rouge">padding</code>은 오히려 <code class="language-plaintext highlighter-rouge">softmax</code> 레이어를 계산할 때 방해가 된다. 따라서 모든 <code class="language-plaintext highlighter-rouge">padding</code> 값을 <code class="language-plaintext highlighter-rouge">softmax</code>의 확률 계산에서 완전히 제외시키기 위해 <code class="language-plaintext highlighter-rouge">Input Embedding</code>에서 <code class="language-plaintext highlighter-rouge">padding token</code>의 인덱스를 저장하고 해당되는 모든 원소를 <code class="language-plaintext highlighter-rouge">-∞</code> 로 마스킹하는 과정이 필요하다.</p>

<p>이 때 마스킹 처리는 열벡터에만 적용한다. 그 이유는 바로 <code class="language-plaintext highlighter-rouge">softmax</code> 계산을 어차피 행벡터 방향으로만 할 것이기 때문이다. 행벡터 방향의 <code class="language-plaintext highlighter-rouge">padding token</code>에도 동일하게 마스킹 적용하는 것은 상관 없으나 열벡터와 행벡터 동시에 마스킹 적용하는 동작을 구현하는 것은 생각보다 많이 까다로우며, 나중에 손실값 계산하는 단계에서 <code class="language-plaintext highlighter-rouge">ignore_index</code> 옵션을 사용해 행벡터의 <code class="language-plaintext highlighter-rouge">padding token</code>을 무시하는 것이 훨씬 효율적이다. 한편, <code class="language-plaintext highlighter-rouge">ignore_index</code> 옵션은 <code class="language-plaintext highlighter-rouge">nn.CrossEntropyLoss</code> 에 매개변수로 구현 되어 있다.</p>

<p><strong><code class="language-plaintext highlighter-rouge">📈 Stage 4. Softmax &amp; Score•V</code></strong></p>

\[Score = \begin{bmatrix}
  0.90 &amp; 0.07 &amp; 0.03 \\
  0.025 &amp; 0.95 &amp; 0.025 \\
  0.21 &amp; 0.03 &amp; 0.76 
\end{bmatrix}, \ \  V=\begin{bmatrix}
  67.85 &amp; 90 &amp; 91 &amp; ..... \\
  62 &amp; 40 &amp; 50 &amp; ..... \\
  37 &amp; 41 &amp; 20 &amp; .....
\end{bmatrix},\ \  Z = score \ • \ V\]

\[{\overset{}{z_{1}^{}}} = {\overset{}{Score_{11}^{}}}({\overset{}{V_{11}^{}}}\ + \ {\overset{}{V_{12}^{}}}\ + \ ...) \ + \ {\overset{}{Score_{12}^{}}}({\overset{}{V_{21}^{}}}\ + \ {\overset{}{V_{22}^{}}}\ + \ ...)\ + \ ....... \\
{\overset{}{z_{2}^{}}} = {\overset{}{Score_{21}^{}}}({\overset{}{V_{11}^{}}}\ + \ {\overset{}{V_{12}^{}}}\ + \ ...) \ + \ {\overset{}{Score_{22}^{}}}({\overset{}{V_{21}^{}}}\ + \ {\overset{}{V_{22}^{}}}\ + \ ...)\ + \ ....... \\
{\overset{}{z_{3}^{}}} = {\overset{}{Score_{31}^{}}}({\overset{}{V_{11}^{}}}\ + \ {\overset{}{V_{12}^{}}}\ + \ ...) \ + \ {\overset{}{Score_{32}^{}}}({\overset{}{V_{21}^{}}}\ + \ {\overset{}{V_{22}^{}}}\ + \ ...)\ + \ ....... \\\]

<p>계산된 <code class="language-plaintext highlighter-rouge">유사도(내적 결과, 중요도, 가중치)</code>, $\frac{Q•K^T}{\sqrt{d_h}}$는 이후에 행렬 $V$와 다시 곱해져 행벡터 $Z_n$(n번째 토큰)에서 토큰에 대한 어텐션 정도를 나타내는 <code class="language-plaintext highlighter-rouge">가중치</code>의 역할을 하게 된다. 그러나 계산된 유사도는 비정규화된 형태다. 수식에는 편의상 이미 <code class="language-plaintext highlighter-rouge">softmax</code>를 적용한 형태의 행렬을 적었지만, 실제로는 원소값의 분산이 너무 커서 가중치로는 쓰기 힘든 수준이다. 따라서 행벡터 단위로 <code class="language-plaintext highlighter-rouge">softmax</code>에 통과시켜 결과의 합이 1인 확률값으로 <code class="language-plaintext highlighter-rouge">변환(정규화)</code>해 행렬 $V$의 가중치로 사용한다.</p>

<p>이제 두번째 수식을 보자. $Score_{11}$에 해당하는 <code class="language-plaintext highlighter-rouge">0.90</code>가 행렬 $V$의 첫번째 행벡터와 곱해지고 있다. 행렬 $V$의 첫번째 행벡터는 토큰 <code class="language-plaintext highlighter-rouge">“I”</code> 를 <code class="language-plaintext highlighter-rouge">512</code>차원으로 표현한 것이다. 그 다음 $Score_{12}$는 행렬 $V$의 두번째 행벡터와, $Score_{13}$은 행렬 $V$의 세번째 행벡터와 각각 곱해진다.</p>

<p>이 행위의 의미는 무엇일까?? $Score_{11}$, $Score_{12}$, $Score_{13}$은 모두 첫번째 토큰인 <code class="language-plaintext highlighter-rouge">“I”</code>에 의미를 파악하는데 <code class="language-plaintext highlighter-rouge">“I”</code>, <code class="language-plaintext highlighter-rouge">“am”</code>, <code class="language-plaintext highlighter-rouge">“dog”</code>를 어느 정도로 어텐션해야 하는지, 즉 <code class="language-plaintext highlighter-rouge">“I”</code>의 의미를 표현하는데 세 토큰의 의미를 어느 정도 반영할지 수치로 표현한 것이다. 당연히 자기 자신인 <code class="language-plaintext highlighter-rouge">“I”</code>와 <code class="language-plaintext highlighter-rouge">가중치(유사도, 중요도)</code>가 가장 높기 때문에 행렬 $V$에서 <code class="language-plaintext highlighter-rouge">“I”</code> 에 해당하는 행벡터 가중치에 가장 큰 값이 들어간다고 생각해볼 수 있다. 이렇게 각 토큰마다 가중합을 반복해주면 최종적으로 <code class="language-plaintext highlighter-rouge">“I”</code>, <code class="language-plaintext highlighter-rouge">“am”</code>, <code class="language-plaintext highlighter-rouge">“dog”</code> 을 인코딩한 $Z_1, \ Z_2, \  Z_3$ 값을 얻을 수 있다.</p>

<p><strong><code class="language-plaintext highlighter-rouge">👩‍💻 Implementation</code></strong></p>

<p>이렇게 <code class="language-plaintext highlighter-rouge">Scaled Dot-Product Attention</code> 을 모두 살펴보았다. 해당 레이어는 모델이 손실값이 가장 작아지는 방향으로 최적화한 행렬 $Q, K, V$ 을 이용해, 토큰의 의미를 이해하는데 어떤 맥락과 표현에 좀 더 집중하고 덜 집중해야 하는지를 유사도를 기준으로 판단한다는 것을 꼭 기억하자. 그렇다면 실제 코드는 어떻게 작성 해야하는지 함께 알아보자. 상단의 <code class="language-plaintext highlighter-rouge">class diagram</code> 을 다시 한 번 보고 돌아오자.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Pytorch Implementation of Scaled Dot-Product Self-Attention
</span>
<span class="k">def</span> <span class="nf">scaled_dot_product_attention</span><span class="p">(</span><span class="n">q</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">k</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">v</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">dot_scale</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">mask</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="bp">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
    <span class="s">"""
    Scaled Dot-Product Attention with Masking for Decoder
    Args:
        q: query matrix, shape (batch_size, seq_len, dim_head)
        k: key matrix, shape (batch_size, seq_len, dim_head)
        v: value matrix, shape (batch_size, seq_len, dim_head)
        dot_scale: scale factor for Q•K^T result
        mask: there are three types of mask, mask matrix shape must be same as single attention head
              1) Encoder padded token
              2) Decoder Masked-Self-Attention
              3) Decoder's Encoder-Decoder Attention
    Math:
        A = softmax(q•k^t/sqrt(D_h)), SA(z) = Av
    """</span>
    <span class="n">attention_matrix</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">))</span> <span class="o">/</span> <span class="n">dot_scale</span>
    <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">attention_matrix</span> <span class="o">=</span> <span class="n">attention_matrix</span><span class="p">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">mask</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="s">'-inf'</span><span class="p">))</span>
    <span class="n">attention_dist</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">attention_matrix</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">attention_matrix</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">attention_dist</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">attention_matrix</span>
</code></pre></div></div>

<p>마스킹 옵션의 경우 주석에 정리된 3가지 상황 중에서 한 개 이상에 해당되면 실행되도록 코드를 작성했다. 3가지  상황과 구체적인 마스킹 방법에 대해서는 전체 모델 구조를 보는 때 소개하도록 하겠다.</p>

<p>한편, 인코더나 디코더나 모두 사용하는 입력과 마스킹 방식에 차이는 있지만, <code class="language-plaintext highlighter-rouge">Scaled Dot-Product Attention</code> 연산 자체는 동일한 것을 사용한다. 따라서 여러개의 인코더나 디코더 객체들 혹은 어텐션 해드 객체들이 모두 쉽게 연산에 접근할 수 있게 클래스 외부에 메서드 형태로 구현하게 되었다.</p>

<h4 id="multi-head-attention-block"><strong><code class="language-plaintext highlighter-rouge">👩‍👩‍👧‍👦 Multi-Head Attention Block</code></strong></h4>

<p>지금까지 살펴본 <code class="language-plaintext highlighter-rouge">Self-Attention</code>의 동작은 모두 한 개의 <code class="language-plaintext highlighter-rouge">Attention-Head</code>에서 일어나는 일을 서술한 것이다. 사실 실제 모델에서는 같은 동작이 <code class="language-plaintext highlighter-rouge">N-1</code>개의 다른 해드에서 동시에 일어나는데, 이것이 바로 <code class="language-plaintext highlighter-rouge">Multi-Head Attention</code>이다.</p>

<p><code class="language-plaintext highlighter-rouge">Official Paper</code> 기준으로 <code class="language-plaintext highlighter-rouge">Transformer-base</code>의 <code class="language-plaintext highlighter-rouge">hidden states</code> 차원은 <code class="language-plaintext highlighter-rouge">512</code>이다. 이것을 개당 <code class="language-plaintext highlighter-rouge">64</code>차원을 갖는 <code class="language-plaintext highlighter-rouge">8</code>개의 <code class="language-plaintext highlighter-rouge">Attention-Head</code> 로 쪼갠 뒤, 8개의 <code class="language-plaintext highlighter-rouge">Attention-Head</code> 에서 동시에 <code class="language-plaintext highlighter-rouge">Self-Attention</code> 을 수행한다. 이후 결과를 <code class="language-plaintext highlighter-rouge">concat</code>하여 다시 <code class="language-plaintext highlighter-rouge">hidden states</code> 를 <code class="language-plaintext highlighter-rouge">512</code> 로 만든 뒤, 여러 해드에서 만든 결과를 연결하고 섞어주기 위해 입출력 차원이 <code class="language-plaintext highlighter-rouge">hidden states</code>와 동일한 <code class="language-plaintext highlighter-rouge">linear projection layer</code>에 통과시킨다. 이것이 인코더(혹은 디코더) 블럭 한 개의 최종 <code class="language-plaintext highlighter-rouge">Self-Attention</code> 결과가 된다.</p>

<p align="center">
<img src="/assets/images/transformer/multi_head_result.png" alt="Multi-Head Attention Result Visualization" class="align-center image-caption" width="50%&quot;, height=&quot;50%" />
<strong><em><a href="https://arxiv.org/abs/1706.03762">Multi-Head Attention Result Visualization</a></em></strong>
</p>

<p><strong>그럼 왜 이렇게 여러 해드를 사용했을까?? 바로 집단지성의 효과를 누리기 위함이다.</strong> 생각해보자. 책 하나를 읽어도 사람마다 정말 다양한 해석이 나온다. 모델도 마찬가지다. 여러 해드를 사용해서 좀 더 다양하고 풍부한 의미를 임베딩에 담고 싶었던 것이다. Kaggle을 해보신 독자라면, 여러 전략을 사용해 여러 개의 결과를 도출한 뒤, 마지막에 모두 앙상블하면 전략 하나 하나의 결과보다 더 높은 성적을 얻어본 경험이 있을 것이다. 이것도 비슷한 효과를 의도했다고 생각한다. Vision에서 Conv Filter를 여러 종류 사용해 다양한 Feature Map을 추출하는 것도 비슷한 현상이라 볼 수 있겠다.</p>

<p>위 그림은 저자가 제시한 <code class="language-plaintext highlighter-rouge">Multi-Head Attention</code>의 시각화 결과다. 중간에 있는 여러 색깔의 띠는 개별 해드가 어텐션하는 방향을 가리킨다. 토큰 <code class="language-plaintext highlighter-rouge">“making”</code> 에 대해서 해드들이 서로 다른 토큰에 어텐션하고 있다.</p>

<p align="center">
<img src="/assets/images/transformer/vit_multi_head_result.png" alt="ViT Multi-Head Attention Result Visualization" class="align-center image-caption" width="50%&quot;, height=&quot;50%" />
<strong><em><a href="https://arxiv.org/abs/2010.11929">ViT Multi-Head Attention Result Visualization</a></em></strong>
</p>

<p>위 그림은 Vision Transformer 논문에서 발췌한 그림<a href="https://qcqced123.github.io/cv/vit">(그림의 자세한 의미는 여기서)</a>이다. 역시 마찬가지로 모델의 초반부 인코더에 속한 Multi-Head들이 서로 다양한 토큰에 어텐션을 하고 있음을 알 수 있다. 추가로 후반으로 갈수록 점점 <code class="language-plaintext highlighter-rouge">Attention Distance</code> 가 일정한 수준에 수렴하는 모습을 볼 수 있는데, 이것을 레이어를 통과할수록 개별 해드가 자신이 어떤 토큰에 주의를 기울여야할지 구체적으로 알아가는 과정이라고 해석할 수 있다. 초반부에는 어찌할 바를 몰라서 이토큰 저토큰에 죄다 어텐션하는 것이다.</p>

<p>그래서 <code class="language-plaintext highlighter-rouge">Transformer</code>는 <code class="language-plaintext highlighter-rouge">Bottom Layer</code>에서는 <code class="language-plaintext highlighter-rouge">Global</code>하고 <code class="language-plaintext highlighter-rouge">General</code>한 정보를 포착하고, <code class="language-plaintext highlighter-rouge">Output</code>과 가까운 <code class="language-plaintext highlighter-rouge">Top Layer</code>에서는 <code class="language-plaintext highlighter-rouge">Local</code>하고 <code class="language-plaintext highlighter-rouge">Specific</code>한 정보를 포착한다.</p>

<p><strong><code class="language-plaintext highlighter-rouge">👩‍💻 Implementation</code></strong></p>

<p>이제 구현을 실제로 구현을 해보자. 역시 구현은 파이토치로 진행했다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Pytorch Implemenation of Single Attention Head
</span>
<span class="k">class</span> <span class="nc">AttentionHead</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="s">"""
    In this class, we implement workflow of single attention head
    Args:
        dim_model: dimension of model's latent vector space, default 512 from official paper
        dim_head: dimension of each attention head, default 64 from official paper (512 / 8)
        dropout: dropout rate, default 0.1
    Math:
        [q,k,v]=z•U_qkv, A = softmax(q•k^t/sqrt(D_h)), SA(z) = Av
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim_model</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">512</span><span class="p">,</span> <span class="n">dim_head</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">64</span><span class="p">,</span> <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">AttentionHead</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span> <span class="o">=</span> <span class="n">dim_model</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dim_head</span> <span class="o">=</span> <span class="n">dim_head</span>  <span class="c1"># 512 / 8 = 64
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">dropout</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dot_scale</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">dim_head</span><span class="p">))</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">fc_q</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">dim_head</span><span class="p">)</span>  <span class="c1"># Linear Projection for Query Matrix
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">fc_k</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">dim_head</span><span class="p">)</span>  <span class="c1"># Linear Projection for Key Matrix
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">fc_v</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">dim_head</span><span class="p">)</span>  <span class="c1"># Linear Projection for Value Matrix
</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">mask</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">enc_output</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="bp">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">fc_q</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="bp">self</span><span class="p">.</span><span class="n">fc_k</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="bp">self</span><span class="p">.</span><span class="n">fc_v</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># x is previous layer's output
</span>        <span class="k">if</span> <span class="n">enc_output</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="s">""" For encoder-decoder self-attention """</span>
            <span class="n">k</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">fc_k</span><span class="p">(</span><span class="n">enc_output</span><span class="p">)</span>
            <span class="n">v</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">fc_v</span><span class="p">(</span><span class="n">enc_output</span><span class="p">)</span>
        <span class="n">attention_matrix</span> <span class="o">=</span> <span class="n">scaled_dot_product_attention</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">dot_scale</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="n">mask</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">attention_matrix</span>
</code></pre></div></div>

<p>똑같은 <code class="language-plaintext highlighter-rouge">Attention-Head</code>를 <code class="language-plaintext highlighter-rouge">N</code>개 사용하기 때문에 먼저 <code class="language-plaintext highlighter-rouge">Single Attention Head</code>의 동작을 따로 객체로 만들었다. 이렇게 하면 <code class="language-plaintext highlighter-rouge">MultiHeadAttention</code> 객체에서 <code class="language-plaintext highlighter-rouge">nn.ModuleList</code> 를 사용해 <code class="language-plaintext highlighter-rouge">N</code>개의 해드를 이어붙일 수 있어서 구현이 훨씬 간편해지기 때문이다. <code class="language-plaintext highlighter-rouge">Single Attention Head</code> 객체가 하는 일은 다음과 같다.</p>

<ul>
  <li><strong>1) Linear Projection by Dimension of Single Attention Head</strong></li>
  <li><strong>2) Maksing</strong></li>
  <li><strong>3) Scaled Dot-Product Attention</strong></li>
</ul>

<p>한편, 여러 <code class="language-plaintext highlighter-rouge">Transformer</code> 구현 Git Repo를 살펴보면 구현 방법은 크게 필자처럼 <code class="language-plaintext highlighter-rouge">Single Attention Head</code>를 추상화하거나 <code class="language-plaintext highlighter-rouge">MultiHeadAttention</code> 객체 하나에 모든 동작을 때려넣는 방식으로 나뉘는 것 같다. 사실 구현에 정답은 없지만 개인적으로 후자의 방식은 비효율적이라 생각한다. 저렇게 구현하면 <code class="language-plaintext highlighter-rouge">3*N</code>개의 <code class="language-plaintext highlighter-rouge">linear projector</code>를 클래스 <code class="language-plaintext highlighter-rouge">__init__</code> 에 만들고 관리해줘야 하는데 쉽지 않을 것이다. 물론 <code class="language-plaintext highlighter-rouge">3</code>개의 <code class="language-plaintext highlighter-rouge">linear projector</code> 만 초기화해서 사용하고 대신 출력 차원을 <code class="language-plaintext highlighter-rouge">Dim_Head</code>가 아닌 <code class="language-plaintext highlighter-rouge">Dim_Model</code>로 구현한 뒤, <code class="language-plaintext highlighter-rouge">N</code>개로 차원을 분할하는 방법도 있다. 하지만 차원을 쪼개는 동작을 구현하는 것도 사실 쉽지 않다. 그래서 필자는 전자의 방식을 추천한다.</p>

<p>한편, <code class="language-plaintext highlighter-rouge">forward</code> 메서드에  <code class="language-plaintext highlighter-rouge">if enc_output is not None:</code> 부분은 추후에 디코더에서 <code class="language-plaintext highlighter-rouge">Multi-Head Attention</code>을 구현하기 위해 추가한 코드다. 디코더는 인코더와 다르게 하나의 디코더 블럭에서 <code class="language-plaintext highlighter-rouge">Self-Attention</code>동작을 두번하는데, 두번째 동작은 서로 다른 출처의 값을 이용해 <code class="language-plaintext highlighter-rouge">linear projection</code>을 수행한다. 따라서 그 경우를 처리해주기 위해 구현하게 되었다.</p>

<p>아래는 <code class="language-plaintext highlighter-rouge">MultiHeadAttention</code> 을 구현한 파이토치 코드다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Pytorch Implemenation of Single Attention Head
</span>
<span class="k">class</span> <span class="nc">MultiHeadAttention</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="s">"""
    In this class, we implement workflow of Multi-Head Self-Attention
    Args:
        dim_model: dimension of model's latent vector space, default 512 from official paper
        num_heads: number of heads in MHSA, default 8 from official paper for Transformer
        dim_head: dimension of each attention head, default 64 from official paper (512 / 8)
        dropout: dropout rate, default 0.1
    Math:
        MSA(z) = [SA1(z); SA2(z); · · · ; SAk(z)]•Umsa
    Reference:
        https://arxiv.org/abs/1706.03762
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim_model</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">512</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span> <span class="n">dim_head</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">64</span><span class="p">,</span> <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MultiHeadAttention</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span> <span class="o">=</span> <span class="n">dim_model</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dim_head</span> <span class="o">=</span> <span class="n">dim_head</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">dropout</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">attention_heads</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">ModuleList</span><span class="p">(</span>
            <span class="p">[</span><span class="n">AttentionHead</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">dim_head</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">dropout</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">num_heads</span><span class="p">)]</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">fc_concat</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">mask</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">enc_output</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="bp">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="s">""" x is already passed nn.Layernorm """</span>
        <span class="k">assert</span> <span class="n">x</span><span class="p">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">3</span><span class="p">,</span> <span class="sa">f</span><span class="s">'Expected (batch, seq, hidden) got </span><span class="si">{</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="s">'</span>
        <span class="n">attention_output</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">fc_concat</span><span class="p">(</span>
            <span class="n">torch</span><span class="p">.</span><span class="n">cat</span><span class="p">([</span><span class="n">head</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">mask</span><span class="p">,</span> <span class="n">enc_output</span><span class="p">)</span> <span class="k">for</span> <span class="n">head</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">attention_heads</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">attention_output</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">MultiHeadAttention</code> 객체는 개별 해드들이 도출한 어텐션 결과를 <code class="language-plaintext highlighter-rouge">concat</code>하고 그것을 <code class="language-plaintext highlighter-rouge">connect &amp; mix</code>하려고 <code class="language-plaintext highlighter-rouge">linear projection</code>을 수행한다.</p>

<h4 id="-feed-forward-network"><strong><code class="language-plaintext highlighter-rouge">🔬 Feed Forward Network</code></strong></h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Pytorch Implementation of FeedForward Network
</span>
<span class="k">class</span> <span class="nc">FeedForward</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="s">"""
    Class for Feed-Forward Network module in transformer
    In official paper, they use ReLU activation function, but GELU is better for now
    We change ReLU to GELU &amp; add dropout layer
    Args:
        dim_model: dimension of model's latent vector space, default 512
        dim_ffn: dimension of FFN's hidden layer, default 2048 from official paper
        dropout: dropout rate, default 0.1
    Math:
        FeedForward(x) = FeedForward(LN(x))+x
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim_model</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">512</span><span class="p">,</span> <span class="n">dim_ffn</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2048</span><span class="p">,</span> <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">FeedForward</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">ffn</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">dim_model</span><span class="p">,</span> <span class="n">dim_ffn</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">GELU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">dropout</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">dim_ffn</span><span class="p">,</span> <span class="n">dim_model</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">dropout</span><span class="p">),</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">ffn</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</code></pre></div></div>

<p>피드 포워드는 모델에 <code class="language-plaintext highlighter-rouge">non-linearity</code>를 추가하기 위해 사용하는 레이어다. 원본 모델은 <code class="language-plaintext highlighter-rouge">ReLU</code> 를 사용하지만 최근 <code class="language-plaintext highlighter-rouge">Transformer</code>류 모델에는 <code class="language-plaintext highlighter-rouge">GeLU</code>를 사용하는 것이 좀 더 안정적인 학습을 하는데 도움이 된다고 밝혀져, 필자 역시 <code class="language-plaintext highlighter-rouge">GeLU</code>를 사용해 구현했다. 또한 논문에는 <code class="language-plaintext highlighter-rouge">dropout</code>에 대한 언급이 전혀 없는데, 은닉층의 차원을 저렇게 크게 키웠다 줄이는데 <code class="language-plaintext highlighter-rouge">overfitting</code> 이슈가 있을 것 같아서 <code class="language-plaintext highlighter-rouge">ViT</code> 논문을 참고해 따로 추가해줬다.</p>

<h4 id="add--norm"><strong><code class="language-plaintext highlighter-rouge">➕ Add &amp; Norm</code></strong></h4>

<p><code class="language-plaintext highlighter-rouge">Residual Connection</code>과 <code class="language-plaintext highlighter-rouge">Layernorm</code>을 의미한다.  따로 객체를 만들어서 사용하지는 않고, <code class="language-plaintext highlighter-rouge">EncoderLayer</code> 객체에 라인으로 추가해 구현하기 때문에 여기서는 역할과 의미만 설명하고 넘어가겠다.</p>

<p>먼저 <code class="language-plaintext highlighter-rouge">Skip-Connection</code>으로도 불리는 <code class="language-plaintext highlighter-rouge">Residual Connection</code>은 어떤 레이어를 통과하기 전, 입력 $x$ 를 레이어를 통과하고 나온 결과값 $fx$ 에 더해준다. 따라서 다음 레이어에 통과되는 입력값은 $x+fx$ 가 된다. 왜 이렇게 더해줄까?? 바로 모델의 안정적인 학습을 위해서다. 일단 그전에 명심하고 가야할 전제가 하나 있다. 모델의 레이어가 깊어질수록 레이어마다 값을 조금씩 바꿔나가는 것이 <code class="language-plaintext highlighter-rouge">Robust</code>하고 <code class="language-plaintext highlighter-rouge">Stable</code>한 결과를 도출할 수 있다는 것이다. 직관적으로 레이어마다 결과가 널뛰기하는 모델보다 안정적으로 차근차근 학습해나가는 모델의 일반화 성능이 더 좋을 것이라고 추측해볼 수 있다. 그래서 <code class="language-plaintext highlighter-rouge">Residual Connection</code> 은 입력 $x$ 와 레이어의 이상적인 출력값 $H(x)$ 의 차이가 크지 않음을 가정한다. 만약, 입력 $X$ 를 <code class="language-plaintext highlighter-rouge">10.0</code> , $H(x)$ 를 <code class="language-plaintext highlighter-rouge">10.4</code> 라고 해보자. 그럼 <code class="language-plaintext highlighter-rouge">Residual Connection</code> 을 사용하는 모델은 <code class="language-plaintext highlighter-rouge">0.4</code>에 대해서만 학습을 하면 된다. 한편 이것을 사용하지 않는 모델은 0에서부터 시작해 무려 <code class="language-plaintext highlighter-rouge">10.4</code>를 학습해야 한다. 어떤 모델이 학습하기 쉬울까?? 당연히 전자일 것이다. 이렇게 모델이 이상적인 값과 입력의 차이만 학습하면 되기 때문에 이것을 <code class="language-plaintext highlighter-rouge">잔차 학습</code>이라고 부르는 것이다.</p>

<p align="center">
<img src="/assets/images/transformer/layernorm.png" alt="Layernorm vs Batchnorm" class="align-center image-caption" width="50%&quot;, height=&quot;50%" />
<strong><em><a href="https://paperswithcode.com/method/layer-normalization">Layernorm vs Batchnorm</a></em></strong>
</p>

<p><code class="language-plaintext highlighter-rouge">Batchnorm</code>은 <code class="language-plaintext highlighter-rouge">“Mini-Batch”</code> 단위를 <code class="language-plaintext highlighter-rouge">Channel(Feature)</code>별로 평균과 표준편차를 구한다면, <code class="language-plaintext highlighter-rouge">Layernorm</code>은  <code class="language-plaintext highlighter-rouge">Channel(Feature)</code> 단위를 <code class="language-plaintext highlighter-rouge">개별 인스턴스</code>별로 평균과 표준편차를 구하여 정규화하는 방식이다.</p>

<p>예를 들어 배치로 4개의 문장을 은닉층의 사이즈가 <code class="language-plaintext highlighter-rouge">512</code>인 모델에 입력해줬다고 생각해보자. 그럼 4개의 문장은 각각 <code class="language-plaintext highlighter-rouge">512</code>개의 원소를 갖게 되는데, 이것에 대한 평균과 표준편차를 구한다는 것이다. 한 개의 문장당 평균과 표준편차를 1개씩 구해서, 4개의 문장이니까 총 8개가 나오겠다.</p>

<p>그렇다면 왜 <code class="language-plaintext highlighter-rouge">Transformer</code>는 <code class="language-plaintext highlighter-rouge">Layernorm</code>을 사용했을까?? 자연어 처리는 배치마다 시퀀스의 길이가 고정되어 있지 않아 패딩이나 절삭을 수행한다. 절삭보다는 패딩이 문제가 된다. 패딩은 일반적으로 문장의 끝부분에 해준다. 여기서 <code class="language-plaintext highlighter-rouge">Batchnorm</code>을 사용하면 끝쪽에 위치한 다른 시퀀스에 속한 정상적인 토큰들은 패딩에 의해 값이 왜곡될 가능성이 있다. 그래서 <code class="language-plaintext highlighter-rouge">Batchnorm</code> 대신 <code class="language-plaintext highlighter-rouge">Layernorm</code>을 사용한다. 또한 <code class="language-plaintext highlighter-rouge">Batchnorm</code> 은 배치 크기에 종속적이라서 테스트 상황에서는 그대로 사용할 수 없다. 따라서 배치 사이즈에 독립적인 <code class="language-plaintext highlighter-rouge">Layernorm</code>을 사용하기도 한다.</p>

<p>한편 이러한 정규화를 왜 사용하는지 궁금하시다면 다른 포스트에 정리를 해뒀으니 참고하시길 바란다. <strong>간단하게만 언급하면,</strong> <code class="language-plaintext highlighter-rouge">모델의 비선형성</code><strong>과 그라디언트 크기 사이의 최적의</strong> <code class="language-plaintext highlighter-rouge">Trade-Off</code><strong>를 인간이 아닌 모델보고 찾게 만드는게 목적이라 볼 수 있다.</strong></p>

<h4 id="encoderlayer"><strong><code class="language-plaintext highlighter-rouge">📘 EncoderLayer</code></strong></h4>
<p>이제 <code class="language-plaintext highlighter-rouge">Single Encoder Block</code>을 정의하기에 필요한 모든 재료를 살펴봤다. 지금까지의 내용을 종합해 한 개의 인코더 블럭을 만들어보자.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Pytorch Implementation of Single Encoder Block
</span>
<span class="k">class</span> <span class="nc">EncoderLayer</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="s">"""
    Class for encoder model module in Transformer
    In this class, we stack each encoder_model module (Multi-Head Attention, Residual-Connection, LayerNorm, FFN)
    We apply pre-layernorm, which is different from original paper
    In common sense, pre-layernorm are more effective &amp; stable than post-layernorm
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim_model</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">512</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span> <span class="n">dim_ffn</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2048</span><span class="p">,</span> <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">EncoderLayer</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">self_attention</span> <span class="o">=</span> <span class="n">MultiHeadAttention</span><span class="p">(</span>
            <span class="n">dim_model</span><span class="p">,</span>
            <span class="n">num_heads</span><span class="p">,</span>
            <span class="nb">int</span><span class="p">(</span><span class="n">dim_model</span> <span class="o">/</span> <span class="n">num_heads</span><span class="p">),</span>
            <span class="n">dropout</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">layer_norm1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">dim_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">layer_norm2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">dim_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">dropout</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">ffn</span> <span class="o">=</span> <span class="n">FeedForward</span><span class="p">(</span>
            <span class="n">dim_model</span><span class="p">,</span>
            <span class="n">dim_ffn</span><span class="p">,</span>
            <span class="n">dropout</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">mask</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="n">ln_x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">layer_norm1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">residual_x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">dropout</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">self_attention</span><span class="p">(</span><span class="n">ln_x</span><span class="p">,</span> <span class="n">mask</span><span class="p">))</span> <span class="o">+</span> <span class="n">x</span>

        <span class="n">ln_x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">layer_norm2</span><span class="p">(</span><span class="n">residual_x</span><span class="p">)</span>
        <span class="n">fx</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">ffn</span><span class="p">(</span><span class="n">ln_x</span><span class="p">)</span> <span class="o">+</span> <span class="n">residual_x</span>
        <span class="k">return</span> <span class="n">fx</span>
</code></pre></div></div>

<p>지금까지의 내용을 객체 하나에 모아둔거라 특별히 설명이 필요한 부분은 없지만, 필자가 <code class="language-plaintext highlighter-rouge">add &amp; norm</code>을 언제 사용했는지 주목해보자. 원본 논문은 <code class="language-plaintext highlighter-rouge">Multi-Head Attention</code>과 <code class="language-plaintext highlighter-rouge">FeedForward</code> <code class="language-plaintext highlighter-rouge">Layer</code>를 통과한 이후에 <code class="language-plaintext highlighter-rouge">add &amp; norm</code>을 하는 <code class="language-plaintext highlighter-rouge">post-layernorm</code> 방식을 적용했다. 하지만 필자는 두 레이어 통과 이전에 미리 <code class="language-plaintext highlighter-rouge">add &amp; norm</code> 을 해주는 <code class="language-plaintext highlighter-rouge">pre-layernorm</code> 방식을 채택했다.</p>

<p align="center">
<img src="/assets/images/transformer/prelayernorm.png" alt="pre-layernorm vs post-layernorm" class="align-center image-caption" width="50%&quot;, height=&quot;50%" />
<strong><em><a href="https://github.com/rickiepark/nlp-with-transformers/blob/main/images/chapter03_layer-norm.png">pre-layernorm vs post-layernorm</a></em></strong>
</p>

<p>최근 <code class="language-plaintext highlighter-rouge">Transformer</code>류의 모델에 <code class="language-plaintext highlighter-rouge">pre-layernorm</code>을 적용하는 것이 좀 더 안정적이고 효율적인 학습을 유도할 수 있다고 실험을 통해 밝혀지고 있다. <code class="language-plaintext highlighter-rouge">pre-layernorm</code> 을 사용하면 별다른 <code class="language-plaintext highlighter-rouge">Gradient Explode</code> 현상이 현저히 줄어들어 복잡한 스케줄러(<code class="language-plaintext highlighter-rouge">warmup</code> 기능이 있는 스케줄러)를 사용할 필요가 없어진다고 하니 참고하자.</p>

<p>이렇게 구현한 <code class="language-plaintext highlighter-rouge">Single Encoder Block</code>을 이제 N개 쌓기만 하면 드디어 인코더를 완성할 수 있게 된다.</p>

<h4 id="-encoder"><strong><code class="language-plaintext highlighter-rouge">📚 Encoder</code></strong></h4>

<p>드디어 대망의 <code class="language-plaintext highlighter-rouge">Encoder</code> 객체 구현을 살펴볼 시간이다.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Pytorch Implementation of Encoder(Stacked N EncoderLayer)
</span>
<span class="k">class</span> <span class="nc">Encoder</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="s">"""
    In this class, encode input sequence and then we stack N EncoderLayer
    First, we define "positional embedding" and then add to input embedding for making "word embedding"
    Second, forward "word embedding" to N EncoderLayer and then get output embedding
    In official paper, they use positional encoding, which is base on sinusoidal function(fixed, not learnable)
    But we use "positional embedding" which is learnable from training
    Args:
        max_seq: maximum sequence length, default 512 from official paper
        N: number of EncoderLayer, default 6 for base model
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">max_seq</span><span class="p">:</span> <span class="mi">512</span><span class="p">,</span> <span class="n">N</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">6</span><span class="p">,</span> <span class="n">dim_model</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">512</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span> <span class="n">dim_ffn</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2048</span><span class="p">,</span> <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Encoder</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">max_seq</span> <span class="o">=</span> <span class="n">max_seq</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">scale</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">dim_model</span><span class="p">))</span>  <span class="c1"># scale factor for input embedding from official paper
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">positional_embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">max_seq</span><span class="p">,</span> <span class="n">dim_model</span><span class="p">)</span>  <span class="c1"># add 1 for cls token
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">num_layers</span> <span class="o">=</span> <span class="n">N</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span> <span class="o">=</span> <span class="n">dim_model</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dim_ffn</span> <span class="o">=</span> <span class="n">dim_ffn</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">dropout</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">encoder_layers</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">ModuleList</span><span class="p">(</span>
            <span class="p">[</span><span class="n">EncoderLayer</span><span class="p">(</span><span class="n">dim_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">dim_ffn</span><span class="p">,</span> <span class="n">dropout</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">num_layers</span><span class="p">)]</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">layer_norm</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">dim_model</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">mask</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
        <span class="s">"""
        inputs: embedding from input sequence, shape =&gt; [BS, SEQ_LEN, DIM_MODEL]
        mask: mask for Encoder padded token for speeding up to calculate attention score
        """</span>
        <span class="n">layer_output</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">pos_x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">max_seq</span><span class="p">).</span><span class="n">repeat</span><span class="p">(</span><span class="n">inputs</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]).</span><span class="n">to</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">dropout</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">scale</span> <span class="o">*</span> <span class="n">inputs</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">positional_embedding</span><span class="p">(</span><span class="n">pos_x</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">encoder_layers</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span>
            <span class="n">layer_output</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">encoded_x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">layer_norm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># from official paper &amp; code by Google Research
</span>        <span class="n">layer_output</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">stack</span><span class="p">(</span><span class="n">layer_output</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">).</span><span class="n">to</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>  <span class="c1"># For Weighted Layer Pool: [N, BS, SEQ_LEN, DIM]
</span>        <span class="k">return</span> <span class="n">encoded_x</span><span class="p">,</span> <span class="n">layer_output</span>
</code></pre></div></div>

<p>역시 지금까지 내용을 종합한 것뿐이라서 크게 특이한 내용은 없고, 구현상 놓치기 쉬운 부분만 알고 넘어가면 된다. <code class="language-plaintext highlighter-rouge">forward</code> 메서드의 변수 <code class="language-plaintext highlighter-rouge">x</code>를 초기화하는 코드 라인을 주목해보자. 이것이 바로 <code class="language-plaintext highlighter-rouge">Input Embedding</code>과 <code class="language-plaintext highlighter-rouge">Position Embedding</code>을 더하는(행렬 합) 연산을 구현한 것이다. 이 때 놓치기 쉬운 부분이 바로 <code class="language-plaintext highlighter-rouge">Input Embedding</code>에 <code class="language-plaintext highlighter-rouge">scale factor</code>를 곱해준다는 것이다. 저자의 주장에 따르면 <code class="language-plaintext highlighter-rouge">Input Embedding</code>에만 <code class="language-plaintext highlighter-rouge">scale factor</code>를 사용하는 것이 안정적인 학습에 도움이 된다고 하니 참고하자.</p>

<p>한편, 마지막 인코더 블럭에서 나온 임베딩을 다시 한 번 <code class="language-plaintext highlighter-rouge">layernorm</code>에 통과하도록 구현했다. 이 부분도 원본 논문에 있는 내용은 아니고  <code class="language-plaintext highlighter-rouge">ViT</code>의 논문 내용을 참고해 추가했다.</p>

<h4 id="decoderlayer"><strong><code class="language-plaintext highlighter-rouge">📘 DecoderLayer</code></strong></h4>

<p>이번에는 디코더에 사용된 블럭의 동작 방식과 의미 그리고 구현까지 알아보자. 사실 디코더도 지금까지 공부한 내용과 크게 다른게 없다. 다만 인코더와는 목적이 다르기 때문에 발생하는 미세한 동작의 차이에 주목해보자. 먼저 <code class="language-plaintext highlighter-rouge">Single Decoder Block</code>은 <code class="language-plaintext highlighter-rouge">Single Encoder Block</code>과 다르게 <code class="language-plaintext highlighter-rouge">Self-Attention</code>을 두 번 수행한다. 지겹겠지만 다시 한 번 Transformer의 목적을 상기시켜보자. 바로 대상 언어를 타겟 언어로 잘 번역하는 것이었다. 일단 인코더를 통해 대상 언어는 잘 이해하게 되었다. 그럼 이제 타겟 언어도 잘 이해해야하지 않은가?? 그래서 타겟 언어를 이해하기 위해 <code class="language-plaintext highlighter-rouge">Self-Attention</code>을 한 번, 그리고 대상 언어를 타겟 언어로 번역하기 위해 <code class="language-plaintext highlighter-rouge">Self-Attention</code>을 한 번, 총 2번 수행하는 것이다. 첫번째  <code class="language-plaintext highlighter-rouge">Self-Attention</code> 을 <code class="language-plaintext highlighter-rouge">Masked Multi-Head Attention</code>, 두번째를 <code class="language-plaintext highlighter-rouge">Encoder-Decoder Multi-Head Attention</code>이라고 부른다.</p>

<p><strong><code class="language-plaintext highlighter-rouge">🎭 Masked Multi-Head Attention</code></strong><br />
인코더의 <code class="language-plaintext highlighter-rouge">Multi-Head Attention와</code> 행렬 $Q,K,V$ 의 출처가 다르다. 디코더는 출처가 타겟 언어인 <code class="language-plaintext highlighter-rouge">linear projection matrix</code>를 사용한다. 또한 인코더와 다르게 개별 시점에 맞는 마스킹 행렬이 필요하다. 디코더의 과업은 결국 대상 언어를 잘 이해하고 그것에 가장 잘 들어맞는 타겟 언어 시퀀스를 <code class="language-plaintext highlighter-rouge">generate</code>하는 것이다. 즉, <code class="language-plaintext highlighter-rouge">Next Token Prediction</code>을 통해 시퀀스를 만들어내야 한다. 그런데 현재 시점에서 미래 시점에 디코더가 예측해야할 토큰을 미리 알고 있으면 그것을 예측이라고 할 수 있을까?? 디코더가 현재 시점의 토큰을 예측하는데 미래 시점의 <code class="language-plaintext highlighter-rouge">Context</code>를 반영하지 못하도록 막기 위해 미리 마스킹 행렬을 정의해 <code class="language-plaintext highlighter-rouge">Word_Embedding</code>에 적용해준다. 이렇게 마스킹이 적용된 임베딩 행렬을 가지고 <code class="language-plaintext highlighter-rouge">linear projection &amp; self-attention</code>을 수행하기 때문에 이름 앞에 <code class="language-plaintext highlighter-rouge">masked</code>를 붙이게 되었다.</p>

<p align="center">
<img src="/assets/images/transformer/decoder_mask.png" alt="Decoder Language Modeling Mask" class="align-center image-caption" width="50%&quot;, height=&quot;50%" />
<strong><em><a href="https://paul-hyun.github.io/transformer-02/">Decoder Language Modeling Mask</a></em></strong>
</p>

<p>위 그림은 마스킹을 적용한 <code class="language-plaintext highlighter-rouge">Word_Embedding</code>의 모습이다. 첫 번째 시점에서 모델은 자기 자신을 제외한 나머지 <code class="language-plaintext highlighter-rouge">Context</code>를 예측에 활용할 수 없다. 그래서 이하 나머지 토큰을 전부 마스킹 처리해줬다. 두번째 시점에서는 직전 시점인 첫번째 토큰과 자기 자신만 참고할 수 있다. 한편, 이렇게 직전 <code class="language-plaintext highlighter-rouge">Context</code>만 가지고 현재 토큰을 추론하는 것을 <code class="language-plaintext highlighter-rouge">Language Modeling</code>이라 부른다. 그리고 마찬가지로 디코더 역시 시퀀스에 패딩 처리를 해주기 때문에 인코더와 동일한 원리로 만든 <code class="language-plaintext highlighter-rouge">decoder padding mask</code> 역시 필요하다.</p>

<p>마스킹 행렬 구현은 최상위 객체인 <code class="language-plaintext highlighter-rouge">Transformer</code>의 내부 메서드로 만들었으니, 그 때 자세히 설명하겠다. 이하 나머지 디테일은 인코더의 것과 동일하다.</p>

<p><strong><code class="language-plaintext highlighter-rouge">🪢 Encoder-Decoder Multi-Head Attention</code></strong><br />
인코더를 통해 이해한 대상 언어 시퀀스와 바로 직전 <code class="language-plaintext highlighter-rouge">Self-Attention</code>을 통해 이해한 타겟 언어 시퀀스를 서로 대조하는 레이어다. 우리의 지금 목적은 <code class="language-plaintext highlighter-rouge">타겟 언어</code>와 가장 유사한 <code class="language-plaintext highlighter-rouge">대상 언어</code>를 찾아 문장을 완성하는 것이다. 따라서 어텐션 계산에 사용될 행렬 $Q$ 의 출처는 직전 레이어인 <code class="language-plaintext highlighter-rouge">Masked Multi-Head Attention</code> 의 반환값을 사용하고, 행렬 $K,V$ 는 인코더의 최종 반환값을 사용한다.</p>

<p>한편, 여기 레이어에는 마스킹 행렬이 세 종류나 필요하다. 그 이유는 서로 출처가 다른 두가지 행렬을 계산에 사용하기 때문이다. 지금은 여전히 디코더의 역할을 수행하는 것이기 때문에 직전 레이어에서 사용한 2개의 마스킹 행렬이 그대로 필요하다. 그리고 인코더에서 넘어온 값을 사용한다는 것은 인코더의 패딩 역시 처리가 필요하다는 의미다. 따라서 <code class="language-plaintext highlighter-rouge">lm_mask</code>, <code class="language-plaintext highlighter-rouge">dec_pad_mask</code>, <code class="language-plaintext highlighter-rouge">enc_pad_mask</code>가 필요하다. 역시 마스킹 구현은 최상위 객체 설명 때 함께 살펴보겠다.</p>

<p><strong><code class="language-plaintext highlighter-rouge">👩‍💻 Implementation</code></strong><br />
이제 <code class="language-plaintext highlighter-rouge">Single Decoder Block</code>의 구현을 살펴보자. 역시 파이토치로 구현했다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Pytorch Implementation of Single Decoder Block
</span>
<span class="k">class</span> <span class="nc">DecoderLayer</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="s">"""
    Class for decoder model module in Transformer
    In this class, we stack each decoder_model module (Masked Multi-Head Attention, Residual-Connection, LayerNorm, FFN)
    We apply pre-layernorm, which is different from original paper
    References:
        https://arxiv.org/abs/1706.03762
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim_model</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">512</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span> <span class="n">dim_ffn</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2048</span><span class="p">,</span> <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">DecoderLayer</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">masked_attention</span> <span class="o">=</span> <span class="n">MultiHeadAttention</span><span class="p">(</span>
            <span class="n">dim_model</span><span class="p">,</span>
            <span class="n">num_heads</span><span class="p">,</span>
            <span class="nb">int</span><span class="p">(</span><span class="n">dim_model</span> <span class="o">/</span> <span class="n">num_heads</span><span class="p">),</span>
            <span class="n">dropout</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">enc_dec_attention</span> <span class="o">=</span> <span class="n">MultiHeadAttention</span><span class="p">(</span>
            <span class="n">dim_model</span><span class="p">,</span>
            <span class="n">num_heads</span><span class="p">,</span>
            <span class="nb">int</span><span class="p">(</span><span class="n">dim_model</span> <span class="o">/</span> <span class="n">num_heads</span><span class="p">),</span>
            <span class="n">dropout</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">layer_norm1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">dim_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">layer_norm2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">dim_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">layer_norm3</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">dim_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">dropout</span><span class="p">)</span>  <span class="c1"># dropout is not learnable layer
</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">ffn</span> <span class="o">=</span> <span class="n">FeedForward</span><span class="p">(</span>
            <span class="n">dim_model</span><span class="p">,</span>
            <span class="n">dim_ffn</span><span class="p">,</span>
            <span class="n">dropout</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">dec_mask</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">enc_dec_mask</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">enc_output</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="n">ln_x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">layer_norm1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">residual_x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">dropout</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">masked_attention</span><span class="p">(</span><span class="n">ln_x</span><span class="p">,</span> <span class="n">dec_mask</span><span class="p">))</span> <span class="o">+</span> <span class="n">x</span>

        <span class="n">ln_x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">layer_norm2</span><span class="p">(</span><span class="n">residual_x</span><span class="p">)</span>
        <span class="n">residual_x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">dropout</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">enc_dec_attention</span><span class="p">(</span><span class="n">ln_x</span><span class="p">,</span> <span class="n">enc_dec_mask</span><span class="p">,</span> <span class="n">enc_output</span><span class="p">))</span> <span class="o">+</span> <span class="n">x</span>  <span class="c1"># for enc_dec self-attention
</span>
        <span class="n">ln_x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">layer_norm3</span><span class="p">(</span><span class="n">residual_x</span><span class="p">)</span>
        <span class="n">fx</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">ffn</span><span class="p">(</span><span class="n">ln_x</span><span class="p">)</span> <span class="o">+</span> <span class="n">residual_x</span>
        <span class="k">return</span> <span class="n">fx</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">Self-Attention</code> 레이어가 인코더보다 하나 더 추가되어 <code class="language-plaintext highlighter-rouge">add &amp; norm</code> 을 총 3번 해줘야 한다는 것을 제외하고는 크게 구현상의 특이점은 없다. 그저 지금까지 살펴본 블럭을 요리조리 다시 쌓으면 된다.</p>

<h4 id="decoder"><strong><code class="language-plaintext highlighter-rouge">📚 Decoder</code></strong></h4>

<p><code class="language-plaintext highlighter-rouge">Single Decoder Block</code>을 <code class="language-plaintext highlighter-rouge">N</code>개 쌓고 전체 디코더 동작을 수행하는 <code class="language-plaintext highlighter-rouge">Decoder</code> 객체의 구현을 알아보자.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Pytorch Implementation of Decoder(N Stacked Single Decoder Block)
</span>
<span class="k">class</span> <span class="nc">Decoder</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="s">"""
    In this class, decode encoded embedding from encoder by outputs (target language, Decoder's Input Sequence)
    First, we define "positional embedding" for Decoder's Input Sequence,
    and then add them to Decoder's Input Sequence for making "decoder word embedding"
    Second, forward "decoder word embedding" to N DecoderLayer and then pass to linear &amp; softmax for OutPut Probability
    Args:
        vocab_size: size of vocabulary for output probability
        max_seq: maximum sequence length, default 512 from official paper
        N: number of EncoderLayer, default 6 for base model
    References:
        https://arxiv.org/abs/1706.03762
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">vocab_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">max_seq</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">512</span><span class="p">,</span>
        <span class="n">N</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">6</span><span class="p">,</span>
        <span class="n">dim_model</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">512</span><span class="p">,</span>
        <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span>
        <span class="n">dim_ffn</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2048</span><span class="p">,</span>
        <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Decoder</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">max_seq</span> <span class="o">=</span> <span class="n">max_seq</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">scale</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">dim_model</span><span class="p">))</span>  <span class="c1"># scale factor for input embedding from official paper
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">positional_embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">max_seq</span><span class="p">,</span> <span class="n">dim_model</span><span class="p">)</span>  <span class="c1"># add 1 for cls token
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">num_layers</span> <span class="o">=</span> <span class="n">N</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span> <span class="o">=</span> <span class="n">dim_model</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dim_ffn</span> <span class="o">=</span> <span class="n">dim_ffn</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">dropout</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">decoder_layers</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">ModuleList</span><span class="p">(</span>
            <span class="p">[</span><span class="n">DecoderLayer</span><span class="p">(</span><span class="n">dim_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">dim_ffn</span><span class="p">,</span> <span class="n">dropout</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">num_layers</span><span class="p">)]</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">layer_norm</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">dim_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">fc_out</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">dim_model</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">)</span>  <span class="c1"># In Pytorch, nn.CrossEntropyLoss already has softmax function
</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">dec_mask</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">enc_dec_mask</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">enc_output</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
        <span class="s">"""
        inputs: embedding from input sequence, shape =&gt; [BS, SEQ_LEN, DIM_MODEL]
        dec_mask: mask for Decoder padded token for Language Modeling
        enc_dec_mask: mask for Encoder-Decoder Self-Attention, from encoder padded token
        """</span>
        <span class="n">layer_output</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">pos_x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">max_seq</span><span class="p">).</span><span class="n">repeat</span><span class="p">(</span><span class="n">inputs</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]).</span><span class="n">to</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">dropout</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">scale</span> <span class="o">*</span> <span class="n">inputs</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">positional_embedding</span><span class="p">(</span><span class="n">pos_x</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">decoder_layers</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dec_mask</span><span class="p">,</span> <span class="n">enc_dec_mask</span><span class="p">,</span> <span class="n">enc_output</span><span class="p">)</span>
            <span class="n">layer_output</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">decoded_x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">fc_out</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">layer_norm</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>  <span class="c1"># Because of pre-layernorm
</span>        <span class="n">layer_output</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">stack</span><span class="p">(</span><span class="n">layer_output</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">).</span><span class="n">to</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>  <span class="c1"># For Weighted Layer Pool: [N, BS, SEQ_LEN, DIM]
</span>        <span class="k">return</span> <span class="n">decoded_x</span><span class="p">,</span> <span class="n">layer_output</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">Encoder</code> 객체와 모든 부분이 동일하다. 디테일한 설정만 디코더에 맞게 변경되었을 뿐이다. <code class="language-plaintext highlighter-rouge">self.fc_out</code> 에 주목해보자. 디코더는 현재 시점에 가장 적합한 토큰을 예측해야 하기 때문에 디코더의 출력부분에 로짓 계산을 위한 레이어가 필요하다. 그 역할을 하는 것이 바로 <code class="language-plaintext highlighter-rouge">self.fc_out</code>이다. 한편, <code class="language-plaintext highlighter-rouge">self.fc_out</code>의 출력 차원이 <code class="language-plaintext highlighter-rouge">vocab_size</code>으로 되어있는데, 디코더는 디코더가 가진 전체 <code class="language-plaintext highlighter-rouge">vocab</code> 을 현재 시점에 적합한 토큰 후보군으로 사용하기 때문이다.</p>

<h4 id="-transformer"><strong><code class="language-plaintext highlighter-rouge">🦾 Transformer</code></strong></h4>
<p>이제 대망의 마지막… 모델의 가장 최상위 객체인 <code class="language-plaintext highlighter-rouge">Transformer</code>에 대해서 살펴보자. 객체의 동작은 정리하면 다음과 같다.</p>

<ul>
  <li><strong>1) Make <code class="language-plaintext highlighter-rouge">Input Embedding</code> for Encoder &amp; Decoder respectively, Init <code class="language-plaintext highlighter-rouge">Encoder &amp; Decoder</code> Class</strong></li>
  <li><strong>2) Make 3 types of Masking: <code class="language-plaintext highlighter-rouge">Encoder Padding Mask</code>, <code class="language-plaintext highlighter-rouge">Decoder LM &amp; Padding Mask</code>, <code class="language-plaintext highlighter-rouge">Encoder-Decoder Mask</code></strong></li>
  <li><strong>3) Return <code class="language-plaintext highlighter-rouge">Output</code> from Encoder &amp; Decoder</strong></li>
</ul>

<p>특히 계속 미뤄왔던 마스킹 구현에 대해서 살펴보자. 나머지는 이미 앞에서 많이 설명했으니까 넘어가도록 하겠다. 일단 먼저 코드를 읽어보자. 추가로 <code class="language-plaintext highlighter-rouge">Input Embedding</code> 구현은 사용자의 <code class="language-plaintext highlighter-rouge">vocab</code> 구축 방식에 따라 달라진다. 필자의 경우 대상 언어와 타겟 언어의 <code class="language-plaintext highlighter-rouge">vocab</code>을 분리해 사용하는 것을 가정하고 코드를 만들어 임베딩 레이어를 따로 따로 구현해줬다. <code class="language-plaintext highlighter-rouge">vocab</code>을 통합으로 구축하시는 분이라면 하나만 정의해도 상관없다. 대신 나중에 디코더의 로짓값 계산을 위해 개별 언어의 토큰 사이즈는 알고 있어야 할 것이다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Pytorch Implementation of Transformer
</span>
<span class="k">class</span> <span class="nc">Transformer</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="s">"""
    Main class for Pure Transformer, Pytorch implementation
    There are two Masking Method for padding token
        1) Row &amp; Column masking
        2) Column masking only at forward time, Row masking at calculating loss time
    second method is more efficient than first method, first method is complex &amp; difficult to implement
    Args:
        enc_vocab_size: size of vocabulary for Encoder Input Sequence
        dec_vocab_size: size of vocabulary for Decoder Input Sequence
        max_seq: maximum sequence length, default 512 from official paper
        enc_N: number of EncoderLayer, default 6 for base model
        dec_N: number of DecoderLayer, default 6 for base model
    Reference:
        https://arxiv.org/abs/1706.03762
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">enc_vocab_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">dec_vocab_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">max_seq</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">512</span><span class="p">,</span>
        <span class="n">enc_N</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">6</span><span class="p">,</span>
        <span class="n">dec_N</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">6</span><span class="p">,</span>
        <span class="n">dim_model</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">512</span><span class="p">,</span>
        <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span>
        <span class="n">dim_ffn</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2048</span><span class="p">,</span>
        <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Transformer</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">enc_input_embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">enc_vocab_size</span><span class="p">,</span> <span class="n">dim_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dec_input_embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">dec_vocab_size</span><span class="p">,</span> <span class="n">dim_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">Encoder</span><span class="p">(</span><span class="n">max_seq</span><span class="p">,</span> <span class="n">enc_N</span><span class="p">,</span> <span class="n">dim_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">dim_ffn</span><span class="p">,</span> <span class="n">dropout</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="n">Decoder</span><span class="p">(</span><span class="n">dec_vocab_size</span><span class="p">,</span> <span class="n">max_seq</span><span class="p">,</span> <span class="n">dec_N</span><span class="p">,</span> <span class="n">dim_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">dim_ffn</span><span class="p">,</span> <span class="n">dropout</span><span class="p">)</span>

    <span class="o">@</span><span class="nb">staticmethod</span>
    <span class="k">def</span> <span class="nf">enc_masking</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">enc_pad_index</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="s">""" make masking matrix for Encoder Padding Token """</span>
        <span class="n">enc_mask</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span> <span class="o">!=</span> <span class="n">enc_pad_index</span><span class="p">).</span><span class="nb">int</span><span class="p">().</span><span class="n">repeat</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]).</span><span class="n">view</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
        <span class="k">return</span> <span class="n">enc_mask</span>

    <span class="o">@</span><span class="nb">staticmethod</span>
    <span class="k">def</span> <span class="nf">dec_masking</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">dec_pad_index</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="s">""" make masking matrix for Decoder Masked Multi-Head Self-Attention """</span>
        <span class="n">pad_mask</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span> <span class="o">!=</span> <span class="n">dec_pad_index</span><span class="p">).</span><span class="nb">int</span><span class="p">().</span><span class="n">repeat</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]).</span><span class="n">view</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
        <span class="n">lm_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tril</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">ones</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>
        <span class="n">dec_mask</span> <span class="o">=</span> <span class="n">pad_mask</span> <span class="o">*</span> <span class="n">lm_mask</span>
        <span class="k">return</span> <span class="n">dec_mask</span>

    <span class="o">@</span><span class="nb">staticmethod</span>
    <span class="k">def</span> <span class="nf">enc_dec_masking</span><span class="p">(</span><span class="n">enc_x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">dec_x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">enc_pad_index</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="s">""" make masking matrix for Encoder-Decoder Multi-Head Self-Attention in Decoder """</span>
        <span class="n">enc_dec_mask</span> <span class="o">=</span> <span class="p">(</span><span class="n">enc_x</span> <span class="o">!=</span> <span class="n">enc_pad_index</span><span class="p">).</span><span class="nb">int</span><span class="p">().</span><span class="n">repeat</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">dec_x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]).</span><span class="n">view</span><span class="p">(</span>
            <span class="n">enc_x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">dec_x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">enc_x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">enc_dec_mask</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">enc_inputs</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">dec_inputs</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">enc_pad_index</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">dec_pad_index</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
        <span class="n">enc_mask</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">enc_masking</span><span class="p">(</span><span class="n">enc_inputs</span><span class="p">,</span> <span class="n">enc_pad_index</span><span class="p">)</span>  <span class="c1"># enc_x.shape[1] == encoder input sequence length
</span>        <span class="n">dec_mask</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">dec_masking</span><span class="p">(</span><span class="n">dec_inputs</span><span class="p">,</span> <span class="n">dec_pad_index</span><span class="p">)</span>  <span class="c1"># dec_x.shape[1] == decoder input sequence length
</span>        <span class="n">enc_dec_mask</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">enc_dec_masking</span><span class="p">(</span><span class="n">enc_inputs</span><span class="p">,</span> <span class="n">dec_inputs</span><span class="p">,</span> <span class="n">enc_pad_index</span><span class="p">)</span>

        <span class="n">enc_x</span><span class="p">,</span> <span class="n">dec_x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">enc_input_embedding</span><span class="p">(</span><span class="n">enc_inputs</span><span class="p">),</span> <span class="bp">self</span><span class="p">.</span><span class="n">dec_input_embedding</span><span class="p">(</span><span class="n">dec_inputs</span><span class="p">)</span>

        <span class="n">enc_output</span><span class="p">,</span> <span class="n">enc_layer_output</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">enc_x</span><span class="p">,</span> <span class="n">enc_mask</span><span class="p">)</span>
        <span class="n">dec_output</span><span class="p">,</span> <span class="n">dec_layer_output</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">decoder</span><span class="p">(</span><span class="n">dec_x</span><span class="p">,</span> <span class="n">dec_mask</span><span class="p">,</span> <span class="n">enc_dec_mask</span><span class="p">,</span> <span class="n">enc_output</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">enc_output</span><span class="p">,</span> <span class="n">dec_output</span><span class="p">,</span> <span class="n">enc_layer_output</span><span class="p">,</span> <span class="n">dec_layer_output</span>
</code></pre></div></div>

<p>마스킹의 필요성이나 동작 방식은 이미 위에서 모두 설명했기 때문에 구현상 특징만 설명하려한다. 세가지 마스킹 모두 공통적으로 구현 코드 라인에 <code class="language-plaintext highlighter-rouge">.int()</code> 가 들어가 있다. 그 이유는 $\frac{Q•K^T}{\sqrt{d_h}}$에 마스킹을 적용할 때 <code class="language-plaintext highlighter-rouge">torch.masked_fill</code> 메서드를 사용하기 때문이다. 무슨 이유 때문인지는 모르겠으나 <code class="language-plaintext highlighter-rouge">torch.masked_fill</code>의 경우 마스킹 조건으로 <code class="language-plaintext highlighter-rouge">boolean</code>을 전달하면 마스킹이 제대로 구현되지 않는 현상이 있었다. 한편, 정수값으로 조건을 구현하면 의도한대로 구현이 되는 것을 확인했다. 그래서 패딩에 해당되는 토큰이 위치한 곳의 원소값을 정수형 <code class="language-plaintext highlighter-rouge">Binary</code> 로 만들어주기 위해 <code class="language-plaintext highlighter-rouge">int()</code> 를 사용한 것이다.</p>

<p><strong><code class="language-plaintext highlighter-rouge">🎭 Decoder Mask</code></strong><br />
디코더는 총 2가지의 마스킹이 필요하다고 언급했었다. <code class="language-plaintext highlighter-rouge">pad_mask</code>의 경우는 인코더의 것과 동일한 원리를 사용하기 때문에 설명을 생략하겠다. <code class="language-plaintext highlighter-rouge">lm_mask</code> 의 경우는 <code class="language-plaintext highlighter-rouge">torch.tril</code>을 이용해 하삼각행렬 형태로 마스킹 행렬 정의가 쉽게 가능하다.<br />
한편, 2개의 마스킹을 동시에 디코더 객체에 넘기는 것은 매우 비효율적이다. 따라서 <code class="language-plaintext highlighter-rouge">pad_mask</code> 와 <code class="language-plaintext highlighter-rouge">lm_mask</code>의 합집합에 해당하는 행렬을 만들어 최종 디코더의 마스킹으로 전달한다.</p>

<p><strong><code class="language-plaintext highlighter-rouge">🙌 Encoder-Decoder Mask</code></strong><br />
이번 경우는 마스킹의 행방향 차원은 디코더 입력값의 시퀀스 길이, 열방향 차원은 인코더 입력값의 시퀀스 길이로 설정해야 한다. 그 이유는 다른 <code class="language-plaintext highlighter-rouge">Self-Attention</code> 레이어와 다르게 서로 다른 출처를 통해 만든 행렬을 사용하기 때문에 $\frac{Q•K^T}{\sqrt{d_h}}$의 모양이 정사각행렬이 아닐 수도 있다. 예를 들어 한국어 문장을 영어로 바꾸는 경우를 생각해보자. 같은 뜻이 담긴 문장이라고 해서 두 문장의 길이가 같은가?? 아니다. 서로 다른 언어라면 거의 대부분의 경우 길이가 다를 것이다. 따라서 $\frac{Q•K^T}{\sqrt{d_h}}$의 행방향은 디코더의 시퀀스 길이에 따르고 열방향은 인코더의 시퀀스 길이에 따르도록 마스킹 역시 구현해줘야 한다.<br />
그리고 이번 마스킹을 만드는 목적이 인코더의 패딩을 마스킹 처리해주기 위함이기 때문에 <code class="language-plaintext highlighter-rouge">enc_pad_index</code> 매개변수에는 인코더 <code class="language-plaintext highlighter-rouge">vocab</code>에서 정의한 <code class="language-plaintext highlighter-rouge">pad_token_ID</code>를 전달하면 된다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># scaled_dot_product_attention의 일부
</span>
<span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
		<span class="n">attention_matrix</span> <span class="o">=</span> <span class="n">attention_matrix</span><span class="p">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">mask</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="s">'-inf'</span><span class="p">))</span>
</code></pre></div></div>

<p>이렇게 구현된 마스킹은 <code class="language-plaintext highlighter-rouge">scaled_dot_product_attention</code> 메서드에 구현된 조건문을 통해 마스킹 대상을 -∞으로 변환하는 역할을 하게 된다.</p>

        
      </section>

      <footer class="page__meta">
        
        
  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      <a href="/tags/#decoder" class="page__taxonomy-item p-category" rel="tag">Decoder</a><span class="sep">, </span>
    
      <a href="/tags/#encoder" class="page__taxonomy-item p-category" rel="tag">Encoder</a><span class="sep">, </span>
    
      <a href="/tags/#natural-language-process" class="page__taxonomy-item p-category" rel="tag">Natural Language Process</a><span class="sep">, </span>
    
      <a href="/tags/#self-attention" class="page__taxonomy-item p-category" rel="tag">Self-Attention</a><span class="sep">, </span>
    
      <a href="/tags/#seq2seq" class="page__taxonomy-item p-category" rel="tag">Seq2Seq</a><span class="sep">, </span>
    
      <a href="/tags/#transformer" class="page__taxonomy-item p-category" rel="tag">Transformer</a>
    
    </span>
  </p>




  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> Categories: </strong>
    <span itemprop="keywords">
    
      <a href="/categories/#nlp" class="page__taxonomy-item p-category" rel="tag">NLP</a>
    
    </span>
  </p>


        

  <p class="page__date"><strong><i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> Updated:</strong> <time class="dt-published" datetime="2023-08-03">August 3, 2023</time></p>

      </footer>

      <section class="page__share">
  
    <h4 class="page__share-title">Share on</h4>
  

  <a href="https://twitter.com/intent/tweet?text=%F0%9F%A4%96%C2%A0%5BTransformer%5D+Attention+Is+All+You+Need%20http%3A%2F%2Flocalhost%3A4000%2Fnlp%2Ftransformer" class="btn btn--twitter" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Twitter"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=http%3A%2F%2Flocalhost%3A4000%2Fnlp%2Ftransformer" class="btn btn--facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Facebook"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=http%3A%2F%2Flocalhost%3A4000%2Fnlp%2Ftransformer" class="btn btn--linkedin" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on LinkedIn"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>


      
  <nav class="pagination">
    
      <a href="/nlp/deberta" class="pagination--pager" title="🪢 [DeBERTa] DeBERTa: Decoding-Enhanced BERT with Disentangled-Attention
">Previous</a>
    
    
      <a href="/python/attribute_function" class="pagination--pager" title="👨‍💻🐍 [Python] Object Attribute &amp; Assertion Function
">Next</a>
    
  </nav>

    </div>

    
  </article>

  
  
    <div class="page__related">
      <h2 class="page__related-title">You may also enjoy</h2>
      <div class="grid__wrapper">
        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
      <div class="archive__item-teaser">
        <img src="/assets/images/huggingface_emoji.png" alt="">
      </div>
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/nlp/lora" rel="permalink">🔪 [LoRA] Low-Rank Adaptation of Large Language Models
</a>
      
    </h2>
    <!-- 

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          5 minute read
        
      </span>
    
  </p>

 -->
    
      <p class="page__meta"><i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i> March 28 2024</p>
    
    <p class="archive__item-excerpt" itemprop="description">LoRA Official Paper Review with Pytorch Implementation
</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
      <div class="archive__item-teaser">
        <img src="/assets/images/huggingface_emoji.png" alt="">
      </div>
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/python/time_complexity2" rel="permalink">👨⏰🐍 [Python] 시간복잡도 2
</a>
      
    </h2>
    <!-- 

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          4 minute read
        
      </span>
    
  </p>

 -->
    
      <p class="page__meta"><i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i> March 26 2024</p>
    
    <p class="archive__item-excerpt" itemprop="description">시간 복잡도 줄이기
</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
      <div class="archive__item-teaser">
        <img src="/assets/images/huggingface_emoji.png" alt="">
      </div>
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/python/time_complexity1" rel="permalink">👨⏰🐍 [Python] 시간복잡도 1
</a>
      
    </h2>
    <!-- 

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          1 minute read
        
      </span>
    
  </p>

 -->
    
      <p class="page__meta"><i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i> March 26 2024</p>
    
    <p class="archive__item-excerpt" itemprop="description">시간 복잡도에 대한 이해
</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
      <div class="archive__item-teaser">
        <img src="/assets/images/huggingface_emoji.png" alt="">
      </div>
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/nlp/linear_attention" rel="permalink">🌆 [Linear Attention] Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention
</a>
      
    </h2>
    <!-- 

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          8 minute read
        
      </span>
    
  </p>

 -->
    
      <p class="page__meta"><i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i> March 14 2024</p>
    
    <p class="archive__item-excerpt" itemprop="description">Linear Attention Official Paper Review with Pytorch Implementation
</p>
  </article>
</div>

        
      </div>
    </div>
  
  
</div>

    </div>

    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>Follow:</strong></li>
    

    
      
        
      
        
      
        
      
        
      
        
      
        
      
    

    
      <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
    
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2024 qcqced. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>







    <script>
  'use strict';

  (function() {
    var commentContainer = document.querySelector('#utterances-comments');

    if (!commentContainer) {
      return;
    }

    var script = document.createElement('script');
    script.setAttribute('src', 'https://utteranc.es/client.js');
    script.setAttribute('repo', 'qcqced123/qcqced123.github.io');
    script.setAttribute('issue-term', 'pathname');
    
    script.setAttribute('theme', 'github-light');
    script.setAttribute('crossorigin', 'anonymous');

    commentContainer.appendChild(script);
  })();
</script>

  




<script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML">
</script>

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"] ],
    displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
    processEscapes: true
    },
    "HTML-CSS": { availableFonts: ["TeX"] }
});
</script>

  </body>
</html>
