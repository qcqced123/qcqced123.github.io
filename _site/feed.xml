<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2023-07-26T23:24:57+09:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">AI/Business Study Log</title><subtitle>NLP, Marketing</subtitle><author><name>qcqced</name><email>qcqced123@gmail.com</email></author><entry><title type="html">🌆 [ViT] An Image Is Worth 16x16 Words: Transformers For Image Recognition At Scale</title><link href="http://localhost:4000/cv/vit" rel="alternate" type="text/html" title="🌆 [ViT] An Image Is Worth 16x16 Words: Transformers For Image Recognition At Scale" /><published>2023-07-26T00:00:00+09:00</published><updated>2023-07-27T02:00:00+09:00</updated><id>http://localhost:4000/cv/ViT</id><content type="html" xml:base="http://localhost:4000/cv/vit"><![CDATA[<h3 id="overview"><code class="language-plaintext highlighter-rouge">🔭 Overview</code></h3>

<p>시작하기 앞서, 본 논문 리뷰를 수월하게 읽으려면 <code class="language-plaintext highlighter-rouge">Transformer</code> 에 대한 선이해가 필수적이다. 아직 <code class="language-plaintext highlighter-rouge">Transformer</code> 에 대해서 잘 모른다면 필자가 작성한 포스트를 읽고 오길 권장한다. 또한 본문 내용을 작성하면서 참고한 논문과 여러 포스트의 링크를 맨 밑 하단에 첨부했으니 참고 바란다. 시간이 없으신 분들은 중간의 코드 구현부를 생략하고 <code class="language-plaintext highlighter-rouge">Insight</code> 부터 읽기를 권장한다.</p>

<p><code class="language-plaintext highlighter-rouge">Vision Transformer</code>(이하 <code class="language-plaintext highlighter-rouge">ViT</code>)는 2020년 10월 Google에서 발표한 컴퓨터 비전용 모델이다. 자연어 처리에서 대성공을 거둔 트렌스포머 구조와 기법을 거의 그대로 비전 분야에 이식했다는 점에서 큰 의의가 있으며, 이후 컴퓨터 비전 분야의 트렌스포머 전성시대가 열리게 된 계기로 작용한다.</p>

<p>한편, <code class="language-plaintext highlighter-rouge">ViT</code> 의 설계 철학은 바로 <code class="language-plaintext highlighter-rouge">scalability(범용성)</code>이다. 신경망 설계에서 범용성이란, 모델의 확장 가능성을 말한다. 예를 들면 학습 데이터보다 더 크고 복잡한 데이터 세트를 사용하거나 모델의 파라미터를 늘려 사이즈를 키워도 여전히 유효한 추론 결과를 도출하거나 더 나은 성능을 보여주고 나아가 개선의 여지가 여전히 남아있을 때 <code class="language-plaintext highlighter-rouge">“확장성이 높다”</code> 라고 표현한다. 저자들은 논문 초반에 콕 찝어서 컴퓨터 비전 분야의 <code class="language-plaintext highlighter-rouge">scalability</code> 높이는 것이 이번 모델 설계의 목표였다고 밝히고 있다. <code class="language-plaintext highlighter-rouge">범용성</code>은 신경망 모델 설계에서 가장 큰 화두가 되는데 도메인마다 정의하는 의미에 차이가 미세하게 존재한다. 따라서  <code class="language-plaintext highlighter-rouge">ViT</code>의 저자들이 말하는 <code class="language-plaintext highlighter-rouge">범용성</code>이 무엇을 의미하는지 알아보는 것은 구체적인 모델 구조를 이해하는데 큰 도움이 될 것이다.</p>

<h3 id="scalability-in-vit"><code class="language-plaintext highlighter-rouge">🧠 Scalability in ViT</code></h3>

<p>논문 초반부에서 다음과 같은 문장이 서술 되어있다.</p>

<p><code class="language-plaintext highlighter-rouge">“Our Vision Transformer (ViT) attains excellent results when pre-trained at sufficient scale and transferred to tasks with fewer datapoints"</code></p>

<p>이 구문이 <code class="language-plaintext highlighter-rouge">ViT</code> 의 <code class="language-plaintext highlighter-rouge">Scalability</code>를 가장 잘 설명하고 있다고 생각한다. 저자들이 말하는 범용성은 결국 <code class="language-plaintext highlighter-rouge">backbone</code> 구조의 활용을 의미한다. 자연어 처리에 익숙한 독자라면 쉽게 이해가 가능할 것이다. <code class="language-plaintext highlighter-rouge">Transformer</code>, <code class="language-plaintext highlighter-rouge">GPT</code>, <code class="language-plaintext highlighter-rouge">BERT</code>의 등장 이후, 자연어 처리는 범용성을 갖는 데이터 세트로 사전 훈련한 모델을 활용해 <code class="language-plaintext highlighter-rouge">Task-Agnostic</code>하게 하나의 <code class="language-plaintext highlighter-rouge">backbone</code>으로 거의 모든 Task를 수행할 수 있으며, 작은 사이즈의 데이터라도 상당히 높은 수준의 추론 성능을 낼 수 있었다. 그러나 당시 컴퓨터 비전의 메인이었던 <code class="language-plaintext highlighter-rouge">Conv</code> 기반 모델들은 파인튜닝해도 데이터 크기가 작으면 일반화 성능이 매우 떨어지고, Task에 따라서 다른 아키텍처를 갖는 모델을 새롭게 정의하거나 불러와 사용해야 하는 번거로움이 있었다. 예를 들면 <code class="language-plaintext highlighter-rouge">Image Classfication</code> 에는 <code class="language-plaintext highlighter-rouge">ResNet</code>, <code class="language-plaintext highlighter-rouge">Segmentation</code> 에는 <code class="language-plaintext highlighter-rouge">U-Net</code>, <code class="language-plaintext highlighter-rouge">Object Detection</code> 은 <code class="language-plaintext highlighter-rouge">YOLO</code> 를 사용하는 것처럼 말이다. 반면 자연어 처리는 사전 학습된 모델 하나로 모든 NLU, 심지어는 NLG Task도 수행할 수 있다. 저자들은 이러한 범용성을 컴퓨터 비전에도 이식 시키고 싶었던 것 같다. 그렇다면 먼저 자연어 처리에서 트랜스포머 계열이 범용성을 가질 수 있었던 이유는 무엇인지 간단히 살펴보자.</p>

<p>저자들은 <code class="language-plaintext highlighter-rouge">self-attention</code>(내적)의 효율성, 모델의 구조적 탁월성 그리고 <code class="language-plaintext highlighter-rouge">self-supervised task</code>의 존재를 꼽는다. 그럼 이것들이 왜 범용성을 높이는데 도움이 될까??</p>

<p><code class="language-plaintext highlighter-rouge">self-attention(내적)</code>은 행렬 간 곱셉으로 정의 되어 설계가 매우 간편하고 병렬로 한번에 처리하는 것이 가능하기 때문에 효율적으로 전체 데이터를 모두 고려한 연산 결과를 얻을 수 있다.</p>

<p><code class="language-plaintext highlighter-rouge">Multi-Head Attention</code> 구조는 여러 차원의 의미 관계를 동시에 포착하고 그것을 앙상블한 것과 같은(실제로는 MLP) 결과를 얻을 수 있다는 점에서 구조적으로 탁월하다.</p>

<p>마지막으로 <code class="language-plaintext highlighter-rouge">MLM</code>, <code class="language-plaintext highlighter-rouge">Auto-Regression(LM) Task</code>는 데이터 세트에 별도의 인간의 개입(라벨링)이 필요하지 않기 때문에 가성비 있게 데이터와 모델의 사이즈를 늘릴 수 있게 된다.<br />
이제 논문에서 트랜스포머 계열이 가진 범용성을 어떻게 비전 분야에 적용했는지 주목하면서 모델 구조를 하나 하나 살펴보자.</p>

<h3 id="modeling"><code class="language-plaintext highlighter-rouge">🌟 Modeling</code></h3>

<p align="center">
<img src="/assets/images/vision_transformer/modeling_overview.png" alt="ViT Model Structure" class="align-center image-caption" width="100%&quot;, height=&quot;50%" />
<strong><em><a href="https://arxiv.org/abs/2010.11929">ViT Model Structure</a></em></strong>
</p>

<ul>
  <li><strong>1) Transfer <code class="language-plaintext highlighter-rouge">Scalability</code> from pure <code class="language-plaintext highlighter-rouge">Transformer</code> to Computer Vision</strong>
    <ul>
      <li><strong>Overcome <code class="language-plaintext highlighter-rouge">reliance</code> on Convolution(<code class="language-plaintext highlighter-rouge">Inductive Bias</code>) in Computer Vision</strong></li>
      <li><strong>Apply Self-Attention &amp; Architecture from vanilla NLP Transformers as <code class="language-plaintext highlighter-rouge">closely</code> as possible</strong></li>
      <li><strong>Treat Image as sequence of text token</strong></li>
      <li><strong>Make $P$ sub-patches from whole image, playing same role as token in NLP Transformer</strong></li>
    </ul>
  </li>
</ul>

<p>저자들은 먼저 <code class="language-plaintext highlighter-rouge">Conv</code> 에 대한 의존을 버릴 것을 주장한다. <code class="language-plaintext highlighter-rouge">Conv</code>가 가진 <code class="language-plaintext highlighter-rouge">Inductive Bias</code> 때문에 파인튜닝 레벨에서 데이터 크기가 작으면 일반화 성능이 떨어지는 것이라고 설명하고 있다. 이 말을 이해하려면 <code class="language-plaintext highlighter-rouge">Inductive Bias</code>에 대해서 먼저 알아야 한다. <code class="language-plaintext highlighter-rouge">Inductive Bias</code>란, 주어진 데이터로부터 일반화 성능을 높이기 위해 <code class="language-plaintext highlighter-rouge">‘입력되는 데이터는 ~ 할 것이다’</code>, <code class="language-plaintext highlighter-rouge">‘이런 특징을 갖고 있을 것이다’</code>와 같은 가정, 가중치, 가설 등을 기계학습 알고리즘에 적용하는 것을 말한다.</p>

<p><code class="language-plaintext highlighter-rouge">Conv</code> 연산 자체 (가중치 공유, 풀링 있는 <code class="language-plaintext highlighter-rouge">Conv Block</code>이 <code class="language-plaintext highlighter-rouge">Invariance</code>)의 기본 가정은 <code class="language-plaintext highlighter-rouge">translation equivariance</code>, <code class="language-plaintext highlighter-rouge">locality</code>이다. 사실 저자의 주장을 이해하는데 <code class="language-plaintext highlighter-rouge">equivariance</code>와 <code class="language-plaintext highlighter-rouge">locality</code>의 뜻이 무엇인지 파악하는 것은 크게 의미가 없다 (<code class="language-plaintext highlighter-rouge">equivariance</code>와 <code class="language-plaintext highlighter-rouge">invariance</code>에 대해서는 다른 포스팅에서 자세히 살펴보도록 하겠다). <strong><u>중요한 것은 입력 데이터에 가정을 더한다는 점이다.</u></strong> 만약 주어진 입력이 미리 가정한 <code class="language-plaintext highlighter-rouge">Inductive Bias</code> 에 벗어난다면 어떻게 될까??</p>

<p>아마 오버피팅 되거나 모델 학습이 수렴성을 갖지 못하게 될 것이다. 이미지 데이터도 Task에 따라 필요한 <code class="language-plaintext highlighter-rouge">Inductive Bias</code>가 달라진다. 예를 들어 <code class="language-plaintext highlighter-rouge">Segmentation</code>, <code class="language-plaintext highlighter-rouge">Detection</code> 의 경우는 이미지 속 객체의 위치, 픽셀 사이의 <code class="language-plaintext highlighter-rouge">spatial variance</code> 정보가 매우 중요하다. 한편, <code class="language-plaintext highlighter-rouge">Classification</code>은 <code class="language-plaintext highlighter-rouge">spatial invariance</code>가 중요하다. 목표 객체의 위치와 주변 특징보다 타겟 자체를 신경망이 인식하는 것이 중요하기 때문이다. 따라서  <code class="language-plaintext highlighter-rouge">ViT</code> 저자들은 어떤 Bias던 상관없이 편향을 갖고 데이터를 본다는 것 자체에 의문을 표하며, 이미지 역시 <code class="language-plaintext highlighter-rouge">Inductive Bias</code>에서 벗어나, 주어진 데이터 전체 특징(패치) 사이의 관계를 파악하는 과정에서 <code class="language-plaintext highlighter-rouge">scalability</code>를 획득할 수 있다고 주장한다.</p>

<p>그래서 <code class="language-plaintext highlighter-rouge">Conv</code>의 대안으로 상대적으로 <code class="language-plaintext highlighter-rouge">Inductive Bias</code> 가 부족한 <code class="language-plaintext highlighter-rouge">Self-Attention</code>, <code class="language-plaintext highlighter-rouge">Transformer Architecture</code>를 사용한다. 두가지의 효용성에 대해서는 이미 위에서 언급했기 때문에 생략하고, 여기서 짚고 넘어가야할 점은 <code class="language-plaintext highlighter-rouge">Self-Attention</code>이 <code class="language-plaintext highlighter-rouge">Conv</code> 대비 <code class="language-plaintext highlighter-rouge">Inductive Bias</code>가 적다는 점이다. Self-Attention 과정에는 여러 연산, 스케일 조정값들이 포함되지만 본질적으로 <code class="language-plaintext highlighter-rouge">“내적”</code> 이 중심이다. 내적은 그 어떤 편향 (<code class="language-plaintext highlighter-rouge">Conv</code>와 대조하려고 이렇게 서술했지만 사실 <code class="language-plaintext highlighter-rouge">Position Embedding</code> 더하는 것도 일종의 약한 <code class="language-plaintext highlighter-rouge">Inductive Bias</code>)이 존재하지 않는다. 일단 주어진 모든 데이터에 대해서 내적값을 산출하고 그 다음에 관계가 있다고 생각되는 정보를 추리기 때문이다. <code class="language-plaintext highlighter-rouge">Conv</code> 때와 달리 <code class="language-plaintext highlighter-rouge">‘입력되는 데이터는 ~ 할 것이다’</code>, <code class="language-plaintext highlighter-rouge">‘이런 특징을 갖고 있을 것이다’</code> 라는 가정이 없다. 이번 포스팅의 마지막 쯤에서 다시 다루겠지만 그래서 <code class="language-plaintext highlighter-rouge">ViT</code>는 인스턴스 사이의 모든 관계를 뽑아보는 <code class="language-plaintext highlighter-rouge">Self-Attention(내적)</code> 을 기반으로 만들어졌기 때문에 이미지의 <code class="language-plaintext highlighter-rouge">Global Information</code>을 포착하는데 탁월한 성능을 보이고, <code class="language-plaintext highlighter-rouge">Conv</code> 는 <strong><u>“중요한 정보는 근처 픽셀에 몰려있다라는”</u></strong> <code class="language-plaintext highlighter-rouge">Inductive Bias</code>  덕분에 <code class="language-plaintext highlighter-rouge">Local Information</code>을 포착하는데 탁월한 성능을 낸다.</p>

<p>그렇다면 픽셀 하나 하나끼리 내적해준다는 것일까?? 아니다 여기서 논문의 제목이 <code class="language-plaintext highlighter-rouge">An Image Is Worth 16x16 Words</code> 인 이유가 드러난다. 일단 픽셀 하나 하나끼리 유사도를 측정하는 것이 유의미할까 생각해보자. 자연어의 토큰과 달리 이미지의 단일 픽셀 한 개는 큰 인사이트를 얻기 힘들다. 픽셀은 말 그대로 점 하나일 뿐이다. 픽셀을 여러 개 묶어 패치 단위로 묶는다면 이야기는 달라진다. 일정 크기 이상의 패치라면 자연어의 토큰처럼 그 자체로 어떤 의미를 담을 수 있다. 따라서 저자는 전체 이미지를 여러 개의 16x16 혹은 14x14 사이즈 패치로 나누어 하나 하나를 토큰으로 간주해 이미지 시퀀스를 만들고 그것을 모델의 Input으로 사용한다.</p>

<p align="center">
<img src="/assets/images/vision_transformer/class_diagram.png" alt="Class Diagram" class="align-center image-caption" width="50%&quot;, height=&quot;50%" />
<strong><em>Class Diagram</em></strong>
</p>

<p>모델 구조의 뼈대가 되는 내용들을 모두 살펴보았고, 위에서 서술한 내용을 구현하기 위해 어떤 블록들을 사용했는지 필자가 직접 논문을 보고 따라 구현한 코드와 함께 알아보도록 하자. 위에 첨부한 모델 모식도에 나와 있는 블록들 하나 하나 살펴볼 예정이다. 여담으로 Google Research의 Official Repo 역시 함께 참고했는데, 코드가 모두 구글이 요새 새롭게 미는 <code class="language-plaintext highlighter-rouge">Jax</code>, <code class="language-plaintext highlighter-rouge">Flax</code> 로 구현 되어 있었다. 파이토치나 좀 써본 필자 입장에서는 정말 … 지옥불을 경험했다. 오늘도 다시 한 번 페이스북 파이토치 개발팀에 큰절 드리고 싶다.</p>

<h4 id="linear-projection-of-flattened-patches"><code class="language-plaintext highlighter-rouge">🔬 Linear Projection of Flattened Patches</code></h4>

\[x_p \in R^{N * (P^2•C)}\]

\[z_{0} = [x_{class}; x_p^1E;x_p^2E;x_p^3E....x_p^NE]\]

\[N = \frac{H*W}{P*P}\]

<p><code class="language-plaintext highlighter-rouge">ViT</code>의 입력 임베딩을 생성하는 역할을 한다. <code class="language-plaintext highlighter-rouge">ViT</code>는 $x \in R^{H * W * C}$(H: height, W: width, C: channel)의 형상을 갖는 이미지를 입력으로 받아 가로 세로 길이가 $P$, 채널 개수 $C$인 $N$개의 패치로 <code class="language-plaintext highlighter-rouge">reshape</code> 한다. 필자가 코드 구현 중 가장 혼동한 부분이 바로 패치 개수 $N$이었다. 직관적으로 패치 개수라고 하면, 전체 이미지 사이즈에서 패치 크기를 나눈 값이라고 생각하기 쉽기 때문이다. 예를 들면 <code class="language-plaintext highlighter-rouge">512x512</code>짜리 이미지를 <code class="language-plaintext highlighter-rouge">16x16</code> 사이즈의 패치로 나눈다고 해보자. 필자는 단순히 <code class="language-plaintext highlighter-rouge">512/16=32</code> 라는 결과를 이용해 $N=32$로 설정하고 실험을 진행하다가 텐서 차원이 맞지 않아 발생하는 에러 로그를 마주했었다. 그러나 논문 속 수식을 확인해보면,  $H * W / P^2$이 바로 패치 개수$N$으로 정의된다. 그래서 만약 <code class="language-plaintext highlighter-rouge">512x512</code> 사이즈의 <code class="language-plaintext highlighter-rouge">RGB</code> 이미지 <code class="language-plaintext highlighter-rouge">10장</code>을 ViT 입력 임베딩에 맞게 차원 변환한다면 결과는 <code class="language-plaintext highlighter-rouge">[10, 3, 1024, 768]</code> 이 될 것이다. (이 예시를 앞으로 계속 이용하겠다)</p>

<p>이렇게 차원을 바꿔준 이미지를 <code class="language-plaintext highlighter-rouge">nn.Linear((channels * patch_size**2), dim_model)</code> 를 통해 <code class="language-plaintext highlighter-rouge">ViT</code>의 임베딩 레이어에 선형 투영해준다. 여기서 자연어 처리와 파이토치를 자주 사용하시는 독자라면 왜 <code class="language-plaintext highlighter-rouge">nn.Embedding</code>을 사용하지 않았는가 의문을 가질 수 있다.</p>

<p>자연어 처리에서 입력 임베딩을 만들때는 모델의 토크나이저에 의해 사전 정의된 vocab의 사이즈가 입력 문장에 속한 토큰 개수보다 훨씬 크기 때문에 데이터 룩업 테이블 방식의 <code class="language-plaintext highlighter-rouge">nn.Embedding</code> 을 사용하게 된다. 이게 무슨 말이냐면, 토크나이저에 의해 사전에 정의된 <code class="language-plaintext highlighter-rouge">vocab</code> 전체가 <code class="language-plaintext highlighter-rouge">nn.Embedding(vocab_size, dim_model)</code>로 투영 되어 가로는 vocab 사이즈, 세로는 모델의 차원 크기에 해당하는 룩업 테이블이 생성되고, 내가 입력한 토큰들은 전체 <code class="language-plaintext highlighter-rouge">vocab</code>의 일부분일테니 전체 임베딩 룩업 테이블에서 내가 임베딩하고 싶은 토큰들의 인덱스만 알아낸다는 것이다.</p>

<p>그래서 <code class="language-plaintext highlighter-rouge">nn.Embedding</code> 에 정의된 차원과 실제 입력 데이터의 차원이 맞지 않아도 함수가 동작하게 되는 것이다. 그러나 비전의 경우, 사전에 정의된 <code class="language-plaintext highlighter-rouge">vocab</code>이라는 개념이 전혀 없고 입력 이미지 역시 항상 고정된 크기의 차원으로 들어오기 때문에 <code class="language-plaintext highlighter-rouge">nn.Embedding</code>이 아닌  <code class="language-plaintext highlighter-rouge">nn.Linear</code> 을 사용해 곧바로 선형 투영을 구현한 것이다. 두 메서드에 대한 자세한 비교는 파이토치 관련 포스트에서 다시 한 번 자세히 다루도록 하겠다.</p>

<p>한편, <code class="language-plaintext highlighter-rouge">Position Embedding</code>을 더하기 전, <code class="language-plaintext highlighter-rouge">Input Embedding</code>의 차원은 <code class="language-plaintext highlighter-rouge">[10, 1024, 1024]</code> 이 된다. 지금까지 설명한 부분(<code class="language-plaintext highlighter-rouge">Linear Projection of Flattened Patches</code> )을 파이토치 코드로 구현하면 다음과 같다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">VisionTransformer</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="p">...</span>
    <span class="n">중략</span>
    <span class="p">...</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">num_patches</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">image_size</span> <span class="o">/</span> <span class="n">patch_size</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">input_embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">((</span><span class="n">channels</span> <span class="o">*</span> <span class="n">patch_size</span><span class="o">**</span><span class="mi">2</span><span class="p">),</span> <span class="n">dim_model</span><span class="p">)</span> <span class="c1"># Projection Layer for Input Embedding
</span>    <span class="p">...</span>
    <span class="n">중략</span>
    <span class="p">...</span>  
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">any</span><span class="p">:</span>
        <span class="s">""" For cls pooling """</span>
        <span class="k">assert</span> <span class="n">inputs</span><span class="p">.</span><span class="n">ndim</span> <span class="o">!=</span> <span class="mi">4</span><span class="p">,</span> <span class="sa">f</span><span class="s">"Input shape should be [BS, CHANNEL, IMAGE_SIZE, IMAGE_SIZE], but got </span><span class="si">{</span><span class="n">inputs</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="s">"</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">inputs</span> 
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">input_embedding</span><span class="p">(</span>
            <span class="n">x</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="p">.</span><span class="n">num_patches</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">patch_size</span><span class="o">**</span><span class="mi">2</span> <span class="o">*</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span> <span class="c1"># Projection Layer for Input Embedding
</span>        <span class="p">)</span>
				<span class="n">cls_token</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">,</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>  <span class="c1"># can change init method
</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cat</span><span class="p">([</span><span class="n">cls_token</span><span class="p">,</span> <span class="n">x</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="p">...</span>
</code></pre></div></div>

<p>임베딩 레이어를 객체로 따로 구현해도 되지만, 필자는 굳이 추상화가 필요하지 않다고 생각해 ViT의 최상위 클래스인 <code class="language-plaintext highlighter-rouge">VisionTransformer</code>의 <code class="language-plaintext highlighter-rouge">forward</code> 메서드 맨 초반부에 구현하게 되었다. 입력 받은 이미지 텐서를 <code class="language-plaintext highlighter-rouge">torch.reshape</code> 을 통해 <code class="language-plaintext highlighter-rouge">[패치 개수, 픽셀개수*채널개수]</code> 로 바꾼 뒤, 미리 정의해둔 <code class="language-plaintext highlighter-rouge">self.input_embedding</code> 에 매개변수로 전달해 <code class="language-plaintext highlighter-rouge">“위치 임베딩”</code> 값이 더해지기 전 <code class="language-plaintext highlighter-rouge">Input Embedding</code>을 만든다.</p>

<p>한편, <code class="language-plaintext highlighter-rouge">CLS Pooling</code>을 위해 마지막에 <code class="language-plaintext highlighter-rouge">[batch, 1, image_size]</code> 의 차원을 갖는 <code class="language-plaintext highlighter-rouge">cls_token</code> 을 정의해 패치 시퀀스와 <code class="language-plaintext highlighter-rouge">concat</code> (맨 앞에)해준다. 이 때 논문에 제시된 수식 상, <code class="language-plaintext highlighter-rouge">CLS Token</code>은 선형 투영하지 않으며, 패치 시퀀스에 선형 투영이 이뤄지고 난 뒤에 맨 앞에 <code class="language-plaintext highlighter-rouge">Concat</code> 하게 된다.</p>

<p><code class="language-plaintext highlighter-rouge">CLS Token</code>까지 더한 최종 <code class="language-plaintext highlighter-rouge">Input Embedding</code> 의 텐서 차원은 <code class="language-plaintext highlighter-rouge">[10, 1025, 1024]</code> 가 된다.</p>

<h4 id="positional-embedding"><code class="language-plaintext highlighter-rouge">🔢 Positional Embedding</code></h4>

\[E_{pos} \in R^{(N+1)*D}\]

<p>이미지를 패치 단위의 임베딩으로 만들었다면 이제 위치 임베딩을 정의해서 더해주면 모식도 속 <code class="language-plaintext highlighter-rouge">Embedded Patches</code> , 즉 인코더에 들어갈 최종 <code class="language-plaintext highlighter-rouge">Patch Embedding</code> 이 완성 된다. 위치 임베딩을 만드는 방식은 기존 <code class="language-plaintext highlighter-rouge">Transformer</code>, <code class="language-plaintext highlighter-rouge">BERT</code> 와 동일하다. 아래 <code class="language-plaintext highlighter-rouge">VisionEncoder</code> 클래스를 구현한 코드를 살펴보자.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="k">class</span> <span class="nc">VisionEncoder</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="p">...</span>
    <span class="n">중략</span>
    <span class="p">...</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">positional_embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Embedding</span><span class="p">((</span><span class="bp">self</span><span class="p">.</span><span class="n">num_patches</span> <span class="o">+</span> <span class="mi">1</span><span class="p">),</span> <span class="n">dim_model</span><span class="p">)</span>  <span class="c1"># add 1 for cls token
</span>    <span class="p">...</span>
    <span class="n">중략</span>
    <span class="p">...</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
        <span class="n">layer_output</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">pos_x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">num_patches</span> <span class="o">+</span> <span class="mi">1</span><span class="p">).</span><span class="n">repeat</span><span class="p">(</span><span class="n">inputs</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]).</span><span class="n">to</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>  <span class="c1"># inputs.shape[0] = Batch Size of Input
</span>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">dropout</span><span class="p">(</span>
            <span class="n">inputs</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">positional_embedding</span><span class="p">(</span><span class="n">pos_x</span><span class="p">)</span>
        <span class="p">)</span>
    <span class="p">...</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">Input Embedding</code>과 다르게 위치 임베딩은 <code class="language-plaintext highlighter-rouge">nn.Embedding</code>으로 구현했는데, 여기서도 사실 <code class="language-plaintext highlighter-rouge">nn.Linear</code>를 사용해도 무방하다. 그것보다 <code class="language-plaintext highlighter-rouge">nn.Embedding</code>의 입력 차원인 <code class="language-plaintext highlighter-rouge">self.num_patches + 1</code> 에 주목해보자. 왜 1을 더해준 값을 사용했을까??</p>

<p><code class="language-plaintext highlighter-rouge">ViT</code>는 BERT의 <code class="language-plaintext highlighter-rouge">CLS Token Pooling</code> 을 차용하기 위해 패치 시퀀스 맨 앞에 CLS 토큰을 추가하기 때문이다. 이렇게 추가된 <code class="language-plaintext highlighter-rouge">CLS Token</code>은 인코더를 거쳐 최종 <code class="language-plaintext highlighter-rouge">MLP Head</code>에 흘러들어가 로짓으로 변환된다. 만약 독자께서 <code class="language-plaintext highlighter-rouge">CLS Token Pooling</code> 대신 다른 풀링 방식을 사용할거라면 1을 추가해줄 필요는 없다.</p>

<p>애초에 객체 인스턴스 초기화 당시에 <code class="language-plaintext highlighter-rouge">CLS Token</code> 을 추가를 반영한 값을 전달하면 되지 않는가하는 의문이 들 수도 있다. 하지만 <code class="language-plaintext highlighter-rouge">VisionEncoder</code> 객체 인스턴스 초기화 당시에는 <code class="language-plaintext highlighter-rouge">num_patches</code> 값으로 <code class="language-plaintext highlighter-rouge">CLS Token</code>이 추가되기 이전 값(+1 반영이 안되어 있음)을 전달하도록 설계 되어 있어서  <code class="language-plaintext highlighter-rouge">CLS Pooling</code>을 사용할거라면 1 추가를 꼭 해줘야 한다.</p>

<p align="center">
<img src="/assets/images/vision_transformer/insight5.png" alt="Performance Table by making Position Embedding method" class="align-center image-caption" width="75%&quot;, height=&quot;50%" />
<strong><em><a href="https://arxiv.org/abs/2010.11929">Performance Table by making Position Embedding method</a></em></strong>
</p>

<p>한편 저자는 <code class="language-plaintext highlighter-rouge">2D Postion Embedding</code>, <code class="language-plaintext highlighter-rouge">Relative Position Embedding</code> 방식도 적용해봤지만, 구현 복잡도 &amp; 연산량 대비 성능 향상 폭이 매우 미미해 일반적인 <code class="language-plaintext highlighter-rouge">1D Position Embedding</code>을 사용할 것을 추천하고 있다.</p>

<h4 id="-multi-head-attention"><code class="language-plaintext highlighter-rouge">👩‍👩‍👧‍👦 Multi-Head Attention</code></h4>

\[z_t^{'} = MSA(LN(z_{t-1}) + z_{t-1})\]

\[MSA(z) = [SA_1();SA_2();SA_3()...SA_k()]*U_{msa}, \ \ U_{msa} \in R^{(k*D_h)*D} \\\]

<p>트랜스포머 계열 모델의 핵심 <code class="language-plaintext highlighter-rouge">Multi-Head Self-Attention</code> 모듈에 대해서 알아보자. 사실 기존 자연어 처리 <code class="language-plaintext highlighter-rouge">Transformer</code>, <code class="language-plaintext highlighter-rouge">BERT</code> 등의 동작 방식과 완전히 동일하며, 코드로 구현할 때 역시 동일하게 만들어주면 된다. 자세한 원리와 동작 방식은 <strong><u>Attention Is All You Need</u></strong> 리뷰 포스트에서 설명했기 때문에 생략하고 넘어가겠다. 한편 파이토치로 구현한 <code class="language-plaintext highlighter-rouge">Multi-Head Self-Attention</code> 블럭에 대한 코드는 다음과 같다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">scaled_dot_product_attention</span><span class="p">(</span><span class="n">q</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">k</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">v</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">dot_scale</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
    <span class="s">"""
    Scaled Dot-Product Attention
    Args:
        q: query matrix, shape (batch_size, seq_len, dim_head)
        k: key matrix, shape (batch_size, seq_len, dim_head)
        v: value matrix, shape (batch_size, seq_len, dim_head)
        dot_scale: scale factor for Q•K^T result, same as pure transformer
    Math:
        A = softmax(q•k^t/sqrt(D_h)), SA(z) = Av
    """</span>
    <span class="n">attention_dist</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="n">softmax</span><span class="p">(</span>
        <span class="n">torch</span><span class="p">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">))</span> <span class="o">/</span> <span class="n">dot_scale</span><span class="p">,</span>
        <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span>
    <span class="p">)</span>
    <span class="n">attention_matrix</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">attention_dist</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">attention_matrix</span>

<span class="k">class</span> <span class="nc">AttentionHead</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="s">"""
    In this class, we implement workflow of single attention head
    Args:
        dim_model: dimension of model's latent vector space, default 1024 from official paper
        dim_head: dimension of each attention head, default 64 from official paper (1024 / 16)
        dropout: dropout rate, default 0.1
    Math:
        [q,k,v]=z•U_qkv, A = softmax(q•k^t/sqrt(D_h)), SA(z) = Av
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim_model</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span>  <span class="mi">1024</span><span class="p">,</span> <span class="n">dim_head</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">64</span><span class="p">,</span> <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">AttentionHead</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span> <span class="o">=</span> <span class="n">dim_model</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dim_head</span> <span class="o">=</span> <span class="n">dim_head</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">dropout</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dot_scale</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">dim_head</span><span class="p">))</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">fc_q</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">dim_head</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">fc_k</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">dim_head</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">fc_v</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">dim_head</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="n">attention_matrix</span> <span class="o">=</span> <span class="n">scaled_dot_product_attention</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">fc_q</span><span class="p">(</span><span class="n">x</span><span class="p">),</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">fc_k</span><span class="p">(</span><span class="n">x</span><span class="p">),</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">fc_v</span><span class="p">(</span><span class="n">x</span><span class="p">),</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">dot_scale</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">attention_matrix</span>

<span class="k">class</span> <span class="nc">MultiHeadAttention</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="s">"""
    In this class, we implement workflow of Multi-Head Self-Attention
    Args:
        dim_model: dimension of model's latent vector space, default 1024 from official paper
        num_heads: number of heads in MHSA, default 16 from official paper for ViT-Large
        dim_head: dimension of each attention head, default 64 from official paper (1024 / 16)
        dropout: dropout rate, default 0.1
    Math:
        MSA(z) = [SA1(z); SA2(z); · · · ; SAk(z)]•Umsa
    Reference:
        https://arxiv.org/abs/2010.11929
        https://arxiv.org/abs/1706.03762
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim_model</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1024</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span> <span class="n">dim_head</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">64</span><span class="p">,</span> <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MultiHeadAttention</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span> <span class="o">=</span> <span class="n">dim_model</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dim_head</span> <span class="o">=</span> <span class="n">dim_head</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">dropout</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">attention_heads</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">ModuleList</span><span class="p">(</span>
            <span class="p">[</span><span class="n">AttentionHead</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">dim_head</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">dropout</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">num_heads</span><span class="p">)]</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">fc_concat</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="s">""" x is already passed nn.Layernorm """</span>
        <span class="k">assert</span> <span class="n">x</span><span class="p">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">3</span><span class="p">,</span> <span class="sa">f</span><span class="s">'Expected (batch, seq, hidden) got </span><span class="si">{</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="s">'</span>
        <span class="n">attention_output</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">fc_concat</span><span class="p">(</span>
            <span class="n">torch</span><span class="p">.</span><span class="n">cat</span><span class="p">([</span><span class="n">head</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">head</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">attention_heads</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># concat all dim_head = num_heads * dim_head
</span>        <span class="p">)</span>
        <span class="k">return</span> <span class="n">attention_output</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">MultiHeadAttention</code>을 가장 최상위 객체로 두고, 하위에 <code class="language-plaintext highlighter-rouge">AttentionHead</code>객체를 따로 구현했다. 이렇게 구현하면, 어텐션 해드별로 쿼리, 키, 벨류 선영 투영 행렬(<code class="language-plaintext highlighter-rouge">nn.Linear</code>)을 따로 구현해줄 필요가 없어지며, <code class="language-plaintext highlighter-rouge">nn.ModuleList</code> 를 통해 개별 해드를 한 번에 그룹핑하고 <code class="language-plaintext highlighter-rouge">loop</code> 를 통해 출력 결과를 <code class="language-plaintext highlighter-rouge">concat</code> 해줄 수 있어 복잡하고 많은 에러를 유발하는 <strong><u>텐서 차원 조작을 피할 수 있으며</u></strong>, 코드의 가독성이 올라가는 효과가 있다.</p>

<h4 id="️-mlp"><code class="language-plaintext highlighter-rouge">🗳️ MLP</code></h4>

\[z_{t} = MLP(LN(z_{t}^{'}) + z_{t}^{'})\]

<p>이름만 <code class="language-plaintext highlighter-rouge">MLP</code>로 바뀌었을 뿐, 기존 트랜스포머의 피드 포워드 블럭과 동일한 역할을 한다. 역시 자세한 동작 방식은 여기 포스트에서 확인하자. 파이토치로 구현한 코드는 다음과 같다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">MLP</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="s">"""
    Class for MLP module in ViT-Large
    Args:
        dim_model: dimension of model's latent vector space, default 512
        dim_mlp: dimension of FFN's hidden layer, default 2048 from official paper
        dropout: dropout rate, default 0.1
    Math:
        MLP(x) = MLP(LN(x))+x
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim_model</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1024</span><span class="p">,</span> <span class="n">dim_mlp</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">4096</span><span class="p">,</span> <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MLP</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">mlp</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">dim_model</span><span class="p">,</span> <span class="n">dim_mlp</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">GELU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">dropout</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">dim_mlp</span><span class="p">,</span> <span class="n">dim_model</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">dropout</span><span class="p">),</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">mlp</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</code></pre></div></div>

<p>특이한 점은 <code class="language-plaintext highlighter-rouge">Activation Function</code>으로 <code class="language-plaintext highlighter-rouge">GELU</code>를 사용(기존 트랜스포머는 <code class="language-plaintext highlighter-rouge">RELU</code>)했다는 점이다.</p>

<h4 id="-vision-encoder-layer"><code class="language-plaintext highlighter-rouge">📘 Vision Encoder Layer</code></h4>

<p><code class="language-plaintext highlighter-rouge">ViT</code> 인코더 블럭 1개에 해당하는 하위 모듈과 동작을 구현한 객체이다. 구현한 코드는 아래와 같다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">VisionEncoderLayer</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="s">"""
    Class for encoder_model module in ViT-Large
    In this class, we stack each encoder_model module (Multi-Head Attention, Residual-Connection, Layer Normalization, MLP)
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim_model</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1024</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">16</span><span class="p">,</span> <span class="n">dim_mlp</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">4096</span><span class="p">,</span> <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">VisionEncoderLayer</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">self_attention</span> <span class="o">=</span> <span class="n">MultiHeadAttention</span><span class="p">(</span>
            <span class="n">dim_model</span><span class="p">,</span>
            <span class="n">num_heads</span><span class="p">,</span>
            <span class="nb">int</span><span class="p">(</span><span class="n">dim_model</span> <span class="o">/</span> <span class="n">num_heads</span><span class="p">),</span>
            <span class="n">dropout</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">layer_norm</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">dim_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">dropout</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">mlp</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">(</span>
            <span class="n">dim_model</span><span class="p">,</span>
            <span class="n">dim_mlp</span><span class="p">,</span>
            <span class="n">dropout</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="n">ln_x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">layer_norm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">residual_x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">dropout</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">self_attention</span><span class="p">(</span><span class="n">ln_x</span><span class="p">))</span> <span class="o">+</span> <span class="n">x</span>

        <span class="n">ln_x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">layer_norm</span><span class="p">(</span><span class="n">residual_x</span><span class="p">)</span>
        <span class="n">fx</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">layer_norm</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">mlp</span><span class="p">(</span><span class="n">ln_x</span><span class="p">)</span> <span class="o">+</span> <span class="n">residual_x</span><span class="p">)</span>  <span class="c1"># from official paper &amp; code by Google Research
</span>        <span class="k">return</span> <span class="n">fx</span>
</code></pre></div></div>

<p>특이점은 마지막 <code class="language-plaintext highlighter-rouge">MLP Layer</code>와 <code class="language-plaintext highlighter-rouge">Residual</code> 결과를 더한 뒤, 다음 인코더 블록에 전달하기 전에 층 정규화를 한 번 더 적용한다는 것이다. 모델 모식도에는 나와 있지 않지만, 본문에 해당 내용이 실려 있다.</p>

<h4 id="-visionencoder"><code class="language-plaintext highlighter-rouge">📚 VisionEncoder</code></h4>

<p>입력 이미지를 <code class="language-plaintext highlighter-rouge">Patch Embedding</code>으로 인코딩 하고 N개의 <code class="language-plaintext highlighter-rouge">VisionEncoderLayer</code>를 쌓기 위해 구현된 객체이다. <code class="language-plaintext highlighter-rouge">Patch Embedding</code>을 만드는 부분은 이미 위에서 설명했기 때문에 넘어가고, 인코더 블럭을 N개 쌓는 방법은 역시나 <code class="language-plaintext highlighter-rouge">nn.ModuleList</code> 를 사용하면 간편하게 구현할 수 있다. 아래 코드를 살펴보자.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">VisionEncoder</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="s">"""
    In this class, encode input sequence(Image) and then we stack N VisionEncoderLayer
    This model is implemented by cls pooling method for classification
    First, we define "positional embedding" and then add to input embedding for making patch embedding
    Second, forward patch embedding to N EncoderLayer and then get output embedding
    Args:
        num_patches: number of patches in input image =&gt; (image_size / patch_size)**2
        N: number of EncoderLayer, default 24 for large model
    """</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_patches</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">N</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">24</span><span class="p">,</span> <span class="n">dim_model</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1024</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">16</span><span class="p">,</span> <span class="n">dim_mlp</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">4096</span><span class="p">,</span> <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">VisionEncoder</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">num_patches</span> <span class="o">=</span> <span class="n">num_patches</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">scale</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">dim_model</span><span class="p">))</span>  <span class="c1"># scale factor for input embedding
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">positional_embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Embedding</span><span class="p">((</span><span class="bp">self</span><span class="p">.</span><span class="n">num_patches</span> <span class="o">+</span> <span class="mi">1</span><span class="p">),</span> <span class="n">dim_model</span><span class="p">)</span>  <span class="c1"># add 1 for cls token
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">num_layers</span> <span class="o">=</span> <span class="n">N</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span> <span class="o">=</span> <span class="n">dim_model</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dim_mlp</span> <span class="o">=</span> <span class="n">dim_mlp</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">dropout</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">encoder_layers</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">ModuleList</span><span class="p">(</span>
            <span class="p">[</span><span class="n">VisionEncoderLayer</span><span class="p">(</span><span class="n">dim_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">dim_mlp</span><span class="p">,</span> <span class="n">dropout</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">num_layers</span><span class="p">)]</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
        <span class="n">layer_output</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">pos_x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">num_patches</span> <span class="o">+</span> <span class="mi">1</span><span class="p">).</span><span class="n">repeat</span><span class="p">(</span><span class="n">inputs</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]).</span><span class="n">to</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">dropout</span><span class="p">(</span>
            <span class="n">inputs</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">positional_embedding</span><span class="p">(</span><span class="n">pos_x</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">encoder_layers</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            <span class="n">layer_output</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">layer_output</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">stack</span><span class="p">(</span><span class="n">layer_output</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">).</span><span class="n">to</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>  <span class="c1"># For Weighted Layer Pool: [N, BS, SEQ_LEN, DIM]
</span>        <span class="k">return</span> <span class="n">x</span><span class="p">,</span> <span class="n">layer_output</span>
</code></pre></div></div>

<h4 id="-visiontransformer"><code class="language-plaintext highlighter-rouge">🤖 VisionTransformer</code></h4>

<p align="center">
<img src="/assets/images/vision_transformer/model_variant.png" alt="ViT Model Variant" class="align-center image-caption" width="75%&quot;, height=&quot;50%" />
<strong><em><a href="https://arxiv.org/abs/2010.11929">ViT Model Variant</a></em></strong>
</p>

<p><code class="language-plaintext highlighter-rouge">ViT</code> 모델의 가장 최상위 객체로, 앞에서 설명한 모든 모듈들의 동작이 이뤄지는 곳이다. 사용자로부터 하이퍼파라미터를 입력 받아 모델의 크기, 깊이, 패치 크기, 이미지 임베딩 추출 방식을 지정한다. 그리고 입력 이미지를 전달받아 임베딩을 만들고 인코더에 전달한 뒤, <code class="language-plaintext highlighter-rouge">MLP Head</code> 를 통해 최종 예측 결과를 반환하는 역할을 한다.</p>

<p>이미지 임베딩 추출 방식은 <code class="language-plaintext highlighter-rouge">Linear Projection</code>과 <code class="language-plaintext highlighter-rouge">Convolution</code>이 있다. 전자가 논문에서 말하는 일반적인 <code class="language-plaintext highlighter-rouge">ViT</code>를 말하며 후자는 저자가 <code class="language-plaintext highlighter-rouge">Hybrid ViT</code>라고 따로 명명하는 모델이다. 임베딩 추출 방식 이외에 다른 차이는 전혀 없다. <code class="language-plaintext highlighter-rouge">extractor</code> 매개변수를 통해 임베딩 추출 방식을 지정할 수 있으니 아래 코드를 확인해보자.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">VisionTransformer</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="s">"""
    Main class for ViT of cls pooling, Pytorch implementation
    We implement pure ViT, Not hybrid version which is using CNN for extracting patch embedding
    input must be [BS, CHANNEL, IMAGE_SIZE, IMAGE_SIZE]
    In NLP, input_sequence is always smaller than vocab size
    But in vision, input_sequence is always same as image size, not concept of vocab in vision
    So, ViT use nn.Linear instead of nn.Embedding for input_embedding
    Args:
        num_classes: number of classes for classification task
        image_size: size of input image, default 512
        patch_size: size of patch, default 16 from official paper for ViT-Large
        extractor: option for feature extractor, default 'base' which is crop &amp; just flatten
                   if you want to use Convolution for feature extractor, set extractor='cnn' named hybrid ver in paper
        classifier: option for pooling method, default token meaning that do cls pooling
                    if you want to use mean pooling, set classifier='mean'
        mode: option for train type, default fine-tune, if you want pretrain, set mode='pretrain'
              In official paper &amp; code by Google Research, they use different classifier head for pretrain, fine-tune
    Math:
        image2sequence: [batch, channel, image_size, image_size] -&gt; [batch, patch, patch_size^2*channel]
        input_embedding: R^(P^2 ·C)×D
    Reference:
        https://arxiv.org/abs/2010.11929
        https://arxiv.org/abs/1706.03762
        https://github.com/google-research/vision_transformer/blob/main/vit_jax/models_vit.py#L184
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span>
            <span class="n">num_classes</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
            <span class="n">channels</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span>
            <span class="n">image_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">512</span><span class="p">,</span>
            <span class="n">patch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">16</span><span class="p">,</span>
            <span class="n">num_layers</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">24</span><span class="p">,</span>
            <span class="n">dim_model</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1024</span><span class="p">,</span>
            <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">16</span><span class="p">,</span>
            <span class="n">dim_mlp</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">4096</span><span class="p">,</span>
            <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
            <span class="n">extractor</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s">'base'</span><span class="p">,</span>
            <span class="n">classifier</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s">'token'</span><span class="p">,</span>
            <span class="n">mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s">'fine_tune'</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">VisionTransformer</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">num_patches</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">image_size</span> <span class="o">/</span> <span class="n">patch_size</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">num_layers</span> <span class="o">=</span> <span class="n">num_layers</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">patch_size</span> <span class="o">=</span> <span class="n">patch_size</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span> <span class="o">=</span> <span class="n">dim_model</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dim_mlp</span> <span class="o">=</span> <span class="n">dim_mlp</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">dropout</span><span class="p">)</span>

        <span class="c1"># Input Embedding
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">extractor</span> <span class="o">=</span> <span class="n">extractor</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">input_embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">((</span><span class="n">channels</span> <span class="o">*</span> <span class="n">patch_size</span><span class="o">**</span><span class="mi">2</span><span class="p">),</span> <span class="n">dim_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">conv</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Conv2d</span><span class="p">(</span>
            <span class="n">in_channels</span><span class="o">=</span><span class="n">channels</span><span class="p">,</span>
            <span class="n">out_channels</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span><span class="p">,</span>
            <span class="n">kernel_size</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">patch_size</span><span class="p">,</span>
            <span class="n">stride</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">patch_size</span>
        <span class="p">)</span>

        <span class="c1"># Encoder Multi-Head Self-Attention
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">VisionEncoder</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">num_patches</span><span class="p">,</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">num_layers</span><span class="p">,</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span><span class="p">,</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">num_heads</span><span class="p">,</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">dim_mlp</span><span class="p">,</span>
            <span class="n">dropout</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">classifier</span> <span class="o">=</span> <span class="n">classifier</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">pretrain_classifier</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">Tanh</span><span class="p">(),</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">fine_tune_classifier</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">mode</span> <span class="o">=</span> <span class="n">mode</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">any</span><span class="p">:</span>
        <span class="s">""" For cls pooling """</span>
        <span class="k">assert</span> <span class="n">inputs</span><span class="p">.</span><span class="n">ndim</span> <span class="o">!=</span> <span class="mi">4</span><span class="p">,</span> <span class="sa">f</span><span class="s">"Input shape should be [BS, CHANNEL, IMAGE_SIZE, IMAGE_SIZE], but got </span><span class="si">{</span><span class="n">inputs</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="s">"</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">inputs</span>

        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">extractor</span> <span class="o">==</span> <span class="s">'cnn'</span><span class="p">:</span>
            <span class="c1"># self.conv(x).shape == [batch, dim, image_size/patch_size, image_size/patch_size]
</span>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">conv</span><span class="p">(</span><span class="n">x</span><span class="p">).</span><span class="n">reshape</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">num_patches</span><span class="o">**</span><span class="mi">2</span><span class="p">).</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># self.extractor == 'base':
</span>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">input_embedding</span><span class="p">(</span>
                <span class="n">x</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="p">.</span><span class="n">num_patches</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">patch_size</span><span class="o">**</span><span class="mi">2</span> <span class="o">*</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
            <span class="p">)</span>
        <span class="n">cls_token</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">,</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>  <span class="c1"># can change init method
</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cat</span><span class="p">([</span><span class="n">cls_token</span><span class="p">,</span> <span class="n">x</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="n">x</span><span class="p">,</span> <span class="n">layer_output</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># output
</span>
        <span class="c1"># classification
</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">,</span> <span class="p">:]</span>  <span class="c1"># select cls token, which is position 0 in sequence
</span>        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">mode</span> <span class="o">==</span> <span class="s">'fine_tune'</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">fine_tune_classifier</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">mode</span> <span class="o">==</span> <span class="s">'pretrain'</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">fine_tune_classifier</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">pretrain_classifier</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">x</span>
</code></pre></div></div>

<p>한편, 코드에서 눈여겨봐야 할 점은 <code class="language-plaintext highlighter-rouge">MLP Head</code>로, 저자는 <code class="language-plaintext highlighter-rouge">pre-train</code> 시점과 <code class="language-plaintext highlighter-rouge">fine-tune</code> 시점에 서로 다른 <code class="language-plaintext highlighter-rouge">Classifier Head</code>를 사용한다. 전자에는 <code class="language-plaintext highlighter-rouge">Activation Function</code> 1개와 두 개의 <code class="language-plaintext highlighter-rouge">MLP Layer</code>를 사용하고, 후자에는 1개의 <code class="language-plaintext highlighter-rouge">MLP Layer</code>를 사용한다.</p>

<p>다만, <code class="language-plaintext highlighter-rouge">pretrain_classifier</code>의 입출력 차원에 대한 정확한 수치를 논문이나 official repo code를 확인해도 찾을 수 없었다, 그래서 임시로 모델의 차원과 똑같이 세팅하게 되었다.</p>

<p>또한 저자는 <code class="language-plaintext highlighter-rouge">CLS Pooling</code>과 더불어 <code class="language-plaintext highlighter-rouge">GAP</code> 방식도 제시하는데, <code class="language-plaintext highlighter-rouge">GAP</code> 방식은 추후에 따로 추가가 필요하다. 그리고 사전 훈련과 파인 튜닝 모두 분류 테스크를 수행했는데 (심지어 같은 데이터 세트를 사용함) 왜 굳이 서로 다른 <code class="language-plaintext highlighter-rouge">Classifier Head</code>를 정의했는지 의도를 알 수 없어 논문을 다시 읽어봤지만, 이유에 대해서 상세히 언급하는 부분이 없었다.</p>

<p><code class="language-plaintext highlighter-rouge">ViT</code>는 입력 임베딩을 정의하는 부분을 제외하면 저자의 의도대로 기존 트랜스포머와 동일한 모델 구조를 가졌다. 완전히 다른 데이터인 이미지와 텍스트에 같은 구조의 모델을 적용한다는 것이 정말 쉽지 않아 보였는데, 패치 개념을 만들어 자연어의 토큰처럼 간주하고 사용한 것이 의도대로 구현하는데 직관적이면서도 정말 효과적이었다고 생각한다. 이제 이렇게 만들어진 모델을 통해 진행한 여러 실험 결과에 어떤 인사이트가 담겨 있는지 알아보자.</p>

<h3 id="insight-from-experiment"><code class="language-plaintext highlighter-rouge">🔬 Insight from Experiment</code></h3>

<h4 id="insight-1-vit의-scalability-증명"><code class="language-plaintext highlighter-rouge">💡 Insight 1. ViT의 Scalability 증명</code></h4>
<ul>
  <li><strong><code class="language-plaintext highlighter-rouge">Pre-Train</code>에 사용되는 이미지 데이터 세트의 크기가 커질수록 <code class="language-plaintext highlighter-rouge">Fine-Tune Stage</code>에서 <code class="language-plaintext highlighter-rouge">ViT</code>가 <code class="language-plaintext highlighter-rouge">CNN</code>보다 높은 성능</strong></li>
  <li><strong>같은 성능이라면 <code class="language-plaintext highlighter-rouge">ViT</code>가 상대적으로 적은 연산량을 기록</strong></li>
</ul>

<p align="center">
<img src="/assets/images/vision_transformer/insight1.png" alt="Performance per Dataset Scale" class="align-center image-caption" width="75%&quot;, height=&quot;50%" />
<strong><em><a href="https://arxiv.org/abs/2010.11929">Performance per Dataset Scale</a></em></strong>
</p>

<p>위 도표는 <code class="language-plaintext highlighter-rouge">Pre-Train Stage</code>에 사용된 이미지 데이터 세트에 따른 모델의 <code class="language-plaintext highlighter-rouge">Fine-Tune</code> 성능 추이를 나타낸 자료다. 사전 훈련 데이터 스케일이 크지 않을 때는 <code class="language-plaintext highlighter-rouge">Conv</code> 기반의 <code class="language-plaintext highlighter-rouge">ResNet</code> 시리즈가 <code class="language-plaintext highlighter-rouge">ViT</code> 시리즈를 압도하는 모습을 보여준다. 하지만 데이터 세트의 크기가 커질수록 점점 <code class="language-plaintext highlighter-rouge">ViT</code> 시리즈의 성능이 <code class="language-plaintext highlighter-rouge">ResNet</code>을 능가하는 결과를 볼 수 있다.</p>

<p>한편, ViT &amp; ResNet 성능 결과 모두 ImageNet과 JFT-Image로 사전 훈련 및 파인 튜닝을 거쳐 나왔다고 하니 참고하자. <strong><u>추가로 파인 튜닝 과정에서 사전 훈련 때보다 이미지 사이즈를 키워서 훈련을 시켰다고 논문에서 밝히고 있는데, 이는 저자의 실험 결과에 기인한 것이다</u></strong>. 논문에 따르면 파인 튜닝 때 사전 훈련 당시보다 더 높은 해상도의 이미지를 사용하면 성능이 향상 된다고 하니 기억했다가  써먹어보자.</p>

<p align="center">
<img src="/assets/images/vision_transformer/insight1_2.png" alt="Performance per FLOPs Scale" class="align-center image-caption" width="75%&quot;, height=&quot;50%" />
<strong><em><a href="https://arxiv.org/abs/2010.11929">Performance per FLOPs</a></em></strong>
</p>

<p>위 도표는 연산량 변화에 따른 모델의 성능 추이를 나타낸 그림이다. 두 지표 모두 같은 점수라면 <code class="language-plaintext highlighter-rouge">ViT</code> 시리즈의 연산량이 현저히 적음을 알 수 있다. 또한 정확도 95% 이하 구간에서 같은 성능이라면  <code class="language-plaintext highlighter-rouge">ViT</code>의 <code class="language-plaintext highlighter-rouge">Hybrid</code> 버전 모델의 연산량이 일반 <code class="language-plaintext highlighter-rouge">ViT</code> 버전보다 현저히 적음을 확인할 수 있다. 이러한 사실은 추후에 <code class="language-plaintext highlighter-rouge">Swin-Transformer</code> 설계에 영감을 준다.</p>

<p>두 개의 실험 결과를 종합했을 때, <code class="language-plaintext highlighter-rouge">ViT</code>가 <code class="language-plaintext highlighter-rouge">ResNet</code>보다 일반화 성능이 더 높으며(도표 1) 모델의 <code class="language-plaintext highlighter-rouge">Saturation</code> 현상이 두드러지지 않아 성능의 한계치(도표 2) 역시 더 높다고 볼 수 있다. 따라서 기존 트랜스포머의 연산•구조적 측면에서 <code class="language-plaintext highlighter-rouge">Scalability</code>를 성공적으로 이식했다고 평가할 수 있겠다.</p>

<h4 id="insight-2-pure-self-attention은-좋은-이미지-피처를-추출하기에-충분하다"><code class="language-plaintext highlighter-rouge">💡 Insight 2. Pure Self-Attention은 좋은 이미지 피처를 추출하기에 충분하다</code></h4>
<ul>
  <li><strong>Patch Embedding Layer의 PCA 결과, 패치의 기저가 되는 차원과 유사한 모양을 추출</strong>
    <ul>
      <li><strong><code class="language-plaintext highlighter-rouge">Convolution</code> 없이 <code class="language-plaintext highlighter-rouge">Self-Attention</code>만으로도 충분히 이미지의 좋은 피처를 추출하는 것이 가능</strong></li>
      <li><strong><code class="language-plaintext highlighter-rouge">Vision</code>에서 <code class="language-plaintext highlighter-rouge">Convolution</code>에 대한 <code class="language-plaintext highlighter-rouge">reliance</code> 탈피 가능</strong></li>
    </ul>
  </li>
</ul>

<p align="center">
<img src="/assets/images/vision_transformer/insight2.png" alt="Patch Embedding Layer’s Filter" class="align-center image-caption" width="75%&quot;, height=&quot;50%" />
<strong><em><a href="https://arxiv.org/abs/2010.11929">Patch Embedding Layer’s Filter</a></em></strong>
</p>

<p>위 자료는 충분한 학습을 거치고 난 <code class="language-plaintext highlighter-rouge">ViT</code>의 <code class="language-plaintext highlighter-rouge">Patch Embedding Layer</code>의 필터를 <code class="language-plaintext highlighter-rouge">PCA</code>한 결과 중에서 특잇값이 높은 상위 28개의 피처를 나열한 그림이다. 이미지의 기본 뼈대가 되기에 적합해 보이는 피처들이 추출된 모습을 볼 수 있다.</p>

<p>따라서 <code class="language-plaintext highlighter-rouge">Inductive Bias</code> 없이, 단일  <code class="language-plaintext highlighter-rouge">Self-Attention</code>만으로 이미지의 피처를 추출하는 것이 충분히 가능하다. 비전 분야에 만연한 <code class="language-plaintext highlighter-rouge">Convolution</code> 의존에서 벗어나 새로운 아키텍처의 도입이 가능함을 시사한 부분이라고 할 수 있겠다.</p>

<h4 id="insight-3-bottom2general-information-top2specific-information"><code class="language-plaintext highlighter-rouge">💡 Insight 3. Bottom2General Information, Top2Specific Information</code></h4>

<ul>
  <li><strong><code class="language-plaintext highlighter-rouge">입력</code>과 가까운 인코더일수록 <code class="language-plaintext highlighter-rouge">Global &amp; General</code>한 Information을 포착</strong></li>
  <li><strong><code class="language-plaintext highlighter-rouge">출력</code>과 가까운 인코더일수록 <code class="language-plaintext highlighter-rouge">Local &amp; Specific</code>한 Information을 포착</strong></li>
</ul>
<p align="center">
<img src="/assets/images/vision_transformer/insight3.png" alt="Multi-Head Attention Distance per Network Depth" class="align-center image-caption" width="75%&quot;, height=&quot;50%" />
<strong><em><a href="https://arxiv.org/abs/2010.11929">Multi-Head Attention Distance per Network Depth</a></em></strong>
</p>

<p>다음 자료는 인코더의 개수 변화에 따른 개별 어텐션 해드의 어텐션 거리 변화 추이를 나타낸 그림이다. 여기서 어텐션 거리란, 해드가 얼마나 멀리 떨어진 패치를 어텐션했는지 픽셀 단위로 표현한 지표다. 해당 값이 높을수록 거리상 멀리 떨어진 패치와 어텐션을, 작을수록 가까운 패치와 어텐션 했다는 것을 의미한다. 다시 도표를 살펴보자. 입력과 가까운 인코더일수록(Depth 0) 해드별 어텐션 거리의 분산이 커지고, 출력과 가까운 인코더일수록(Depth 23) 분산이 점자 줄어들다가 거의 한 점에 수렴하는듯한 양상을 보여준다. 다시 말해, 입력과 가까운 <code class="language-plaintext highlighter-rouge">Bottom Encoder</code>는 멀리 떨어진 패치부터 가까운 패치까지 모두 전역적(<code class="language-plaintext highlighter-rouge">Global</code>)으로 어텐션을 수행해 <code class="language-plaintext highlighter-rouge">General</code> 한 정보를 포착하게 되고 출력과 가까운 <code class="language-plaintext highlighter-rouge">Top Encoder</code>는 개별 해드들이 모두 비슷한 거리에 위치한 패치(<code class="language-plaintext highlighter-rouge">Local</code>)에 어텐션을 수행해 <code class="language-plaintext highlighter-rouge">Specific</code> 한 정보를 포착하게 된다.</p>

<p>이 때 <code class="language-plaintext highlighter-rouge">Global</code>과 <code class="language-plaintext highlighter-rouge">Local</code>이라는 용어 때문에 <code class="language-plaintext highlighter-rouge">Bottom Encoder</code> 는 멀리 떨어진 패치와 어텐션하고, <code class="language-plaintext highlighter-rouge">Top Encoder</code>는 가까운 패치와 어텐션한다고 착각하기 쉽다. <strong><u>그러나 개별 해드들의 어텐션 거리가 얼마나 분산되어 있는가가 바로 </u></strong><code class="language-plaintext highlighter-rouge">Global</code>, <code class="language-plaintext highlighter-rouge">Local</code><strong><u>을 구분하는 기준이 된다.</u></strong> 입력부에 가까운 레이어들은 헤드들의 어텐션 거리 분산이 매우 큰 편인데, 이것을 이패치 저패치 모두 어텐션 해보고 비교해본다고 해석해서 <code class="language-plaintext highlighter-rouge">Global</code>이라고 부르고, 출력부에 가까운 레이어는 헤드들의 어텐션 거리 분산이 매우 작은 편인데, 이게 바로 각각의 헤드들이 어떤 정보에 주목해야할지(분류 손실이 가장 작아지는 패치) 범위를 충분히 좁힌 상태에서 특정 부분에만 집중한다는 의미로 해석해 <code class="language-plaintext highlighter-rouge">Local</code> 이라고 부르게 되었다.</p>

<p>&lt;<strong><a href="https://arxiv.org/abs/2006.05987">Revisiting Few-sample BERT Fine-tuning</a></strong>&gt;도 위와 비슷한 맥락의 사실에 대해 언급하고 있으니 참고해보자. 이러한 사실은 트랜스포머 인코더 계열 모델을 튜닝할 때 <code class="language-plaintext highlighter-rouge">Depth</code> 별로 다른 <code class="language-plaintext highlighter-rouge">Learning Rate</code>을 적용하는 <code class="language-plaintext highlighter-rouge">Layerwise Learning Rate Decay</code> 의 초석이 되기도 한다. <code class="language-plaintext highlighter-rouge">Layerwise Learning Rate Decay</code> 에 대해서는 <strong><a href="https://towardsdatascience.com/advanced-techniques-for-fine-tuning-transformers-82e4e61e16e">여기 포스트</a></strong>를 참고하도록 하자.</p>

<p align="center">
<img src="/assets/images/vision_transformer/insight3_2.png" alt="Output from Last Encoder" class="align-center image-caption" width="40%&quot;, height=&quot;10%" />
<strong><em><a href="https://arxiv.org/abs/2010.11929">Output from Last Encoder</a></em></strong>
</p>

<p>한편 논문에는 언급되지 않은, 필자의 뇌피셜에 가깝지만, <strong><u>출력에 가까운 인코더들의 해드가 가진</u></strong> <code class="language-plaintext highlighter-rouge">Attention Distance</code><strong><u>이 모두 비슷하다는 사실로 이미지 분류에 결정적인 역할을 하는 피처가 이미지의 특정 구역에 모여 있으며, 그 스팟은 이미지의 중앙 부근일 가능성이 높다고 추측 해볼 수 있다.</u></strong> 모든 해드의 픽셀 거리가 서로 비슷하려면 일단 비슷한 위치의 패치에 어텐션을 해야하기 때문에 분류 손실값을 최소로 줄여주는 피처는 보통 한 구역(패치)에 몰려 있을 것이라고 유추가 가능하다. 또한 특정 스팟이 중앙에 위치할수록 어텐션 거리의 분산이 줄어들것이라고 생각 해볼 수도 있었다. 저자는 <code class="language-plaintext highlighter-rouge">Attention Rollout</code>이라는 개념을 통해 <code class="language-plaintext highlighter-rouge">Attention Distance</code>을 산출했다고 언급하는데, 자세한 내용은 옆에 두 링크를 참고해보자(<a href="https://hongl.tistory.com/234">한국어 설명 블로그</a>,  <a href="https://arxiv.org/abs/2005.00928">원논문</a>). 이러한 필자의 가설이 맞다면, <code class="language-plaintext highlighter-rouge">Convolution</code> 의 <code class="language-plaintext highlighter-rouge">Inductive Bias</code>  중 <code class="language-plaintext highlighter-rouge">Locality</code> 의 효과성을 <code class="language-plaintext highlighter-rouge">Self-Attention</code>을 통해 입증이 가능하며, 반대로 <code class="language-plaintext highlighter-rouge">Convolution</code>에 대한 의존에서 벗어나 단일 <code class="language-plaintext highlighter-rouge">Self-Attention</code> 으로도 같은 효과를 낼 수 있다는 증거 중 하나가 될 것이다.</p>

<h4 id="insight-4-vit는-cls-pooling-사용하는게-효율적"><code class="language-plaintext highlighter-rouge">💡 Insight 4. ViT는 CLS Pooling 사용하는게 효율적</code></h4>

<ul>
  <li><strong><code class="language-plaintext highlighter-rouge">CLS Pooling</code>은 <code class="language-plaintext highlighter-rouge">GAP</code> 보다 2배 이상 큰 학습률을 사용해도 비슷한 성능을 기록</strong>
    <ul>
      <li><strong><u>학습 속도는 더 빠르되 성능이 비슷하기 때문에</u></strong> <code class="language-plaintext highlighter-rouge">CLS Pooling</code> <strong><u>이 더 효율적</u></strong></li>
    </ul>
  </li>
</ul>

<p align="center">
<img src="/assets/images/vision_transformer/insight4.png" alt="Performance Trend by Pooling Method with LR" class="align-center image-caption" width="75%&quot;, height=&quot;50%" />
<strong><em><a href="https://arxiv.org/abs/2010.11929">Performance Trend by Pooling Method with LR</a></em></strong>
</p>

<p>다음 도표는 풀링 방식과 학습률의 변동에 따른 정확도 변화 추이를 나타낸 그림이다. 비슷한 성능이라면 <code class="language-plaintext highlighter-rouge">CLS Pooling</code>이 <code class="language-plaintext highlighter-rouge">GAP</code>보다 2배 이상 큰 학습률을 사용했다. 학습률이 크면 모델의 수렴 속도가 빨라져 학습 속도가 빨라지는 장점이 있다. 그런데 성능까지 비슷하다면 <code class="language-plaintext highlighter-rouge">ViT</code>는 <code class="language-plaintext highlighter-rouge">CLS Pooling</code>을 사용하는 것이 더 효율적이라고 할 수 있겠다.</p>

<p>나중에 시간이 된다면 다른 풀링 방식, 예를 들면 <code class="language-plaintext highlighter-rouge">Weighted Layer Pooling</code>, <code class="language-plaintext highlighter-rouge">GeM Pooling</code>, <code class="language-plaintext highlighter-rouge">Attention Pooling</code> 같은 것을 적용해 실험해보겠다.</p>

<h4 id="insight-5-vit는-absolute-1d-position-embedding-사용하는게-가장-효율적"><code class="language-plaintext highlighter-rouge">💡 Insight 5. ViT는 Absolute 1D-Position Embedding 사용하는게 가장 효율적</code></h4>

<ul>
  <li><strong>어떤 형태로든 위치 임베딩 값을 정의해준다면, 형태와 종류에 상관없이 거의 비슷한 성능을 보임</strong></li>
  <li><strong>성능이 비슷하면, 직관적이고 구현이 간편한 <code class="language-plaintext highlighter-rouge">Absolute 1D-Position Embedding</code> 방법을 사용하는 것이 가장 효율적</strong></li>
  <li><strong><code class="language-plaintext highlighter-rouge">ViT</code>는 <code class="language-plaintext highlighter-rouge">Patch-Level</code> 사용해, <code class="language-plaintext highlighter-rouge">Pixel-Level</code>보다 상대적으로 시퀀스 길이가 짧아 위치•공간 정보를 인코딩하는 방식에 영향을 덜 받음</strong></li>
</ul>

<p align="center">
<img src="/assets/images/vision_transformer/insight5.png" alt="Performance Table by making Position Embedding method" class="align-center image-caption" width="75%&quot;, height=&quot;50%" />
<strong><em><a href="https://arxiv.org/abs/2010.11929">Performance Table by making Position Embedding method</a></em></strong>
</p>

<p>위 실험 결과는 <code class="language-plaintext highlighter-rouge">Position Embedding</code> 인코딩 방식에 따른 <code class="language-plaintext highlighter-rouge">ViT</code> 모델의 성능 변화 추이를 나타낸 자료다. 인코딩 형태와 상관없이 위치 임베딩의 유무가 성능에 큰 영향을 미친다는 사실을 알려주고 있다. 한편, 인코딩 형태 변화에 따른 유의미한 성능 변화는 없었다. 하지만 <code class="language-plaintext highlighter-rouge">Absolute 1D-Position Embedding</code>의 컨셉이 가장 직관적이며 구현하기 편하고 연산량이 다른 인코딩보다 적다는 것을 감안하면 ViT에 가장 효율적인 위치 임베딩 방식이라고 판단할 수 있다.</p>

<p>논문은 결과에 대해 <code class="language-plaintext highlighter-rouge">ViT</code>가 사용하는 <code class="language-plaintext highlighter-rouge">Patch-Level Embedding</code>이 <code class="language-plaintext highlighter-rouge">Pixel-Level</code>보다 상대적으로 짧은 시퀀스 길이를 갖기 때문이라고 설명한다. 예를 들어 <code class="language-plaintext highlighter-rouge">224x224</code> 사이즈의 이미지를 <code class="language-plaintext highlighter-rouge">16x16</code> 사이즈의 패치 여러장으로 만든다고 생각해보자. 임베딩 차원에 들어가는 $N$ 은 $(224/16)^2$ , 즉 <code class="language-plaintext highlighter-rouge">196</code>이 된다. 한편 이것을 <code class="language-plaintext highlighter-rouge">Pixel-Level</code>로 임베딩 하게 되면 $224^2$, 즉 <code class="language-plaintext highlighter-rouge">50176</code> 개의 시퀀스가 생긴다. 따라서 <code class="language-plaintext highlighter-rouge">Pixel-Level</code> 에 비하면 훨씬 짧은 시퀀스 길이를 갖기 때문에 <code class="language-plaintext highlighter-rouge">Absolute 1D-Position Embedding</code> 만으로도 충분히 <code class="language-plaintext highlighter-rouge">Spatial Relation</code>을 학습할 수 있는 것이다.</p>

<p align="center">
<img src="/assets/images/vision_transformer/insight5_2.png" alt="Absolute 1D-Position Embedding" class="align-center image-caption" width="50%&quot;, height=&quot;50%" />
<strong><em><a href="https://arxiv.org/abs/2010.11929">Absolute 1D-Position Embedding</a></em></strong>
</p>

<p>하지만, 필자는 자연어 처리의 <code class="language-plaintext highlighter-rouge">Transformer-XL</code>, <code class="language-plaintext highlighter-rouge">XLNet</code>, <code class="language-plaintext highlighter-rouge">DeBERTa</code> 같은 모델들이 <code class="language-plaintext highlighter-rouge">Relative Position Embedding</code> 방식을 적용해 큰 성공을 거둔 바가 있다는 점을 생각하면 이런 결과가 납득이 가면서도 의아했다.</p>

<p>저자는 실험에 사용한 모든 데이터 세트를 <code class="language-plaintext highlighter-rouge">224x224</code>로 <code class="language-plaintext highlighter-rouge">resize</code> 했다고 밝히고 있는데, 만약 이미지 사이즈가 <code class="language-plaintext highlighter-rouge">512x512</code>정도만 되더라도 $N$ 값이 <code class="language-plaintext highlighter-rouge">1024</code> 이라서 위 결과와 상당히 다른 양상이 나타나지 않을까 하는 생각이 든다. 추후에 시간이 된다면 이 부분도 꼭 실험해봐야겠다. 예측컨데 이미자 사이즈가 커질수록 <code class="language-plaintext highlighter-rouge">2D Position Embedding</code> 혹은 <code class="language-plaintext highlighter-rouge">Relative Position Embedding</code>이 더 효율적일 것이라 예상한다.</p>

<h3 id="️conclusion"><code class="language-plaintext highlighter-rouge">🧑‍⚖️ Conclusion</code></h3>

<p>이렇게 <code class="language-plaintext highlighter-rouge">ViT</code> 모델을 제안한 <a href="https://arxiv.org/abs/2010.11929">&lt;An Image Is Worth 16x16 Words: Transformers For Image Recognition At Scale&gt;</a>에 실린 내용을 모두 살펴보았다. <code class="language-plaintext highlighter-rouge">Conv</code> 에 대한 의존을 탈피 했다는 점에서 매우 의미가 있는 시도였으며, Self-Attention &amp; Transformer 구조 채택만으로도 컴퓨터 비전 영역에 어느 정도  <code class="language-plaintext highlighter-rouge">scalability</code> 를  이식하는데 성공했다는 점에서 후대 연구에 중요한 시사점을 남겼다. 상대적으로 정체(??)되어 있던 비전 영역이 성능의 한계를 한단계 뛰어넘을 수 있는 초석을 마련해준 셈이다.</p>

<p>하지만, <code class="language-plaintext highlighter-rouge">ViT</code>의 <code class="language-plaintext highlighter-rouge">Pretrain Stage</code>에 적합한 <code class="language-plaintext highlighter-rouge">Self-Supervised Learning</code> 방법을 찾지 못해 여전히 <code class="language-plaintext highlighter-rouge">Supervised Learning</code> 방식을 채택한 점은 매우 아쉬웠다. <strong><u>이는 결국 데이터</u></strong> <code class="language-plaintext highlighter-rouge">Scale</code> <strong><u>확장에 한계를 의미하기 때문이다.</u></strong> 오늘날 BERT와 GPT의 성공 신화는 비단 <code class="language-plaintext highlighter-rouge">Self-Attention</code>와 <code class="language-plaintext highlighter-rouge">Transformer</code>의 구조적 탁월성에 의해서만 탄생한게 아니다. 이에 못지 않게(개인적으로 제일 중요하다 생각) 주요했던 것이 바로 데이터 <code class="language-plaintext highlighter-rouge">Scale</code> 확장이다.  <code class="language-plaintext highlighter-rouge">MLM</code>, <code class="language-plaintext highlighter-rouge">AR</code> 등의 <code class="language-plaintext highlighter-rouge">Self-Supervised Learning</code> 덕분에 데이터 <code class="language-plaintext highlighter-rouge">Scale</code>을 효율적으로 스케일 업 시킬 수 있었고, 사전 훈련 데이터의 증가는 모델 깊이, 너비, 차원까지 더욱 크케 키우는데 기여했다.</p>

<p>또한 <code class="language-plaintext highlighter-rouge">ViT</code>는 선천적으로 <code class="language-plaintext highlighter-rouge">Patch-Level Embedding</code>을 사용하기 때문에 다양한 이미지 테스크에 적용하는 것이 힘들다. <code class="language-plaintext highlighter-rouge">Segmentation</code>, <code class="language-plaintext highlighter-rouge">Object Detection</code> 같은 Task는 픽셀 단위로 예측을 수행해 객체를 탐지하거나 분할해야 한다. 하지만 <code class="language-plaintext highlighter-rouge">Patch</code> 단위로 훈련을 수행했던 <code class="language-plaintext highlighter-rouge">ViT</code>는 <code class="language-plaintext highlighter-rouge">Pixel</code> 단위의 예측을 수행하는데 어려움을 겪는다.</p>

<p>마지막으로 <code class="language-plaintext highlighter-rouge">Self-Attention</code> 자체의 <code class="language-plaintext highlighter-rouge">Computational Overhead</code>가 너무 심해 고해상도의 이미지를 적절히 다루기 힘들다. 위에서도 언급했지만 이미지의 사이즈가 <code class="language-plaintext highlighter-rouge">512x512</code>만 되어도 이미 패치의 개수가 <code class="language-plaintext highlighter-rouge">1024</code>가 된다. 사이즈가 커질수록 시퀀스 길이 역시 기하급수적으로 커지는데다가 <code class="language-plaintext highlighter-rouge">Self-Attention</code> 는 쿼리와 키 행렬을 내적 (자기 자신과 곱이라 볼 수 있음) 하기 때문에 <code class="language-plaintext highlighter-rouge">Computational Overhead</code>가 $N^2$이 된다.</p>

<p>필자는 <code class="language-plaintext highlighter-rouge">ViT</code>를 절반의 성공이라고 평하고 싶다. 본래 <code class="language-plaintext highlighter-rouge">ViT</code>의 설계 목적은 비전 분야의 <code class="language-plaintext highlighter-rouge">Conv</code>에 대한 의존을 탈피하면서, 퓨어한 <code class="language-plaintext highlighter-rouge">Self-Attention</code>을 도입해 <code class="language-plaintext highlighter-rouge">Scalabilty</code> 를 이식하는 것이었다. <code class="language-plaintext highlighter-rouge">Self-Attention</code>을 도입하는데는 성공했지만, 여전히 다룰 수 있는 이미지 사이즈나 Task에는 한계가 분명하며 결정적으로 <code class="language-plaintext highlighter-rouge">Self-Supervised Learning</code> 방식을 도입하지 못했다. <code class="language-plaintext highlighter-rouge">Scalabilty</code> 라는 단어의 의미를 생각하면, 방금 말한 부분에서까지 확장성이 있어야 설계 의도에 부합하는 결과라고 생각한다.</p>]]></content><author><name>qcqced</name><email>qcqced123@gmail.com</email></author><category term="Computer Vision" /><category term="Computer Vision" /><category term="Vision Transformer" /><category term="ViT" /><category term="Transformer" /><category term="Self-Attention" /><category term="Image Classification" /><summary type="html"><![CDATA[ViT Official Papaer Review with Pytorch Implementation]]></summary></entry><entry><title type="html">🔢 Vector Space: Column Space, Basis, Rank, Null Space</title><link href="http://localhost:4000/linear-algebra/vector-subspace" rel="alternate" type="text/html" title="🔢 Vector Space: Column Space, Basis, Rank, Null Space" /><published>2023-07-19T00:00:00+09:00</published><updated>2023-07-10T13:00:00+09:00</updated><id>http://localhost:4000/linear-algebra/vector_space</id><content type="html" xml:base="http://localhost:4000/linear-algebra/vector-subspace"><![CDATA[<h3 id="-column-space"><code class="language-plaintext highlighter-rouge">🔢 Column Space</code></h3>

\[C(A) = Range(A)\]

<p>열벡터가 <code class="language-plaintext highlighter-rouge">span</code>하는 공간을 의미한다. <code class="language-plaintext highlighter-rouge">span</code> 이란, 벡터의 집합에 의해 생성된 모든 <code class="language-plaintext highlighter-rouge">linear combination</code>의 결과로 생성할 수 있는 부분 공간을 말한다. 따라서 <code class="language-plaintext highlighter-rouge">column space</code> 는 열벡터의 <code class="language-plaintext highlighter-rouge">linear combination</code> 결과로 생성할 수 있는 <code class="language-plaintext highlighter-rouge">vector space</code>의 부분 공간을 말한다.</p>
<h3 id="-basis"><code class="language-plaintext highlighter-rouge">🍖 Basis</code></h3>
<figure class="half">
  <a href="https://twlab.tistory.com/24"><img src="/assets/images/linear_independent.png" title="Linear Independent" /></a>
  <a href="https://twlab.tistory.com/24"><img src="/assets/images/linear_dependent.png" title="Linear Independent" /></a>
</figure>
<p>기저에 대해 알기 위해서는 먼저 <code class="language-plaintext highlighter-rouge">linear independent(선형 독립)</code>의 의미를 알아야 한다. 선형독립이란, 왼쪽 그림처럼 서로 다른 벡터들이 관련성 없이 독립적으로 존재하는 상태를 말한다. 따라서 서로 다른 두 벡터가 선형 독립이라면 한 벡터의 선형조합으로 다른 벡터를 표현할 수 없다. 반대로 선형 종속 상태면 오른쪽 그림처럼 벡터를 다른 벡터의 선형조합으로 표현 가능하다.</p>

<p>이제 기저에 대해 알아보자. 기저란 선형 독립이면서 벡터 공간을 <code class="language-plaintext highlighter-rouge">span</code> 하는 벡터 집합을 말한다. 다시 말해, 공간 또는 차원을 표현하는데 필요한 요소들의 집합이라고 볼 수 있다. 예를 들어 2차원 공간을 표현하고 싶다면 서로 선형 독립인 벡터 2개가 필요하다. 오른쪽 그림처럼 벡터 2개가 존재해도 서로 종속 관계라면 표현(span)할 수 있는 공간은 1차원의 직선이 되기 때문이다. 정리하면, $N$차원 공간의 기저란 선형 독립이면서 벡터 공간을 <code class="language-plaintext highlighter-rouge">span</code>하는 벡터가 $N$개 있는 상태다. 추가로, $N$차원 공간의 기저는<code class="language-plaintext highlighter-rouge">NxN</code> 크기의 <code class="language-plaintext highlighter-rouge">Invertable</code>한 행렬과 동치를 이룬다. 뒤에서 더 자세히 다루겠지만 역행렬은 좌표평면 상에서 <code class="language-plaintext highlighter-rouge">reverse linear combination</code> 의 역할을 하기 때문이다.<br />
한편 기저는 유일하지 않다. 위에서 언급한 $N$차원 기저의 필요충분조건을 만족하는 모든 벡터 집합은 모두 기저가 될 수 있다.</p>

<h3 id="-standard-basis"><code class="language-plaintext highlighter-rouge">🦴 Standard Basis</code></h3>

\[I= 
   \begin{pmatrix} 
   1 &amp; 0 &amp; 0  \\
   0 &amp; 1 &amp; 0  \\
   0 &amp; 0 &amp; 1  \\
   \end{pmatrix}\]

<p>표준 기저란, 기저가 표현하는 차원의 축이 우리가 흔히 아는 <code class="language-plaintext highlighter-rouge">x축, y축, z축</code> 이 되는 기저 벡터를 말한다. 수학적으로는 주대각성분의 값이 모두 1인 대각행렬 $D$, 즉 단위 행렬 $I$가 기저일 때 우리는 표준 기저라고 정의한다.</p>

<h3 id="-rank"><code class="language-plaintext highlighter-rouge">🧮 Rank</code></h3>

<p align="center">
<img src="/assets/images/column_space.png" alt="Column Space Image" class="align-center image-caption" width="100%&quot;, height=&quot;50%" />
<strong><em><a href="https://www.researchgate.net/figure/Example-of-a-projection-of-a-matrix-3-2-on-the-column-space_fig2_220103928">Column Space Image</a></em></strong>
</p>

<p>행렬에서 <code class="language-plaintext highlighter-rouge">independent</code>한 <code class="language-plaintext highlighter-rouge">column</code>의 개수를 의미하며, 기하학적으로는 <code class="language-plaintext highlighter-rouge">column space</code>가 실제 <code class="language-plaintext highlighter-rouge">span</code>하는 공간의 차원을 말한다. <code class="language-plaintext highlighter-rouge">Rank Theorem</code> 에 의해, 행렬 $A$ column vector는 행렬 $A^T$의 row vector와 같다. 따라서 column rank와 row rank 값 역시 항상 동일하다. 행렬 $A$의 랭크는 $rank(A)$로 표기한다.</p>

<p>행렬의 랭크는 행렬의 생김새에 따라 부르는 명칭이 조금씩 바뀐다. 예를 들어 열벡터가 모두 선형 독립이면서 크기가 <code class="language-plaintext highlighter-rouge">10x3</code> 인 행렬 $C$가 있다고 가정해보자. 모든 열벡터가 선형 독립이기 때문에 우리는 행렬 $C$의 랭크가 3이라는 것을 알 수 있다. 이 때 행렬 $C$를  <code class="language-plaintext highlighter-rouge">full-column rank</code> 라고 부른다. 그리고 행벡터의 랭크 역시 랭크 정리 이론에 의해 3이 될 것이다. 이번에는 행렬 $C$의 열벡터 랭크가 2라고 가정해보자. 우리는 이 때 행렬 $C$를 <code class="language-plaintext highlighter-rouge">rank-deficient</code>로 정의한다. 만약 행렬 $C$의 열벡터가 모두 선형독립이고 그 크기가 <code class="language-plaintext highlighter-rouge">10x10</code>이라면 뭐라고 부를까?? 이 때는 열벡터, 행벡터 모두 랭크가 10이 되기 때문에 <code class="language-plaintext highlighter-rouge">full-rank</code> 라고 부른다.</p>

<p>정리하면 행렬의 랭크란, 행렬의 행의 크기 M 그리고 열의 크기 N 중에서 더 작은값보다 같거나 작으면서 <code class="language-plaintext highlighter-rouge">independent</code>한 <code class="language-plaintext highlighter-rouge">column</code>의 개수라는 의미를 내포한 개념이라고 볼 수 있겠다.</p>

<p>추가로, column vector와 row vector를 순서대로 곱하면 항상 $Rank = 1$인 행렬 $A$가 만들어진다는 것이다. 그렇게 만들어진 행렬의 원소가 두 벡터의 <code class="language-plaintext highlighter-rouge">linear combination</code>  으로 구성된 것이라서 당연한 소리라고 생각할 수 있지만, 이것은 선형대수학에서 매우 중요한 성질이 된다. 뒤집어서 보면 어떤 행렬의 $Rank=1$이라는 것은 그 행렬이 어떤 다른 행렬의 기본 단위 요소가 된다는 의미이기 때문이다. 어떤 행렬의 랭크가 4라는 것은 랭크 1짜리 행렬 4개의 조합이라고 생각해볼 수 있다.</p>

<h3 id="-null-space"><code class="language-plaintext highlighter-rouge">👌 Null Space</code></h3>

\[Ax=0\]

<p>위 수식을 만족하는 벡터 $x$의 집합을 말한다. 다시 말해, 선형 변환 $A$(크기가 MxN인 행렬)를 통해 0이 되는 벡터 집합 $x$가 바로 <code class="language-plaintext highlighter-rouge">null space(영공간)</code>이다. 영공간은 선형변환 $A$의 랭크와 무관하며 선형변환 A의 열차원인 $R^N$상에 존재하는 공간이다. 그래서 $Ax=0$을 행렬과 벡터의 내적으로 해석하면 영공간은 선형변환 $A$의 row space와 수직이다라는 사실을 알 수 있다.</p>

\[N_A = dim(Null(A)) - rank(A)\]

<p>한편, 영공간이 <code class="language-plaintext highlighter-rouge">span</code> 하는 공간의 차원과 션형변환 $A$의 랭크를 더하면 션형변환 $A$의 열차원을 알 수 있다. 수식으로 표현하면 다음과 같다.</p>

<h3 id="-left-null-space"><code class="language-plaintext highlighter-rouge">🫲 Left Null Space</code></h3>

\[A^Tx=0\]

<p>선형변환 $A$의 크기가 MxN일 때, $A$의 좌 영공간은 $A$의 모든 열들에 대해 선형 조합으로 0 벡터(영벡터)가 되는 모든 벡터 집합 $x$의 공간을 <code class="language-plaintext highlighter-rouge">좌영공간</code>이라고 한다. $A$의 열벡터에 대한 영공간이라는 것이 포인트가 된다. 따라서 좌영공간은 선형변환 $A$의 전치행렬인 $A^T$의 영공간을 구하는 것과 같으며, 선형변환 $A$의 column space와 수직하게 된다.</p>]]></content><author><name>qcqced</name><email>qcqced123@gmail.com</email></author><category term="Linear Algebra" /><category term="Linear Algebra" /><category term="linear independent" /><category term="vector space" /><category term="rank" /><category term="column space" /><category term="null space" /><category term="basis" /><summary type="html"><![CDATA[💡 Concept of main sub-space]]></summary></entry><entry><title type="html">🎲 RuntimeError: CUDA error: device-side assert triggered</title><link href="http://localhost:4000/framework-library/mismatch-dimension" rel="alternate" type="text/html" title="🎲 RuntimeError: CUDA error: device-side assert triggered" /><published>2023-07-17T00:00:00+09:00</published><updated>2023-07-18T07:00:00+09:00</updated><id>http://localhost:4000/framework-library/dim_mismatch</id><content type="html" xml:base="http://localhost:4000/framework-library/mismatch-dimension"><![CDATA[<h3 id="-사전에-정의-입출력-차원--실제-입출력-차원"><code class="language-plaintext highlighter-rouge">😵 사전에 정의 입출력 차원 ≠ 실제 입출력 차원</code></h3>

<p>다양한 원인이 있다고 알려져 있는 에러지만, 필자의 경우 위 에러는 사전에 정의한 데이터의 입출력 차원과 실제 입출력 데이터 차원이 서로 상이할 때 발생했다. 하지만 원인을 확실히 특정하고 싶다면 아래 예시 코드를 먼저 추가한 뒤, 다시 한 번 에러 로그를 확인해보길 권장한다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">os</span>
<span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="s">'CUDA_LAUNCH_BLOCKING'</span><span class="p">]</span> <span class="o">=</span> <span class="s">"1"</span>
<span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="s">"CUDA_VISIBLE_DEVICES"</span><span class="p">]</span> <span class="o">=</span> <span class="s">"0"</span>
</code></pre></div></div>
<p>예시 코드처럼 환경변수를 추가하면 에러가 어느 부분에서 발생했는지 로그가 좀 더 구체적으로 나온다. 거의 대부분이 입출력 차원 문제일테니 귀찮으면 바로 차원을 수정하도록 하자.</p>]]></content><author><name>qcqced</name><email>qcqced123@gmail.com</email></author><category term="Framework &amp; Library" /><category term="Pytorch" /><category term="Dimension Mismatch" /><category term="CUDA" /><category term="Error Handling" /><summary type="html"><![CDATA[Pytorch Error: Mis-match between pre-defined dimension and input dimension]]></summary></entry><entry><title type="html">🎲 RuntimeError: CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling cublasCreate(hand≤)</title><link href="http://localhost:4000/framework-library/mismatch-embedding" rel="alternate" type="text/html" title="🎲 RuntimeError: CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling cublasCreate(hand≤)" /><published>2023-07-17T00:00:00+09:00</published><updated>2023-07-18T02:00:00+09:00</updated><id>http://localhost:4000/framework-library/embedding_mismatch</id><content type="html" xml:base="http://localhost:4000/framework-library/mismatch-embedding"><![CDATA[<h3 id="-nnembedding-차원--실제-데이터-입력-차원"><code class="language-plaintext highlighter-rouge">😵 nn.Embedding 차원 ≠ 실제 데이터 입력 차원</code></h3>
<p><code class="language-plaintext highlighter-rouge">torch.nn.Embedding</code>에서 정의한 입출력 차원과 실제 데이터의 차원이 다른 경우에 발생하는 에러다. 다양한 상황에서 마주할 수 있는 에러지만, 필자의 경우 <code class="language-plaintext highlighter-rouge">Huggingface</code>에서 불러온<code class="language-plaintext highlighter-rouge">pretrained tokenizer</code>에 <code class="language-plaintext highlighter-rouge">special token</code> 을 추가해 사용할 때, 토큰을 추가했다는 사실을 잊고 <code class="language-plaintext highlighter-rouge">nn.Embedding</code> 에 정의한 입출력 차원을 변경하지 않아서 발생하는 경우가 많았다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoConfig</span><span class="p">,</span> <span class="n">AutoModel</span>

<span class="k">class</span> <span class="nc">CFG</span><span class="p">:</span>
    <span class="n">model_name</span> <span class="o">=</span> <span class="s">'microsoft/deberta-v3-large'</span>
    <span class="n">config</span> <span class="o">=</span> <span class="n">AutoConfig</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>
    <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">AutoModel</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">add_markdown_token</span><span class="p">(</span><span class="n">cfg</span><span class="p">:</span> <span class="n">sCFG</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
    <span class="s">"""
    Add MarkDown token to pretrained tokenizer ('[MD]')
    Args:
        cfg: CFG, needed to load tokenizer from Huggingface AutoTokenizer
    """</span>
    <span class="n">markdown_token</span> <span class="o">=</span> <span class="s">'[MD]'</span>
    <span class="n">special_tokens_dict</span> <span class="o">=</span> <span class="p">{</span><span class="s">'additional_special_tokens'</span><span class="p">:</span> <span class="p">[</span><span class="sa">f</span><span class="s">'</span><span class="si">{</span><span class="n">markdown_token</span><span class="si">}</span><span class="s">'</span><span class="p">]}</span>
    <span class="n">cfg</span><span class="p">.</span><span class="n">tokenizer</span><span class="p">.</span><span class="n">add_special_tokens</span><span class="p">(</span><span class="n">special_tokens_dict</span><span class="p">)</span>
    <span class="n">markdown_token_id</span> <span class="o">=</span> <span class="n">cfg</span><span class="p">.</span><span class="n">tokenizer</span><span class="p">(</span><span class="sa">f</span><span class="s">'</span><span class="si">{</span><span class="n">markdown_token</span><span class="si">}</span><span class="s">'</span><span class="p">,</span> <span class="n">add_special_tokens</span><span class="o">=</span><span class="bp">False</span><span class="p">)[</span><span class="s">'input_ids'</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>

    <span class="nb">setattr</span><span class="p">(</span><span class="n">cfg</span><span class="p">.</span><span class="n">tokenizer</span><span class="p">,</span> <span class="s">'markdown_token'</span><span class="p">,</span> <span class="sa">f</span><span class="s">'</span><span class="si">{</span><span class="n">markdown_token</span><span class="si">}</span><span class="s">'</span><span class="p">)</span>
    <span class="nb">setattr</span><span class="p">(</span><span class="n">cfg</span><span class="p">.</span><span class="n">tokenizer</span><span class="p">,</span> <span class="s">'markdown_token_id'</span><span class="p">,</span> <span class="n">markdown_token_id</span><span class="p">)</span>
    <span class="n">cfg</span><span class="p">.</span><span class="n">tokenizer</span><span class="p">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="sa">f</span><span class="s">'</span><span class="si">{</span><span class="n">cfg</span><span class="p">.</span><span class="n">checkpoint_dir</span><span class="si">}</span><span class="s">/tokenizer/'</span><span class="p">)</span>


<span class="n">add_markdown_token</span><span class="p">(</span><span class="n">CFG</span><span class="p">)</span>
<span class="n">CFG</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">resize_token_embeddings</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">))</span>
</code></pre></div></div>
<p>구글링해보니 해결하는 방법은 다양한 것 같은데, <code class="language-plaintext highlighter-rouge">torch.nn.Embedding</code>에 정의된 입출력 차원을 실제 데이터 차원과 맞춰주면 간단하게 해결된다. 필자처럼 <code class="language-plaintext highlighter-rouge">special token</code> 을 추가해 사용하다 해당 에러가 발생하는 상황이라면 새로운 토큰이 추가된 토크나이저의 길이를 다시 측정한 뒤 값을 <code class="language-plaintext highlighter-rouge">resize_token_embeddings</code> 메서드에 전달해 <code class="language-plaintext highlighter-rouge">nn.Embedding</code>을 업데이트 해주면 된다.</p>]]></content><author><name>qcqced</name><email>qcqced123@gmail.com</email></author><category term="Framework &amp; Library" /><category term="Pytorch" /><category term="Dimension Mismatch" /><category term="nn.Embedding" /><category term="CUDA" /><category term="Error Handling" /><summary type="html"><![CDATA[Pytorch Error: Mis-match between pre-defined dimension and input dimension]]></summary></entry><entry><title type="html">🤔 RuntimeError: Function ‘LogSoftmaxBackward0’ returned nan values in its 0th output</title><link href="http://localhost:4000/framework-library/backward-nan/" rel="alternate" type="text/html" title="🤔 RuntimeError: Function ‘LogSoftmaxBackward0’ returned nan values in its 0th output" /><published>2023-07-11T00:00:00+09:00</published><updated>2023-07-11T13:00:00+09:00</updated><id>http://localhost:4000/framework-library/backward-nan</id><content type="html" xml:base="http://localhost:4000/framework-library/backward-nan/"><![CDATA[<h3 id="-pytorch-backward-과정에서-nan-발생하는-문제"><code class="language-plaintext highlighter-rouge">🔥 Pytorch Backward 과정에서 NaN 발생하는 문제</code></h3>

<p>커스텀으로 모델, 여러 풀링, 매트릭, 손실 함수들을 정의하면서부터 제일 많이 마주하게 되는 에러다. 진심으로 요즘 <code class="language-plaintext highlighter-rouge">CUDA OOM</code> 보다 훨씬 자주 보는 것 같다. 해당 에러는 <code class="language-plaintext highlighter-rouge">LogSoftmax</code> 레이어에 전달된 입력값 중에서 <code class="language-plaintext highlighter-rouge">nan</code>, <code class="language-plaintext highlighter-rouge">inf</code> 가 포함되어 연산을 진행할 수 없다는 것을 의미한다. 딥러닝 실험을 진행하면서 가장 해결하기 까다로운 녀석으로 원인을 특정하기 힘들기 때문이다. 원인을 잡기 어려운 이유는 바로 우리가 지금 하고 있는게 <code class="language-plaintext highlighter-rouge">‘딥러닝’</code> 이라서 그렇다. 위 에러는 대부분 연산자가 우리가 의도하지 않은 동작을 하는 케이스 때문인데, 하나 하나 디버깅하기에는 너무나도 연산자가 많다. 또한 딥러닝은 입출력으로 엄청나게 큰 사이즈의 행렬을 사용한다. 우리가 <code class="language-plaintext highlighter-rouge">nan</code>, <code class="language-plaintext highlighter-rouge">inf</code> 값 존재에 대해서 인지하기 쉽지 않다.</p>

<p><strong><u>위 에러는 필자의 경험상 대부분 커스텀으로 정의한 레이어에서 발생하는 경우가 많았으며 특히</u></strong> <code class="language-plaintext highlighter-rouge">분수</code>, <code class="language-plaintext highlighter-rouge">각도</code>, <code class="language-plaintext highlighter-rouge">제곱근</code>, <code class="language-plaintext highlighter-rouge">지수</code> <strong><u>개념을 사용하는 연산자가 대부분 원인이었다.</u></strong> 예를 들어 코사인 유사도를 구하는 과정에서 연산 대상 벡터값에  <code class="language-plaintext highlighter-rouge">zero-value</code> 가 포함된 경우 분모가 0이 되기 때문에 연산 정의가 되지 않아 <code class="language-plaintext highlighter-rouge">nan</code> 을 반환해 위와 같은 에러가 발생하는 경우가 있다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">check_nan</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
    <span class="s">""" Check if there is NaN in tensor """</span>
    <span class="n">checker</span> <span class="o">=</span> <span class="bp">False</span>
    <span class="k">if</span> <span class="bp">True</span> <span class="ow">in</span> <span class="n">torch</span><span class="p">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
        <span class="n">checker</span> <span class="o">=</span> <span class="bp">True</span>
    <span class="k">return</span> <span class="n">checker</span>

<span class="k">def</span> <span class="nf">zero_filtering</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="s">"""
    Add eps value for zero embedding, because competition metric is cosine similarity
    Cosine Similarity will be returned NaN, when input value has zero, like as torch.clamp()
    """</span>
    <span class="n">eps</span> <span class="o">=</span> <span class="mf">1e-4</span>
    <span class="n">x</span><span class="p">[</span><span class="n">x</span> <span class="o">&lt;=</span> <span class="n">eps</span><span class="p">]</span> <span class="o">=</span> <span class="n">eps</span>
    <span class="k">return</span> <span class="n">x</span>

<span class="k">def</span> <span class="nf">nan_filtering</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">eps</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-4</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="s">"""
    Change eps value for NaN Embedding, because competition metric is cosine similarity
    Cosine Similarity will be returned NaN
    """</span>
    <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">nan</span><span class="o">=</span><span class="n">eps</span><span class="p">)</span>

<span class="k">class</span> <span class="nc">CLIPGEMPooling</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="s">"""
    Generalized Mean Pooling for Natural Language Processing
    This class version of GEMPooling for CLIP, Transfer from NLP Task Code
    ViT don't use attention mask, because input image shape will be same

    Mean Pooling &lt;= GEMPooling &lt;= Max Pooling
    Because of doing exponent to each token embeddings, GEMPooling is like as weight to more activation token

    In original paper, they use p=3, but in this class, we use p=4 because torch doesn't support pow calculation
    for negative value tensor, only for non-negative value in odd number exponent
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">auto_cfg</span><span class="p">:</span> <span class="n">AutoConfig</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">CLIPGEMPooling</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>

    <span class="o">@</span><span class="nb">staticmethod</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">last_hidden_state</span><span class="p">,</span> <span class="n">p</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="s">"""
        last_hidden_state.size: [batch_size, patches_sequence, hidden_size]
        1) Pow last_hidden_state with p and then take a averaging
        2) pow sum_embeddings with 1/p
        """</span>
        <span class="n">p_embeddings</span> <span class="o">=</span> <span class="n">zero_filtering</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nb">pow</span><span class="p">(</span><span class="n">last_hidden_state</span><span class="p">,</span> <span class="n">p</span><span class="p">))</span>
        <span class="c1"># Check NaN value in Embedding after applying torch.pow
</span>        <span class="k">if</span> <span class="n">check_nan</span><span class="p">(</span><span class="n">p_embeddings</span><span class="p">):</span>
            <span class="n">p_embeddings</span> <span class="o">=</span> <span class="n">nan_filtering</span><span class="p">(</span><span class="n">p_embeddings</span><span class="p">)</span>
        <span class="n">sum_embeddings</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">p_embeddings</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

        <span class="n">gem_embeddings</span> <span class="o">=</span> <span class="n">zero_filtering</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nb">pow</span><span class="p">(</span><span class="n">sum_embeddings</span><span class="p">,</span> <span class="mf">1.</span> <span class="o">/</span> <span class="n">p</span><span class="p">))</span>
        <span class="c1"># Check NaN value in Embedding after applying torch.pow
</span>        <span class="k">if</span> <span class="n">check_nan</span><span class="p">(</span><span class="n">gem_embeddings</span><span class="p">):</span>
            <span class="n">gem_embeddings</span> <span class="o">=</span> <span class="n">nan_filtering</span><span class="p">(</span><span class="n">gem_embeddings</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">gem_embeddings</span>

<span class="k">class</span> <span class="nc">CLIPMultipleNegativeRankingLoss</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="s">"""
    Multiple Negative Ranking Loss for CLIP Model
    main concept is same as original one, but append suitable for other type of model (Not Sentence-Transformers)
    if you set more batch size, you can get more negative pairs for each anchor &amp; positive pair
    Args:
        scale: output of similarity function is multiplied by this value =&gt; I don't know why this is needed
        similarity_fct: standard of distance metrics, default cosine similarity
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">reduction</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">scale</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">20.0</span><span class="p">,</span> <span class="n">similarity_fct</span><span class="o">=</span><span class="n">cos_sim</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">reduction</span> <span class="o">=</span> <span class="n">reduction</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">scale</span> <span class="o">=</span> <span class="n">scale</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">similarity_fct</span> <span class="o">=</span> <span class="n">similarity_fct</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">reduction</span> <span class="o">=</span> <span class="n">reduction</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">cross_entropy_loss</span> <span class="o">=</span> <span class="n">CrossEntropyLoss</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">reduction</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">embeddings_a</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">embeddings_b</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="n">similarity_scores</span> <span class="o">=</span> <span class="n">zero_filtering</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">similarity_fct</span><span class="p">(</span><span class="n">embeddings_a</span><span class="p">,</span> <span class="n">embeddings_b</span><span class="p">))</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">scale</span>
        <span class="k">if</span> <span class="n">check_nan</span><span class="p">(</span><span class="n">similarity_scores</span><span class="p">):</span>
            <span class="s">""" Check NaN Value in similarity_scores """</span>
            <span class="n">similarity_scores</span> <span class="o">=</span> <span class="n">nan_filtering</span><span class="p">(</span><span class="n">similarity_scores</span><span class="p">)</span>

        <span class="n">labels</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">(</span>
            <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">similarity_scores</span><span class="p">)),</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">long</span><span class="p">,</span>
            <span class="n">device</span><span class="o">=</span><span class="n">similarity_scores</span><span class="p">.</span><span class="n">device</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">cross_entropy_loss</span><span class="p">(</span><span class="n">similarity_scores</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
</code></pre></div></div>

<p>필자의 경우, 두 개의 입력 행렬에 각각  <code class="language-plaintext highlighter-rouge">sqrt()</code> 를 적용하고 두 행렬의 개별 원소 사이의 코사인 유사도를 구해야 했던 적이 있다. <code class="language-plaintext highlighter-rouge">sqrt</code> <strong><u>과정에서 너무 작은 값들이 입력으로 들어가</u></strong> <code class="language-plaintext highlighter-rouge">underflow</code> <strong><u>가 발생해 행렬에</u></strong> <code class="language-plaintext highlighter-rouge">zero-value</code> <strong><u>가 생겼고, 이를 모른채 코사인 유사도를 구하다가 한참을 위 에러와 싸웠던 적이 있다.</u></strong> 심지어 연산속도 향상을 위해서 <strong><code class="language-plaintext highlighter-rouge">torch.autocast</code></strong> 클래스의 <code class="language-plaintext highlighter-rouge">grad_scaler(float32 to float16)</code> 까지 적용하고 있었다.</p>

<h3 id="️-내가-해결한-방법"><code class="language-plaintext highlighter-rouge">🖍️ 내가 해결한 방법</code></h3>
<p>이 글을 읽는 당신이 만약 <code class="language-plaintext highlighter-rouge">sqrt</code> 혹은 <code class="language-plaintext highlighter-rouge">pow</code>를 활용하는 경우, <code class="language-plaintext highlighter-rouge">underflow</code> 방지를 위해서 <del>위 예시 코드처럼 꼭 적당한 입실론 값을 연산 전후에 필요에 따라 더해줄 것을 권장한다.</del> 입실론 값의 설정은 현재 자신이 사용하고 있는 부동 소수점 정확도에 맞게 설정해주면 될 것 같다. <code class="language-plaintext highlighter-rouge">float32</code> 를 사용하는 경우에는 대부분 <code class="language-plaintext highlighter-rouge">1e-6</code> 을 많이 사용하는 것 같다. 필자도 정확히 어떤 값이 적당한지 아직 잘 모르겠다… 그리고 딥러닝 실험하면서 <code class="language-plaintext highlighter-rouge">overflow</code> 때문에 <code class="language-plaintext highlighter-rouge">inf</code> 이 발생했던 적은 없었다.</p>

<p>입실론 값을 문제가 되는 연산 전에 일괄적으로 더할 경우, 아무리 작은 값이라도 연산 종류에 따라서 결과가 크게 왜곡되는 경우가 발생한다. 따라서 연산을 먼저 적용한 뒤 결과에 <code class="language-plaintext highlighter-rouge">NaN</code>, <code class="language-plaintext highlighter-rouge">Inf</code>, <code class="language-plaintext highlighter-rouge">Zero</code>가 발생하는지 체크하고, 발생한 부분에 한해서 입실론 값을 더해주는 커스텀 <code class="language-plaintext highlighter-rouge">function</code>울 정의해 문제를 해결했다.<br />
(위의 코드 예제 <code class="language-plaintext highlighter-rouge">check_nan</code>, <code class="language-plaintext highlighter-rouge">zero_filtering</code>, <code class="language-plaintext highlighter-rouge">nan_filtering</code>)</p>

<p>한편 <code class="language-plaintext highlighter-rouge">torch.autograd.set_detect_anomaly(True)</code> 를 훈련 루프 초반에 정의해주면, <code class="language-plaintext highlighter-rouge">NaN</code>이 발생하는 즉시 실행이 멈추고 <code class="language-plaintext highlighter-rouge">NaN</code>을 유발한 라인을 출력해준다. 꼭 활용해보자.</p>]]></content><author><name>qcqced</name><email>qcqced123@gmail.com</email></author><category term="Framework &amp; Library" /><category term="Pytorch" /><category term="Logsoftmax" /><category term="NaN" /><category term="Error Handling" /><summary type="html"><![CDATA[Pytorch Error: Backward NaN values]]></summary></entry><entry><title type="html">🖥️ RuntimeError: Attempting to deserialize object on CUDA device 2 but torch.cuda.device_count() is 1. Please use torch.load with map_location to map your storages to an existing device</title><link href="http://localhost:4000/framework-library/cuda-num/" rel="alternate" type="text/html" title="🖥️ RuntimeError: Attempting to deserialize object on CUDA device 2 but torch.cuda.device_count() is 1. Please use torch.load with map_location to map your storages to an existing device" /><published>2023-07-11T00:00:00+09:00</published><updated>2023-07-11T13:00:00+09:00</updated><id>http://localhost:4000/framework-library/cuda-device-num</id><content type="html" xml:base="http://localhost:4000/framework-library/cuda-num/"><![CDATA[<h3 id="-pytorch-잘못된-cuda-장치-번호-사용-문제"><code class="language-plaintext highlighter-rouge">🔢 Pytorch 잘못된 CUDA 장치 번호 사용 문제</code></h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span><span class="p">.</span><span class="n">load_state_dict</span><span class="p">(</span>
    <span class="n">torch</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">map_location</span><span class="o">=</span><span class="s">'cuda:0'</span><span class="p">)</span> 
<span class="p">)</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">pretrained model</code>, <code class="language-plaintext highlighter-rouge">weight</code>를 <code class="language-plaintext highlighter-rouge">load</code>하거나 혹은 훈련 루프를 <code class="language-plaintext highlighter-rouge">resume</code> 을 위해 <code class="language-plaintext highlighter-rouge">torch.load()</code> 를 사용할 때 마주할 수 있는 에러 로그다. 발생하는 이유는 현재 <code class="language-plaintext highlighter-rouge">GPU</code> 에 할당하려는 모델이 사전 훈련때 할당 되었던 <code class="language-plaintext highlighter-rouge">GPU</code> 번호와 현재 할당하려는 <code class="language-plaintext highlighter-rouge">GPU</code> 번호가 서로 상이하기 때문이다. 따라서 <code class="language-plaintext highlighter-rouge">torch.load</code>의 <code class="language-plaintext highlighter-rouge">map_location</code>인자에 현재 자신이 사용하려는 <code class="language-plaintext highlighter-rouge">GPU</code> 번호를 입력해주자.</p>]]></content><author><name>qcqced</name><email>qcqced123@gmail.com</email></author><category term="Framework &amp; Library" /><category term="Pytorch" /><category term="CUDA" /><summary type="html"><![CDATA[Pytorch Error: Wrong CUDA Device Number]]></summary></entry><entry><title type="html">🪢 assert len(optimizer_state[“found_inf_per_device”]) &amp;gt; 0, “No inf checks were recorded for this optimizer.” AssertionError: No inf checks were recorded for this optimizer.</title><link href="http://localhost:4000/framework-library/inf-per-device" rel="alternate" type="text/html" title="🪢 assert len(optimizer_state[“found_inf_per_device”]) &amp;gt; 0, “No inf checks were recorded for this optimizer.” AssertionError: No inf checks were recorded for this optimizer." /><published>2023-07-11T00:00:00+09:00</published><updated>2023-07-11T13:00:00+09:00</updated><id>http://localhost:4000/framework-library/found_inf_per_device</id><content type="html" xml:base="http://localhost:4000/framework-library/inf-per-device"><![CDATA[<h3 id="-optimizer가-손실값을-제대로-backward-할-수-없는-문제"><code class="language-plaintext highlighter-rouge">🤔 Optimizer가 손실값을 제대로 Backward 할 수 없는 문제</code></h3>

<p>텐서의 계산 그래프가 중간에 끊어져 옵티마이저가 그라디언트를 제대로 <code class="language-plaintext highlighter-rouge">Backward</code> 하지 못해 발생하는 에러다. 공부를 시작하고 정말 처음 마주하는 에러라서 정말 많이 당황했다. 래퍼런스 자료 역시 거의 없어서 해결하는데 애를 먹었던  쓰라린 사연이 있는 에러다. 이 글을 읽는 독자라면 대부분 텐서의 계산 그래프가 중간에 끊어진다는 것이 무슨 의미일지 이해하시지 못할거라 생각한다. 그게 정상이다. 필자 역시 알고 싶지 않았으나 욕심만 많고 멍청한 탓에… 알게 되었다. 아래 예시 코드를 먼저 살펴보자.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Before Append
</span><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">:</span> <span class="nb">dict</span><span class="p">,</span> <span class="n">position_list</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">feature</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
        <span class="n">feature</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">.</span><span class="n">last_hidden_state</span>
        <span class="n">pred</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">batch_size</span><span class="p">):</span>
            <span class="s">""" Apply Pooling &amp; Fully Connected Layer for each unique cell in batch (one notebook_id) """</span>
            <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">position_list</span><span class="p">[</span><span class="n">i</span><span class="p">])):</span>
                <span class="n">src</span><span class="p">,</span> <span class="n">end</span> <span class="o">=</span> <span class="n">position_list</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">idx</span><span class="p">]</span>
                <span class="n">embedding</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">pooling</span><span class="p">(</span><span class="n">feature</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">src</span><span class="p">:</span><span class="n">end</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="p">:].</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>  <span class="c1"># maybe don't need mask
</span>                <span class="n">logit</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">fc</span><span class="p">(</span><span class="n">embedding</span><span class="p">)</span>
                <span class="n">pred</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">logit</span><span class="p">)</span>  
            <span class="n">pred</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">pred</span>

<span class="c1"># After Append
</span><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">:</span> <span class="nb">dict</span><span class="p">,</span> <span class="n">position_list</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">feature</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
        <span class="n">feature</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">.</span><span class="n">last_hidden_state</span>
        <span class="n">pred</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([],</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">batch_size</span><span class="p">):</span>
            <span class="s">""" Apply Pooling &amp; Fully Connected Layer for each unique cell in batch (one notebook_id) """</span>
            <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">position_list</span><span class="p">[</span><span class="n">i</span><span class="p">])):</span>
                <span class="n">src</span><span class="p">,</span> <span class="n">end</span> <span class="o">=</span> <span class="n">position_list</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">idx</span><span class="p">]</span>
                <span class="n">embedding</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">pooling</span><span class="p">(</span><span class="n">feature</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">src</span><span class="p">:</span><span class="n">end</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="p">:].</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>  <span class="c1"># maybe don't need mask
</span>                <span class="n">logit</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">fc</span><span class="p">(</span><span class="n">embedding</span><span class="p">)</span>
                <span class="n">pred</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cat</span><span class="p">([</span><span class="n">pred</span><span class="p">,</span> <span class="n">logit</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">pred</span>
</code></pre></div></div>

<p>다음 코드는 필자가 공부를 위해 만든 모델 클래스의 <code class="language-plaintext highlighter-rouge">forward</code> 메서드이다. 전자는 이번 포스팅의 주제인 에러를 일으킨 주인공이고, 후자는 에러를 수정한 이후 정상적으로 작동하는 코드다. 독자 여러분들도 두 코드에 어떤 차이가 있는지 스스로 질문을 던지면서 읽어주시길 바란다.</p>
<p align="center">
<img src="/assets/images/marginrankingloss.png" alt="Model Overview" class="align-center image-caption" width="100%&quot;, height=&quot;50%" />
<em>Modeling Overview</em>
</p>

<p>위의 코드들은 <code class="language-plaintext highlighter-rouge">DeBERTa-V3-Large</code> 의 마지막 인코더 레이어가 반환하는 <code class="language-plaintext highlighter-rouge">last_hidden_state</code> 를 미리 설정한 서브 구간별로 나누고 개별적으로 <code class="language-plaintext highlighter-rouge">pooling &amp; fully connected layer</code> 에 통과시켜 로짓값으로 변환하기 위해 만들었다. 쉽게 말해 입력으로 토큰(단어) 384개 짜리 문장을 하나 넣었고, 모델은 384개의 개별 토큰에 대한 임베딩 값을 반환했는데 그것을 전부 이용하는 것이 아니라 예를 들어 <code class="language-plaintext highlighter-rouge">2번~4번</code> 토큰을 1번 구간, <code class="language-plaintext highlighter-rouge">6번~20번</code> 토큰을 2번 구간, <code class="language-plaintext highlighter-rouge">30번~50번</code> 토큰을 3번 구간 … <code class="language-plaintext highlighter-rouge">370번~380번</code> 토큰을 30번 구간으로 설정하고 구간 별로 따로 <code class="language-plaintext highlighter-rouge">pooling &amp; fully connected layer</code> 에 통과시켜 로짓을 구하는 것이다. 일반적이라면 1개의 문장에서 1개의 최종 로짓값이 도출되는 것이라면, 위 코드는 30개의 로짓값이 도출된다.</p>

<h3 id="️-내가-해결한-방법"><code class="language-plaintext highlighter-rouge">🖍️ 내가 해결한 방법</code></h3>

<p>코드 이해를 위한 설명은 마쳤으니 본격적으로 본 에러와 어떤 연관이 있는지 살펴보자. <code class="language-plaintext highlighter-rouge">Before</code> 코드는 <code class="language-plaintext highlighter-rouge">pred</code> 라는 리스트에 개별 구간에 대한 로짓값을 <code class="language-plaintext highlighter-rouge">append</code> 하고 마지막에 <code class="language-plaintext highlighter-rouge">torch.as_tensor</code>를 활용해 텐서로 변환하고 있다. 한편 후자는 <code class="language-plaintext highlighter-rouge">pred</code> 를 깡통 텐서로 선언한 뒤, <code class="language-plaintext highlighter-rouge">torch.cat</code>으로 모든 구간에 대한 로짓값을 하나의 텐서 구조체에 담고 있다.</p>

<p>얼핏보면 크게 다른점이 없어 보인다. 하지만 전자는 텐서 구조체를 새로 정의 하면서 <code class="language-plaintext highlighter-rouge">torch.Tensor[[logit1], [logit2], ….]</code> 형태를 갖고 후자는 <code class="language-plaintext highlighter-rouge">torch.Tensor[logit1, logit2, …]</code> 형태를 갖는다. 서로 다른 텐서 구조체를 그대로 모델 객체의 <code class="language-plaintext highlighter-rouge">forward</code> 메서드 및 <code class="language-plaintext highlighter-rouge">loss function</code>에 통과시키고 오차 역전을 하면 어떤 일이 생기는지 지금부터 알아보자.</p>

<p>전자의 경우는 도출된 손실함수의 미분값이 정의된 계산 그래프를 타고 역전될 수 없다. 이유는 전자의 <code class="language-plaintext highlighter-rouge">pred</code> 가 forward 메서드 내부에서 새로이 정의 되었기 때문이다. 후자 역시 마찬가지 아닌가 싶을 것이다. 후자의 <code class="language-plaintext highlighter-rouge">pred</code> 역시 <code class="language-plaintext highlighter-rouge">forward</code> 메서드 내부에서 정의된 것은 맞지만 <code class="language-plaintext highlighter-rouge">torch.cat</code>을 사용하면서 구간의 로짓값들 위에 새로이 차원을 덮어쓰는것이 아니게 된다. 이것이 매우 중요한 차이가 되는데, 후자와 같은 형태가 되는 경우, 손실값으로 부터 <code class="language-plaintext highlighter-rouge">Backward</code> 되는 미분값들이 곧바로 <code class="language-plaintext highlighter-rouge">forward</code> 과정에서 기록된 자신의 계산 그래프로 찾아 갈 수 있다. 한편 전자의 경우 새롭게 덮어 쓰여진 차원 때문에 미분값들이 자신의 계산 그래프로 찾아갈 수 없게 된다. 따라서 옵티마이저가 더 이상 <code class="language-plaintext highlighter-rouge">Backward</code> 를 수행할 수 없어 제목과 같은 에러를 반환하게 되는 것이다.</p>

<p>처음 이 에러를 마주했을 때는  <code class="language-plaintext highlighter-rouge">found_inf_per_device</code>, <code class="language-plaintext highlighter-rouge">No inf checks</code> 라는 키워드에 꽂혀 (특히 <code class="language-plaintext highlighter-rouge">inf</code>)  <code class="language-plaintext highlighter-rouge">&lt;RuntimeError: Function 'LogSoftmaxBackward0' returned nan values in its 0th output&gt;</code> 이것과 유사한 종류의 에러라 생각하고 열심히 연산 과정에 문제가 없는지, 어디서 NaN이 발생하는지, 학습률을 너무 크게 설정했는지 등을 검토하며 하루를 날렸었던 기억이 있다.</p>]]></content><author><name>qcqced</name><email>qcqced123@gmail.com</email></author><category term="Framework &amp; Library" /><category term="Pytorch" /><category term="CUDA" /><category term="Error Handling" /><summary type="html"><![CDATA[Pytorch Error: Optimizer can't backward loss]]></summary></entry><entry><title type="html">🚚 RuntimeError: stack expects each tensor to be equal size, but got [32] at entry 0 and [24] at entry 1</title><link href="http://localhost:4000/framework-library/dataloader-collatefn" rel="alternate" type="text/html" title="🚚 RuntimeError: stack expects each tensor to be equal size, but got [32] at entry 0 and [24] at entry 1" /><published>2023-07-11T00:00:00+09:00</published><updated>2023-07-12T13:00:00+09:00</updated><id>http://localhost:4000/framework-library/dataloader-collate</id><content type="html" xml:base="http://localhost:4000/framework-library/dataloader-collatefn"><![CDATA[<h3 id="-가변-길이의-텐서를-데이터로더에-전달하는-경우-"><code class="language-plaintext highlighter-rouge">📏 가변 길이의 텐서를 데이터로더에 전달하는 경우 </code></h3>

<p>커스텀 데이터 클래스와 데이터로더를 통해 반환되는 데이터 인스턴스의 텐서 크기가 일정하지 않아 발생하는 에러다. 특히 자연어 처리에서 자주 찾아 볼 수 있는데 데이터로더 객체 선언 시, 매개변수 옵션 중에 <code class="language-plaintext highlighter-rouge">collate_fn=collate</code> 를 추가해주면 해결 가능한 에러다. 이 때 매개변수 <code class="language-plaintext highlighter-rouge">collate_fn</code> 에 전달하는 값(메서드)은 사용자가 직접 정의해줘야 한다. 허깅페이스 라이브리러에 상황에 맞게 미리 제작된 <code class="language-plaintext highlighter-rouge">collate</code> 메서드를 지원해주고 있기 때문에 잘 이용하면 된다. 필자의 경우에는 커스텀으로 직접 정의한 메서드, 객체를 사용하고 있다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 데이터 로더 예시
</span><span class="n">loader_train</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span>
            <span class="n">train_dataset</span><span class="p">,</span>
            <span class="n">batch_size</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">batch_size</span><span class="p">,</span>
            <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
            <span class="n">worker_init_fn</span><span class="o">=</span><span class="n">seed_worker</span><span class="p">,</span>
            <span class="n">collate_fn</span><span class="o">=</span><span class="n">MiniBatchCollate</span><span class="p">,</span>  <span class="c1"># 여기에 사용하려는 collate function 혹은 객체를 전달하자!!
</span>            <span class="n">generator</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">generator</span><span class="p">,</span>
            <span class="n">num_workers</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">num_workers</span><span class="p">,</span>
            <span class="n">pin_memory</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
            <span class="n">drop_last</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
        <span class="p">)</span>

<span class="c1"># collate 메서드 예시: 
</span><span class="k">class</span> <span class="nc">MiniBatchCollate</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="s">"""
    Collate class for torch.utils.data.DataLoader  
    This class object to use variable data such as NLP text sequence
    If you use static padding with AutoTokenizer, you don't need this class 
    But if you use dynamic padding with AutoTokenizer, you must use this class object &amp; call
    Args:
        batch: data instance from torch.utils.data.DataSet
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">DataLoader</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">batch</span> <span class="o">=</span> <span class="n">batch</span>

    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">dict</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">],</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
        <span class="n">inputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">position_list</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">batch</span>
        <span class="n">labels</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="n">rnn</span><span class="p">.</span><span class="n">pad_sequence</span><span class="p">(</span>
            <span class="n">labels</span><span class="p">,</span>
            <span class="n">batch_first</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
            <span class="n">padding_value</span><span class="o">=-</span><span class="mi">1</span>
        <span class="p">)</span>
        <span class="n">position_list</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="n">rnn</span><span class="p">.</span><span class="n">pad_sequence</span><span class="p">(</span>
            <span class="n">position_list</span><span class="p">,</span>
            <span class="n">batch_first</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
            <span class="n">padding_value</span><span class="o">=-</span><span class="mi">1</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">position_list</span>

<span class="k">def</span> <span class="nf">collate</span><span class="p">(</span><span class="n">inputs</span><span class="p">):</span>
    <span class="s">"""
    slice input sequence by maximum length sequence in mini-batch, used for speed up training
    if you want slice other variable such as label feature, you can add param on them
    This Function should be used after DataLoader return mini-batch instance
    Args:
        inputs: list of dict, dict has keys of "input_ids", "attention_mask", "token_type_ids"    
    """</span>
    <span class="n">mask_len</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">inputs</span><span class="p">[</span><span class="s">"attention_mask"</span><span class="p">].</span><span class="nb">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">).</span><span class="nb">max</span><span class="p">())</span>
    <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">inputs</span><span class="p">.</span><span class="n">items</span><span class="p">():</span>
        <span class="n">inputs</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="n">k</span><span class="p">][:,</span> <span class="p">:</span><span class="n">mask_len</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">inputs</span>
</code></pre></div></div>

<p>일반적으로 <code class="language-plaintext highlighter-rouge">collate</code> 는 메서드로 구현해서 사용하지만, 위 코드처럼 객체로 구현하고 내부에 <code class="language-plaintext highlighter-rouge">__call__</code> 를 정의해 사용하는 방법도 있다. 필자 역시 단일 메서드 형태를 계속해서 사용하다가 최근 들어 에폭 한 번에 서로 다른 데이터 세트 및 모델을 훈련 시켜야 하는 상황을 마주한 이후 객체 형태로 다시 구현해 사용하고 있다.</p>

<p>한편 예시 코드 가장 마지막 <code class="language-plaintext highlighter-rouge">collate</code> 메서드는 입력 시퀀스가 huggingface의 <code class="language-plaintext highlighter-rouge">AutoTokenizer.encode_plus</code> 를 이용해 사용자 지정 <code class="language-plaintext highlighter-rouge">max_len</code>까지 패딩을 마친 상태라는 가정하에 구현 되었다. 해당 메서드는 위에 발생한 에러를 해결하기 위함보다, 미니 배치에 속한 전체 데이터 중에서 최대 길이가 사용자 지정 <code class="language-plaintext highlighter-rouge">max_len</code>까지 미치지 못하는데 패딩이 된 경우에 사용하기 위해 만들었다. 불필요한 패딩을 <code class="language-plaintext highlighter-rouge">trucation</code> 하여 뉴럴 네트워크의 학습 속도를 높이기 위함이다. 해당 메서드는 포스팅의 제목에 달린 에러를 해결하는데 사용할 수는 없지만  <code class="language-plaintext highlighter-rouge">collate</code> 기능을 언급하는 김에 생각이나 같이 정리해봤다. 이 메서드는 <code class="language-plaintext highlighter-rouge">torch.utils.data.DataLoader</code> 의 인자가 아니라, 메인 학습 루프 내부에 사용한다. 다시 말해, 데이터로더가 배치 인스턴스를 반환한 다음 사용하면 된다는 것이다. 패딩방식과 <code class="language-plaintext highlighter-rouge">collate</code> 기능에 대한 자세한 설명은 다른 포스팅에서 다루도록 하겠다.</p>

<p>반면 <code class="language-plaintext highlighter-rouge">MiniBatchCollate</code> 객체는 <code class="language-plaintext highlighter-rouge">torch.utils.data.DataLoader</code> 의 <code class="language-plaintext highlighter-rouge">collate_fn</code> 인자에 전달하면 된다. 필자의 경우는 <code class="language-plaintext highlighter-rouge">Dynamic Padding</code> 기법을 사용하기 때문에 미니 배치 내부의 인스턴스들이 서로 다른 시퀀스 길이를 갖는 경우가 발생한다. 데이터로더는 미니 배치에 속하는 데이터의 길이가 통일되지 않으면 배치 단위로 데이터를 묶을 수 없게 된다. 따라서 미니 배치 단위의 길이 통일을 위해 <code class="language-plaintext highlighter-rouge">torch.nn.utils.rnn.pad_sequence</code> 메서드를 사용한다. 이 메서드는 입력한 미니 배치 데이터 중에서 가장 긴 시퀀스를 기준으로 모든 데이터 길이를 통일한다. <code class="language-plaintext highlighter-rouge">batch_first=True</code> 를 주목하자. 이 인자를 <code class="language-plaintext highlighter-rouge">False</code> 로 설정할 경우, 배치 차원이 맨 앞이 아니라 중간에 정의된다. 일반적으로는 배치 차원을 맨 앞에 두는 워크플로우를 사용하기 때문에 꼭 해당 인자를 <code class="language-plaintext highlighter-rouge">True</code> 로 설정하고 사용하자.</p>]]></content><author><name>qcqced</name><email>qcqced123@gmail.com</email></author><category term="Framework &amp; Library" /><category term="Pytorch" /><category term="DataLoader" /><category term="collate_fn" /><category term="Dynamic Padding" /><category term="Padding" /><summary type="html"><![CDATA[Pytorch Error: Dataloader get non-equal size of tensor]]></summary></entry><entry><title type="html">📐 Inner Product: Projection Matrix, Least Sqaure Method</title><link href="http://localhost:4000/linear-algebra/inner-product" rel="alternate" type="text/html" title="📐 Inner Product: Projection Matrix, Least Sqaure Method" /><published>2023-07-10T00:00:00+09:00</published><updated>2023-07-11T13:00:00+09:00</updated><id>http://localhost:4000/linear-algebra/Inner_Product%20copy</id><content type="html" xml:base="http://localhost:4000/linear-algebra/inner-product"><![CDATA[<h3 id="concept-of-inner-product"><code class="language-plaintext highlighter-rouge">💡 Concept of Inner Product</code></h3>

\[a^Tb = ||a||•||b||cos\theta\]

<p>내적은 <code class="language-plaintext highlighter-rouge">Inner Product</code>, <code class="language-plaintext highlighter-rouge">Dot Product</code>, <code class="language-plaintext highlighter-rouge">Scalar Product</code>로 불리며 두 벡터의 유사도, 즉 닮은 정도를 구하는데 사용되는 벡터•행렬 연산의 한 종류다. 두 벡터의 정사영과도 동일한 개념으로 사용된다. 위 수식의 우변에 주목해보자. 
$||a||cos\theta$ 는 벡터 $a$를 벡터 $b$에 정사영 내린 크기로 해석할 수 있다. 한편 $||b||$ 는 벡터 $b$의 길이이므로, 결국 내적이란 한 벡터를 다른 벡터에 정사영 해준 결과와 벡터 크기의 곱이 된다.</p>

<p align="center">
<img src="/assets/images/inner_product.png" alt="Inner Product Image" class="align-center image-caption" width="50%&quot;, height=&quot;50%" />
<strong><em><a href="https://wikidocs.net/22384">Inner Product Image</a></em></strong>
</p>

<p>내적을 기하학적으로 생각해보자. $\theta$는 두 벡터 사이의 끼인각이다. 그렇다면 끼인각과 내적의 크기 사이의 상관관계는 어떻게 될까?? 내적의 의미는 서로 같은 정도가 아니라 <code class="language-plaintext highlighter-rouge">“서로 닮은 정도”</code>라고 했다. 중학교 때 배웠던 닮음 개념을 떠올려보자. 닮음이란 유클리드 공간에서 모든 각을 보존하며 모든 거리를 일정한 비율로 확대 또는 축소시키는 아핀 변환이다. 다시 말해, 어떤 두 도형을 닮았다고 말하는데 절대적인 거리 혹은 길이가 같을 필요가 없다. 그래서 두 벡터의 닮은 정도를 파악하려면 우리는 벡터의 길이 대신 방향이라는 물리량에 주목해야 한다. 벡터는 직선으로 표현되기 때문에 두 벡터가 완전히 닮았다고 말하려면, 끼인각의 크기가 0이 되어야 한다. 따라서 끼인각의 크기가 작아질수록 두 벡터의 닮은 정도는 커지게 되고, $\theta=0$ 에서 내적값은 최대가 된다. 만약 $\theta=90$ 이라면 내적값은 어떻게 될까?? 삼각비 정의에 의해 $cos \theta = 0$ 이 될 것이다. 따라서 내적값은 0이 되고, 두 벡터는 서로 전혀 닮지 않았다고 판단할 수 있다. 한편 $\theta=180$ 일 때 내적값은 최소가 되고, 두 벡터는 음의 방향으로 닮은 상태를 갖는다.</p>

\[N_a=\frac{a}{\sqrt{a^Ta}} = \frac{a}{||a||}\]

<p>한편 내적을 벡터 정규화에도 사용할 수 있는데, 방법은 위 수식과 같다. 일반적으로 벡터를 정규화하는 방법은 벡터를 벡터의 크기로 나누면 된다고만 알고 있을 것이다. 하지만 벡터의 전치와 벡터와의 내적으로도 벡터의 크기를 구할 수 있기 때문에 (벡터의 전치와 벡터는 $cos\theta=0$이 되기 때문) 위 등식이 성립한다. 한편, 벡터 정규화 결과는 벡터의 길이가 1이 되기 때문에 <code class="language-plaintext highlighter-rouge">‘단위 벡터’</code> 라고 정의한다. 벡터의 길이가 1이라는 점을 이용하면, 단위 벡터에는 방향에 대한 물리량만 남아 있다는 사실을 알 수 있다. 그래서 우리가 어떤 벡터의 방향 정보를 얻고 싶을 때, 벡터의 정규화를 사용하면 간단하게 구할 수 있다.</p>

\[\frac{b^Ta}{||b||} * \frac{b}{||b||} = \frac{b^Ta}{\sqrt{b^Tb}} * \frac{b}{\sqrt{b^Tb}} = \frac{b^Ta}{b^Tb}*b\]

<p>내적 공식과 벡터 정규화 공식을 함께 사용하면 벡터 $a$를 벡터 $b$에 정사영 내렸을 때 도출되는 벡터 또한 직접 구할 수 있다. 위 수식을 살펴보자. 각 변의 좌측 항은 내적 공식에 의해 
$||a||\cos\theta$ 가 된다. 지금 우리가 구하고 싶은 것은 정사영 내린 벡터 자체인데, $||a||\cos\theta$ 은 크기에 대한 물리량만 담고 있을뿐 방향에 대한 정보가 담겨 있지 않다. 앞에서 언급했듯이, 벡터 정규화 결과는 해당 벡터의 방향에 대한 물리량을 의미한다. 따라서 우변에 벡터 $b$ 의 방향(정규화 결과)를 곱하게 되는 것이다.</p>

\[(a - \hat{x}•b)^T•\ b\hat{x} = 0 \\
\hat{x} = \frac{b^Ta}{b^Tb} \\
\hat{x}•b = \frac{b^Ta}{b^Tb}*b\]

<p>한편, 정사영 내린 벡터(벡터 
$\frac{a^Tb}{b^Tb}*b$)를 기준 벡터(벡터 $b$)의 스칼라배 해준 벡터로 생각해도 쉽게 구할 수 있다. 스칼라를 미지수로 두고 정사영 내린 벡터를 $b•\hat{x}$라고 정의하면 우리는 벡터 $a$를 빗변, 정사영 내린 벡터를 밑변으로 하는 직각삼각형의 높이를 구할 수 있게 된다. 정사영 내린 벡터 $b•\hat{x}$에 마이너스 부호를 취해 반대 방향으로 뒤집어주면 우리는 직각삼각형의 높이를 밑변과 빗변의 합($a - b•\hat{x}$)으로 나타낼 수 있기 때문이다. 따라서 밑변과 높이의 끼인각이 수직이라는 점을 내적 공식에 적용해 우리는 다음 수식을 풀어내면 정사영 내린 벡터를 얻을 수 있다. 정사영 내린 벡터를 실제로는 <code class="language-plaintext highlighter-rouge">투영 벡터</code>라고 정의한다.</p>

<h3 id="-what-is-projection-matrix"><code class="language-plaintext highlighter-rouge">🔢 What is Projection Matrix</code></h3>

<p>벡터 $a$와 벡터 $b$의 변화에 따른 투영 벡터 크기의 추이를 살펴보자. 벡터 $a$가 만약 2배 커진다면 $a$가 분자에만 있기 때문에 투영 벡터 역시 그대로 2배 증가할 것이다. 반면 벡터 $b$는 2배가 커져도 분자•분모 모두 동일하게 2개씩  $b$가 있기 때문에 이전과 변화가 없다. 따라서 투영 벡터의 크기는 전적으로 벡터 $a$에 의존적이다. 다시 말해, 벡터 $a$는 무언가 매개체에 의해 벡터 $b$로 정사영 되고 있으며 그 매개체를 우리는 <code class="language-plaintext highlighter-rouge">projection matrix</code> 라고 한다.</p>

\[P = \frac{bb^T}{b^Tb}\]

<p>분모는 행벡터와 열벡터의 내적이라서 스칼라(상수), 분자는 열벡터와 행벡터의 곱이라서 행렬 형태가 될 것이다. 따라서 끝에 <code class="language-plaintext highlighter-rouge">Matrix</code> 라는 단어를 붙이게 되었다. 이러한 <code class="language-plaintext highlighter-rouge">Projection Matrix</code>는 $n$차원 벡터에도 동일하게 적용할 수 있기 때문에 훗날 차원축소 혹은 선형변환 같은 테크닉으로 머신러닝, 딥러닝에서 유용하게 사용된다.</p>

<h3 id="️-least-square-method"><code class="language-plaintext highlighter-rouge">🖍️ Least Square Method</code></h3>

\[Measurement = Ax + n\]

<p>$(a - \hat{x}•b)^T$
를 에러 벡터 $e$로 치환하면 내적•정사영을 <code class="language-plaintext highlighter-rouge">Least Square Method</code> (최소 자승법)으로 해석할 수도 있다. 예를 들어 5차원의 공간에서 2차원의 공간으로 <code class="language-plaintext highlighter-rouge">span</code>하는 행렬 $A$(<code class="language-plaintext highlighter-rouge">[5x2]</code>, <code class="language-plaintext highlighter-rouge">full column rank</code>) 그리고 5차원 공간에서 정의되는 벡터 $b$가 있다고 가정해보자. 현재 상황은 $C(A)$와 벡터 $b$ 사이의 해가 존재하지 않는 상태, 즉 모델의 예측값과 실제 정답이 일치하지 않는다는 것이다. 비록 해가 없지만 예측값을 그래도 최대한 정답에 가깝게 위치 시켜보자는 취지에서 나온 것이 바로 <code class="language-plaintext highlighter-rouge">최소 자승법</code>이다. 최소자승법은 $||e||$를 모델의 예측과 정답 사이 오차로 정의하는데, 이 때 $L_2$의 제곱근 연산을 피하기 위해서 $||e||^2$을 최적화 한다. 오차의 제곱을 최소화한다는 동작 방식에 맞게 <code class="language-plaintext highlighter-rouge">Least Square</code> 라는 이름을 갖게 되었다.<br />
한편, 이 오차를 최대한 줄이는 방법은 무엇일까?? $C(A)$ 상에서 정의되는 벡터 $b\hat{x}$와 에러 벡터 $e$ 사이의 내적(두 벡터가 수직)값이 0일 때 벡터 $e$의 길이가 최소 된다는 점을 이용해 미지수 $\hat{x}$를 찾고 최적화 식에 넣어 오차 제곱이 최소가 되는 계수를 구해주면 된다.</p>

<p>마지막으로 <code class="language-plaintext highlighter-rouge">Dot Product</code>, <code class="language-plaintext highlighter-rouge">Scalar Product</code> , <code class="language-plaintext highlighter-rouge">Inner Product</code>는 넓은 의미에서 모두 내적에 포함된다. 하지만 미세한 의미 차이는 있다. 하나 하나 살펴보자. 먼저 <code class="language-plaintext highlighter-rouge">Dot Product</code>란, 내적을 유클리드 좌표계에서 정의할 때 사용되는 명칭이며, 연산 기호가 점곱이라는 점을 강조하기 위해 <code class="language-plaintext highlighter-rouge">Dot</code> 이라는 단어를 사용하게 되었다. 한편, <code class="language-plaintext highlighter-rouge">Scalar Product</code> 역시 내적을 유클리드 좌표계에서 정의할 때 사용하는 명칭이지만, 그 결과가 스칼라 값이라는 점을 강조하기 위해 <code class="language-plaintext highlighter-rouge">Scalar Product</code>라고 명명했다. 마지막으로 <code class="language-plaintext highlighter-rouge">Inner Product</code>는 벡터 공간에서 정의 되어 행렬 같은 다른 개체들에 대해서 확장 적용이 가능하다는 점에서 <code class="language-plaintext highlighter-rouge">Dot Product</code>  , <code class="language-plaintext highlighter-rouge">Scalar Product</code> 보다 더 일반적인 개념으로 볼 수 있겠다.</p>]]></content><author><name>qcqced</name><email>qcqced123@gmail.com</email></author><category term="Linear Algebra" /><category term="Linear Algebra" /><category term="Inner Product" /><category term="Projection Matrix" /><category term="내적" /><category term="정사영" /><summary type="html"><![CDATA[💡 Concept & Insight of Inner Product]]></summary></entry><entry><title type="html">📏 Lp-Norm: Concept &amp;amp; Insight</title><link href="http://localhost:4000/linear-algebra/lp-norm" rel="alternate" type="text/html" title="📏 Lp-Norm: Concept &amp;amp; Insight" /><published>2023-07-04T00:00:00+09:00</published><updated>2023-07-05T13:00:00+09:00</updated><id>http://localhost:4000/linear-algebra/Lp_Norm</id><content type="html" xml:base="http://localhost:4000/linear-algebra/lp-norm"><![CDATA[\[||x||_p = (∑_{i=1}^n |x_i|^p)^{1/p}\]

<p><strong><code class="language-plaintext highlighter-rouge">Lp-Norm</code></strong>은 <code class="language-plaintext highlighter-rouge">Lebesgue</code>라는 프랑스 수학자에 의해 고안된 개념으로<strong>,</strong> 기계학습을 공부하는 사람이라면 지겹도록 듣는 <code class="language-plaintext highlighter-rouge">L2-Norm</code>, <code class="language-plaintext highlighter-rouge">L1-Norm</code>을 일반화 버전이라고 생각하면 된다. <strong><u>다시 말해, 벡터의 크기를 나타내는 표현식을 일반화한</u></strong> 것이 바로 <code class="language-plaintext highlighter-rouge">Lp-Norm</code> 이며 수식은 위와 같다.</p>

<p><code class="language-plaintext highlighter-rouge">p=1</code>이라고 가정하고 수식을 전개해보자. 
$||x||_1 = (|x_1|^1 + |x_2|^1+ … + |x_n|^1)^{1/1}$이 된다. 우리가 아는 <code class="language-plaintext highlighter-rouge">L1-Norm</code> 의 수식과 동일하다.</p>

<p>그렇다면 <code class="language-plaintext highlighter-rouge">p=2</code>일 때 수식을 살펴보자. 
$||x||_2 = (|x_1|^2 + |x_2|^2+ … + |x_n|^2)^{1/2}$으로 전개 된다는 것을 알 수 있다. 역시 우리가 맨날 보는 <code class="language-plaintext highlighter-rouge">L2-Norm</code> 과 동일하다.</p>

<p><code class="language-plaintext highlighter-rouge">L1-Norm</code>은 맨허튼 거리, <code class="language-plaintext highlighter-rouge">L2-Norm</code> 은 유클리드 거리를 의미한다는 것은 익히 들어 봤을 것이다. 만약 $p=∞$라면, 수식은 어떻게 될까, 과연 어떤 의미를 갖고 있을까??</p>

<p>이전과 똑같이 전개해보면
\(||x||_∞ = (|x_1|^∞ + |x_2|^∞+ ... + |x_n|^∞)^{1/∞}\), 이렇게 식이 도출될 것이다. 이제 괄호 내부 원소들의 지수가 <code class="language-plaintext highlighter-rouge">무한대</code>라는 점에 주목해보자. 직관적으로 무한대 값들 사이의 덧셈, 곱셈의 결과는 <code class="language-plaintext highlighter-rouge">무한대</code> 라는 것을 알 수 있다. 그렇다면 우리는 위 수식에서 절대값이 가장 큰 $|x_i|$만 남겨도 역시 무한대 값을 얻을 수 있다. <strong><u>무한대는 미지수 개념에 가깝지 실제 실수 개념은 아니기 때문이다.</u></strong> 따라서 괄호 내부에는 $|x_i|^p$ 값만 남게 되고 괄호 밖의 $1/p$와 남은 연산을 해주면 결국 $|x_i|$만 남게 된다. 따라서 $||x||_∞ = max(|x_1|, \ |x_2|, \ … \ , |x_n|)$가 된다.</p>

<p>이와 같은 성질 때문에 <code class="language-plaintext highlighter-rouge">Lp-Norm</code> 은 <code class="language-plaintext highlighter-rouge">Lp-Pooling</code> 으로도 해석할 수 있으며, 수식의 우변에  $1/n$을 곱해주면 <code class="language-plaintext highlighter-rouge">Generalized Mean Pooling</code> 이 된다는 사실을 알 수 있다. 결국 <code class="language-plaintext highlighter-rouge">Norm</code>과 <code class="language-plaintext highlighter-rouge">Pooling</code> 은 같은 개념이었던 것이다. 
<strong>그래서 위에서 살펴본 $ L_∞ $ 역시 <code class="language-plaintext highlighter-rouge">Max Pooling</code> 이라 해석이 가능해진다.</strong></p>

<p>여담으로 맨앞의 대문자 L은 <code class="language-plaintext highlighter-rouge">Lebesgue</code> 의 이름에서 본따왔다고 알려져 있다. 그리고 예전부터 $L_2$값을 수식으로 표현할 때 왜 짝대기 두개를 사용할까 항상 궁금했는데  $L_p$와 <code class="language-plaintext highlighter-rouge">일반 절대값</code>을 구분하기 위해 짝대기를 두 개 사용하게 되었다고 한다.</p>
<p align="center">
<img src="/assets/images/220px-Vector-p-Norms_qtl1.svg.png" alt="Lp-Norm Image" class="align-center image-caption" width="50%&quot;, height=&quot;50%" />
<em>Lp-Norm</em>
</p>

<p>위 자료는 
$L_p$
norm을 p값 변화 추이에 따라 기하학적으로 표현한 그림이다. <code class="language-plaintext highlighter-rouge">p=1</code> 일 때는 $L_1: |x| + |y| =1$가 되기 때문에 마름모 형태의 영역을 갖는다. 한편 <code class="language-plaintext highlighter-rouge">p=2</code> 일 때는 $L_2: x^2 + y^2 =1^2$가 되기 때문에 원의 영역을 갖는다. $p=∞$ 일 때는 $L_∞: max(|x_1|…|x_n|) = 1$ 이 되기 때문에 정사각형 형태의 영역을 갖게 될 것이다.</p>]]></content><author><name>qcqced</name><email>qcqced123@gmail.com</email></author><category term="Linear Algebra" /><category term="Linear Algebra" /><category term="Norm" /><category term="Pooling" /><summary type="html"><![CDATA[Concept of Lp-Norm & GeM Pool]]></summary></entry></feed>