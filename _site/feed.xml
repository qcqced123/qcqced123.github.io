<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2023-08-17T17:16:51+09:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">AI/Business Study Log</title><subtitle>NLP, Marketing</subtitle><author><name>qcqced</name><email>qcqced123@gmail.com</email></author><entry><title type="html">ğŸ‘¨â€ğŸ’»ğŸÂ [Python] Object Attribute &amp;amp; Assertion Function</title><link href="http://localhost:4000/python/attribute_function" rel="alternate" type="text/html" title="ğŸ‘¨â€ğŸ’»ğŸÂ [Python] Object Attribute &amp;amp; Assertion Function" /><published>2023-08-17T00:00:00+09:00</published><updated>2023-08-18T02:00:00+09:00</updated><id>http://localhost:4000/python/python_object_attribute_func</id><content type="html" xml:base="http://localhost:4000/python/attribute_function"><![CDATA[<h3 id="-attribute-function"><code class="language-plaintext highlighter-rouge">ğŸ§§ Attribute Function</code></h3>

<p>ì´ë²ˆ í¬ìŠ¤íŒ…ì€ <code class="language-plaintext highlighter-rouge">Python</code> ì½”ë“œë¥¼ ì‘ì„±í•˜ë©´ì„œ ê°ì²´ì™€ ë‚´ë¶€ ë©”ì„œë“œì— ê´€ë ¨í•œ ì²˜ë¦¬ê°€ í•„ìš”í•  ë•Œ ê°€ì¥ ë§ì´ ì‚¬ìš©í•˜ê²Œ ë˜ëŠ” <code class="language-plaintext highlighter-rouge">getattr</code>, <code class="language-plaintext highlighter-rouge">setattr</code> , <code class="language-plaintext highlighter-rouge">delattr</code> , <code class="language-plaintext highlighter-rouge">hasttr</code> í•¨ìˆ˜ë“¤ì˜ ì‚¬ìš©ë²•ì— ëŒ€í•´ ë‹¤ë¤„ë³´ë ¤ í•œë‹¤. íŠ¹íˆ <code class="language-plaintext highlighter-rouge">getattr</code>, <code class="language-plaintext highlighter-rouge">setattr</code> ì˜ ê²½ìš° ë¨¸ì‹ ëŸ¬ë‹ í˜¹ì€ ë”¥ëŸ¬ë‹ ê´€ë ¨ ì½”ë“œë¥¼ ì½ë‹¤ê°€ ì‹¬ì‹¬ì¹˜ ì•Šê²Œ ì°¾ì•„ë³¼ ìˆ˜ ìˆë‹¤. ëª¨ë¸ì˜ <code class="language-plaintext highlighter-rouge">hyper-parameter</code>ë¥¼ íŠœë‹í•˜ê±°ë‚˜ ê¸°íƒ€ ì‹¤í—˜ì„ í•  ë•Œ ì •ì˜í•œ ê°ì²´ì˜ ë³€ìˆ˜ í˜¹ì€ ë©”ì„œë“œì— ì‰½ê³  ê°„ê²°í•˜ê²Œ ì ‘ê·¼í•˜ê¸° ìœ„í•´ ì‚¬ìš©ë˜ê³  ìˆê¸° ë•Œë¬¸ì´ë‹¤.</p>

<h4 id="-getattr"><strong><code class="language-plaintext highlighter-rouge">ğŸ“Œ getattr</code></strong></h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="s">""" getattr(object, attribute_name, default) """</span>

<span class="k">class</span> <span class="nc">CFG</span><span class="p">:</span>
    <span class="s">"""--------[Common]--------"""</span>
    <span class="n">wandb</span><span class="p">,</span> <span class="n">train</span><span class="p">,</span> <span class="n">competition</span><span class="p">,</span> <span class="n">seed</span><span class="p">,</span> <span class="n">cfg_name</span> <span class="o">=</span> <span class="bp">True</span><span class="p">,</span> <span class="bp">True</span><span class="p">,</span> <span class="s">'UPPPM'</span><span class="p">,</span> <span class="mi">42</span><span class="p">,</span> <span class="s">'CFG'</span>
    <span class="n">device</span><span class="p">,</span> <span class="n">gpu_id</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">device</span><span class="p">(</span><span class="s">'cuda'</span> <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s">'cpu'</span><span class="p">),</span> <span class="mi">0</span>
    <span class="n">num_workers</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="s">""" Mixed Precision, Gradient Check Point """</span>
    <span class="n">amp_scaler</span> <span class="o">=</span> <span class="bp">True</span>
    <span class="n">gradient_checkpoint</span> <span class="o">=</span> <span class="bp">True</span> <span class="c1"># save parameter
</span>    <span class="n">output_dir</span> <span class="o">=</span> <span class="s">'./output/'</span>
    <span class="s">""" Clipping Grad Norm, Gradient Accumulation """</span>
    <span class="n">clipping_grad</span> <span class="o">=</span> <span class="bp">True</span> <span class="c1"># clip_grad_norm
</span>    <span class="n">n_gradient_accumulation_steps</span> <span class="o">=</span> <span class="mi">1</span> <span class="c1"># Gradient Accumulation
</span>    <span class="n">max_grad_norm</span> <span class="o">=</span> <span class="n">n_gradient_accumulation_steps</span> <span class="o">*</span> <span class="mi">1000</span>
    <span class="s">""" Model """</span>
    <span class="n">model_name</span> <span class="o">=</span> <span class="s">'microsoft/deberta-v3-large'</span>
    <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>
<span class="c1">#    pooling = 'attention'
</span>    <span class="n">max_len</span> <span class="o">=</span> <span class="mi">512</span>
    <span class="s">""" CV, Epoch, Batch Size """</span>
    <span class="n">n_folds</span> <span class="o">=</span> <span class="mi">4</span>
    <span class="n">epochs</span> <span class="o">=</span> <span class="mi">180</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="mi">64</span>
</code></pre></div></div>

<p>ìœ„ì˜ ê°ì²´ëŠ” ì‹¤ì œ ì œê°€ ìºê¸€ ëŒ€íšŒë¥¼ ì¤€ë¹„í•˜ë©´ì„œ ì‚¬ìš©í–ˆë˜ <a href="http://config.py"><code class="language-plaintext highlighter-rouge">config.py</code></a> ë¥¼ ê°€ì ¸ì™”ë‹¤.</p>

<p><code class="language-plaintext highlighter-rouge">getattr(object: object, attribute_name: str, default: Any)</code> í•¨ìˆ˜ëŠ” ì‚¬ìš©ìê°€ ì§€ì •í•œ ê°ì²´ì— ë§¤ê°œë³€ìˆ˜ë¡œ ì „ë‹¬í•œ <code class="language-plaintext highlighter-rouge">attribute</code>ê°€ ì¡´ì¬í•˜ëŠ”ì§€ ì—¬ë¶€ë¥¼ íŒë‹¨í•˜ê³ , ì¡´ì¬í•œë‹¤ë©´ í•´ë‹¹ <code class="language-plaintext highlighter-rouge">attribute</code>ì˜ <code class="language-plaintext highlighter-rouge">value</code>ë¥¼ ë°˜í™˜í•œë‹¤. í•œí¸ ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë©´ <code class="language-plaintext highlighter-rouge">default</code>ë¡œ ì„¸íŒ…í•œ ê°’ì„ ë°˜í™˜í•œë‹¤.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">getattr</span><span class="p">(</span><span class="n">CFG</span><span class="p">,</span> <span class="s">'epochs'</span><span class="p">,</span> <span class="s">"This Attribute doesn't find"</span><span class="p">)</span>
<span class="nb">getattr</span><span class="p">(</span><span class="n">CFG</span><span class="p">,</span> <span class="s">'MPL'</span><span class="p">,</span> <span class="s">"This Attribute doesn't find"</span><span class="p">)</span>
<span class="o">---------------</span> <span class="n">Result</span> <span class="o">---------------</span> 
<span class="mi">180</span>
<span class="n">This</span> <span class="n">Attribute</span> <span class="n">doesn</span><span class="s">'t find
</span></code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">if-else</code> êµ¬ë¬¸ë³´ë‹¤ í›¨ì”¬ ê°„ê²°í•˜ê²Œ ê°ì²´ì˜ ë©”ì„œë“œì— ì ‘ê·¼í•˜ëŠ” ê²ƒì´ ê°€ëŠ¥í•´ì¡Œìœ¼ë©°, <code class="language-plaintext highlighter-rouge">default</code> ê°’ì„ ë§¤ê°œë³€ìˆ˜ë¡œ ì „ë‹¬ ë°›ê¸° ë•Œë¬¸ì— í´ë¼ì´ì–¸íŠ¸ê°€ ì§€ì •í•œ <code class="language-plaintext highlighter-rouge">attribute</code> ê°€ ê°ì²´ ë‚´ë¶€ì— ì—†ì–´ë„ <code class="language-plaintext highlighter-rouge">AttributeError</code> ë¥¼ ë°œìƒì‹œí‚¤ì§€ ì•Šì•„ ì˜ˆì™¸ ì²˜ë¦¬ë¥¼ ë³„ë„ë¡œ ì§€ì •í•  í•„ìš”ê°€ ì‚¬ë¼ì ¸ ì½”ë“œ ê°€ë…ì„± ë° ìœ ì§€ë³´ìˆ˜ì— ìš©ì´í•˜ë‹¤ëŠ” ì¥ì ì´ ìˆë‹¤.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Exmple</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">test1</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">test2</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">def</span> <span class="nf">A</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">print</span><span class="p">(</span><span class="s">"A"</span><span class="p">)</span>  
    <span class="k">def</span> <span class="nf">B</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">print</span><span class="p">(</span><span class="s">"B"</span><span class="p">)</span>  
    <span class="k">def</span> <span class="nf">C</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">print</span><span class="p">(</span><span class="s">"C"</span><span class="p">)</span>

<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="s">'__main__'</span><span class="p">:</span>
    <span class="n">exmple</span> <span class="o">=</span> <span class="n">Exmple</span><span class="p">()</span>
    <span class="n">class_list</span> <span class="o">=</span> <span class="p">[</span><span class="s">'A'</span><span class="p">,</span><span class="s">'B'</span><span class="p">,</span><span class="s">'C'</span><span class="p">]</span>

    <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">class_list</span><span class="p">:</span>
        <span class="nb">getattr</span><span class="p">(</span><span class="n">exmple</span><span class="p">,</span> <span class="n">c</span><span class="p">)()</span>
</code></pre></div></div>

<p>í•œí¸ <code class="language-plaintext highlighter-rouge">getattr()</code> ë’¤ì— ê´„í˜¸ë¥¼ í•˜ë‚˜ ë” ë¶™ì—¬ì„œ ì‚¬ìš©í•˜ê¸°ë„(ë¨¸ì‹ ëŸ¬ë‹, ë”¥ëŸ¬ë‹ í›ˆë ¨ ë£¨í”„ ì½”ë“œì— ì¢…ì¢… ë³´ì„) í•˜ëŠ”ë°,  í•´ë‹¹ ê´„í˜¸ëŠ” ì§€ì • <code class="language-plaintext highlighter-rouge">attribute</code> ì˜ í˜¸ì¶œì— í•„ìš”í•œ ë§¤ê°œë³€ìˆ˜ë¥¼ ì „ë‹¬í•˜ê¸° ìœ„í•œ ìš©ë„ë¡œ ì“°ì¸ë‹¤. ì´ë²ˆ ì˜ˆì‹œì˜ ê°ì²´ ë‚´ë¶€ ë©”ì„œë“œë“¤ì€ í˜¸ì¶œì— í•„ìš”í•œ ë§¤ê°œë³€ìˆ˜ê°€ ì •ì˜ë˜ì–´ ìˆì§€ ì•Šê¸° ë•Œë¬¸ì— ê´„í˜¸ ì•ˆì„ ë¹„ì›Œë’€ë‹¤.</p>

<h4 id="ï¸-setattr"><strong><code class="language-plaintext highlighter-rouge">âœ‚ï¸ setattr</code></strong></h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="s">""" setattr(object, attribute_name, value) """</span>

<span class="k">class</span> <span class="nc">CFG</span><span class="p">:</span>
    <span class="s">"""--------[Common]--------"""</span>
    <span class="n">wandb</span><span class="p">,</span> <span class="n">train</span><span class="p">,</span> <span class="n">competition</span><span class="p">,</span> <span class="n">seed</span><span class="p">,</span> <span class="n">cfg_name</span> <span class="o">=</span> <span class="bp">True</span><span class="p">,</span> <span class="bp">True</span><span class="p">,</span> <span class="s">'UPPPM'</span><span class="p">,</span> <span class="mi">42</span><span class="p">,</span> <span class="s">'CFG'</span>
    <span class="n">device</span><span class="p">,</span> <span class="n">gpu_id</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">device</span><span class="p">(</span><span class="s">'cuda'</span> <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s">'cpu'</span><span class="p">),</span> <span class="mi">0</span>
    <span class="n">num_workers</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="s">""" Mixed Precision, Gradient Check Point """</span>
    <span class="n">amp_scaler</span> <span class="o">=</span> <span class="bp">True</span>
    <span class="n">gradient_checkpoint</span> <span class="o">=</span> <span class="bp">True</span> <span class="c1"># save parameter
</span>    <span class="n">output_dir</span> <span class="o">=</span> <span class="s">'./output/'</span>
    <span class="s">""" Clipping Grad Norm, Gradient Accumulation """</span>
    <span class="n">clipping_grad</span> <span class="o">=</span> <span class="bp">True</span> <span class="c1"># clip_grad_norm
</span>    <span class="n">n_gradient_accumulation_steps</span> <span class="o">=</span> <span class="mi">1</span> <span class="c1"># Gradient Accumulation
</span>    <span class="n">max_grad_norm</span> <span class="o">=</span> <span class="n">n_gradient_accumulation_steps</span> <span class="o">*</span> <span class="mi">1000</span>
    <span class="s">""" Model """</span>
    <span class="n">model_name</span> <span class="o">=</span> <span class="s">'microsoft/deberta-v3-large'</span>
    <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>
<span class="c1">#    pooling = 'attention'
</span>    <span class="n">max_len</span> <span class="o">=</span> <span class="mi">512</span>
    <span class="s">""" CV, Epoch, Batch Size """</span>
    <span class="n">n_folds</span> <span class="o">=</span> <span class="mi">4</span>
    <span class="n">epochs</span> <span class="o">=</span> <span class="mi">180</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="mi">64</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">setattr(object: object, attribute_name: str, value: Any)</code> ëŠ” ì§€ì • ê°ì²´ì˜ ì§€ì • ë©”ì„œë“œ í˜¹ì€ ë³€ìˆ˜ì— ì ‘ê·¼í•˜ê³  ì œì–´í•˜ëŠ” ìš©ë„ë¡œ ì‚¬ìš©í•˜ëŠ” í•¨ìˆ˜ë‹¤. ì§€ì • ê°ì²´ ë‹¨ìœ„ë¡œ ì ‘ê·¼ ê°€ëŠ¥í•˜ê¸° ë•Œë¬¸ì— ëª¨ë¸ì„ íŠœë‹í•  ë•Œ ì •ë§ ë§ì´ ì‚¬ìš©í•˜ê²Œ ëœë‹¤. <code class="language-plaintext highlighter-rouge">setattr()</code> ë¥¼ í™œìš©í•´ ìƒí™©ì— ë§ëŠ” íŒŒë¼ë¯¸í„°ë¥¼ ëª¨ë¸ì— ì£¼ì…í•˜ê³  í•´ë‹¹ <code class="language-plaintext highlighter-rouge">config</code>ë¥¼ <code class="language-plaintext highlighter-rouge">json</code> í˜¹ì€ <code class="language-plaintext highlighter-rouge">yaml</code> í˜•ì‹ìœ¼ë¡œ ì €ì¥í•´ë‘ë©´ ëª¨ë¸ì˜ ë²„ì „ë³„ íŒŒë¼ë¯¸í„° ê°’ì„ íš¨ìœ¨ì ìœ¼ë¡œ ê´€ë¦¬í•  ìˆ˜ ìˆìœ¼ë‹ˆ ê¸°ì–µí•´ë‘ì.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">CFG</span><span class="p">.</span><span class="n">wandb</span>
<span class="nb">setattr</span><span class="p">(</span><span class="n">CFG</span><span class="p">,</span> <span class="s">'wandb'</span><span class="p">,</span> <span class="bp">False</span><span class="p">)</span>
<span class="n">CFG</span><span class="p">.</span><span class="n">wandb</span>
<span class="nb">setattr</span><span class="p">(</span><span class="n">CFG</span><span class="p">,</span> <span class="s">'wandb'</span><span class="p">,</span> <span class="bp">True</span><span class="p">)</span>
<span class="n">CFG</span><span class="p">.</span><span class="n">wandb</span>

<span class="o">---------------</span> <span class="n">Result</span> <span class="o">---------------</span> 
<span class="bp">True</span>
<span class="bp">False</span>
<span class="bp">True</span>
</code></pre></div></div>

<h4 id="-hasattr"><strong><code class="language-plaintext highlighter-rouge">ğŸ“Œ hasattr</code></strong></h4>

<p><code class="language-plaintext highlighter-rouge">hasattr(object, attribute_name)</code> ëŠ” ì§€ì • ê°ì²´ì— ë§¤ê°œë³€ìˆ˜ë¡œ ì „ë‹¬í•œ <code class="language-plaintext highlighter-rouge">attribute</code> ê°€ ì¡´ì¬í•˜ë©´ <code class="language-plaintext highlighter-rouge">True</code>, ì—†ë‹¤ë©´ <code class="language-plaintext highlighter-rouge">False</code> ë¥¼ ë°˜í™˜í•œë‹¤. ì‚¬ìš©ë²•ì€ <code class="language-plaintext highlighter-rouge">getattr()</code> ì™€ ë§¤ìš° ìœ ì‚¬í•˜ê¸° ë•Œë¬¸ì— ìƒëµí•œë‹¤.</p>

<h4 id="ï¸-delattr"><strong><code class="language-plaintext highlighter-rouge">âœï¸ delattr</code></strong></h4>

<p><code class="language-plaintext highlighter-rouge">delattr(object, attribute_name)</code> ëŠ” ì§€ì • ê°ì²´ì— ë§¤ê°œë³€ìˆ˜ë¡œ ì „ë‹¬í•œ <code class="language-plaintext highlighter-rouge">attribute</code>ë¥¼ ê°ì²´ ë‚´ë¶€ì—ì„œ ì‚­ì œí•˜ëŠ” ì—­í• ì„ í•œë‹¤. ì‚¬ìš© ì˜ˆì‹œëŠ” ì•„ë˜ì™€ ê°™ë‹¤.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">delattr</span><span class="p">(</span><span class="n">CFG</span><span class="p">,</span> <span class="s">'epochs'</span><span class="p">)</span>
<span class="nb">hasattr</span><span class="p">(</span><span class="n">CFG</span><span class="p">,</span> <span class="s">'epochs'</span><span class="p">)</span>

<span class="o">---------------</span> <span class="n">Result</span> <span class="o">---------------</span> 
<span class="bp">False</span>
</code></pre></div></div>

<p>í•œí¸, ëª¨ë“ˆ(ex: config,py, model.py, model_utils.py ë“±)ë„ ê°ì²´ë¡œ ê°„ì£¼ë˜ê¸° ë•Œë¬¸ì— ìœ„ì—ì„œ ì‚´í´ë³¸ 4ê°€ì§€ functionì€ ëª¨ë“ˆ ë ˆë²¨ì—ì„œë„ ë™ì¼í•˜ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.</p>

<h3 id="ï¸-assertion"><code class="language-plaintext highlighter-rouge">âš ï¸ Assertion</code></h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">assert</span> <span class="n">ì¡°ê±´</span><span class="p">,</span> <span class="n">ë©”ì„¸ì§€</span> 
</code></pre></div></div>

<p>ì¡°ê±´ì´ Trueì´ë©´ ì•„ë¬´ëŸ° ì¼ì´ ì¼ì–´ë‚˜ì§€ ì•ŠëŠ”ë‹¤. í•˜ì§€ë§Œ ì¡°ê±´ì´ Falseì´ë©´ AssertionErrorê°€ ë°œìƒí•˜ê³  ì§€ì •í•œ ë©”ì„¸ì§€ê°€ ì¶œë ¥ëœë‹¤. ë©”ì„¸ì§€ë¥¼ ì§€ì •í•˜ì§€ ì•Šì•˜ë‹¤ë©´ <code class="language-plaintext highlighter-rouge">AssertionError</code>ê°€ ë™ì¼í•˜ê²Œ ë°œìƒí•˜ì§€ë§Œ êµ¬ì²´ì ì¸ ì—ëŸ¬ ëª…ì‹œë€ì€ ë¹„ì›Œì§„ ì±„ë¡œ ë¡œê·¸ê°€ ì¶œë ¥ëœë‹¤.</p>

<p><code class="language-plaintext highlighter-rouge">assert</code>ëŠ” ì½”ë“œì˜ ì˜¤ë¥˜ë¥¼ ì°¾ëŠ” ë° ìœ ìš©í•˜ë‹¤. ë˜í•œ ì½”ë“œì˜ ì˜ë„ë¥¼ ëª…í™•í•˜ê²Œ í‘œí˜„í•˜ëŠ” ë°ì—ë„ ìœ ìš©í•˜ë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ë³€ìˆ˜ì˜ ê°’ì´ íŠ¹ì • ì¡°ê±´ì„ ë§Œì¡±í•´ì•¼ í•œë‹¤ëŠ” ê²ƒì„ <code class="language-plaintext highlighter-rouge">assert</code>ë¥¼ ì‚¬ìš©í•´ í‘œí˜„í•  ìˆ˜ ìˆë‹¤.</p>

<p><code class="language-plaintext highlighter-rouge">assert</code>ëŠ” ì—ëŸ¬ ë¡œê·¸ë¥¼ ë°˜í™˜í•˜ë©´ì„œ ê°œë°œìê°€ í”„ë¡œê·¸ë¨ì„ ë§Œë“œëŠ” ê³¼ì •ì— ê´€ì—¬í•œë‹¤. ì›í•˜ëŠ” ì¡°ê±´ì˜ ë³€ìˆ˜ ê°’ì„ ë³´ì¦ë°›ì„ ë•Œê¹Œì§€ <code class="language-plaintext highlighter-rouge">assert</code>ë¡œ í…ŒìŠ¤íŠ¸ í•  ìˆ˜ ìˆë‹¤. ì´ëŠ” ë°ì´í„° ìœ íš¨ì„± ê²€ì‚¬ì²˜ëŸ¼ ë‹¨ìˆœíˆ ì—ëŸ¬ë¥¼ ì°¾ëŠ”ê²ƒì´ ì•„ë‹ˆë¼ ê°’ì„ ë³´ì¦í•˜ê¸° ìœ„í•´ ì‚¬ìš©ëœë‹¤. ì˜ˆë¥¼ ë“¤ì–´ í•¨ìˆ˜ì˜ ì…ë ¥ ê°’ì´ ì–´ë–¤ ì¡°ê±´ì˜ ì°¸ì„ì„ ë³´ì¦í•˜ê¸° ìœ„í•´ ì‚¬ìš©í•  ìˆ˜ ìˆê³  í•¨ìˆ˜ì˜ ë°˜í™˜ ê°’ì´ ì–´ë–¤ ì¡°ê±´ì— ë§Œì¡±í•˜ë„ë¡ ë§Œë“¤ ìˆ˜ ìˆë‹¤. í˜¹ì€ ë³€ìˆ˜ ê°’ì´ ë³€í•˜ëŠ” ê³¼ì •ì—ì„œ íŠ¹ì • ë¶€ë¶„ì€ ë°˜ë“œì‹œ ì–´ë–¤ ì˜ì—­ì— ì†í•˜ëŠ” ê²ƒì„ ë³´ì¦í•˜ê¸° ìœ„í•´ ê°€ì • ì„¤ì •ë¬¸ì„ í†µí•´ í™•ì¸ í•  ìˆ˜ë„ ìˆë‹¤. <code class="language-plaintext highlighter-rouge">assert</code>ëŠ” ì‹¤ìˆ˜ë¥¼ ê°€ì •í•´ ê°’ì„ ë³´ì¦í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ì½”ë”© í•˜ê¸° ë•Œë¬¸ì— <code class="language-plaintext highlighter-rouge">'ë°©ì–´ì  í”„ë¡œê·¸ë˜ë°'</code>ì— ì†í•œë‹¤. ë°©ì–´ì  í”„ë¡œê·¸ë˜ë°ì— ëŒ€í•œ ìì„¸í•œ ë‚´ìš©ì€ ë‹¤ìŒ í¬ìŠ¤íŠ¸ì—ì„œ ì‚´í´ë³´ë„ë¡ í•˜ì.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Python assert ë°ì´í„° ìœ íš¨ì„± ê²€ì‚¬ ì˜ˆì‹œ
</span><span class="k">class</span> <span class="nc">DeBERTa</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,):</span>
    <span class="p">...</span><span class="n">ì¤‘ëµ</span><span class="p">...</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">mask</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">):</span>
        <span class="k">assert</span> <span class="n">inputs</span><span class="p">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">3</span><span class="p">,</span> <span class="sa">f</span><span class="s">'Expected (batch, sequence, vocab_size) got </span><span class="si">{</span><span class="n">inputs</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="s">'</span>
    <span class="p">...</span><span class="n">ì¤‘ëµ</span><span class="p">...</span>
</code></pre></div></div>

<p>ìœ„ì˜ ì½”ë“œëŠ” í•„ìê°€ ë…¼ë¬¸ì„ ë³´ê³  ë”°ë¼ êµ¬í˜„í•œ <code class="language-plaintext highlighter-rouge">DeBERTa</code> ëª¨ë¸ ìµœìƒìœ„ ê°ì²´ì˜ ì½”ë“œ ì¼ë¶€ë¶„ì´ë‹¤. ìµœìƒìœ„ ê°ì²´ëŠ” ëª¨ë¸ì˜ ì…ë ¥ ì„ë² ë”© ì¸µê³¼ ìœ„ì¹˜ ì„ë² ë”© ì¸µì„ ì •ì˜í•´ì¤˜ì•¼ í•˜ê¸° ë•Œë¬¸ì— ë°˜ë“œì‹œ ì…ë ¥ê°’ì„ ë¯¸ë¦¬ ì •í•´ì§„ ì°¨ì› í˜•ì‹ì— ë§ê²Œ ê°ì²´ì˜ ë§¤ê°œ ë³€ìˆ˜ë¡œ ë„˜ê²¨ì¤˜ì•¼ í•œë‹¤. ì§€ì • í˜•ì‹ì—ì„œ ë²—ì–´ë‚œ í…ì„œëŠ” ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©ë  ìˆ˜ ì—†ê²Œ ë§Œë“¤ê¸° ìœ„í•´ ê°ì²´ì˜ <code class="language-plaintext highlighter-rouge">forward</code> ë©”ì„œë“œ ì‹œì‘ë¶€ë¶„ì— <code class="language-plaintext highlighter-rouge">assert</code> í•¨ìˆ˜ë¥¼ ë‘ì–´ ë°ì´í„° ìœ íš¨ì„± ê²€ì‚¬ë¥¼ í•˜ë„ë¡ êµ¬í˜„í–ˆë‹¤. ì§€ì •ëœ ì°¨ì› í˜•íƒœì— ë§ì§€ ì•ŠëŠ” ë°ì´í„°ë¥¼ ì…ë ¥í•˜ê²Œ ë˜ë©´ <code class="language-plaintext highlighter-rouge">AssertionError</code>ì™€ í•¨ê»˜ í•„ìê°€ ì§€ì •í•œ ì—ëŸ¬ ë©”ì„¸ì§€ë¥¼ ë°˜í™˜ ë°›ê²Œ ë  ê²ƒì´ë‹¤.</p>

<p>í•œí¸ <code class="language-plaintext highlighter-rouge">AssertionError</code>ëŠ” í”„ë¡œê·¸ë˜ë¨¸ê°€ ì˜ë„ì— ë§ì§€ ì•ŠëŠ” ë©”ì„œë“œ í˜¹ì€ ê°ì²´ ì‚¬ìš©ì„ ë§‰ê¸° ìœ„í•´ ì„ ì œì ìœ¼ë¡œ ëŒ€ì‘í•œ ê²ƒì´ë¼ê³  ë³¼ ìˆ˜ ìˆë‹¤. ì´ëŠ” í”„ë¡œê·¸ë˜ë¨¸ê°€ ë§Œë“  ê·œì¹™ì— í•´ë‹¹í•  ë¿, ì‹¤ì œ íŒŒì´ì¬ì´ë‚˜ ì»´í“¨í„° ë‚´ë¶€ ë™ì‘ ë¬¸ë²•ì— í‹€ë ¸ë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•˜ëŠ” ê²ƒì€ ì•„ë‹ˆë‹¤.</p>]]></content><author><name>qcqced</name><email>qcqced123@gmail.com</email></author><category term="Python" /><category term="Python" /><category term="Object" /><category term="Attribute" /><category term="Assertion" /><category term="ML" /><category term="Deep Learning" /><summary type="html"><![CDATA[getattr, setattr, delattr, hasattr, Assertion ì‚¬ìš©ë°©ë²•]]></summary></entry><entry><title type="html">ğŸ”¥Â Pytorch Tensor Indexing ìì£¼ ì‚¬ìš©í•˜ëŠ” ë©”ì„œë“œ ëª¨ìŒì§‘</title><link href="http://localhost:4000/framework-library/torch-indexing-function" rel="alternate" type="text/html" title="ğŸ”¥Â Pytorch Tensor Indexing ìì£¼ ì‚¬ìš©í•˜ëŠ” ë©”ì„œë“œ ëª¨ìŒì§‘" /><published>2023-08-04T00:00:00+09:00</published><updated>2023-08-05T02:00:00+09:00</updated><id>http://localhost:4000/framework-library/Pytorch-Tensor-Indexing-Function</id><content type="html" xml:base="http://localhost:4000/framework-library/torch-indexing-function"><![CDATA[<p>íŒŒì´í† ì¹˜ì—ì„œ í•„ìê°€ ìì£¼ ì‚¬ìš©í•˜ëŠ” í…ì„œ ì¸ë±ì‹± ê´€ë ¨ ë©”ì„œë“œì˜ ì‚¬ìš©ë²• ë° ì‚¬ìš© ì˜ˆì‹œë¥¼ í•œë°©ì— ì •ë¦¬í•œ í¬ìŠ¤íŠ¸ë‹¤. ë©”ì„œë“œ í•˜ë‚˜ë‹¹ í•˜ë‚˜ì˜ í¬ìŠ¤íŠ¸ë¡œ ë§Œë“¤ê¸°ì—ëŠ” ë„ˆë¬´ ê¸¸ì´ê°€ ì§§ë‹¤ ìƒê°í•´ í•œ í˜ì´ì§€ì— ëª¨ë‘ ë„£ê²Œ ë˜ì—ˆë‹¤. ì§€ì†ì ìœ¼ë¡œ ì—…ë°ì´íŠ¸ ë  ì˜ˆì •ì´ë‹¤. ë˜í•œ í…ì„œ ì¸ë±ì‹± ë§ê³ ë„ ë‹¤ë¥¸ ì£¼ì œë¡œë„ ê´€ë ¨ ë©”ì„œë“œë¥¼ ì •ë¦¬í•´ ì˜¬ë¦´ ì˜ˆì •ì´ë‹ˆ ë§ì€ ê´€ì‹¬ ë¶€íƒë“œë¦°ë‹¤.</p>

<h3 id="torchargmax"><code class="language-plaintext highlighter-rouge">ğŸ”Â torch.argmax</code></h3>

<p>ì…ë ¥ í…ì„œì—ì„œ ê°€ì¥ í° ê°’ì„ ê°–ê³  ìˆëŠ” ì›ì†Œì˜ ì¸ë±ìŠ¤ë¥¼ ë°˜í™˜í•œë‹¤. ìµœëŒ€ê°’ì„ ì°¾ì„ ì°¨ì›ì„ ì§€ì •í•´ì¤„ ìˆ˜ ìˆë‹¤. ì•„ë˜ ì˜ˆì‹œ ì½”ë“œë¥¼ í™•ì¸í•´ë³´ì.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># torch.argmax params
</span><span class="n">torch</span><span class="p">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

<span class="c1"># torch.argmax example 1
</span><span class="n">test</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">29</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">45</span><span class="p">,</span><span class="mi">22</span><span class="p">,</span><span class="mi">3</span><span class="p">])</span>
<span class="n">torch</span><span class="p">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">test</span><span class="p">)</span>
<span class="n">torch</span><span class="p">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">test2</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="o">&lt;</span><span class="n">Result</span><span class="o">&gt;</span>
<span class="n">tensor</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>

<span class="c1"># torch.argmax example 2
</span><span class="n">test</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
                     <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">]])</span>

<span class="n">torch</span><span class="p">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">test</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="o">&lt;</span><span class="n">Result</span><span class="o">&gt;</span>
<span class="n">tensor</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>

<span class="c1"># torch.argmax example 3
</span><span class="n">test</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
                     <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">]])</span>

<span class="n">torch</span><span class="p">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">test</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">tensor</span><span class="p">([[</span><span class="mi">0</span><span class="p">],</span>
        <span class="p">[</span><span class="mi">2</span><span class="p">]])</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">dim</code> ë§¤ê°œë³€ìˆ˜ì— ì›í•˜ëŠ” ì°¨ì›ì„ ì…ë ¥í•˜ë©´ í•´ë‹¹ ì°¨ì› ë·°ì—ì„œ ê°€ì¥ í° ì›ì†Œë¥¼ ì°¾ì•„ ì¸ë±ìŠ¤ ê°’ì„ ë°˜í™˜í•´ì¤„ ê²ƒì´ë‹¤. ì´ ë•Œ <code class="language-plaintext highlighter-rouge">keepdim=True</code> ë¡œ ì„¤ì •í•œë‹¤ë©´ ì…ë ¥ ì°¨ì›ì—ì„œ ê°€ì¥ í° ì›ì†Œì˜ ì¸ë±ìŠ¤ë¥¼ ë°˜í™˜í•˜ë˜ ì›ë³¸ í…ì„œì˜ ì°¨ì›ê³¼ ë™ì¼í•œ í˜•íƒœë¡œ ì¶œë ¥í•´ì¤€ë‹¤. <code class="language-plaintext highlighter-rouge">example 2</code> ì˜ ê²½ìš° <code class="language-plaintext highlighter-rouge">dim=0</code> ë¼ì„œ í–‰ì´ ëˆ„ì ëœ ë°©í–¥ìœ¼ë¡œ í…ì„œë¥¼ ë°”ë¼ë´ì•¼ í•œë‹¤. í–‰ì´ ëˆ„ì ëœ ë°©í–¥ìœ¼ë¡œ í…ì„œë¥¼ ë³´ê²Œ ë˜ë©´ <code class="language-plaintext highlighter-rouge">tensor([[0, 1, 1]])</code>ì´ ëœë‹¤.</p>

<h3 id="torchstack"><code class="language-plaintext highlighter-rouge">ğŸ“šÂ torch.stack</code></h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="s">"""
torch.stack
Args:
	tensors(sequence of Tensors): í…ì„œê°€ ë‹´ê¸´ íŒŒì´ì¬ ì‹œí€€ìŠ¤ ê°ì²´
	dim(int): ì¶”ê°€í•  ì°¨ì› ë°©í–¥ì„ ì„¸íŒ…, ê¸°ë³¸ê°’ì€ 0
"""</span>
<span class="n">torch</span><span class="p">.</span><span class="n">stack</span><span class="p">(</span><span class="n">tensors</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</code></pre></div></div>

<p>ë§¤ê°œë³€ìˆ˜ë¡œ ì£¼ì–´ì§„ íŒŒì´ì¬ ì‹œí€€ìŠ¤ ê°ì²´(ë¦¬ìŠ¤íŠ¸, íŠœí”Œ)ë¥¼ ì‚¬ìš©ìê°€ ì§€ì •í•œ ìƒˆë¡œìš´ ì°¨ì›ì— ìŒ“ëŠ” ê¸°ëŠ¥ì„ í•œë‹¤. ë§¤ê°œë³€ìˆ˜ <code class="language-plaintext highlighter-rouge">tensors</code> ëŠ” í…ì„œê°€ ë‹´ê¸´ íŒŒì´ì¬ì˜ ì‹œí€€ìŠ¤ ê°ì²´ë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ëŠ”ë‹¤. <code class="language-plaintext highlighter-rouge">dim</code> ì€ ì‚¬ìš©ìê°€ í…ì„œ ì ì¬ë¥¼ í•˜ê³  ì‹¶ì€ ìƒˆë¡œìš´ ì°¨ì›ì„ ì§€ì •í•´ì£¼ë©´ ëœë‹¤. ê¸°ë³¸ê°’ì€ 0ì°¨ì›ìœ¼ë¡œ ì§€ì • ë˜ì–´ìˆìœ¼ë©°, í…ì„œì˜ ë§¨ ì•ì°¨ì›ì´ ìƒˆë¡­ê²Œ ìƒê¸°ê²Œ ëœë‹¤. <code class="language-plaintext highlighter-rouge">torch.stack</code> ì€ ê¸°ê³„í•™ìŠµ, íŠ¹íˆ ë”¥ëŸ¬ë‹ì—ì„œ ì •ë§ ìì£¼ ì‚¬ìš©ë˜ê¸° ë•Œë¬¸ì— ì‚¬ìš©ë²• ë° ì‚¬ìš©ìƒí™©ì„ ìµí˜€ë‘ë©´ ë„ì›€ì´ ëœë‹¤. ì˜ˆì‹œë¥¼ í†µí•´ í•´ë‹¹ ë©”ì„œë“œë¥¼ ì–´ë–¤ ìƒí™©ì—ì„œ ì–´ë–»ê²Œ ì‚¬ìš©í•˜ëŠ”ì§€ ì•Œì•„ë³´ì.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="s">""" torch.stack example """</span>

<span class="k">class</span> <span class="nc">Projector</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="s">"""
    Making projection matrix(Q, K, V) for each attention head
    When you call this class, it returns projection matrix of each attention head
    For example, if you call this class with 8 heads, it returns 8 set of projection matrices (Q, K, V)
    Args:
        num_heads: number of heads in MHA, default 8
        dim_head: dimension of each attention head, default 64
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span> <span class="n">dim_head</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">64</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Projector</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span> <span class="o">=</span> <span class="n">num_heads</span> <span class="o">*</span> <span class="n">dim_head</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dim_head</span> <span class="o">=</span> <span class="n">dim_head</span>

    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">fc_q</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">dim_head</span><span class="p">)</span>
        <span class="n">fc_k</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">dim_head</span><span class="p">)</span>
        <span class="n">fc_v</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">dim_head</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">fc_q</span><span class="p">,</span> <span class="n">fc_k</span><span class="p">,</span> <span class="n">fc_v</span>

<span class="n">num_heads</span> <span class="o">=</span> <span class="mi">8</span>
<span class="n">dim_head</span> <span class="o">=</span> <span class="mi">64</span>
<span class="n">projector</span> <span class="o">=</span> <span class="n">Projector</span><span class="p">(</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">dim_head</span><span class="p">)</span>  <span class="c1"># init instance
</span><span class="n">projector_list</span> <span class="o">=</span> <span class="p">[</span><span class="nb">list</span><span class="p">(</span><span class="n">projector</span><span class="p">())</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_heads</span><span class="p">)]</span>  <span class="c1"># call instance
</span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">512</span><span class="p">)</span> <span class="c1"># x.shape: [Batch_Size, Sequence_Length, Dim_model]
</span><span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">V</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[],</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_heads</span><span class="p">):</span>
    <span class="n">Q</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">projector_list</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">0</span><span class="p">](</span><span class="n">x</span><span class="p">))</span> <span class="c1"># [10, 512, 64]
</span>    <span class="n">K</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">projector_list</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">1</span><span class="p">](</span><span class="n">x</span><span class="p">))</span> <span class="c1"># [10, 512, 64]
</span>	  <span class="n">V</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">projector_list</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">2</span><span class="p">](</span><span class="n">x</span><span class="p">))</span> <span class="c1"># [10, 512, 64]
</span> 
<span class="n">Q</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">stack</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># Q.shape: [10, 8, 512, 64]
</span><span class="n">K</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">stack</span><span class="p">(</span><span class="n">K</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># K.shape: [10, 8, 512, 64]
</span><span class="n">V</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">stack</span><span class="p">(</span><span class="n">V</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># V.shape: [10, 8, 512, 64]
</span></code></pre></div></div>

<p>ìœ„ ì½”ë“œëŠ” <code class="language-plaintext highlighter-rouge">Transformer</code> ì˜ <code class="language-plaintext highlighter-rouge">Multi-Head Attention</code> êµ¬í˜„ì²´ ì¼ë¶€ë¥¼ ë°œì·Œí•´ì˜¨ ê²ƒì´ë‹¤. <code class="language-plaintext highlighter-rouge">Multi-Head Attention</code> ì€ ê°œë³„ ì–´í…ì…˜ í•´ë“œë³„ë¡œ í–‰ë ¬ $Q, K, V$ë¥¼ ê°€ì ¸ì•¼ í•œë‹¤. ë”°ë¼ì„œ ì…ë ¥ ì„ë² ë”©ì„ ê°œë³„ ì–´í…ì…˜ í—¤ë“œì— <code class="language-plaintext highlighter-rouge">Linear Combination</code> í•´ì¤˜ì•¼ í•˜ëŠ”ë° í—¤ë“œ ê°œìˆ˜ê°€ 8ê°œë‚˜ ë˜ê¸° ë•Œë¬¸ì— ê°œë³„ì ìœ¼ë¡œ <code class="language-plaintext highlighter-rouge">Projection Matrix</code> ë¥¼ ì„ ì–¸í•´ì£¼ëŠ” ê²ƒì€ ë§¤ìš° ë¹„íš¨ìœ¨ì ì´ë‹¤. ë”°ë¼ì„œ ê°ì²´  <code class="language-plaintext highlighter-rouge">Projector</code> ì— í–‰ë ¬ $Q, K, V$ì— ëŒ€í•œ <code class="language-plaintext highlighter-rouge">Projection Matrix</code> ë¥¼ ì •ì˜í•´ì¤¬ë‹¤. ì´í›„ í—¤ë“œ ê°œìˆ˜ë§Œí¼ ê°ì²´  <code class="language-plaintext highlighter-rouge">Projector</code> ë¥¼ í˜¸ì¶œí•´ ë¦¬ìŠ¤íŠ¸ì— í•´ë“œë³„ <code class="language-plaintext highlighter-rouge">Projection Matrix</code> ë¥¼ ë‹´ì•„ì¤€ë‹¤. ê·¸ ë‹¤ìŒ <code class="language-plaintext highlighter-rouge">torch.stack</code>ì„ ì‚¬ìš©í•´ <code class="language-plaintext highlighter-rouge">Attention Head</code> ë°©í–¥ì˜ ì°¨ì›ìœ¼ë¡œ ë¦¬ìŠ¤íŠ¸ ë‚´ë¶€ í…ì„œë“¤ì„ ìŒ“ì•„ì£¼ë©´ ëœë‹¤.</p>

<h3 id="torcharange"><code class="language-plaintext highlighter-rouge">ğŸ”¢Â torch.arange</code></h3>

<p>ì‚¬ìš©ìê°€ ì§€ì •í•œ ì‹œì‘ì ë¶€í„° ëì ê¹Œì§€ ì¼ì •í•œ ê°„ê²©ìœ¼ë¡œ í…ì„œë¥¼ ë‚˜ì—´í•œë‹¤. Pythonì˜ ë‚´ì¥ ë©”ì„œë“œ <code class="language-plaintext highlighter-rouge">range</code>ì™€ ë™ì¼í•œ ì—­í• ì„ í•˜ëŠ”ë°, ëŒ€ì‹  í…ì„œ ê·¸ ê²°ê³¼ë¥¼ í…ì„œ êµ¬ì¡°ì²´ë¡œ ë°˜í™˜í•œë‹¤ê³  ìƒê°í•˜ë©´ ë˜ê² ë‹¤.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># torch.arange usage
</span><span class="n">torch</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="n">start</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">end</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
<span class="n">tensor</span><span class="p">([</span> <span class="mi">0</span><span class="p">,</span>  <span class="mi">1</span><span class="p">,</span>  <span class="mi">2</span><span class="p">,</span>  <span class="mi">3</span><span class="p">,</span>  <span class="mi">4</span><span class="p">])</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">tensor</span><span class="p">([</span> <span class="mi">1</span><span class="p">,</span>  <span class="mi">2</span><span class="p">,</span>  <span class="mi">3</span><span class="p">])</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">)</span>
<span class="n">tensor</span><span class="p">([</span> <span class="mf">1.0000</span><span class="p">,</span>  <span class="mf">1.5000</span><span class="p">,</span>  <span class="mf">2.0000</span><span class="p">])</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">step</code> ë§¤ê°œë³€ìˆ˜ë¡œ ì›ì†Œê°„ ê°„ê²© ì¡°ì •ì„ í•  ìˆ˜ ìˆëŠ”ë°, ê¸°ë³¸ì€ 1ë¡œ ì§€ì • ë˜ì–´ ìˆìœ¼ë‹ˆ ì°¸ê³ í•˜ì. í•„ìì˜ ê²½ìš°ì—ëŠ” <code class="language-plaintext highlighter-rouge">nn.Embedding</code>ì˜ ì…ë ¥ í…ì„œë¥¼ ë§Œë“¤ ë•Œ ê°€ì¥ ë§ì´ ì‚¬ìš©í–ˆë‹¤. <code class="language-plaintext highlighter-rouge">nn.Embedding</code> ì˜ ê²½ìš° Inputìœ¼ë¡œ <code class="language-plaintext highlighter-rouge">IntTensor</code>, <code class="language-plaintext highlighter-rouge">LongTensor</code>ë¥¼ ë°›ê²Œ ë˜ì–´ ìˆìœ¼ë‹ˆ ì•Œì•„ë‘ì.</p>

<h3 id="torchrepeat"><code class="language-plaintext highlighter-rouge">ğŸ”Â torch.repeat</code></h3>

<p>ì…ë ¥ê°’ìœ¼ë¡œ ì£¼ì–´ì§„ í…ì„œë¥¼ ì‚¬ìš©ìê°€ ì§€ì •í•œ ë°˜ë³µ íšŸìˆ˜ë§Œí¼ íŠ¹ì • ì°¨ì› ë°©í–¥ìœ¼ë¡œ ëŠ˜ë¦°ë‹¤. ì˜ˆë¥¼ ë“¤ë©´ <code class="language-plaintext highlighter-rouge">[1,2,3] * 3</code>ì˜ ê²°ê³¼ëŠ” <code class="language-plaintext highlighter-rouge">[1, 2, 3, 1, 2, 3, 1, 2, 3]</code> ì¸ë°, ì´ê²ƒì„ ì‚¬ìš©ìê°€ ì§€ì •í•œ ë°˜ë³µ íšŸìˆ˜ë§Œí¼ íŠ¹ì • ì°¨ì›ìœ¼ë¡œ ìˆ˜í–‰í•˜ê² ë‹¤ëŠ” ê²ƒì´ë‹¤. ì•„ë˜ ì‚¬ìš© ì˜ˆì œë¥¼ í™•ì¸í•´ë³´ì.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># torch.repeat example
</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">x</span><span class="p">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">tensor</span><span class="p">([[</span> <span class="mi">1</span><span class="p">,</span>  <span class="mi">2</span><span class="p">,</span>  <span class="mi">3</span><span class="p">,</span>  <span class="mi">1</span><span class="p">,</span>  <span class="mi">2</span><span class="p">,</span>  <span class="mi">3</span><span class="p">],</span>
        <span class="p">[</span> <span class="mi">1</span><span class="p">,</span>  <span class="mi">2</span><span class="p">,</span>  <span class="mi">3</span><span class="p">,</span>  <span class="mi">1</span><span class="p">,</span>  <span class="mi">2</span><span class="p">,</span>  <span class="mi">3</span><span class="p">],</span>
        <span class="p">[</span> <span class="mi">1</span><span class="p">,</span>  <span class="mi">2</span><span class="p">,</span>  <span class="mi">3</span><span class="p">,</span>  <span class="mi">1</span><span class="p">,</span>  <span class="mi">2</span><span class="p">,</span>  <span class="mi">3</span><span class="p">],</span>
        <span class="p">[</span> <span class="mi">1</span><span class="p">,</span>  <span class="mi">2</span><span class="p">,</span>  <span class="mi">3</span><span class="p">,</span>  <span class="mi">1</span><span class="p">,</span>  <span class="mi">2</span><span class="p">,</span>  <span class="mi">3</span><span class="p">]])</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">x</span><span class="p">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">tensor</span><span class="p">([[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
         <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]],</span>

        <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
         <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]],</span>

        <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
         <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]],</span>

        <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
         <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]]])</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">x</span><span class="p">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">).</span><span class="n">size</span>
<span class="n">torch</span><span class="p">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">x</span><span class="p">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">tensor</span><span class="p">([[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
         <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]],</span>

        <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
         <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]],</span>

        <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
         <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]],</span>

        <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
         <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]]])</span>
</code></pre></div></div>

<p>$t$ë¥¼ ì–´ë–¤ í…ì„œ êµ¬ì¡°ì²´ $x$ì˜ ìµœëŒ€ ì°¨ì›ì´ë¼ê³  í–ˆì„ , $x_t$ë¥¼ ê°€ì¥ ì™¼ìª½ì— ë„£ê³  ê°€ì¥ ë‚®ì€ ì°¨ì›ì¸ 0ì°¨ì›ì— ëŒ€í•œ ë°˜ë³µ íšŸìˆ˜ë¥¼ ì˜¤ë¥¸ìª½ ëì— ëŒ€ì…í•´ì„œ ì‚¬ìš©í•˜ë©´ ëœë‹¤. (<code class="language-plaintext highlighter-rouge">torch.repeat(</code>$x_t, x_{t-1}, â€¦ x_2, x_1, x_0$<code class="language-plaintext highlighter-rouge">))</code>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># torch.arange &amp; torch.repeate usage example
</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">pos_x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">num_patches</span> <span class="o">+</span> <span class="mi">1</span><span class="p">).</span><span class="n">repeat</span><span class="p">(</span><span class="n">inputs</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]).</span><span class="n">to</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">pos_x</span><span class="p">.</span><span class="n">shape</span>
<span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">16</span><span class="p">,</span> <span class="mi">1025</span><span class="p">])</span>
</code></pre></div></div>

<p>í•„ìì˜ ê²½ìš°, <code class="language-plaintext highlighter-rouge">position embedding</code>ì˜ ì…ë ¥ì„ ë§Œë“¤ê³  ì‹¶ì„ ë•Œ <code class="language-plaintext highlighter-rouge">torch.arange</code> ì™€ ì—°ê³„í•´ ìì£¼ ì‚¬ìš© í–ˆë˜ ê²ƒ ê°™ë‹¤. ìœ„ ì½”ë“œë¥¼ ì°¸ê³ í•˜ì.</p>

<h3 id="torchclamp"><code class="language-plaintext highlighter-rouge">ğŸ”¬Â torch.clamp</code></h3>

<p>ì…ë ¥ í…ì„œì˜ ì›ì†Œê°’ì„ ì‚¬ìš©ìê°€ ì§€ì •í•œ ìµœëŒ€â€¢ìµœì†Œê°’ ë²”ìœ„ ì´ë‚´ë¡œ ì œí•œí•˜ëŠ” ë©”ì„œë“œë‹¤.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># torch.clamp params
</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">clamp</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="nb">min</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span> <span class="err">â†’</span> <span class="n">Tensor</span>

<span class="c1"># torch.clamp usage example
</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">a</span>
<span class="n">tensor</span><span class="p">([</span><span class="o">-</span><span class="mf">1.7120</span><span class="p">,</span>  <span class="mf">0.1734</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0478</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0922</span><span class="p">])</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="nb">min</span><span class="o">=-</span><span class="mf">0.5</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">tensor</span><span class="p">([</span><span class="o">-</span><span class="mf">0.5000</span><span class="p">,</span>  <span class="mf">0.1734</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0478</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0922</span><span class="p">])</span>
</code></pre></div></div>

<p>ì…ë ¥ëœ í…ì„œì˜ ì›ì†Œë¥¼ ì§€ì • ìµœëŒ€â€¢ìµœì†Œ ì„¤ì •ê°’ê³¼ í•˜ë‚˜ í•˜ë‚˜ ëŒ€ì¡°í•´ì„œ í…ì„œ ë‚´ë¶€ì˜ ëª¨ë“  ì›ì†Œê°€ ì§€ì • ë²”ìœ„ ì•ˆì— ë“¤ë„ë¡ ë§Œë“¤ì–´ì¤€ë‹¤. <code class="language-plaintext highlighter-rouge">torch.clamp</code> ì—­ì‹œ ë‹¤ì–‘í•œ ìƒí™©ì—ì„œ ì‚¬ìš©ë˜ëŠ”ë°, í•„ìì˜ ê²½ìš° ëª¨ë¸ ë ˆì´ì–´ ì¤‘ê°„ì— ì œê³±ê·¼ì´ë‚˜ ì§€ìˆ˜, ë¶„ìˆ˜ í˜¹ì€ ê°ë„ ê´€ë ¨ ì—°ì‚°ì´ ë“¤ì–´ê°€ <code class="language-plaintext highlighter-rouge">Backward Pass</code>ì—ì„œ <code class="language-plaintext highlighter-rouge">NaN</code>ì´ ë°œìƒí•  ìˆ˜ ìˆëŠ” ê²½ìš°ì— ì•ˆì „ì¥ì¹˜ë¡œ ë§ì´ ì‚¬ìš©í•˜ê³  ìˆë‹¤. (<a href="https://qcqced123.github.io/framework-library/backward-nan/">ìì„¸íˆ ì•Œê³  ì‹¶ë‹¤ë©´ í´ë¦­</a>)</p>

<h3 id="torchgather"><code class="language-plaintext highlighter-rouge">ğŸ‘©â€ğŸ‘©â€ğŸ‘§â€ğŸ‘¦Â torch.gather</code></h3>

<p>í…ì„œ ê°ì²´ ë‚´ë¶€ì—ì„œ ì›í•˜ëŠ” ì¸ë±ìŠ¤ì— ìœ„ì¹˜í•œ ì›ì†Œë§Œ ì¶”ì¶œí•˜ê³  ì‹¶ì„ ë•Œ ì‚¬ìš©í•˜ë©´ ë§¤ìš° ìœ ìš©í•œ ë©”ì„œë“œë‹¤. í…ì„œ ì—­ì‹œ <code class="language-plaintext highlighter-rouge">iterable</code> ê°ì²´ë¼ì„œ <code class="language-plaintext highlighter-rouge">loop</code> ë¥¼ ì‚¬ìš©í•´ ì ‘ê·¼í•˜ëŠ” ê²ƒì´ ì§ê´€ì ìœ¼ë¡œ ë³´ì¼ ìˆ˜ ìˆìœ¼ë‚˜, í†µìƒì ìœ¼ë¡œ í…ì„œë¥¼ ì‚¬ìš©í•˜ëŠ” ìƒí™©ì´ë¼ë©´ ê°ì²´ì˜ ì°¨ì›ì´ ì–´ë§ˆë¬´ì‹œ í•˜ê¸° ë•Œë¬¸ì— ë£¨í”„ë¡œ ì ‘ê·¼í•´ ê´€ë¦¬í•˜ëŠ” ê²ƒì€ ë§¤ìš° ë¹„íš¨ìœ¨ì ì´ë‹¤. ë£¨í”„ë¥¼ í†µí•´ ì ‘ê·¼í•˜ë©´ íŒŒì´ì¬ì˜ ë‚´ì¥ ë¦¬ìŠ¤íŠ¸ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒê³¼ ë³„ë°˜ ë‹¤ë¥¼ê²Œ ì—†ì–´ì§€ê¸° ë•Œë¬¸ì—, í…ì„œë¥¼ ì‚¬ìš©í•˜ëŠ” ë©”ë¦¬íŠ¸ê°€ ì‚¬ë¼ì§„ë‹¤. ë¹„êµì  í¬ì§€ ì•Šì€ 2~3ì°¨ì›ì˜ í…ì„œ ì •ë„ë¼ë©´ ì‚¬ìš©í•´ë„ í¬ê²Œ ë¬¸ì œëŠ” ì—†ì„ê±°ë¼ ìƒê°í•˜ì§€ë§Œ ê·¸ë˜ë„ ì½”ë“œì˜ ì¼ê´€ì„±ì„ ìœ„í•´ <code class="language-plaintext highlighter-rouge">torch.gather</code> ì‚¬ìš©ì„ ê¶Œì¥í•œë‹¤. ì´ì œ <code class="language-plaintext highlighter-rouge">torch.gather</code>ì˜ ì‚¬ìš©ë²•ì— ëŒ€í•´ ì•Œì•„ë³´ì.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># torch.gather params
</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">gather</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">sparse_grad</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">dim</code>ê³¼ <code class="language-plaintext highlighter-rouge">index</code>ì— ì£¼ëª©í•´ë³´ì. ë¨¼ì € <code class="language-plaintext highlighter-rouge">dim</code>ì€ ì‚¬ìš©ìê°€ ì¸ë±ì‹±ì„ ì ìš©í•˜ê³  ì‹¶ì€ ì°¨ì›ì„ ì§€ì •í•´ì£¼ëŠ” ì—­í• ì„ í•œë‹¤. <code class="language-plaintext highlighter-rouge">index</code> ë§¤ê°œë³€ìˆ˜ë¡œ ì „ë‹¬í•˜ëŠ” í…ì„œ ì•ˆì—ëŠ” ì›ì†Œì˜ <code class="language-plaintext highlighter-rouge">â€˜ì¸ë±ìŠ¤â€™</code>ë¥¼ ì˜ë¯¸í•˜ëŠ” ìˆ«ìë“¤ì´ ë§ˆêµ¬ì¡ì´ë¡œ ë‹´ê²¨ìˆëŠ”ë°, í•´ë‹¹ ì¸ë±ìŠ¤ê°€ ëŒ€ìƒ í…ì„œì˜ ì–´ëŠ ì°¨ì›ì„ ê°€ë¦¬í‚¬ ê²ƒì¸ì§€ë¥¼ ì»´í“¨í„°ì—ê²Œ ì•Œë ¤ì¤€ë‹¤ê³  ìƒê°í•˜ë©´ ëœë‹¤. <code class="language-plaintext highlighter-rouge">index</code> ëŠ” ì•ì—ì„œ ì„¤ëª…í–ˆë“¯ì´ ì›ì†Œì˜ <code class="language-plaintext highlighter-rouge">â€˜ì¸ë±ìŠ¤â€™</code>ë¥¼ ì˜ë¯¸í•˜ëŠ” ìˆ«ìë“¤ì´ ë‹´ê¸´ í…ì„œë¥¼ ì…ë ¥ìœ¼ë¡œ í•˜ëŠ” ë§¤ê°œë³€ìˆ˜ë‹¤. ì´ ë•Œ ì£¼ì˜í•  ì ì€ ëŒ€ìƒ í…ì„œ(<code class="language-plaintext highlighter-rouge">input</code>)ì™€ ì¸ë±ìŠ¤ í…ì„œì˜ ì°¨ì› í˜•íƒœê°€ ë°˜ë“œì‹œ ë™ì¼í•´ì•¼ í•œë‹¤ëŠ” ê²ƒì´ë‹¤. ì—­ì‹œ ë§ë¡œë§Œ ë“¤ìœ¼ë©´ ì´í•´í•˜ê¸° í˜ë“œë‹ˆ ì‚¬ìš© ì˜ˆì‹œë¥¼ í•¨ê¼ ì‚´í´ë³´ì.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># torch.gather usage example
</span><span class="o">&gt;&gt;&gt;</span> <span class="n">q</span><span class="p">,</span> <span class="n">kr</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1024</span><span class="p">,</span> <span class="mi">64</span><span class="p">),</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1024</span><span class="p">,</span> <span class="mi">64</span><span class="p">)</span> <span class="c1"># [batch, sequence, dim_head], [batch, 2*sequence, dim_head]
</span><span class="o">&gt;&gt;&gt;</span> <span class="n">tmp_c2p</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">kr</span><span class="p">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">))</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">tmp_c2p</span><span class="p">,</span> <span class="n">tmp_c2p</span><span class="p">.</span><span class="n">shape</span>
<span class="p">(</span><span class="n">tensor</span><span class="p">([[</span><span class="o">-</span><span class="mf">2.6477</span><span class="p">,</span> <span class="o">-</span><span class="mf">4.7478</span><span class="p">,</span> <span class="o">-</span><span class="mf">5.3250</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mf">1.6062</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.9717</span><span class="p">,</span>  <span class="mf">3.8004</span><span class="p">],</span>
         <span class="p">[</span> <span class="mf">0.0662</span><span class="p">,</span>  <span class="mf">1.5240</span><span class="p">,</span>  <span class="mf">0.1182</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mf">0.1653</span><span class="p">,</span>  <span class="mf">2.8476</span><span class="p">,</span>  <span class="mf">1.6337</span><span class="p">],</span>
         <span class="p">[</span><span class="o">-</span><span class="mf">0.5010</span><span class="p">,</span> <span class="o">-</span><span class="mf">4.2267</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.1179</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mf">1.1447</span><span class="p">,</span>  <span class="mf">1.7845</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1493</span><span class="p">],</span>
         <span class="p">...,</span>
         <span class="p">[</span><span class="o">-</span><span class="mf">2.1073</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2149</span><span class="p">,</span> <span class="o">-</span><span class="mf">4.8630</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mf">0.8238</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5833</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2066</span><span class="p">],</span>
         <span class="p">[</span> <span class="mf">2.1747</span><span class="p">,</span>  <span class="mf">3.2924</span><span class="p">,</span>  <span class="mf">6.5808</span><span class="p">,</span>  <span class="p">...,</span> <span class="o">-</span><span class="mf">0.2926</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2511</span><span class="p">,</span>  <span class="mf">2.6996</span><span class="p">],</span>
         <span class="p">[</span><span class="o">-</span><span class="mf">2.8362</span><span class="p">,</span>  <span class="mf">2.8700</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.9729</span><span class="p">,</span>  <span class="p">...,</span> <span class="o">-</span><span class="mf">4.9913</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3616</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1708</span><span class="p">]],</span>
        <span class="n">grad_fn</span><span class="o">=&lt;</span><span class="n">MmBackward0</span><span class="o">&gt;</span><span class="p">)</span>
<span class="n">torch</span><span class="p">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">1024</span><span class="p">,</span> <span class="mi">1024</span><span class="p">]))</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">max_seq</span><span class="p">,</span> <span class="n">max_relative_position</span> <span class="o">=</span> <span class="mi">1024</span><span class="p">,</span> <span class="mi">512</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">q_index</span><span class="p">,</span> <span class="n">k_index</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="n">max_seq</span><span class="p">),</span> <span class="n">torch</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">max_relative_position</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">q_index</span><span class="p">,</span> <span class="n">k_index</span>
<span class="p">(</span><span class="n">tensor</span><span class="p">([</span>   <span class="mi">0</span><span class="p">,</span>    <span class="mi">1</span><span class="p">,</span>    <span class="mi">2</span><span class="p">,</span>  <span class="p">...,</span> <span class="mi">1021</span><span class="p">,</span> <span class="mi">1022</span><span class="p">,</span> <span class="mi">1023</span><span class="p">]),</span>
 <span class="n">tensor</span><span class="p">([</span>   <span class="mi">0</span><span class="p">,</span>    <span class="mi">1</span><span class="p">,</span>    <span class="mi">2</span><span class="p">,</span>  <span class="p">...,</span> <span class="mi">1021</span><span class="p">,</span> <span class="mi">1022</span><span class="p">,</span> <span class="mi">1023</span><span class="p">]))</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">tmp_pos</span> <span class="o">=</span> <span class="n">q_index</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="o">-</span> <span class="n">k_index</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">rel_pos_matrix</span> <span class="o">=</span> <span class="n">tmp_pos</span> <span class="o">+</span> <span class="n">max_relative_position</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">rel_pos_matrix</span>
<span class="n">tensor</span><span class="p">([[</span> <span class="mi">512</span><span class="p">,</span>  <span class="mi">511</span><span class="p">,</span>  <span class="mi">510</span><span class="p">,</span>  <span class="p">...,</span> <span class="o">-</span><span class="mi">509</span><span class="p">,</span> <span class="o">-</span><span class="mi">510</span><span class="p">,</span> <span class="o">-</span><span class="mi">511</span><span class="p">],</span>
        <span class="p">[</span> <span class="mi">513</span><span class="p">,</span>  <span class="mi">512</span><span class="p">,</span>  <span class="mi">511</span><span class="p">,</span>  <span class="p">...,</span> <span class="o">-</span><span class="mi">508</span><span class="p">,</span> <span class="o">-</span><span class="mi">509</span><span class="p">,</span> <span class="o">-</span><span class="mi">510</span><span class="p">],</span>
        <span class="p">[</span> <span class="mi">514</span><span class="p">,</span>  <span class="mi">513</span><span class="p">,</span>  <span class="mi">512</span><span class="p">,</span>  <span class="p">...,</span> <span class="o">-</span><span class="mi">507</span><span class="p">,</span> <span class="o">-</span><span class="mi">508</span><span class="p">,</span> <span class="o">-</span><span class="mi">509</span><span class="p">],</span>
        <span class="p">...,</span>
        <span class="p">[</span><span class="mi">1533</span><span class="p">,</span> <span class="mi">1532</span><span class="p">,</span> <span class="mi">1531</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mi">512</span><span class="p">,</span>  <span class="mi">511</span><span class="p">,</span>  <span class="mi">510</span><span class="p">],</span>
        <span class="p">[</span><span class="mi">1534</span><span class="p">,</span> <span class="mi">1533</span><span class="p">,</span> <span class="mi">1532</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mi">513</span><span class="p">,</span>  <span class="mi">512</span><span class="p">,</span>  <span class="mi">511</span><span class="p">],</span>
        <span class="p">[</span><span class="mi">1535</span><span class="p">,</span> <span class="mi">1534</span><span class="p">,</span> <span class="mi">1533</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mi">514</span><span class="p">,</span>  <span class="mi">513</span><span class="p">,</span>  <span class="mi">512</span><span class="p">]])</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">rel_pos_matrix</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">rel_pos_matrix</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="o">*</span><span class="n">max_relative_position</span> <span class="o">-</span> <span class="mi">1</span><span class="p">).</span><span class="n">repeat</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">tmp_c2p</span> <span class="o">=</span> <span class="n">tmp_c2p</span><span class="p">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">rel_pos_matrix</span><span class="p">,</span> <span class="n">rel_pos_matrix</span><span class="p">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">tmp_c2p</span><span class="p">.</span><span class="n">shape</span> 
<span class="p">(</span><span class="n">tensor</span><span class="p">([[[</span> <span class="mi">512</span><span class="p">,</span>  <span class="mi">511</span><span class="p">,</span>  <span class="mi">510</span><span class="p">,</span>  <span class="p">...,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">],</span>
          <span class="p">[</span> <span class="mi">513</span><span class="p">,</span>  <span class="mi">512</span><span class="p">,</span>  <span class="mi">511</span><span class="p">,</span>  <span class="p">...,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">],</span>
          <span class="p">[</span> <span class="mi">514</span><span class="p">,</span>  <span class="mi">513</span><span class="p">,</span>  <span class="mi">512</span><span class="p">,</span>  <span class="p">...,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">],</span>
          <span class="p">...,</span>
          <span class="p">[</span><span class="mi">1023</span><span class="p">,</span> <span class="mi">1023</span><span class="p">,</span> <span class="mi">1023</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mi">512</span><span class="p">,</span>  <span class="mi">511</span><span class="p">,</span>  <span class="mi">510</span><span class="p">],</span>
          <span class="p">[</span><span class="mi">1023</span><span class="p">,</span> <span class="mi">1023</span><span class="p">,</span> <span class="mi">1023</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mi">513</span><span class="p">,</span>  <span class="mi">512</span><span class="p">,</span>  <span class="mi">511</span><span class="p">],</span>
          <span class="p">[</span><span class="mi">1023</span><span class="p">,</span> <span class="mi">1023</span><span class="p">,</span> <span class="mi">1023</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mi">514</span><span class="p">,</span>  <span class="mi">513</span><span class="p">,</span>  <span class="mi">512</span><span class="p">]],</span>
 
         <span class="p">[[</span> <span class="mi">512</span><span class="p">,</span>  <span class="mi">511</span><span class="p">,</span>  <span class="mi">510</span><span class="p">,</span>  <span class="p">...,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">],</span>
          <span class="p">[</span> <span class="mi">513</span><span class="p">,</span>  <span class="mi">512</span><span class="p">,</span>  <span class="mi">511</span><span class="p">,</span>  <span class="p">...,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">],</span>
          <span class="p">[</span> <span class="mi">514</span><span class="p">,</span>  <span class="mi">513</span><span class="p">,</span>  <span class="mi">512</span><span class="p">,</span>  <span class="p">...,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">],</span>
          <span class="p">...,</span>
          <span class="p">[</span><span class="mi">1023</span><span class="p">,</span> <span class="mi">1023</span><span class="p">,</span> <span class="mi">1023</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mi">512</span><span class="p">,</span>  <span class="mi">511</span><span class="p">,</span>  <span class="mi">510</span><span class="p">],</span>
          <span class="p">[</span><span class="mi">1023</span><span class="p">,</span> <span class="mi">1023</span><span class="p">,</span> <span class="mi">1023</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mi">513</span><span class="p">,</span>  <span class="mi">512</span><span class="p">,</span>  <span class="mi">511</span><span class="p">],</span>
          <span class="p">[</span><span class="mi">1023</span><span class="p">,</span> <span class="mi">1023</span><span class="p">,</span> <span class="mi">1023</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mi">514</span><span class="p">,</span>  <span class="mi">513</span><span class="p">,</span>  <span class="mi">512</span><span class="p">]],</span>
<span class="n">torch</span><span class="p">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1024</span><span class="p">,</span> <span class="mi">1024</span><span class="p">]),</span>
<span class="n">torch</span><span class="p">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1024</span><span class="p">,</span> <span class="mi">1024</span><span class="p">]))</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">gather</span><span class="p">(</span><span class="n">tmp_c2p</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">rel_pos_matrix</span><span class="p">)</span>
<span class="n">tensor</span><span class="p">([[[</span><span class="o">-</span><span class="mf">0.8579</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2178</span><span class="p">,</span>  <span class="mf">1.6323</span><span class="p">,</span>  <span class="p">...,</span> <span class="o">-</span><span class="mf">2.6477</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.6477</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.6477</span><span class="p">],</span>
         <span class="p">[</span> <span class="mf">1.1601</span><span class="p">,</span>  <span class="mf">2.1752</span><span class="p">,</span>  <span class="mf">0.7187</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mf">0.0662</span><span class="p">,</span>  <span class="mf">0.0662</span><span class="p">,</span>  <span class="mf">0.0662</span><span class="p">],</span>
         <span class="p">[</span> <span class="mf">3.4379</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2573</span><span class="p">,</span>  <span class="mf">0.1375</span><span class="p">,</span>  <span class="p">...,</span> <span class="o">-</span><span class="mf">0.5010</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5010</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5010</span><span class="p">],</span>
         <span class="p">...,</span>
         <span class="p">[</span><span class="o">-</span><span class="mf">1.2066</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2066</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2066</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mf">0.5943</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5169</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.0820</span><span class="p">],</span>
         <span class="p">[</span> <span class="mf">2.6996</span><span class="p">,</span>  <span class="mf">2.6996</span><span class="p">,</span>  <span class="mf">2.6996</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mf">0.2014</span><span class="p">,</span>  <span class="mf">1.1458</span><span class="p">,</span>  <span class="mf">3.2626</span><span class="p">],</span>
         <span class="p">[</span><span class="o">-</span><span class="mf">0.1708</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1708</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1708</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mf">1.9955</span><span class="p">,</span>  <span class="mf">4.1549</span><span class="p">,</span>  <span class="mf">2.6356</span><span class="p">]],</span>
</code></pre></div></div>

<p>ìœ„ ì½”ë“œëŠ” <code class="language-plaintext highlighter-rouge">DeBERTa</code> ì˜ <code class="language-plaintext highlighter-rouge">Disentangled Self-Attention</code>ì„ êµ¬í˜„í•œ ì½”ë“œì˜ ì¼ë¶€ë¶„ì´ë‹¤. ìì„¸í•œ ì›ë¦¬ëŠ” <code class="language-plaintext highlighter-rouge">DeBERTa</code> ë…¼ë¬¸ ë¦¬ë·° í¬ìŠ¤íŒ…ì—ì„œ í™•ì¸í•˜ë©´ ë˜ê³ , ìš°ë¦¬ê°€ ì§€ê¸ˆ ì£¼ëª©í•  ë¶€ë¶„ì€ ë°”ë¡œ <code class="language-plaintext highlighter-rouge">tmp_c2p</code>, <code class="language-plaintext highlighter-rouge">rel_pos_matrix</code> ê·¸ë¦¬ê³  ë§ˆì§€ë§‰ ì¤„ì— ìœ„ì¹˜í•œ <code class="language-plaintext highlighter-rouge">torch.gather</code> ë‹¤. <code class="language-plaintext highlighter-rouge">[10, 1024, 1024]</code> ëª¨ì–‘ì„ ê°€ì§„ ëŒ€ìƒ í…ì„œ <code class="language-plaintext highlighter-rouge">tmp_c2p</code> ì—ì„œ ë‚´ê°€ ì›í•˜ëŠ” ì›ì†Œë§Œ ì¶”ì¶œí•˜ë ¤ëŠ” ìƒí™©ì¸ë°, ì¶”ì¶œí•´ì•¼í•  ì›ì†Œì˜ ì¸ë±ìŠ¤ ê°’ì´ ë‹´ê¸´ í…ì„œë¥¼ <code class="language-plaintext highlighter-rouge">rel_pos_matrix</code> ë¡œ ì •ì˜í–ˆë‹¤. <code class="language-plaintext highlighter-rouge">rel_pos_matrix</code> ì˜ ì°¨ì›ì€ <code class="language-plaintext highlighter-rouge">[10, 1024, 1024]</code>ë¡œ <code class="language-plaintext highlighter-rouge">tmp_c2p</code>ì™€ ë™ì¼í•˜ë‹¤. ì°¸ê³ ë¡œ ì¶”ì¶œí•´ì•¼ í•˜ëŠ” ì°¨ì› ë°©í–¥ì€ ê°€ë¡œ ë°©í–¥(ë‘ ë²ˆì§¸ 1024)ì´ë‹¤.</p>

<p>ì´ì œ <code class="language-plaintext highlighter-rouge">torch.gather</code>ì˜ ë™ì‘ì„ ì‚´í´ë³´ì. ìš°ë¦¬ê°€ í˜„ì¬ ì¶”ì¶œí•˜ê³  ì‹¶ì€ ëŒ€ìƒì€ 3ì°¨ì› í…ì„œì˜ ê°€ë¡œ ë°©í–¥(ë‘ ë²ˆì§¸ 1024, í…ì„œì˜ í–‰ ë²¡í„°), ì¦‰ <code class="language-plaintext highlighter-rouge">2 * max_sequence_length</code> ë¥¼ ì˜ë¯¸í•˜ëŠ” ì°¨ì› ë°©í–¥ì˜ ì›ì†Œë‹¤. ë”°ë¼ì„œ <code class="language-plaintext highlighter-rouge">dim=-1</code>ìœ¼ë¡œ ì„¤ì •í•´ì¤€ë‹¤. ì´ì œ ë©”ì„œë“œê°€ ì˜ë„ëŒ€ë¡œ ì ìš©ë˜ì—ˆëŠ”ì§€ í™•ì¸í•´ë³´ì. <code class="language-plaintext highlighter-rouge">rel_pos_matrix</code> ì˜ 0ë²ˆ ë°°ì¹˜, 0ë²ˆì§¸ ì‹œí€€ìŠ¤ì˜ ê°€ì¥ ë§ˆì§€ë§‰ ì°¨ì›ì˜ ê°’ì€ <code class="language-plaintext highlighter-rouge">0</code>ìœ¼ë¡œ ì´ˆê¸°í™” ë˜ì–´ ìˆë‹¤. ë‹¤ì‹œ ë§í•´, ëŒ€ìƒ í…ì„œì˜ ëŒ€ìƒ ì°¨ì›ì—ì„œ 0ë²ˆì§¸ ì¸ë±ìŠ¤ì— í•´ë‹¹í•˜ëŠ” ê°’ì„ ê°€ì ¸ì˜¤ë¼ëŠ” ì˜ë¯¸ë¥¼ ë‹´ê³  ìˆë‹¤. ê·¸ë ‡ë‹¤ë©´ <code class="language-plaintext highlighter-rouge">torch.gather</code> ì‹¤í–‰ ê²°ê³¼ê°€ <code class="language-plaintext highlighter-rouge">tmp_c2p</code>ì˜ 0ë²ˆ ë°°ì¹˜, 0ë²ˆì§¸ ì‹œí€€ìŠ¤ì˜ 0ë²ˆì§¸ ì°¨ì› ê°’ê³¼ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸í•´ë³´ì. ë‘˜ ë‹¤ <code class="language-plaintext highlighter-rouge">-2.6477</code>, <code class="language-plaintext highlighter-rouge">-2.6477</code> ìœ¼ë¡œ ê°™ì€ ê°’ì„ ë‚˜íƒ€ë‚´ê³  ìˆë‹¤. ë”°ë¼ì„œ ìš°ë¦¬ ì˜ë„ëŒ€ë¡œ ì˜ ì‹¤í–‰ë˜ì—ˆë‹¤ëŠ” ì‚¬ì‹¤ì„ ì•Œ ìˆ˜ ìˆë‹¤.</p>

<h3 id="torchtriu-torchtril"><code class="language-plaintext highlighter-rouge">ğŸ‘©â€ğŸ‘©â€ğŸ‘§â€ğŸ‘¦Â torch.triu, torch.tril</code></h3>

<p>ê°ê° ì…ë ¥ í…ì„œë¥¼ <code class="language-plaintext highlighter-rouge">ìƒì‚¼ê°í–‰ë ¬</code>, <code class="language-plaintext highlighter-rouge">í•˜ì‚¼ê°í–‰ë ¬</code>ë¡œ ë§Œë“ ë‹¤. <code class="language-plaintext highlighter-rouge">triu</code>ë‚˜ <code class="language-plaintext highlighter-rouge">tril</code>ì€ ì‚¬ì‹¤ ë’¤ì§‘ìœ¼ë©´ ê°™ì€ ê²°ê³¼ë¥¼ ë°˜í™˜í•˜ê¸° ë•Œë¬¸ì— <code class="language-plaintext highlighter-rouge">tril</code>ì„ ê¸°ì¤€ìœ¼ë¡œ ì„¤ëª…ì„ í•˜ê² ë‹¤. ë©”ì„œë“œì˜ ë§¤ê°œë³€ìˆ˜ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># torch.triu, tril params
</span><span class="n">upper_tri_matrix</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">triu</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span> <span class="n">diagonal</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
<span class="n">lower_tri_matrix</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tril</span><span class="p">(</span><span class="n">input_tensors</span><span class="p">,</span> <span class="n">diagonal</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">diagonal</code> ì— ì£¼ëª©í•´ë³´ì. ì–‘ìˆ˜ë¥¼ ì „ë‹¬í•˜ë©´ ì£¼ëŒ€ê°ì„±ë¶„ì—ì„œ í•´ë‹¹í•˜ëŠ” ê°’ë§Œí¼ ë–¨ì–´ì§„ ê³³ì˜ ëŒ€ê°ì„±ë¶„ê¹Œì§€ ê·¸ ê°’ì„ ì‚´ë ¤ë‘”ë‹¤. í•œí¸ ìŒìˆ˜ë¥¼ ì „ë‹¬í•˜ë©´ ì£¼ëŒ€ê°ì„±ë¶„ì„ í¬í•¨í•´ ì£¼ì–´ì§„ ê°’ë§Œí¼ ë–¨ì–´ì§„ ê³³ê¹Œì§€ì˜ ëŒ€ê°ì„±ë¶„ì„ ëª¨ë‘ 0ìœ¼ë¡œ ë§Œë“¤ì–´ë²„ë¦°ë‹¤. ê¸°ë³¸ì€ 0ìœ¼ë¡œ ì„¤ì •ë˜ì–´ ìˆìœ¼ë©°, ì´ëŠ” ì£¼ëŒ€ê°ì„±ë¶„ë¶€í„° ì™¼ìª½ í•˜ë‹¨ì˜ ì›ì†Œë¥¼ ëª¨ë‘ ì‚´ë ¤ë‘ê² ë‹¤ëŠ” ì˜ë¯¸ê°€ ëœë‹¤.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># torch.tril usage example
</span><span class="o">&gt;&gt;&gt;</span> <span class="n">lm_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tril</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">ones</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">lm_mask</span>
<span class="mi">1</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span>
<span class="mi">1</span> <span class="mi">1</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span>
<span class="mi">1</span> <span class="mi">1</span> <span class="mi">1</span> <span class="mi">0</span> <span class="mi">0</span>
<span class="mi">1</span> <span class="mi">1</span> <span class="mi">1</span> <span class="mi">1</span> <span class="mi">0</span>
</code></pre></div></div>

<p>ë‘ ë©”ì„œë“œëŠ” ì„ í˜•ëŒ€ìˆ˜í•™ì´ í•„ìš”í•œ ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ ì‚¬ìš©ë˜ëŠ”ë°, í•„ìì˜ ê²½ìš°, <code class="language-plaintext highlighter-rouge">GPT</code>ì²˜ëŸ¼ <code class="language-plaintext highlighter-rouge">Transformer</code>ì˜ <code class="language-plaintext highlighter-rouge">Decoder</code> ë¥¼ ì‚¬ìš©í•˜ëŠ” ëª¨ë¸ì„ ë¹Œë“œí•  ë•Œ ê°€ì¥ ë§ì´ ì‚¬ìš©í–ˆë˜ ê²ƒ ê°™ë‹¤. <code class="language-plaintext highlighter-rouge">Decoder</code>ë¥¼ ì‚¬ìš©í•˜ëŠ” ëª¨ë¸ì€ ëŒ€ë¶€ë¶„ êµ¬ì¡°ìƒ <code class="language-plaintext highlighter-rouge">Language Modeling</code>ì„ ìœ„í•´ì„œ <code class="language-plaintext highlighter-rouge">Masked Multi-Head Self-Attention Block</code>ì„ ì‚¬ìš©í•˜ëŠ”ë° ì´ ë•Œ ë¯¸ë˜ ì‹œì ì˜ í† í° ì„ë² ë”© ê°’ì— ë§ˆìŠ¤í‚¹ì„ í•´ì£¼ê¸° ìœ„í•´ <code class="language-plaintext highlighter-rouge">torch.tril</code> ì„ ì‚¬ìš©í•˜ê²Œ ë˜ë‹ˆ ì°¸ê³ í•˜ì.</p>

<h3 id="torchtensormasked_fill"><code class="language-plaintext highlighter-rouge">ğŸ‘©â€ğŸ‘©â€ğŸ‘§â€ğŸ‘¦Â torch.Tensor.masked_fill</code></h3>

<p>ì‚¬ìš©ìê°€ ì§€ì •í•œ ê°’ì— í•´ë‹¹ë˜ëŠ” ì›ì†Œë¥¼ ëª¨ë‘ ë§ˆìŠ¤í‚¹ ì²˜ë¦¬í•´ì£¼ëŠ” ë©”ì„œë“œë‹¤. ë¨¼ì € ë§¤ê°œë³€ìˆ˜ë¥¼ í™•ì¸í•´ë³´ì.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># torch.Tensor.masked_fill params
</span><span class="n">input_tensors</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">6</span><span class="p">]])</span>
<span class="n">input_tensors</span><span class="p">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">mask</span><span class="p">:</span> <span class="n">BoolTensor</span><span class="p">,</span> <span class="n">value</span><span class="p">:</span> <span class="nb">float</span><span class="p">)</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">masked_fill</code> ì€ í…ì„œ ê°ì²´ì˜ ë‚´ë¶€ <code class="language-plaintext highlighter-rouge">attribute</code> ë¡œ ì •ì˜ë˜ê¸° ë•Œë¬¸ì— í•´ë‹¹ ë©”ì„œë“œë¥¼ ì‚¬ìš©í•˜ê³  ì‹¶ë‹¤ë©´ ë¨¼ì € ë§ˆìŠ¤í‚¹ ëŒ€ìƒ í…ì„œë¥¼ ë§Œë“¤ì–´ì•¼ í•œë‹¤. í…ì„œë¥¼ ì •ì˜í–ˆë‹¤ë©´ í…ì„œ ê°ì²´ì˜ <code class="language-plaintext highlighter-rouge">attributes</code> ì ‘ê·¼ì„ í†µí•´ <code class="language-plaintext highlighter-rouge">masked_fill()</code> ì„ í˜¸ì¶œí•œ ë’¤, í•„ìš”í•œ ë§¤ê°œë³€ìˆ˜ë¥¼ ì „ë‹¬í•´ì£¼ëŠ” ë°©ì‹ìœ¼ë¡œ ì‚¬ìš©í•˜ë©´ ëœë‹¤.</p>

<p><code class="language-plaintext highlighter-rouge">mask</code> ë§¤ê°œë³€ìˆ˜ì—ëŠ” ë§ˆìŠ¤í‚¹ í…ì„œë¥¼ ì „ë‹¬í•´ì•¼ í•˜ëŠ”ë°, ì´ ë•Œ ë‚´ë¶€ ì›ì†ŒëŠ” ëª¨ë‘ <code class="language-plaintext highlighter-rouge">boolean</code>ì´ì–´ì•¼ í•˜ê³  í…ì„œì˜ í˜•íƒœëŠ” ëŒ€ìƒ í…ì„œì™€ ë™ì¼í•´ì•¼ í•œë‹¤(ì™„ì „íˆ ê°™ì„ í•„ìš”ëŠ” ì—†ê³ , ë¸Œë¡œë“œ ìºìŠ¤íŒ…ë§Œ ê°€ëŠ¥í•˜ë©´ ìƒê´€ ì—†ìŒ).</p>

<p><code class="language-plaintext highlighter-rouge">value</code> ë§¤ê°œë³€ìˆ˜ì—ëŠ” ë§ˆìŠ¤í‚¹ ëŒ€ìƒ ì›ì†Œë“¤ì— ì¼ê´„ì ìœ¼ë¡œ ì ìš©í•´ì£¼ê³  ì‹¶ì€ ê°’ì„ ì „ë‹¬í•œë‹¤. ì´ê²Œ ë§ë¡œë§Œ ë“¤ìœ¼ë©´ ì´í•´í•˜ê¸° ì‰½ì§€ ì•Šë‹¤. ì•„ë˜ ì‚¬ìš© ì˜ˆì‹œë¥¼ í•¨ê»˜ ì²¨ë¶€í–ˆìœ¼ë‹ˆ ì°¸ê³  ë°”ë€ë‹¤.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># torch.masked_fill usage
</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">lm_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tril</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">ones</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">lm_mask</span>
<span class="mi">1</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span>
<span class="mi">1</span> <span class="mi">1</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span>
<span class="mi">1</span> <span class="mi">1</span> <span class="mi">1</span> <span class="mi">0</span> <span class="mi">0</span>
<span class="mi">1</span> <span class="mi">1</span> <span class="mi">1</span> <span class="mi">1</span> <span class="mi">0</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">attention_matrix</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">))</span> <span class="o">/</span> <span class="n">dot_scale</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">attention_matrix</span>
<span class="mf">1.22</span> <span class="mf">2.1</span> <span class="mf">3.4</span> <span class="mf">1.2</span> <span class="mf">1.1</span>
<span class="mf">1.22</span> <span class="mf">2.1</span> <span class="mf">3.4</span> <span class="mf">9.9</span> <span class="mf">9.9</span>
<span class="mf">1.22</span> <span class="mf">2.1</span> <span class="mf">3.4</span> <span class="mf">9.9</span> <span class="mf">9.9</span>
<span class="mf">1.22</span> <span class="mf">2.1</span> <span class="mf">3.4</span> <span class="mf">9.9</span> <span class="mf">9.9</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">attention_matrix</span> <span class="o">=</span> <span class="n">attention_matrix</span><span class="p">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">lm_mask</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="s">'-inf'</span><span class="p">))</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">attention_matrix</span>
<span class="mf">1.22</span> <span class="o">-</span><span class="n">inf</span> <span class="o">-</span><span class="n">inf</span> <span class="o">-</span><span class="n">inf</span> <span class="o">-</span><span class="n">inf</span>
<span class="mf">1.22</span> <span class="mf">2.1</span> <span class="o">-</span><span class="n">inf</span> <span class="o">-</span><span class="n">inf</span> <span class="o">-</span><span class="n">inf</span>
<span class="mf">1.22</span> <span class="mf">2.1</span> <span class="mf">3.4</span> <span class="o">-</span><span class="n">inf</span> <span class="o">-</span><span class="n">inf</span>
<span class="mf">1.22</span> <span class="mf">2.1</span> <span class="mf">3.4</span> <span class="mf">9.9</span> <span class="o">-</span><span class="n">inf</span>
</code></pre></div></div>]]></content><author><name>qcqced</name><email>qcqced123@gmail.com</email></author><category term="Framework &amp; Library" /><category term="Pytorch" /><category term="Tensor" /><category term="Linear Algebra" /><summary type="html"><![CDATA[íŒŒì´í† ì¹˜ì—ì„œ ìì£¼ ì‚¬ìš©í•˜ëŠ” í…ì„œ ì¸ë±ì‹± ê´€ë ¨ ë©”ì„œë“œ ëª¨ìŒ]]></summary></entry><entry><title type="html">ğŸª¢Â [DeBERTa] DeBERTa: Decoding-Enhanced BERT with Disentangled-Attention</title><link href="http://localhost:4000/nlp/deberta" rel="alternate" type="text/html" title="ğŸª¢Â [DeBERTa] DeBERTa: Decoding-Enhanced BERT with Disentangled-Attention" /><published>2023-08-04T00:00:00+09:00</published><updated>2023-08-05T02:00:00+09:00</updated><id>http://localhost:4000/nlp/DeBERTa</id><content type="html" xml:base="http://localhost:4000/nlp/deberta"><![CDATA[<h3 id="overview"><code class="language-plaintext highlighter-rouge">ğŸ”­Â Overview</code></h3>

<p><code class="language-plaintext highlighter-rouge">DeBERTa</code>ëŠ” 2020ë…„ <code class="language-plaintext highlighter-rouge">Microsoft</code>ê°€ <code class="language-plaintext highlighter-rouge">ICLR</code>ì—ì„œ ë°œí‘œí•œ ìì—°ì–´ ì²˜ë¦¬ìš© ì‹ ê²½ë§ ëª¨ë¸ì´ë‹¤. <code class="language-plaintext highlighter-rouge">Disentangled Self-Attention</code>, <code class="language-plaintext highlighter-rouge">Enhanced Mask Decoder</code>ë¼ëŠ” ë‘ê°€ì§€ ìƒˆë¡œìš´ í…Œí¬ë‹‰ì„ <code class="language-plaintext highlighter-rouge">BERT</code>, <code class="language-plaintext highlighter-rouge">RoBERTa</code>ì— ì ìš©í•´ ë‹¹ì‹œ <code class="language-plaintext highlighter-rouge">SOTA</code>ë¥¼ ë‹¬ì„±í–ˆìœ¼ë©°, íŠ¹íˆ ì˜ì–´ì²˜ëŸ¼ ë¬¸ì¥ì—ì„œ ìë¦¬í•˜ëŠ” ìœ„ì¹˜ì— ë”°ë¼ ë‹¨ì–´ì˜ ì˜ë¯¸, í˜•íƒœê°€ ê²°ì •ë˜ëŠ” êµ´ì ˆì–´ ê³„ì—´ì— ëŒ€í•œ ì„±ëŠ¥ì´ ì¢‹ì•„ ê¾¸ì¤€íˆ ì‚¬ë‘ë°›ê³  ìˆëŠ” ëª¨ë¸ì´ë‹¤. ë˜í•œ ì¸ì½”ë”© ê°€ëŠ¥í•œ ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´ê°€ <code class="language-plaintext highlighter-rouge">4096</code>ìœ¼ë¡œ ë§¤ìš° ê¸´ í¸ (<code class="language-plaintext highlighter-rouge">DeBERTa-V3-Large</code>) ì— ì†í•´, <code class="language-plaintext highlighter-rouge">Kaggle Competition</code>ì—ì„œ ìì£¼ í™œìš©ëœë‹¤. ì¶œì‹œëœì§€ 2ë…„ì´ ë„˜ë„ë¡ <code class="language-plaintext highlighter-rouge">SuperGLUE</code> ëŒ€ì‹œë³´ë“œì—ì„œ ê¾¸ì¤€íˆ ìƒìœ„ê¶Œì„ ìœ ì§€í•˜ê³  ìˆë‹¤ëŠ” ì ë„ <code class="language-plaintext highlighter-rouge">DeBERTa</code>ê°€ ì–¼ë§ˆë‚˜ ì˜ ì„¤ê³„ëœ ëª¨ë¸ì¸ì§€ ì•Œ ìˆ˜ ìˆëŠ” ëŒ€ëª©ì´ë‹¤.</p>

<p>í•œí¸, <code class="language-plaintext highlighter-rouge">DeBERTa</code>ì˜ ì„¤ê³„ ì² í•™ì€ <code class="language-plaintext highlighter-rouge">Inductive Bias</code> ë‹¤. ê°„ë‹¨í•˜ê²Œ <code class="language-plaintext highlighter-rouge">Inductive Bias</code>ë€, ì£¼ì–´ì§„ ë°ì´í„°ë¡œë¶€í„° ì¼ë°˜í™” ì„±ëŠ¥ì„ ë†’ì´ê¸° ìœ„í•´Â <code class="language-plaintext highlighter-rouge">"ì…ë ¥ë˜ëŠ” ë°ì´í„°ëŠ” ~ í•  ê²ƒì´ë‹¤"</code>,Â <code class="language-plaintext highlighter-rouge">"ì´ëŸ° íŠ¹ì§•ì„ ê°–ê³  ìˆì„ ê²ƒì´ë‹¤"</code>ì™€ ê°™ì€ ê°€ì •, ê°€ì¤‘ì¹˜, ê°€ì„¤ ë“±ì„ ê¸°ê³„í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ì— ì ìš©í•˜ëŠ” ê²ƒì„ ë§í•œë‹¤. <strong><a href="https://qcqced123.github.io/cv/vit"><code class="language-plaintext highlighter-rouge">ViT</code> ë…¼ë¬¸ ë¦¬ë·°</a></strong>ì—ì„œë„ ë°í˜”ë“¯, í“¨ì–´í•œ <code class="language-plaintext highlighter-rouge">Self-Attention</code> ì˜ <code class="language-plaintext highlighter-rouge">Inductive Bias</code> ëŠ” ì‚¬ì‹¤ìƒ ì—†ìœ¼ë©°, ì „ì²´ <code class="language-plaintext highlighter-rouge">Transformer</code> êµ¬ì¡° ë ˆë²¨ì—ì„œ ë´ë„ <code class="language-plaintext highlighter-rouge">Absolute Position Embedding</code>ì„ ì‚¬ìš©í•´ í† í°ì˜ ìœ„ì¹˜ ì •ë³´ë¥¼ ëª¨ë¸ì— ì£¼ì…í•´ì£¼ëŠ” ê²ƒì´ ê·¸ë‚˜ë§ˆ ì•½í•œ <code class="language-plaintext highlighter-rouge">Iniductive Bias</code>ë¼ê³  ë³¼ ìˆ˜ ìˆë‹¤. ë‹¤ë¥¸ í¬ìŠ¤íŒ…ì—ì„œëŠ” ë¶„ëª… <code class="language-plaintext highlighter-rouge">Inductive Bias</code> ê°€ ì ê¸° ë•Œë¬¸ì— ìì—°ì–´ ì²˜ë¦¬ì—ì„œ <code class="language-plaintext highlighter-rouge">Transformer</code> ê°€ ì„±ê³µì„ ê±°ë‘˜ ìˆ˜ ìˆë‹¤ê³  í•´ë†“ê³  ì´ê²Œ ì§€ê¸ˆ ì™€ì„œ ë§ì„ ë’¤ì§‘ëŠ”ë‹¤ê³  ìƒê°í•  ìˆ˜ ìˆë‹¤. í•˜ì§€ë§Œ <code class="language-plaintext highlighter-rouge">Self-Attention</code>ê³¼ <code class="language-plaintext highlighter-rouge">Absolute Position Embedding</code>ì˜ ì˜ë¯¸ë¥¼ ë‹¤ì‹œ í•œ ë²ˆ ìƒê¸°í•´ë³´ë©´, <code class="language-plaintext highlighter-rouge">Inductive Bias</code> ì¶”ê°€ë¥¼ ì£¼ì¥í•˜ëŠ” ì €ìë“¤ì˜ ìƒê°ì´ ê½¤ë‚˜ í•©ë¦¬ì ì´ì—ˆìŒì„ ì•Œ ìˆ˜ ìˆê²Œ ëœë‹¤. êµ¬ì²´ì ì¸ ëª¨ë¸ êµ¬ì¡°ë¥¼ íŒŒì•…í•˜ê¸° ì „ì— ë¨¼ì € <code class="language-plaintext highlighter-rouge">Inductive Bias</code> ì¶”ê°€ê°€ ì™œ í•„ìš”í•˜ë©°, ì–´ë– í•œ ê°€ì •ì´ í•„ìš”í•œì§€ ì•Œì•„ë³´ì.</p>

<h3 id="inducitve-bias-in-deberta"><code class="language-plaintext highlighter-rouge">ğŸª¢Â Inducitve Bias in DeBERTa</code></h3>
<ul>
  <li><strong><code class="language-plaintext highlighter-rouge">Absolute Position + Relative Position</code>ì„ ëª¨ë‘ í™œìš©í•´ í’ë¶€í•˜ê³  ê¹Šì€ ì„ë² ë”© ì¶”ì¶œ</strong></li>
  <li><strong><code class="language-plaintext highlighter-rouge">ë‹¨ì–´ì˜ ë°œìƒ ìˆœì„œ</code> ì„ë² ë”©ê³¼ <code class="language-plaintext highlighter-rouge">ë‹¨ì–´ ë¶„í¬ ê°€ì„¤</code> ì„ë² ë”©ì„ ëª¨ë‘ ì¶”ì¶œí•˜ëŠ” ê²ƒì„ ëª©ì ìœ¼ë¡œ ì„¤ê³„</strong></li>
</ul>

<p>ë³¸ ë…¼ë¬¸ ì´ˆë¡ì—ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ë¬¸ì¥ì´ ì„œìˆ ë˜ì–´ ìˆë‹¤.</p>

<p><code class="language-plaintext highlighter-rouge">motivated by the observation that the attention weight of a word pair depends on not only their contents but their relative positions. For example, the dependency between the words â€œdeepâ€ and â€œlearningâ€ is much stronger when they occur next to each other than when they occur in different sentences.</code></p>

<p>ìœ„ì˜ ë‘ ë¬¸ì¥ì´ <code class="language-plaintext highlighter-rouge">DeBERTa</code>ì˜ <code class="language-plaintext highlighter-rouge">Inducitve Bias</code> ë¥¼ ê°€ì¥ ì˜ ì„¤ëª…í•˜ê³  ìˆë‹¤ê³  ìƒê°í•œë‹¤. ì €ìê°€ ì¶”ê°€ë¥¼ ì£¼ì¥í•˜ëŠ” <code class="language-plaintext highlighter-rouge">Inductive Bias</code>ë€, <code class="language-plaintext highlighter-rouge">relative position</code> ì •ë³´ë¼ëŠ” ê²ƒê³¼ ê¸°ì¡´ ëª¨ë¸ë§ìœ¼ë¡œëŠ” <code class="language-plaintext highlighter-rouge">relative position</code>ì´ ì£¼ëŠ” ë¬¸ë§¥ ì •ë³´ í¬ì°©ì´ ë¶ˆê°€ëŠ¥í•˜ë‹¤ëŠ” ì‚¬ì‹¤ì„ ì•Œ ìˆ˜ ìˆë‹¤.</p>

<p>ê·¸ë ‡ë‹¤ë©´ <code class="language-plaintext highlighter-rouge">relative position</code> ê°€ ì œê³µí•˜ëŠ” ë¬¸ë§¥ ì •ë³´ê°€ ë„ëŒ€ì²´ ë­ê¸¸ë˜ ê¸°ì¡´ì˜ ë°©ì‹ìœ¼ë¡œëŠ” í¬ì°©ì´ ë¶ˆê°€ëŠ¥í•˜ë‹¤ëŠ” ê²ƒì¼ê¹Œ?? ìì—°ì–´ì—ì„œ í¬ì°© ê°€ëŠ¥í•œ ë¬¸ë§¥ë“¤ì˜ ì¢…ë¥˜ì™€ ê¸°ì¡´ì˜ ëª¨ë¸ë§ ë°©ì‹ì— ëŒ€í•œ ì •ë¦¬ë¶€í„° í•´ë³´ì. ì—¬ê¸°ì„œ ë§í•˜ëŠ” ê¸°ì¡´ ë°©ì‹ì´ë€, í“¨ì–´í•œ <code class="language-plaintext highlighter-rouge">Self-Attention</code>ê³¼ <code class="language-plaintext highlighter-rouge">Absolute Position Embedding</code> ì„ ì‚¬ìš©í•˜ëŠ” <code class="language-plaintext highlighter-rouge">Transformer-Encoder-Base</code>  ëª¨ë¸(<code class="language-plaintext highlighter-rouge">BERT</code>, <code class="language-plaintext highlighter-rouge">RoBERTa</code>)ì„ ëœ»í•œë‹¤. ì´ë²ˆ í¬ìŠ¤íŒ…ì—ì„œëŠ” <code class="language-plaintext highlighter-rouge">BERT</code>ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì„¤ëª…í•˜ê² ë‹¤.</p>

<h4 id="-types-of-embedding"><strong><code class="language-plaintext highlighter-rouge">ğŸ“š Types of Embedding</code></strong></h4>
<p><strong>ë¨¼ì € í˜„ì¡´í•˜ëŠ” ëª¨ë“  ì„ë² ë”©(<code class="language-plaintext highlighter-rouge">ë²¡í„°ì— ë¬¸ë§¥ì„ ì£¼ì…í•˜ëŠ”</code>)ê¸°ë²•ë“¤ì„ ì •ë¦¬í•´ë³´ì. ë‹¤ìŒê³¼ ê°™ì´ 3ê°€ì§€ ì¹´í…Œê³ ë¦¬ë¡œ ë¶„ë¥˜ê°€ ê°€ëŠ¥í•˜ë‹¤.</strong></p>

<ul>
  <li><strong>1) ë‹¨ì–´ì˜ ë¹ˆë„ìˆ˜:  ì‹œí€€ìŠ¤ì—ì„œ ì‚¬ìš©ëœ í† í°ë“¤ì˜ ë¹ˆë„ìˆ˜ë¥¼ ì¸¡ì •(<code class="language-plaintext highlighter-rouge">Bag of words</code>)</strong></li>
  <li><strong>2) ë‹¨ì–´ì˜ ë°œìƒ ìˆœì„œ: <code class="language-plaintext highlighter-rouge">corpus</code> ë‚´ë¶€ì˜ íŠ¹ì • <code class="language-plaintext highlighter-rouge">sequence</code> ë“±ì¥ ë¹ˆë„ë¥¼ ì¹´ìš´íŠ¸(<code class="language-plaintext highlighter-rouge">N-Gram</code>), ì£¼ì–´ì§„ ì‹œí€€ìŠ¤ë¥¼ ê°€ì§€ê³  ë‹¤ìŒ ì‹œì ì— ë“±ì¥í•  í† í°ì„ ë§ì¶”ëŠ” ë°©ì‹(<code class="language-plaintext highlighter-rouge">LM</code>)</strong></li>
  <li><strong>3) ë‹¨ì–´ ë¶„í¬ ê°€ì„¤ :  ë‹¨ì–´ì˜ ì˜ë¯¸ëŠ” ì£¼ë³€ ë¬¸ë§¥ì— ì˜í•´ ê²°ì •ëœë‹¤ëŠ” ê°€ì •, ì–´ë–¤ ë‹¨ì–´ ìŒì´ ìì£¼ ê°™ì´ ë“±ì¥í•˜ëŠ”ì§€ ì¹´ìš´íŠ¸í•´ <code class="language-plaintext highlighter-rouge">PMI</code>ë¥¼ ì¸¡ì •í•˜ëŠ” ë°©ì‹(<code class="language-plaintext highlighter-rouge">Word2Vec</code>)</strong></li>
</ul>

<p>ê¸°ì¡´ì˜ ëª¨ë¸ë§ ë°©ì‹ì€ ì–´ë””ì— í¬í•¨ë ê¹Œ?? <code class="language-plaintext highlighter-rouge">BERT</code> ëŠ” ëŒ€ë¶„ë¥˜ ìƒ ì‹ ê²½ë§ì— í¬í•¨ë˜ê³ , <code class="language-plaintext highlighter-rouge">Language Modeling</code>ì„ í†µí•´ ì‹œí€€ìŠ¤ë¥¼ í•™ìŠµí•œë‹¤ëŠ” ì  ê·¸ë¦¬ê³  <code class="language-plaintext highlighter-rouge">Self-Attention</code>ê³¼ <code class="language-plaintext highlighter-rouge">Absolute Position Embedding</code> ì„ ì‚¬ìš©í•œë‹¤ëŠ” ì ì—ì„œ 2ë²ˆ, <code class="language-plaintext highlighter-rouge">ë‹¨ì–´ì˜ ë°œìƒ ìˆœì„œ</code> ì— í¬í•¨ëœë‹¤ê³  ë³¼ ìˆ˜ ìˆë‹¤. <code class="language-plaintext highlighter-rouge">Absolute Position Embedding</code> ê³¼ <code class="language-plaintext highlighter-rouge">Self-Attention</code>ì˜ ì‚¬ìš©ì´ í“¨ì–´í•œ <code class="language-plaintext highlighter-rouge">BERT</code>ê°€ ë¶„ë¥˜ìƒ 2ë²ˆì´ë¼ëŠ” ì‚¬ì‹¤ì„ ë’·ë°›ì¹¨í•˜ëŠ” ì¦ê±°ë¼ëŠ” ì ì—ì„œ ì˜ì•„í•  ìˆ˜ ìˆë‹¤. í•˜ì§€ë§Œ ì˜ ìƒê°í•´ë³´ì.</p>

<p><code class="language-plaintext highlighter-rouge">Absolute Position Embedding</code>ì€ ì£¼ì–´ì§„ ì‹œí€€ìŠ¤ì˜ ê¸¸ì´ë¥¼ ì¸¡ì •í•œ ë’¤, ë‚˜ì—´ëœ ìˆœì„œ ê·¸ëŒ€ë¡œ <code class="language-plaintext highlighter-rouge">forward</code>í•˜ê²Œ <code class="language-plaintext highlighter-rouge">0</code>ë¶€í„° <code class="language-plaintext highlighter-rouge">ê¸¸ì´-1</code>ì˜ ë²ˆí˜¸ë¥¼ ê°œë³„ í† í°ì— í• ë‹¹í•œë‹¤. ë‹¤ì‹œ ë§í•´, ë‹¨ì–´ê°€ ì‹œí€€ìŠ¤ì—ì„œ ë°œìƒí•œ ìˆœì„œë¥¼ ìˆ˜í•™ì ìœ¼ë¡œ í‘œí˜„í•´ ëª¨ë¸ì— ì£¼ì…í•œë‹¤ëŠ” ì˜ë¯¸ê°€ ëœë‹¤. <code class="language-plaintext highlighter-rouge">Self-Attention</code>ì€ <code class="language-plaintext highlighter-rouge">Absolute Position Embedding</code> ì •ë³´ê°€ ì£¼ì…ëœ ì‹œí€€ìŠ¤ ì „ì²´ë¥¼ í•œ ë²ˆì— ë³‘ë ¬ ì²˜ë¦¬í•œë‹¤. ë”°ë¼ì„œ ì¶©ë¶„íˆ <code class="language-plaintext highlighter-rouge">BERT</code> ê°™ì€ <code class="language-plaintext highlighter-rouge">Self-Attention</code>, <code class="language-plaintext highlighter-rouge">Absolute Position Embedding</code> ê¸°ë°˜ ëª¨ë¸ì„ 2ë²ˆì— ë¶„ë¥˜í•  ìˆ˜ ìˆê² ë‹¤.</p>

<p>í•œí¸, í˜¹ìëŠ” <code class="language-plaintext highlighter-rouge">"BERTëŠ” MLM ì„ ì‚¬ìš©í•˜ëŠ”ë° Language Modelingì„ í•œë‹¤ê³  í•˜ëŠ”ê²Œ ë§ë‚˜ìš”"</code>ë¼ê³  ë§í•  ìˆ˜ ìˆë‹¤. í•˜ì§€ë§Œ <code class="language-plaintext highlighter-rouge">MLM</code> ì—­ì‹œ ëŒ€ë¶„ë¥˜ ìƒ <code class="language-plaintext highlighter-rouge">Language Modeling</code> ê¸°ë²•ì— ì†í•œë‹¤. <strong>ë‹¤ë§Œ, <code class="language-plaintext highlighter-rouge">Bi-Directional</code>í•˜ê²Œ ë¬¸ë§¥ì„ íŒŒì•…í•˜ê³  <code class="language-plaintext highlighter-rouge">LM</code>ì„ í•˜ë‹ˆê¹Œ ì •ë§ ì—„ë°€íˆ ë”°ì§€ë©´ 3ë²ˆì˜ ì†ì„±ë„ ì¡°ê¸ˆì€ ìˆë‹¤ê³  ë³´ëŠ”ê²Œ ë¬´ë¦¬ëŠ” ì•„ë‹ˆë¼ ìƒê°í•œë‹¤.</strong> <code class="language-plaintext highlighter-rouge">MLM</code> ì‚¬ìš©ìœ¼ë¡œ ë” ë§ì€ ì •ë³´ë¥¼ í¬ì°©í•´ ì„ë² ë”©ì„ ë§Œë“¤ê¸° ë•Œë¬¸ì— ì´ˆê¸° <code class="language-plaintext highlighter-rouge">BERT</code>ê°€ <code class="language-plaintext highlighter-rouge">GPT</code>ë³´ë‹¤ <code class="language-plaintext highlighter-rouge">NLU</code>ì—ì„œ ìƒëŒ€ì ìœ¼ë¡œ ê°•ì ì„ ê°€ì¡Œë˜ ê²ƒ ì•„ë‹ê¹Œ ì‹¶ë‹¤.</p>

<h4 id="-relative-position-embedding"><strong><code class="language-plaintext highlighter-rouge">ğŸ”¢ Relative Position Embedding</code></strong></h4>
<p>ì´ì œ <code class="language-plaintext highlighter-rouge">Relative Position Embedding</code>ì´ ë¬´ì—‡ì´ê³ , ë„ëŒ€ì²´ ì–´ë–¤ ë¬¸ë§¥ ì •ë³´ë¥¼ í¬ì°©í•œë‹¤ëŠ” ê²ƒì¸ì§€ ì•Œì•„ë³´ì. <code class="language-plaintext highlighter-rouge">Relative Position Embedding</code> ì´ë€, ì‹œí€€ìŠ¤ ë‚´ë¶€ í† í° ì‚¬ì´ì˜ ìœ„ì¹˜ ê´€ê³„ í‘œí˜„ì„ í†µí•´ í† í° ì‚¬ì´ì˜ <code class="language-plaintext highlighter-rouge">relation</code>ì„ <code class="language-plaintext highlighter-rouge">pairwise</code>í•˜ê²Œ í•™ìŠµí•˜ëŠ” ìœ„ì¹˜ ì„ë² ë”© ê¸°ë²•ì„ ë§í•œë‹¤. ì¼ë°˜ì ìœ¼ë¡œ ìƒëŒ€ ìœ„ì¹˜ ê´€ê³„ëŠ” ì„œë¡œ ë‹¤ë¥¸ ë‘ í† í°ì˜ ì‹œí€€ìŠ¤ ì¸ë±ìŠ¤ ê°’ì˜ ì°¨ë¥¼ ì´ìš©í•´ ë‚˜íƒ€ë‚¸ë‹¤. í¬ì°©í•˜ëŠ” ë¬¸ë§¥ ì •ë³´ëŠ” ì˜ˆì‹œì™€ í•¨ê¹¨ ì„¤ëª…í•˜ê² ë‹¤. ë”¥ëŸ¬ë‹ì´ë¼ëŠ” ë‹¨ì–´ëŠ” ì˜ì–´ë¡œ <code class="language-plaintext highlighter-rouge">Deep Learning</code> ì´ë‹¤. ë‘ ë‹¨ì–´ë¥¼ í•©ì³ë†“ê³  ë³´ë©´ <code class="language-plaintext highlighter-rouge">ì‹ ê²½ë§ì„ ì‚¬ìš©í•˜ëŠ” ë¨¸ì‹ ëŸ¬ë‹ ê¸°ë²•ì˜ í•œ ì¢…ë¥˜</code>ë¼ëŠ” ì˜ë¯¸ë¥¼ ê°–ê² ì§€ë§Œ, ë”°ë¡œ ë”°ë¡œ ë³´ë©´ <code class="language-plaintext highlighter-rouge">ê¹Šì€</code>, <code class="language-plaintext highlighter-rouge">ë°°ì›€</code>ì´ë¼ëŠ” ê°œë³„ì ì¸ ì˜ë¯¸ë¡œ ë‚˜ë‰œë‹¤.</p>

<ul>
  <li><strong><code class="language-plaintext highlighter-rouge">1) The Deep Learning is the Best Technique in Computer Science</code></strong></li>
  <li><strong><code class="language-plaintext highlighter-rouge">2) Iâ€™m learning how to swim in the deep ocean</code></strong></li>
</ul>

<p><code class="language-plaintext highlighter-rouge">Deep</code>ê³¼ <code class="language-plaintext highlighter-rouge">Learning</code>ì˜ ìƒëŒ€ì ì¸ ê±°ë¦¬ì— ì£¼ëª©í•˜ë©´ì„œ ë‘ ë¬¸ì¥ì„ í•´ì„í•´ë³´ì. ì²« ë²ˆì§¸ ë¬¸ì¥ì—ì„œ ë‘ ë‹¨ì–´ëŠ” ì´ì›ƒí•˜ê²Œ ìœ„ì¹˜í•´ <code class="language-plaintext highlighter-rouge">ì‹ ê²½ë§ì„ ì‚¬ìš©í•˜ëŠ” ë¨¸ì‹ ëŸ¬ë‹ ê¸°ë²•ì˜ í•œ ì¢…ë¥˜</code> ë¼ëŠ” ì˜ë¯¸ë¥¼ ë§Œë“¤ì–´ë‚´ê³  ìˆë‹¤. í•œí¸ ë‘ ë²ˆì§¸ ë¬¸ì¥ì—ì„œ ë‘ ë‹¨ì–´ëŠ” ë„ì–´ì“°ê¸° ê¸°ì¤€ 5ê°œì˜ í† í°ë§Œí¼ ë–¨ì–´ì ¸ ìœ„ì¹˜í•´ ê°ê° <code class="language-plaintext highlighter-rouge">ë°°ì›€</code>, <code class="language-plaintext highlighter-rouge">ê¹Šì€</code> ì´ë¼ëŠ” ì˜ë¯¸ë¥¼ ë§Œë“¤ì–´ ë‚´ê³  ìˆë‹¤. ì´ì²˜ëŸ¼ ê°œë³„ í† í° ì‚¬ì´ì˜ ìœ„ì¹˜ ê´€ê³„ì— ë”°ë¼ì„œ íŒŒìƒë˜ëŠ” ë¬¸ë§¥ì  ì •ë³´ë¥¼ í¬ì°©í•˜ë ¤ëŠ” ì˜ë„ë¡œ ì„¤ê³„ëœ ê¸°ë²•ì´ ë°”ë¡œ <code class="language-plaintext highlighter-rouge">Relative Position Embedding</code> ì´ë‹¤.</p>

<p><code class="language-plaintext highlighter-rouge">pairwise</code> í•˜ê²Œ <code class="language-plaintext highlighter-rouge">relation</code> ì„ í¬ì°©í•œë‹¤ëŠ” ì ìœ¼ë¡œ ë³´ì•„ <code class="language-plaintext highlighter-rouge">skip-gram</code>ì˜ <code class="language-plaintext highlighter-rouge">negative sampling</code>ê³¼ ë§¤ìš° ìœ ì‚¬í•œ ëŠë‚Œì˜ ì •ë³´ë¥¼ í¬ì°©í•  ê²ƒì´ë¼ê³  ì˜ˆìƒë˜ë©° ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜ìƒ <strong>3ë²ˆ, <code class="language-plaintext highlighter-rouge">ë‹¨ì–´ ë¶„í¬ ê°€ì„¤</code></strong>ì— í¬í•¨ì‹œí‚¬ ìˆ˜ ìˆì„ ê²ƒ ê°™ë‹¤. (í•„ìì˜ ê°œì¸ì ì¸ ì˜ê²¬ì´ë‹ˆ ì´ ë¶€ë¶„ì— ëŒ€í•œ ë‹¤ë¥¸ ì˜ê²¬ì´ ìˆë‹¤ë©´ ê¼­ ëŒ“ê¸€ì— ì ì–´ì£¼ì‹œë©´ ê°ì‚¬í•˜ê² ìŠµë‹ˆë‹¹ğŸ¥°).</p>

<p>ìœ„ ì˜ˆì‹œë§Œìœ¼ë¡œëŠ” ìƒëŒ€ ìœ„ì¹˜ ì„ë² ë”© ê°œë…ì´ ì™€ë‹¿ì§€ ì•Šì„ ìˆ˜ ìˆë‹¤. ê·¸ë ‡ë‹¤ë©´ ì˜†ì— ë§í¬ë¥¼ ë¨¼ì € ì½ê³  ì˜¤ì. (<a href="https://qcqced123.github.io/nlp/deberta#-word-context-vs-relative-position-vs-absolute-position">ë§í¬1</a>)</p>

<p><code class="language-plaintext highlighter-rouge">Relative Position Embedding</code> ì„ ì‹¤ì œ ì–´ë–»ê²Œ ì½”ë“œë¡œ êµ¬í˜„í•˜ëŠ”ì§€, ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ìœ„ì¹˜ ê´€ê³„ë¥¼ ì–´ë–»ê²Œ ì •ì˜í–ˆëŠ”ì§€ <code class="language-plaintext highlighter-rouge">Absolute Position Embedding</code>ì™€ ë¹„êµë¥¼ í†µí•´ ì•Œì•„ë³´ì. ë‹¤ìŒê³¼ ê°™ì€ ë‘ ê°œì˜ ë¬¸ì¥ì´ ìˆì„ ë•Œ, ê°œë³„ ìœ„ì¹˜ ì„ë² ë”© ë°©ì‹ì´ ë¬¸ì¥ì˜ ìœ„ì¹˜ ì •ë³´ë¥¼ ì¸ì½”ë”©í•˜ëŠ” ê³¼ì •ì„ íŒŒì´ì¬ ì½”ë“œë¡œ ì‘ì„±í•´ë´¤ë‹¤. í•¨ê»˜ ì‚´í´ë³´ì.</p>

<ul>
  <li><strong><code class="language-plaintext highlighter-rouge">A) I love studying deep learning so much</code></strong></li>
  <li><strong><code class="language-plaintext highlighter-rouge">B) I love deep cheeze burguer so much</code></strong></li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Absolute Position Embedding
</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">max_length</span> <span class="o">=</span> <span class="mi">7</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">position_embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Embedding</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">512</span><span class="p">)</span> <span class="c1"># [max_seq, dim_model]
</span><span class="o">&gt;&gt;&gt;</span> <span class="n">pos_x</span> <span class="o">=</span> <span class="n">position_embedding</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="n">max_length</span><span class="p">))</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">pos_x</span><span class="p">,</span> <span class="n">pos_x</span><span class="p">.</span><span class="n">shape</span>
<span class="p">(</span><span class="n">tensor</span><span class="p">([[</span> <span class="mf">0.4027</span><span class="p">,</span>  <span class="mf">0.9331</span><span class="p">,</span>  <span class="mf">1.0556</span><span class="p">,</span>  <span class="p">...,</span> <span class="o">-</span><span class="mf">1.7370</span><span class="p">,</span>  <span class="mf">0.7799</span><span class="p">,</span>  <span class="mf">1.9851</span><span class="p">],</span>  <span class="c1"># A,Bì˜ 0ë²ˆ í† í°: I
</span>         <span class="p">[</span><span class="o">-</span><span class="mf">0.2206</span><span class="p">,</span>  <span class="mf">2.1024</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6055</span><span class="p">,</span>  <span class="p">...,</span> <span class="o">-</span><span class="mf">1.1342</span><span class="p">,</span>  <span class="mf">1.3956</span><span class="p">,</span>  <span class="mf">0.9017</span><span class="p">],</span>  <span class="c1"># A,Bì˜ 1ë²ˆ í† í°: love
</span>         <span class="p">[</span><span class="o">-</span><span class="mf">0.9560</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0426</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.8587</span><span class="p">,</span>  <span class="p">...,</span> <span class="o">-</span><span class="mf">0.9406</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1467</span><span class="p">,</span>  <span class="mf">0.1762</span><span class="p">],</span>  <span class="c1"># A,Bì˜ 2ë²ˆ í† í°: studying, deep
</span>         <span class="p">...,</span>                                                           <span class="c1"># A,Bì˜ 3ë²ˆ í† í°: deep, cheeze
</span>         <span class="p">[</span> <span class="mf">0.5999</span><span class="p">,</span>  <span class="mf">0.5235</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3445</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mf">1.9020</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.5003</span><span class="p">,</span>  <span class="mf">0.7535</span><span class="p">],</span>  <span class="c1"># A,Bì˜ 4ë²ˆ í† í°: learning, burger
</span>         <span class="p">[</span> <span class="mf">0.0688</span><span class="p">,</span>  <span class="mf">0.5867</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0340</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mf">0.8547</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.9196</span><span class="p">,</span>  <span class="mf">1.1193</span><span class="p">],</span>  <span class="c1"># A,Bì˜ 5ë²ˆ í† í°: so
</span>         <span class="p">[</span><span class="o">-</span><span class="mf">0.0751</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4133</span><span class="p">,</span>  <span class="mf">0.0256</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mf">0.0788</span><span class="p">,</span>  <span class="mf">1.4665</span><span class="p">,</span>  <span class="mf">0.8196</span><span class="p">]],</span> <span class="c1"># A,Bì˜ 6ë²ˆ í† í°: much
</span>        <span class="n">grad_fn</span><span class="o">=&lt;</span><span class="n">EmbeddingBackward0</span><span class="o">&gt;</span><span class="p">),</span>
 <span class="n">torch</span><span class="p">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">7</span><span class="p">,</span> <span class="mi">512</span><span class="p">]))</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">Absolute Position Embedding</code>ì€ ì£¼ìœ„ ë¬¸ë§¥ì— ìƒê´€ì—†ì´ ê°™ì€ ìœ„ì¹˜ì˜ í† í°ì´ë¼ë©´ ê°™ì€ í¬ì§€ì…˜ ê°’ìœ¼ë¡œ ì¸ì½”ë”©í•˜ê¸° ë•Œë¬¸ì— <code class="language-plaintext highlighter-rouge">512</code>ê°œì˜ ì›ì†Œë¡œ êµ¬ì„±ëœ í–‰ë²¡í„°ë“¤ì˜ ì¸ë±ìŠ¤ë¥¼ ì‹¤ì œ ë¬¸ì¥ì—ì„œ í† í°ì˜ ë“±ì¥ ìˆœì„œì— ë§µí•‘í•´ì£¼ëŠ” ë°©ì‹ìœ¼ë¡œ ìœ„ì¹˜ ì •ë³´ë¥¼ í‘œí˜„í•œë‹¤. ì˜ˆë¥¼ ë“¤ë©´, ë¬¸ì¥ì—ì„œ ê°€ì¥ ë¨¼ì € ë“±ì¥í•˜ëŠ” <code class="language-plaintext highlighter-rouge">0</code>ë²ˆ í† í°ì— <code class="language-plaintext highlighter-rouge">0</code>ë²ˆì§¸ <code class="language-plaintext highlighter-rouge">í–‰ë²¡í„°</code>ë¥¼ ë°°ì •í•˜ê³  ê°€ì¥ ë§ˆì§€ë§‰ì— ë“±ì¥í•˜ëŠ” <code class="language-plaintext highlighter-rouge">N-1</code> ë²ˆì§¸ í† í°ì€ <code class="language-plaintext highlighter-rouge">N-1</code>ë²ˆì§¸ <code class="language-plaintext highlighter-rouge">í–‰ë²¡í„°</code>ë¥¼ ìœ„ì¹˜ ì •ë³´ê°’ìœ¼ë¡œ ê°–ëŠ” ë°©ì‹ì´ë‹¤. ì „ì²´ ì‹œí€€ìŠ¤ ê´€ì ì—ì„œ ê°œë³„ í† í°ì— ë²ˆí˜¸ë¥¼ ë¶€ì—¬í•˜ê¸° ë•Œë¬¸ì— <code class="language-plaintext highlighter-rouge">syntactical</code>í•œ ì •ë³´ë¥¼ ëª¨ë¸ë§ í•´ì£¼ê¸° ì í•©í•˜ë‹¤ëŠ” ì¥ì ì´ ìˆë‹¤.</p>

<p><code class="language-plaintext highlighter-rouge">Absolute Position Embedding</code> ì€ ì¼ë°˜ì ìœ¼ë¡œ <code class="language-plaintext highlighter-rouge">Input Embedding</code>ê³¼ í–‰ë ¬í•© ì—°ì‚°ì„ í†µí•´ <code class="language-plaintext highlighter-rouge">Word Embedding</code> ìœ¼ë¡œ ë§Œë“¤ì–´ ì¸ì½”ë”ì˜ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©í•œë‹¤.</p>

<p>ì•„ë˜ ì½”ë“œëŠ” ì €ìê°€ ë…¼ë¬¸ì—ì„œ ì œì‹œí•œ <code class="language-plaintext highlighter-rouge">DeBERTa</code>ì˜ <code class="language-plaintext highlighter-rouge">Relative Position Embedding</code> êµ¬í˜„ì„ íŒŒì´í† ì¹˜ë¡œ ì˜®ê¸´ ê²ƒì´ë‹¤. <code class="language-plaintext highlighter-rouge">Relative Position Embedding</code> ì€ ì ˆëŒ€ ìœ„ì¹˜ì— ë¹„í•´ ê½¤ë‚˜ ë³µì¡í•œ ê³¼ì •ì„ ê±°ì³ì•¼ í•˜ê¸° ë•Œë¬¸ì— ì½”ë“œ ì—­ì‹œ ê¸´ í¸ì´ë‹¤. í•˜ë‚˜ í•˜ë‚˜ ì²œì²œíˆ ì‚´í´ë³´ì.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Relative Position Embedding
</span><span class="o">&gt;&gt;&gt;</span> <span class="n">position_embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Embedding</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">max_length</span><span class="p">,</span> <span class="n">dim_model</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">x</span><span class="p">,</span> <span class="n">p_x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="n">max_length</span><span class="p">,</span> <span class="n">dim_model</span><span class="p">),</span> <span class="n">position_embedding</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">max_length</span><span class="p">))</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">fc_q</span><span class="p">,</span> <span class="n">fc_kr</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">dim_model</span><span class="p">,</span> <span class="n">dim_head</span><span class="p">),</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">dim_model</span><span class="p">,</span> <span class="n">dim_head</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">q</span><span class="p">,</span> <span class="n">kr</span> <span class="o">=</span> <span class="n">fc_q</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">fc_kr</span><span class="p">(</span><span class="n">p_x</span><span class="p">)</span> <span class="c1"># [batch, max_length, dim_head], [batch, 2*max_length, dim_head]
</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">tmp_c2p</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">kr</span><span class="p">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">))</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">tmp_c2p</span><span class="p">,</span> <span class="n">tmp_c2p</span><span class="p">.</span><span class="n">shape</span>
<span class="p">(</span><span class="n">tensor</span><span class="p">([[</span> <span class="mf">2.8118</span><span class="p">,</span>  <span class="mf">0.8449</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6240</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6516</span><span class="p">,</span>  <span class="mf">3.4009</span><span class="p">,</span>  <span class="mf">1.8296</span><span class="p">,</span>  <span class="mf">0.8304</span><span class="p">,</span>  <span class="mf">1.0164</span><span class="p">,</span>
           <span class="mf">3.5664</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.4208</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.0821</span><span class="p">,</span>  <span class="mf">1.5752</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.9469</span><span class="p">,</span> <span class="o">-</span><span class="mf">7.1767</span><span class="p">],</span>
         <span class="p">[</span><span class="o">-</span><span class="mf">2.1907</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.2801</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.0628</span><span class="p">,</span>  <span class="mf">0.4443</span><span class="p">,</span>  <span class="mf">2.2272</span><span class="p">,</span> <span class="o">-</span><span class="mf">5.6653</span><span class="p">,</span> <span class="o">-</span><span class="mf">4.6036</span><span class="p">,</span>  <span class="mf">1.4134</span><span class="p">,</span>
          <span class="o">-</span><span class="mf">1.1742</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3361</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4586</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.1827</span><span class="p">,</span>  <span class="mf">1.0878</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.5657</span><span class="p">],</span>
         <span class="p">[</span><span class="o">-</span><span class="mf">4.8952</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.5330</span><span class="p">,</span>  <span class="mf">0.0251</span><span class="p">,</span>  <span class="mf">3.5001</span><span class="p">,</span>  <span class="mf">4.1619</span><span class="p">,</span>  <span class="mf">1.7408</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5100</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.4616</span><span class="p">,</span>
          <span class="o">-</span><span class="mf">1.6101</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.8741</span><span class="p">,</span>  <span class="mf">1.1404</span><span class="p">,</span>  <span class="mf">4.9860</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.5350</span><span class="p">,</span>  <span class="mf">1.0999</span><span class="p">],</span>
         <span class="p">[</span><span class="o">-</span><span class="mf">3.3437</span><span class="p">,</span>  <span class="mf">4.2276</span><span class="p">,</span>  <span class="mf">0.4509</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.8911</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.1069</span><span class="p">,</span>  <span class="mf">0.9540</span><span class="p">,</span>  <span class="mf">1.2045</span><span class="p">,</span>  <span class="mf">2.2194</span><span class="p">,</span>
          <span class="o">-</span><span class="mf">2.6509</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.4076</span><span class="p">,</span>  <span class="mf">5.1599</span><span class="p">,</span>  <span class="mf">1.6591</span><span class="p">,</span>  <span class="mf">3.8764</span><span class="p">,</span>  <span class="mf">2.5126</span><span class="p">],</span>
         <span class="p">[</span> <span class="mf">0.8164</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.9171</span><span class="p">,</span>  <span class="mf">0.8217</span><span class="p">,</span>  <span class="mf">1.3953</span><span class="p">,</span>  <span class="mf">1.6260</span><span class="p">,</span>  <span class="mf">3.8104</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0303</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.1631</span><span class="p">,</span>
           <span class="mf">3.9008</span><span class="p">,</span>  <span class="mf">0.5856</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.6212</span><span class="p">,</span>  <span class="mf">1.7220</span><span class="p">,</span>  <span class="mf">2.7997</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.8802</span><span class="p">],</span>
         <span class="p">[</span> <span class="mf">3.4473</span><span class="p">,</span>  <span class="mf">0.9721</span><span class="p">,</span>  <span class="mf">3.9137</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.2055</span><span class="p">,</span>  <span class="mf">0.6963</span><span class="p">,</span>  <span class="mf">1.2761</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2266</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.7274</span><span class="p">,</span>
          <span class="o">-</span><span class="mf">1.4928</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.9257</span><span class="p">,</span> <span class="o">-</span><span class="mf">5.4422</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.8544</span><span class="p">,</span>  <span class="mf">1.8749</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.4923</span><span class="p">],</span>
         <span class="p">[</span> <span class="mf">2.6639</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.4392</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.8818</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.4120</span><span class="p">,</span>  <span class="mf">1.7542</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.8774</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.0795</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2156</span><span class="p">,</span>
          <span class="o">-</span><span class="mf">1.0852</span><span class="p">,</span>  <span class="mf">3.7825</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.5581</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.6989</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.6705</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2262</span><span class="p">]],</span>
        <span class="n">grad_fn</span><span class="o">=&lt;</span><span class="n">MmBackward0</span><span class="o">&gt;</span><span class="p">),</span>
 <span class="n">torch</span><span class="p">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">7</span><span class="p">,</span> <span class="mi">14</span><span class="p">]))</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">max_seq</span><span class="p">,</span> <span class="n">max_pos</span> <span class="o">=</span> <span class="mi">7</span><span class="p">,</span> <span class="n">max_seq</span> <span class="o">*</span> <span class="mi">2</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">q_index</span><span class="p">,</span> <span class="n">k_index</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="n">max_seq</span><span class="p">),</span> <span class="n">torch</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="n">max_seq</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">q_index</span><span class="p">,</span> <span class="n">k_index</span>
<span class="p">(</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">]),</span> <span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">]))</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">tmp_pos</span> <span class="o">=</span> <span class="n">q_index</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="o">-</span> <span class="n">k_index</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">rel_pos_matrix</span> <span class="o">=</span> <span class="n">tmp_pos</span> <span class="o">+</span> <span class="n">max_relative_position</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">rel_pos_matrix</span>
<span class="n">tensor</span><span class="p">([[</span> <span class="mi">7</span><span class="p">,</span>  <span class="mi">6</span><span class="p">,</span>  <span class="mi">5</span><span class="p">,</span>  <span class="mi">4</span><span class="p">,</span>  <span class="mi">3</span><span class="p">,</span>  <span class="mi">2</span><span class="p">,</span>  <span class="mi">1</span><span class="p">],</span>
        <span class="p">[</span> <span class="mi">8</span><span class="p">,</span>  <span class="mi">7</span><span class="p">,</span>  <span class="mi">6</span><span class="p">,</span>  <span class="mi">5</span><span class="p">,</span>  <span class="mi">4</span><span class="p">,</span>  <span class="mi">3</span><span class="p">,</span>  <span class="mi">2</span><span class="p">],</span>
        <span class="p">[</span> <span class="mi">9</span><span class="p">,</span>  <span class="mi">8</span><span class="p">,</span>  <span class="mi">7</span><span class="p">,</span>  <span class="mi">6</span><span class="p">,</span>  <span class="mi">5</span><span class="p">,</span>  <span class="mi">4</span><span class="p">,</span>  <span class="mi">3</span><span class="p">],</span>
        <span class="p">[</span><span class="mi">10</span><span class="p">,</span>  <span class="mi">9</span><span class="p">,</span>  <span class="mi">8</span><span class="p">,</span>  <span class="mi">7</span><span class="p">,</span>  <span class="mi">6</span><span class="p">,</span>  <span class="mi">5</span><span class="p">,</span>  <span class="mi">4</span><span class="p">],</span>
        <span class="p">[</span><span class="mi">11</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span>  <span class="mi">9</span><span class="p">,</span>  <span class="mi">8</span><span class="p">,</span>  <span class="mi">7</span><span class="p">,</span>  <span class="mi">6</span><span class="p">,</span>  <span class="mi">5</span><span class="p">],</span>
        <span class="p">[</span><span class="mi">12</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span>  <span class="mi">9</span><span class="p">,</span>  <span class="mi">8</span><span class="p">,</span>  <span class="mi">7</span><span class="p">,</span>  <span class="mi">6</span><span class="p">],</span>
        <span class="p">[</span><span class="mi">13</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span>  <span class="mi">9</span><span class="p">,</span>  <span class="mi">8</span><span class="p">,</span>  <span class="mi">7</span><span class="p">]])</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">rel_pos_matrix</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">rel_pos_matrix</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">max_pos</span> <span class="o">-</span> <span class="mi">1</span><span class="p">).</span><span class="n">repeat</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">tmp_c2p</span> <span class="o">=</span> <span class="n">tmp_c2p</span><span class="p">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">rel_pos_matrix</span><span class="p">,</span> <span class="n">rel_pos_matrix</span><span class="p">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">tmp_c2p</span><span class="p">.</span><span class="n">shape</span> 
<span class="p">(</span><span class="n">tensor</span><span class="p">([[[</span> <span class="mi">7</span><span class="p">,</span>  <span class="mi">6</span><span class="p">,</span>  <span class="mi">5</span><span class="p">,</span>  <span class="mi">4</span><span class="p">,</span>  <span class="mi">3</span><span class="p">,</span>  <span class="mi">2</span><span class="p">,</span>  <span class="mi">1</span><span class="p">],</span>
          <span class="p">[</span> <span class="mi">8</span><span class="p">,</span>  <span class="mi">7</span><span class="p">,</span>  <span class="mi">6</span><span class="p">,</span>  <span class="mi">5</span><span class="p">,</span>  <span class="mi">4</span><span class="p">,</span>  <span class="mi">3</span><span class="p">,</span>  <span class="mi">2</span><span class="p">],</span>
          <span class="p">[</span> <span class="mi">9</span><span class="p">,</span>  <span class="mi">8</span><span class="p">,</span>  <span class="mi">7</span><span class="p">,</span>  <span class="mi">6</span><span class="p">,</span>  <span class="mi">5</span><span class="p">,</span>  <span class="mi">4</span><span class="p">,</span>  <span class="mi">3</span><span class="p">],</span>
          <span class="p">[</span><span class="mi">10</span><span class="p">,</span>  <span class="mi">9</span><span class="p">,</span>  <span class="mi">8</span><span class="p">,</span>  <span class="mi">7</span><span class="p">,</span>  <span class="mi">6</span><span class="p">,</span>  <span class="mi">5</span><span class="p">,</span>  <span class="mi">4</span><span class="p">],</span>
          <span class="p">[</span><span class="mi">11</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span>  <span class="mi">9</span><span class="p">,</span>  <span class="mi">8</span><span class="p">,</span>  <span class="mi">7</span><span class="p">,</span>  <span class="mi">6</span><span class="p">,</span>  <span class="mi">5</span><span class="p">],</span>
          <span class="p">[</span><span class="mi">12</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span>  <span class="mi">9</span><span class="p">,</span>  <span class="mi">8</span><span class="p">,</span>  <span class="mi">7</span><span class="p">,</span>  <span class="mi">6</span><span class="p">],</span>
          <span class="p">[</span><span class="mi">13</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span>  <span class="mi">9</span><span class="p">,</span>  <span class="mi">8</span><span class="p">,</span>  <span class="mi">7</span><span class="p">]],</span>
<span class="n">torch</span><span class="p">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">10</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">14</span><span class="p">]),</span>
<span class="n">torch</span><span class="p">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">10</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">14</span><span class="p">]))</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">outputs</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">gather</span><span class="p">(</span><span class="n">tmp_c2p</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">rel_pos_matrix</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">.</span><span class="n">shape</span>
<span class="p">(</span><span class="n">tensor</span><span class="p">([[[</span> <span class="mf">1.0164</span><span class="p">,</span>  <span class="mf">0.8304</span><span class="p">,</span>  <span class="mf">1.8296</span><span class="p">,</span>  <span class="mf">3.4009</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6516</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6240</span><span class="p">,</span>  <span class="mf">0.8449</span><span class="p">],</span>
          <span class="p">[</span><span class="o">-</span><span class="mf">1.1742</span><span class="p">,</span>  <span class="mf">1.4134</span><span class="p">,</span> <span class="o">-</span><span class="mf">4.6036</span><span class="p">,</span> <span class="o">-</span><span class="mf">5.6653</span><span class="p">,</span>  <span class="mf">2.2272</span><span class="p">,</span>  <span class="mf">0.4443</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.0628</span><span class="p">],</span>
          <span class="p">[</span><span class="o">-</span><span class="mf">1.8741</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.6101</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.4616</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5100</span><span class="p">,</span>  <span class="mf">1.7408</span><span class="p">,</span>  <span class="mf">4.1619</span><span class="p">,</span>  <span class="mf">3.5001</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">5.1599</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.4076</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.6509</span><span class="p">,</span>  <span class="mf">2.2194</span><span class="p">,</span>  <span class="mf">1.2045</span><span class="p">,</span>  <span class="mf">0.9540</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.1069</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">1.7220</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.6212</span><span class="p">,</span>  <span class="mf">0.5856</span><span class="p">,</span>  <span class="mf">3.9008</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.1631</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0303</span><span class="p">,</span>  <span class="mf">3.8104</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">1.8749</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.8544</span><span class="p">,</span> <span class="o">-</span><span class="mf">5.4422</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.9257</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.4928</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.7274</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2266</span><span class="p">],</span>
          <span class="p">[</span><span class="o">-</span><span class="mf">1.2262</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.6705</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.6989</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.5581</span><span class="p">,</span>  <span class="mf">3.7825</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0852</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2156</span><span class="p">]],</span>
          <span class="p">.....</span>
          <span class="p">[[</span> <span class="mf">1.0164</span><span class="p">,</span>  <span class="mf">0.8304</span><span class="p">,</span>  <span class="mf">1.8296</span><span class="p">,</span>  <span class="mf">3.4009</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6516</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6240</span><span class="p">,</span>  <span class="mf">0.8449</span><span class="p">],</span>
          <span class="p">[</span><span class="o">-</span><span class="mf">1.1742</span><span class="p">,</span>  <span class="mf">1.4134</span><span class="p">,</span> <span class="o">-</span><span class="mf">4.6036</span><span class="p">,</span> <span class="o">-</span><span class="mf">5.6653</span><span class="p">,</span>  <span class="mf">2.2272</span><span class="p">,</span>  <span class="mf">0.4443</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.0628</span><span class="p">],</span>
          <span class="p">[</span><span class="o">-</span><span class="mf">1.8741</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.6101</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.4616</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5100</span><span class="p">,</span>  <span class="mf">1.7408</span><span class="p">,</span>  <span class="mf">4.1619</span><span class="p">,</span>  <span class="mf">3.5001</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">5.1599</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.4076</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.6509</span><span class="p">,</span>  <span class="mf">2.2194</span><span class="p">,</span>  <span class="mf">1.2045</span><span class="p">,</span>  <span class="mf">0.9540</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.1069</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">1.7220</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.6212</span><span class="p">,</span>  <span class="mf">0.5856</span><span class="p">,</span>  <span class="mf">3.9008</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.1631</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0303</span><span class="p">,</span>  <span class="mf">3.8104</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">1.8749</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.8544</span><span class="p">,</span> <span class="o">-</span><span class="mf">5.4422</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.9257</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.4928</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.7274</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2266</span><span class="p">],</span>
          <span class="p">[</span><span class="o">-</span><span class="mf">1.2262</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.6705</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.6989</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.5581</span><span class="p">,</span>  <span class="mf">3.7825</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0852</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2156</span><span class="p">]]],</span>
        <span class="n">grad_fn</span><span class="o">=&lt;</span><span class="n">GatherBackward0</span><span class="o">&gt;</span><span class="p">),</span>
 <span class="n">torch</span><span class="p">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">10</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">7</span><span class="p">]))</span>
</code></pre></div></div>

<p>ì¼ë‹¨ ì ˆëŒ€ ìœ„ì¹˜ì™€ ë™ì¼í•˜ê²Œ <code class="language-plaintext highlighter-rouge">nn.Embedding</code>ì„ ì‚¬ìš©í•´ ì„ë² ë”© ë£©ì—… í…Œì´ë¸”(ë ˆì´ì–´)ë¥¼ ì •ì˜í•˜ì§€ë§Œ, ì…ë ¥ ì°¨ì›ì´ ë‹¤ë¥´ë‹¤. ì ˆëŒ€ ìœ„ì¹˜ ì„ë² ë”©ì€ <code class="language-plaintext highlighter-rouge">forward</code>í•˜ê²Œ ìœ„ì¹˜ê°’ì„ ë§µí•‘í•´ì•¼ í•˜ëŠ” ë°˜ë©´ì— ìƒëŒ€ ìœ„ì¹˜ ì„ë² ë”© ë°©ì‹ì€ <code class="language-plaintext highlighter-rouge">Bi-Directional</code>í•œ ë§µí•‘ì„ í•´ì•¼ í•´ì„œ, ê¸°ì¡´ <code class="language-plaintext highlighter-rouge">max_length</code> ê°’ì˜ ë‘ ë°°ë¥¼ ì…ë ¥ ì°¨ì›(<code class="language-plaintext highlighter-rouge">max_pos</code>)ìœ¼ë¡œ ì‚¬ìš©í–ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ <code class="language-plaintext highlighter-rouge">0</code>ë²ˆ í† í°ê³¼ ë‚˜ë¨¸ì§€ í† í° ì‚¬ì´ì˜ ìœ„ì¹˜ ê´€ê³„ë¥¼ í‘œí˜„í•´ì•¼ í•˜ëŠ” ìƒí™©ì´ë‹¤. ê·¸ë ‡ë‹¤ë©´ ìš°ë¦¬ëŠ” <code class="language-plaintext highlighter-rouge">0</code>ë²ˆ í† í°ê³¼ ë‚˜ë¨¸ì§€ í† í°ê³¼ì˜ ìœ„ì¹˜ ê´€ê³„ë¥¼ <code class="language-plaintext highlighter-rouge">[0, -1, -2, -3, -4, -5, -6]</code> ìœ¼ë¡œ ì¸ì½”ë”©í•  ìˆ˜ ìˆë‹¤.</p>

<p>ë°˜ëŒ€ë¡œ ë§ˆì§€ë§‰ <code class="language-plaintext highlighter-rouge">6</code>ë²ˆ í† í°ê³¼ ë‚˜ë¨¸ì§€ í† í° ì‚¬ì´ì˜ ìœ„ì¹˜ ê´€ê³„ë¥¼ í‘œí˜„í•˜ëŠ” ê²½ìš°ë¼ë©´ ì–´ë–»ê²Œ ë ê¹Œ?? <code class="language-plaintext highlighter-rouge">[6, 5, 4, 3, 2, 1, 0]</code> ìœ¼ë¡œ ì¸ì½”ë”© ë  ê²ƒì´ë‹¤. ë‹¤ì‹œ ë§í•´, ìœ„ì¹˜ ì„ë² ë”© ì›ì†Œ ê°’ì€ <code class="language-plaintext highlighter-rouge">[-max_seq:max_seq]</code> ì‚¬ì´ì—ì„œ ì •ì˜ëœë‹¤ëŠ” ê²ƒì´ë‹¤. ê·¸ëŸ¬ë‚˜ ì›ì†Œê°’ì˜ ë²”ìœ„ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•  ìˆ˜ëŠ” ì—†ë‹¤. ì´ìœ ëŠ” íŒŒì´ì¬ì˜ ë¦¬ìŠ¤íŠ¸, í…ì„œ ê°™ì€ ë°°ì—´í˜• ìë£Œêµ¬ì¡°ëŠ” ìŒì´ ì•„ë‹Œ ì •ìˆ˜ë¥¼ ì¸ë±ìŠ¤ë¡œ í™œìš©í•´ì•¼ <code class="language-plaintext highlighter-rouge">forward</code> í•˜ê²Œ ì›ì†Œì— ì ‘ê·¼í•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì´ë‹¤. ì¼ë°˜ì ìœ¼ë¡œ ë°°ì—´ í˜•íƒœì˜ ìë£Œí˜•ì€ ëª¨ë‘ ì¸ë±ìŠ¤ <code class="language-plaintext highlighter-rouge">0</code>ë¶€í„° <code class="language-plaintext highlighter-rouge">N-1</code>ê¹Œì§€ ìˆœì°¨ì ìœ¼ë¡œ ë§µí•‘ëœë‹¤. ê·¸ë˜ì„œ ì˜ë„í•œëŒ€ë¡œ í† í°ì— ì ‘ê·¼í•˜ë ¤ë©´ ì—­ì‹œ í† í°ì˜ ì¸ë±ìŠ¤ë¥¼ <code class="language-plaintext highlighter-rouge">forward</code> í•œ í˜•íƒœë¡œ ë§Œë“¤ì–´ì¤˜ì•¼ í•œë‹¤.</p>

<p>ë”°ë¼ì„œ ê¸°ì¡´ <code class="language-plaintext highlighter-rouge">[-max_seq:max_seq]</code> ì—  <code class="language-plaintext highlighter-rouge">max_seq</code>ë¥¼ ë”í•´ì¤€ <code class="language-plaintext highlighter-rouge">[0:2*max_seq]</code> (<code class="language-plaintext highlighter-rouge">2 * max_seq</code>)ì„ ì›ì†Œ ê°’ì˜ ë²”ìœ„ë¡œ ì‚¬ìš©í•˜ê²Œ ëœë‹¤. ì—¬ê¸°ê¹Œì§€ê°€ í†µìƒì ìœ¼ë¡œ ë§í•˜ëŠ” <code class="language-plaintext highlighter-rouge">Relative Position Embedding</code> ì— í•´ë‹¹í•œë‹¤. ìœ„ ì½”ë“œìƒìœ¼ë¡œëŠ” <code class="language-plaintext highlighter-rouge">rel_pos_matrix</code> ë¥¼ ë§Œë“  ë¶€ë¶„ì— í•´ë‹¹í•œë‹¤.</p>

\[âˆ‚(i,j)=
\begin{cases}
\ 0 &amp; {(i - j â‰¤ k)} \\ 
\ 2k-1 &amp; {(i - j â‰¥ k)} \\
\ i - j + k &amp; {(others)} \\
\end{cases}\]

<p>ì´ì œë¶€í„° ì €ìê°€ ì£¼ì¥í•˜ëŠ” ìœ„ì¹˜ ê´€ê³„ í‘œí˜„ ë°©ì‹ì— ëŒ€í•´ ì•Œì•„ë³´ì. ì¼ë°˜ì ì¸ <code class="language-plaintext highlighter-rouge">Relative Position Embedding</code>ê³¼ ê±°ì˜ ìœ ì‚¬í•˜ì§€ë§Œ, <code class="language-plaintext highlighter-rouge">rel_pos_matrix</code> ë‚´ë¶€ ì›ì†Œ ê°’ì´ ìŒìˆ˜ê°€ ë˜ê±°ë‚˜ <code class="language-plaintext highlighter-rouge">max_pos</code> ì„ ì´ˆê³¼í•˜ëŠ” ê²½ìš°ë¥¼ ì²˜ë¦¬ í•´ì£¼ê¸° ìœ„í•´ í›„ì²˜ë¦¬ ê³¼ì •ì„ ë„ì…í•´ ì‚¬ìš©í–ˆë‹¤. ì˜ˆì™¸ ìƒí™©ì€ <strong><code class="language-plaintext highlighter-rouge">max_seq &gt; 1/2 * max_pos(==k)</code></strong> ì¼ ë•Œ ë°œìƒí•œë‹¤. <code class="language-plaintext highlighter-rouge">official repo</code> ì˜ ì½”ë“œë¥¼ ë³´ë©´ <code class="language-plaintext highlighter-rouge">max_seq</code>ì™€ <code class="language-plaintext highlighter-rouge">k</code>ë¥¼ ì¼ì¹˜ì‹œì¼œ ëª¨ë¸ë§ í•˜ê¸° ë•Œë¬¸ì— íŒŒì¸íŠœë‹ í•˜ëŠ” ìƒí™©ì´ë¼ë©´ ì´ê²ƒì„ ëª°ë¼ë„ ìƒê´€ì—†ê² ì§€ë§Œ, í•˜ë‚˜ í•˜ë‚˜ ëª¨ë¸ì„ ì§ì ‘ ë§Œë“œëŠ” ì…ì¥ì´ë¼ë©´ ì˜ˆì™¸ ìƒí™©ì„ ë°˜ë“œì‹œ ê¸°ì–µí•˜ì.</p>

<p>í•œí¸, ì´ëŸ¬í•œ ì¸ì½”ë”© ë°©ì‹ì€ <code class="language-plaintext highlighter-rouge">word2vec</code> ì˜ <code class="language-plaintext highlighter-rouge">window size</code> ë„ì…ê³¼ ë¹„ìŠ·í•œ ì›ë¦¬(<code class="language-plaintext highlighter-rouge">ì˜ë¯¸ëŠ” ì£¼ë³€ ë¬¸ë§¥ì— ì˜í•´ ê²°ì •</code>)ë¼ê³  ìƒê°í•˜ë©´ ë˜ëŠ”ë°, ìœˆë„ìš° ì‚¬ì´ì¦ˆ ë²”ìœ„ì—ì„œ ë²—ì–´ë‚œ í† í°ë“¤ì€ ì£¼ë³€ ë¬¸ë§¥ìœ¼ë¡œ ì¸ì • í•˜ì§€ ì•Šê² ë‹¤ëŠ”(<code class="language-plaintext highlighter-rouge">negative sample</code>) ì˜ë„ë¥¼ ê°–ê³  ìˆë‹¤. ì‹¤ì œ êµ¬í˜„ì€ í…ì„œ ë‚´ë¶€ ì›ì†Œê°’ì˜ ë²”ìœ„ë¥¼ ì‚¬ìš©ì ì§€ì • ë²”ìœ„ë¡œ ì œí•œí•  ìˆ˜ ìˆëŠ” <code class="language-plaintext highlighter-rouge">torch.clamp</code> ë¥¼ ì‚¬ìš©í•˜ë©´ <code class="language-plaintext highlighter-rouge">1</code>ì¤„ë¡œ ê¹”ë”í•˜ê²Œ ë§Œë“¤ ìˆ˜ ìˆìœ¼ë‹ˆ ì°¸ê³ í•˜ì.</p>

<p><code class="language-plaintext highlighter-rouge">torch.clamp</code> ê¹Œì§€ ì ìš©í•˜ê³  ë‚œ ìµœì¢… ê²°ê³¼ë¥¼ ì‚´í´ë³´ì. í–‰ë°±í„°, ì—´ë²¡í„° ëª¨ë‘ <code class="language-plaintext highlighter-rouge">[0:2*max_seq]</code> ì‚¬ì´ì—ì„œ ì •ì˜ë˜ê³  ìˆìœ¼ë©°, ê°œë³„ ë°©í–¥ ë²¡í„° ì›ì†Œì˜ ìµœëŒ€ê°’ê³¼ ìµœì†Œê°’ì˜ ì°¨ì´ê°€ í•­ìƒ <code class="language-plaintext highlighter-rouge">k</code> ë¡œ ìœ ì§€ ëœë‹¤. ì˜ë„ëŒ€ë¡œ ì •í™•íˆ ìœˆë„ìš° ì‚¬ì´ì¦ˆë§Œí¼ì˜ ì£¼ë³€ ë§¥ë½ì„ ë°˜ì˜í•´ ì„ë² ë”©ì„ í˜•ì„±í•˜ê³  ìˆìŒì„ ì•Œ ìˆ˜ ìˆë‹¤.</p>

<p>ì •ë¦¬í•˜ë©´, <code class="language-plaintext highlighter-rouge">Relative Position Embedding</code> ë€ ì ˆëŒ€ ìœ„ì¹˜ ë°©ì‹ì²˜ëŸ¼ ì„ë² ë”© ë£©ì—… í…Œì´ë¸”ì„ ë§Œë“¤ë˜, ì‚¬ìš©ìê°€ ì§€ì •í•œ ìœˆë„ìš° ì‚¬ì´ì¦ˆì— í•´ë‹¹í•˜ëŠ” í† í°ì˜ ì„ë² ë”© ê°’ë§Œ ì¶”ì¶œí•´ ìƒˆë¡œìš´ í–‰ë²¡í„°ë¥¼ ì—¬ëŸ¬ ê°œ ë§Œë“¤ì–´ ë‚´ëŠ” ê¸°ë²•ì´ë¼ê³  í•  ìˆ˜ ìˆë‹¤. <strong>ì´ ë•Œ í–‰ë²¡í„°ëŠ” ëŒ€ìƒ í† í°ê³¼ ê·¸ ë‚˜ë¨¸ì§€ í† í° ì‚¬ì´ì˜ ìœ„ì¹˜ ë³€í™”ì— ë”°ë¼ ë°œìƒí•˜ëŠ” íŒŒìƒì ì¸ ë§¥ë½ ì •ë³´ë¥¼ ë‹´ê³  ìˆë‹¤.</strong></p>

<h4 id="-word-context-vs-relative-position-vs-absolute-position"><strong><code class="language-plaintext highlighter-rouge">ğŸ¤” Word Context vs Relative Position vs Absolute Position</code></strong></h4>

<p align="center">
<img src="/assets/images/deberta/line_people.png" alt="ì¤„ ì„œìˆëŠ” ì‚¬ëŒë“¤" class="align-center image-caption" width="40%&quot;, height=&quot;50%" />
<strong><em><a href="https://kr.freepik.com/premium-photo/people-standing-in-line-during-airport-check-in_8754408.htm">ì¤„ ì„œìˆëŠ” ì‚¬ëŒë“¤</a></em></strong>
</p>

<p>ì§€ê¸ˆê¹Œì§€ <code class="language-plaintext highlighter-rouge">Relative Position Embedding</code>ì´ ë¬´ì—‡ì´ê³ , ë„ëŒ€ì²´ ì–´ë–¤ ë¬¸ë§¥ ì •ë³´ë¥¼ í¬ì°©í•œë‹¤ëŠ” ê²ƒì¸ì§€ ì•Œì•„ë´¤ë‹¤. í•„ìì˜ ì„¤ëª…ì´ ë§¤ë„ëŸ½ì§€ ëª»í•˜ê¸°ë„ í•˜ê³  ì˜ˆì‹œë¥¼ í…ìŠ¤íŠ¸ë¡œ ë“¤ê³  ìˆì–´ì„œ ì§ê´€ì ìœ¼ë¡œ <code class="language-plaintext highlighter-rouge">word context</code>ëŠ” ë¬´ì—‡ì¸ì§€, <code class="language-plaintext highlighter-rouge">Position</code> ì •ë³´ì™€ëŠ” ë­ê°€ ë‹¤ë¥¸ì§€, ë‘ ê°€ì§€ <code class="language-plaintext highlighter-rouge">Position</code> ì •ë³´ëŠ” ë­ê°€ ì–´ë–»ê²Œ ë‹¤ë¥¸ì§€ ì™€ë‹¿ì§€ ì•ŠëŠ” ë¶„ë“¤ì´ ë§ìœ¼ì‹¤ ê²ƒ ê°™ë‹¤. ê·¸ë˜ì„œ ìµœëŒ€í•œ ì§ê´€ì ì¸ ì˜ˆì‹œë¥¼ í†µí•´ ì„¸ê°€ì§€ ì •ë³´ì˜ ì°¨ì´ì ì„ ì„¤ëª…í•´ë³´ë ¤ í•œë‹¤. (í•„ì ë³¸ì¸ì´ í–‡ê°ˆë ¤ì„œ ì“°ëŠ” ê±´ ë¹„ë°€ì´ë‹¤)</p>

<p>ì‚¬ëŒ 5ëª…ì´ ê³µí•­ ì²´í¬ì¸ì„ ìœ„í•´ ì„œ ìˆë‹¤. ëª¨ë‘ ì™¼ìª½ì„ ë³´ê³  ìˆëŠ” ê²ƒì„ ë³´ì•„ ì™¼ìª½ì— í‚¤ê°€ ì œì¼ ì‘ì€ ì—¬ìê°€ ê°€ì¥ ì•ì¤„ì´ë¼ê³  ë³¼ ìˆ˜ ìˆê² ë‹¤. ìš°ë¦¬ëŠ” ì¤„ ì„œìˆëŠ” ìˆœì„œëŒ€ë¡œ 5ëª…ì˜ ì‚¬ëŒì—ê²Œ ë²ˆí˜¸ë¥¼ ë¶€ì—¬í•  ê²ƒì´ë‹¤. í¸ì˜ìƒ 0ë²ˆë¶€í„° ì‹œì‘í•´ 4ë²ˆê¹Œì§€ ë²ˆí˜¸ë¥¼ ì£¼ê² ë‹¤. 1ë²ˆì— í•´ë‹¹í•˜ëŠ” ì‚¬ëŒì€ ëˆ„êµ¬ì¸ê°€??  ë°”ë¡œ ì¤„ì˜ 2ë²ˆì§¸ì— ì„œìˆëŠ” ì—¬ìë‹¤. ê·¸ëŸ¼ 2ë²ˆì— í•´ë‹¹í•˜ëŠ” ì‚¬ëŒì€ ëˆ„êµ¬ì¸ê°€?? ì‚¬ì§„ ì† ì¤„ì˜ ê°€ì¥ ì¤‘ê°„ì— ìˆëŠ” ë‚¨ìê°€ 2ë²ˆì´ë‹¤. ì´ë ‡ê²Œ ê·¸ë£¹ ë‹¨ìœ„(ì „ì²´ ì¤„)ì—ì„œ ê°œê°œì¸ì— ì¼ë ¨ì˜ ë²ˆí˜¸ë¥¼ ë¶€ì—¬í•´ ìœ„ì¹˜ë¥¼ í‘œí˜„í•˜ëŠ” ë°©ë²•ì´ ë°”ë¡œ <code class="language-plaintext highlighter-rouge">Absolute Position Embedding</code>ì´ë‹¤.</p>

<p>í•œí¸, ë‹¤ì‹œ 2ë²ˆ ì‚¬ëŒì—ê²Œ ì£¼ëª©í•´ë³´ì. ìš°ë¦¬ëŠ” 2ë²ˆ ë‚¨ìë¥¼ ì „ì²´ ì¤„ì—ì„œ ê°€ìš´ë° ìœ„ì¹˜í•œ ì‚¬ëŒì´ ì•„ë‹ˆë¼, ê²€ì •ìƒ‰ ì–‘ë³µê³¼ êµ¬ë‘ë¥¼ ì‹ ê³  ì†ì— ì¥” ë¬´ì–¸ê°€ë¥¼ ì‘ì‹œí•˜ê³  ìˆëŠ” ì‚¬ëŒì´ë¼ê³  í‘œí˜„í•  ìˆ˜ë„ ìˆë‹¤. ì´ê²ƒì´ ë°”ë¡œ í† í°ì˜ ì˜ë¯¸ ì •ë³´ë¥¼ ë‹´ì€ <code class="language-plaintext highlighter-rouge">word context</code>ì— í•´ë‹¹í•œë‹¤.</p>

<p>ë§ˆì§€ë§‰ìœ¼ë¡œ <code class="language-plaintext highlighter-rouge">Relative Position Embedding</code> ë°©ì‹ìœ¼ë¡œ 2ë²ˆ ë‚¨ìë¥¼ í‘œí˜„í•´ë³´ì. ì˜¤ë¥¸ì†ìœ¼ë¡œëŠ” ì»¤í”¼ë¥¼ ë“¤ê³  ë‹¤ë¥¸ ì†ìœ¼ë¡œëŠ” ìºë¦¬ì–´ë¥¼ ì¡ê³  ìˆìœ¼ë©° ê²€ì •ìƒ‰ í•˜ì´íê³¼ ë² ì´ì§€ìƒ‰ ë°”ì§€ë¥¼ ì…ì€ <strong>1ë²ˆ ì—¬ìì˜ ë’¤ì— ìˆëŠ” ì‚¬ëŒ</strong>, íšŒìƒ‰ ì–‘ë³µê³¼ ê²€ì€ ë¿”í…Œ ì•ˆê²½ì„ ì“°ê³  í•œ ì†ì—ëŠ” ìºë¦¬ì–´ë¥¼ ì¡ê³  ìˆëŠ” <strong>4ë²ˆ ì—¬ìì˜ ì•ì— ìˆëŠ” ì‚¬ëŒ</strong>, ê²€ì •ìƒ‰ ìì¼“ê³¼ ì²­ë°”ì§€ë¥¼ ì…ê³  í•œ ì†ì—ëŠ” íšŒìƒ‰ ì½”íŠ¸ë¥¼ ë“¤ê³  ìˆëŠ” ì¤„ì˜ <strong>ë§¨ ì• ì—¬ìë¡œë¶€í„° 2ë²ˆì§¸ ë’¤ì— ì„œìˆëŠ” ì‚¬ëŒ</strong>, í„±ìˆ˜ì—¼ì´ ê¸¸ê³  ë¨¸ë¦¬ê°€ ê¸´ í¸ì´ë©° íŒŒë€ìƒ‰ ê°€ë””ê±´ì„ ì…ê³  ì´ˆë¡ìƒ‰ê³¼ ê²€ì •ìƒ‰ì´ í˜¼í•©ëœ ê°€ë°©ì„ ì™¼ìª½ìœ¼ë¡œ ë©”ê³  ìˆëŠ” <strong>ë‚¨ìë¡œë¶€í„° 2ë²ˆì§¸ ì•ì— ìˆëŠ” ì‚¬ëŒ.</strong></p>

<p>ì´ì²˜ëŸ¼ í‘œí˜„í•˜ëŠ”ê²Œ ë°”ë¡œ <code class="language-plaintext highlighter-rouge">Relative Position Embedding</code>ì— ëŒ€ì‘ëœë‹¤ê³  ë³¼ ìˆ˜ ìˆë‹¤. ì´ì œ ìœ„ ì˜ˆì‹œë¥¼ ìì—°ì–´ ì²˜ë¦¬ì— ê·¸ëŒ€ë¡œ ëŒ€ì…ì‹œì¼œë³´ë©´ ì´í•´ê°€ í•œê²° ìˆ˜ì›”í•  ê²ƒì´ë‹¤.</p>

<h4 id="-deberta-inductive-bias"><strong><code class="language-plaintext highlighter-rouge">ğŸ¤” DeBERTa Inductive Bias</code></strong></h4>
<p><strong>ê²°êµ­</strong> <code class="language-plaintext highlighter-rouge">DeBERTa</code><strong>ëŠ” ë‘ê°€ì§€ ìœ„ì¹˜ ì •ë³´ í¬ì°© ë°©ì‹ì„ ì ì ˆíˆ ì„ì–´ì„œ ëª¨ë¸ì´ ë”ìš± í’ë¶€í•œ ì„ë² ë”©ì„ ê°–ë„ë¡ í•˜ë ¤ëŠ” ì˜ë„ë¡œ ì„¤ê³„ ë˜ì—ˆë‹¤.</strong> ë˜í•œ ìš°ë¦¬ëŠ” ì´ë¯¸ ëª¨ë¸ì´ ë‹¤ì–‘í•œ ë§¥ë½ ì •ë³´ë¥¼ í¬ì°©í• ìˆ˜ë¡ <code class="language-plaintext highlighter-rouge">NLU Task</code> ì—ì„œ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ê¸°ë¡í•œë‹¤ëŠ” ì‚¬ì‹¤ì„ <code class="language-plaintext highlighter-rouge">BERT</code>ì™€ <code class="language-plaintext highlighter-rouge">GPT</code> ì‚¬ë¡€ì—ì„œ ì•Œ ìˆ˜ ìˆì—ˆë‹¤. ë”°ë¼ì„œ <code class="language-plaintext highlighter-rouge">Relative Position Embedding</code> ì„ ì¶”ê°€í•˜ì—¬ <code class="language-plaintext highlighter-rouge">ë‹¨ì–´ì˜ ë°œìƒ ìˆœì„œ</code> ë¥¼ í¬ì°©í•˜ëŠ” ëª¨ë¸ì— <code class="language-plaintext highlighter-rouge">ë‹¨ì–´ ë¶„í¬ ê°€ì„¤</code> ì ì¸ íŠ¹ì§•ì„ ë”í•´ì£¼ë ¤ëŠ” ì €ìì˜ ì•„ì´ë””ì–´ëŠ” ë§¤ìš° íƒ€ë‹¹í•˜ë‹¤ê³  ë³¼ ìˆ˜ ìˆê² ë‹¤.</p>

<p>ì´ì œ ê´€ê±´ì€ <strong><code class="language-plaintext highlighter-rouge">â€œë‘ê°€ì§€ ìœ„ì¹˜ ì •ë³´ë¥¼ ì–´ë–¤ ë°©ì‹ìœ¼ë¡œ ì¶”ì¶œí•˜ê³  ì„ì–´ì¤„ ê²ƒì¸ê°€â€</code></strong> í•˜ëŠ” ë¬¼ìŒì— ë‹µí•˜ëŠ” ê²ƒì´ë‹¤. ì €ìëŠ” ë¬¼ìŒì— ë‹µí•˜ê¸° ìœ„í•´ <code class="language-plaintext highlighter-rouge">Disentangled Self-Attention</code> ê³¼  <code class="language-plaintext highlighter-rouge">Enhanced Mask Decoder</code> ë¼ëŠ” ìƒˆë¡œìš´ ê¸°ë²• ë‘ê°€ì§€ë¥¼ ì œì‹œí•œë‹¤. ì „ìëŠ” <code class="language-plaintext highlighter-rouge">ë‹¨ì–´ ë¶„í¬ ê°€ì„¤</code> ì— í•´ë‹¹ë˜ëŠ” ë§¥ë½ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ê¸° ìœ„í•œ ê¸°ë²•ì´ê³ , í›„ìëŠ” <code class="language-plaintext highlighter-rouge">ë‹¨ì–´ ë°œìƒ ìˆœì„œ</code> ì— í¬í•¨ë˜ëŠ” ì„ë² ë”©ì„ ëª¨ë¸ì— ì£¼ì…í•˜ê¸° ìœ„í•´ ì„¤ê³„ë˜ì—ˆë‹¤. ëª¨ë¸ë§ íŒŒíŠ¸ì—ì„œëŠ” ë‘ê°€ì§€ ìƒˆë¡œìš´ ê¸°ë²•ì— ëŒ€í•´ì„œ ìì„¸íˆ ì‚´í´ë³¸ ë’¤, ëª¨ë¸ì„ ì½”ë“œë¡œ ë¹Œë“œí•˜ëŠ” ê³¼ì •ì„ ì„¤ëª…í•˜ë ¤ í•œë‹¤.<br />
ì½”ë“œëŠ” ë…¼ë¬¸ì˜ ë‚´ìš©ê³¼ microsoftì˜ ê³µì‹ git repoë¥¼ ì°¸ê³ í•´ ë§Œë“¤ì—ˆìŒì„ ë°íŒë‹¤. ë‹¤ë§Œ, ë…¼ë¬¸ì—ì„œ ëª¨ë¸ êµ¬í˜„ê³¼ ê´€ë ¨í•´ ì„¸ë¶€ì ì¸ ë‚´ìš©ì€ ìƒë‹¹ìˆ˜ ìƒëµí•˜ê³  ìˆìœ¼ë©°, repoì— ê³µê°œëœ ì½”ë“œëŠ” hard codingë˜ì–´ ê·¸ ì˜ë„ë¥¼ ì •í™•í•˜ê²Œ íŒŒì•…í•˜ëŠ”ë° ë§ì€ ì–´ë ¤ì›€ì´ ìˆì—ˆë‹¤. ê·¸ë˜ì„œ ì–´ëŠ ì •ë„ëŠ” í•„ìì˜ ì£¼ê´€ì ì¸ ìƒê°ì´ ë°˜ì˜ëœ ì½”ë“œë¼ëŠ” ì ì„ ë¯¸ë¦¬ ë°íŒë‹¤.</p>

<h3 id="modeling"><code class="language-plaintext highlighter-rouge">ğŸŒŸÂ Modeling</code></h3>

<p align="center">
<img src="/assets/images/deberta/deberta_overview.png" alt="DeBERTa Model Structure" class="align-center image-caption" width="60%&quot;, height=&quot;50%" />
<strong><em><a href="https://www.youtube.com/watch?v=gcMyKUXbY8s&amp;t=838s&amp;ab_channel=%EA%B3%A0%EB%A0%A4%EB%8C%80%ED%95%99%EA%B5%90%EC%82%B0%EC%97%85%EA%B2%BD%EC%98%81%EA%B3%B5%ED%95%99%EB%B6%80DSBA%EC%97%B0%EA%B5%AC%EC%8B%A4">DeBERTa Model Structure</a></em></strong>
</p>

<ul>
  <li><strong>1) Disentangled Self-Attention Encoder Block for <code class="language-plaintext highlighter-rouge">Relative Position Embedding</code></strong></li>
  <li><strong>2) Enhanced Mask Decoder for <code class="language-plaintext highlighter-rouge">Absolute Position Embedding</code></strong></li>
</ul>

<p><code class="language-plaintext highlighter-rouge">DeBERTa</code> ì˜ ì „ë°˜ì ì¸ êµ¬ì¡°ëŠ” ì¼ë°˜ì ì¸ <code class="language-plaintext highlighter-rouge">BERT</code>, <code class="language-plaintext highlighter-rouge">RoBERTa</code>ì™€ í¬ê²Œ ë‹¤ë¥¸ ì ì´ ì—†ë‹¤. ë‹¤ë§Œ, ëª¨ë¸ì˜ ì´ˆë°˜ë¶€ <code class="language-plaintext highlighter-rouge">Input Embedding</code> ì—ì„œ <code class="language-plaintext highlighter-rouge">Absolute Position</code> ì •ë³´ë¥¼ ì¶”ê°€í•˜ëŠ” ë¶€ë¶„ì´ í›„ë°˜ë¶€ <code class="language-plaintext highlighter-rouge">Enhanced Mask Decoder</code>ë¼ ë¶€ë¥´ëŠ” ì¸ì½”ë” ë¸”ë¡ìœ¼ë¡œ ì˜®ê²¨ê°„ ê²ƒê³¼ <code class="language-plaintext highlighter-rouge">Disentangled Self-Attention</code> ì„ ìœ„í•´ ê°œë³„ ì¸ì½”ë” ë¸”ë¡ë§ˆë‹¤ ìƒëŒ€ ìœ„ì¹˜ ì •ë³´ë¥¼ ì¶œì²˜ë¡œ í•˜ëŠ” <code class="language-plaintext highlighter-rouge">linear projection</code> ë ˆì´ì–´ê°€ ì¶”ê°€ë˜ì—ˆìŒì„ ëª…ì‹¬í•˜ì. ë˜í•œ, <code class="language-plaintext highlighter-rouge">DeBERTa</code>ì˜ <code class="language-plaintext highlighter-rouge">pre-train</code> ì€ <code class="language-plaintext highlighter-rouge">RoBERTa</code>ì²˜ëŸ¼ <code class="language-plaintext highlighter-rouge">NSP</code>ë¥¼ ì‚­ì œí•˜ê³  <code class="language-plaintext highlighter-rouge">MLM</code>ë§Œ ì‚¬ìš©í•œ ì ë„ ê¸°ì–µí•˜ì.</p>

<p align="center">
<img src="/assets/images/deberta/deberta_class_diagram.png" alt="DeBERTa Class Diagram" class="align-center image-caption" width="60%&quot;, height=&quot;50%" />
<strong><em>DeBERTa Class Diagram</em></strong>
</p>

<p>ìœ„ ìë£ŒëŠ” í•„ìê°€ êµ¬í˜„í•œ <code class="language-plaintext highlighter-rouge">DeBERTa</code>ì˜ êµ¬ì¡°ë¥¼ í‘œí˜„í•œ ê·¸ë¦¼ì´ë‹¤. ì½”ë“œ ë¦¬ë·°ì— ì°¸ê³ í•˜ì‹œë©´ ì¢‹ì„ ê²ƒ ê°™ë‹¤ ì²¨ë¶€í–ˆë‹¤. ê°€ì¥ ì¤‘ìš”í•œ <code class="language-plaintext highlighter-rouge">Disentangled-Attention</code>ê³¼ <code class="language-plaintext highlighter-rouge">EMD</code>ë¶€í„° ì‚´í´ë³¸ ë’¤, ë‚˜ë¨¸ì§€ ê°ì²´ì— ëŒ€í•´ì„œ ì‚´í´ë³´ì.</p>

<h4 id="disentangled-self-attention"><strong><code class="language-plaintext highlighter-rouge">ğŸª¢Â Disentangled Self-Attention</code></strong></h4>

\[\tilde{A_{ij}} = Q_i^câ€¢K_j^{cT} + Q_i^câ€¢K_{âˆ‚(i,j)}^{rT} + K_j^câ€¢Q_{âˆ‚(i,j)}^{rT} \\
Attention(Q_c,K_c,V_c,Q_r,K_r) = softmax(\frac{\tilde{A}}{\sqrt{3d_h}})*V_c\]

<p><code class="language-plaintext highlighter-rouge">Disentangled Self-Attention</code>ì€ ì €ìê°€ í“¨ì–´í•œ <code class="language-plaintext highlighter-rouge">Input Embedding</code> ì •ë³´ì™€ <code class="language-plaintext highlighter-rouge">Relative Position</code> ì •ë³´ë¥¼ í†µí•©ì‹œí‚¤ê¸° ìœ„í•´ ê³ ì•ˆí•œ ë³€í˜• <code class="language-plaintext highlighter-rouge">Self-Attention</code> ê¸°ë²•ì´ë‹¤. ê¸°ì¡´ì˜ <code class="language-plaintext highlighter-rouge">Self-Attention</code>ê³¼ ë‹¤ë¥´ê²Œ <code class="language-plaintext highlighter-rouge">Position Embedding</code>ì„ <code class="language-plaintext highlighter-rouge">Input Embedding</code>ì™€ ë”í•˜ì§€ ì•Šê³  ë”°ë¡œ ì‚¬ìš©í•œë‹¤. <strong>ì¦‰, ê°™ì€</strong> $d_h$ <strong>ê³µê°„ì— <code class="language-plaintext highlighter-rouge">Input Embedding</code>ê³¼ <code class="language-plaintext highlighter-rouge">Relative Position</code>ì´ë¼ëŠ” ì„œë¡œ ë‹¤ë¥¸ ë‘ ë²¡í„°ë¥¼ ë§µí•‘í•˜ê³  ê·¸ ê´€ê³„ì„±ì„ íŒŒì•…í•´ë³´ê² ë‹¤ëŠ” ëœ»ì´ë‹¤.</strong></p>

<p><code class="language-plaintext highlighter-rouge">Input</code>ê³¼ <code class="language-plaintext highlighter-rouge">Position</code> ì •ë³´ë¥¼ ì„œë¡œ ì£¼ì²´ì ì¸ ì…ì¥ì—ì„œ í•œ ë²ˆì”© ë‚´ì í•œë‹¤ê³  í•´ì„œ <code class="language-plaintext highlighter-rouge">Disentangled</code>ë¼ëŠ” ì´ë¦„ì´ ë¶™ê²Œ ë˜ì—ˆë‹¤. <code class="language-plaintext highlighter-rouge">Transformer-XL</code>, <code class="language-plaintext highlighter-rouge">XLNet</code>ì— ì œì‹œëœ <code class="language-plaintext highlighter-rouge">Cross-Attention</code>ê³¼ ë§¤ìš° ìœ ì‚¬í•œ ê°œë…ì´ë‹¤. ì²«ë²ˆì§¸ ìˆ˜ì‹ì—ì„œ ê°€ì¥ ë§ˆì§€ë§‰ í•­ì„ ì œì™¸í•˜ë©´ <code class="language-plaintext highlighter-rouge">Cross-Attention</code>ê³¼ í¬ì°©í•˜ëŠ” ì •ë³´ê°€ ë™ì¼í•˜ë‹¤ê³  ì €ì ì—­ì‹œ ë°íˆê³  ìˆìœ¼ë‹ˆ ì°¸ê³ í•˜ì.</p>

<p><code class="language-plaintext highlighter-rouge">Disentangled Self-Attention</code> ì€ ì´ 5ê°€ì§€ <code class="language-plaintext highlighter-rouge">linear projection matrix</code>ë¥¼ ì‚¬ìš©í•œë‹¤. <code class="language-plaintext highlighter-rouge">Input Embedding</code> ì„ ì¶œì²˜ë¡œ í•˜ëŠ” $Q^c, k^c, V^c$, ê·¸ë¦¬ê³  <code class="language-plaintext highlighter-rouge">Position Embedding</code>ì„ ì¶œì²˜ë¡œ í•˜ëŠ” $Q^r, K^r$ì´ë‹¤. ì²¨ì $c,r$ì€ ê°ê° <code class="language-plaintext highlighter-rouge">content</code>, <code class="language-plaintext highlighter-rouge">relative</code> ì˜ ì•½ìë¡œ í–‰ë ¬ì˜ ì¶œì²˜ë¥¼ ëœ»í•œë‹¤. í•œí¸ í–‰ë ¬ ì•„ë˜ ì²¨ìì— ì¨ìˆëŠ” $i,j$ëŠ” ê°ê° í˜„ì¬ ì–´í…ì…˜ ëŒ€ìƒ í† í°ì˜ ì¸ë±ìŠ¤ì™€ ê·¸ ë‚˜ë¨¸ì§€ í† í°ì˜ ì¸ë±ìŠ¤ë¥¼ ê°€ë¦¬í‚¨ë‹¤. ê·¸ë˜ì„œ $\tilde{A_{ij}}$ëŠ” <code class="language-plaintext highlighter-rouge">[NxN]</code> í¬ê¸° í–‰ë ¬(<code class="language-plaintext highlighter-rouge">ê¸°ì¡´ ì–´í…ì…˜ì—ì„œ ì¿¼ë¦¬ì™€ í‚¤ì˜ ë‚´ì  ê²°ê³¼ì— í•´ë‹¹</code>)ì˜ $i$ë²ˆì§¸ í–‰ë°±í„°ì˜ $j$ë²ˆì§¸ ì›ì†Œì˜ ê°’ì„ ì˜ë¯¸í•œë‹¤. <code class="language-plaintext highlighter-rouge">Input Embedding</code> ì •ë³´ì™€ <code class="language-plaintext highlighter-rouge">Relative Position</code> ì •ë³´ë¥¼ ë”°ë¡œ ë”°ë¡œ ê´€ë¦¬í•˜ê¸° ë•Œë¬¸ì— ìš°ë¦¬ê°€ ê¸°ì¡´ì— ì•Œê³  ìˆë˜ <code class="language-plaintext highlighter-rouge">Self-Attention</code>ê³¼ëŠ” ì‚¬ë­‡ ë‹¤ë¥¸ ìˆ˜ì‹ì´ë‹¤. ì´ì œë¶€í„° ìˆ˜ì‹ì˜ í•­ í•˜ë‚˜í•˜ë‚˜ì˜ ì˜ë¯¸ë¥¼ êµ¬ì²´ì ì¸ ì˜ˆì‹œì™€ í•¨ê¼ íŒŒí—¤ì³ë³´ì.</p>

<p><strong><code class="language-plaintext highlighter-rouge">â˜ºï¸Â c2c matrix</code></strong><br />
<code class="language-plaintext highlighter-rouge">content2content</code>ì˜ ì•½ìë¡œ ì²«ë²ˆì§¸ ìˆ˜ì‹ ìš°ë³€ì˜ ì²«ë²ˆì§¸ í•­ì„ ê°€ë¦¬í‚¤ëŠ” ë§ì´ë‹¤. ì´ë¦„ì˜ ì˜ë¯¸ëŠ” ë‚´ì ì— ì‚¬ìš©í•˜ëŠ” ë‘ í–‰ë ¬ì˜ ì¶œì²˜ê°€ ëª¨ë‘ <code class="language-plaintext highlighter-rouge">Input Embedding</code> ì´ë¼ëŠ” ì‚¬ì‹¤ì„ ë‚´í¬í•˜ê³  ìˆë‹¤. ê¸°ì¡´ì— ì•Œê³  ìˆë˜ <code class="language-plaintext highlighter-rouge">Self-Attention</code> ì˜ ë‘ë²ˆì§¸ ë‹¨ê³„ì¸ $Qâ€¢K^T$ì™€ ê±°ì˜ ë™ì¼í•œ ì˜ë¯¸ë¥¼ ë‹´ê³  ìˆëŠ” í•­ì´ë¼ê³  ìƒê°í•˜ë©´ ë  ê²ƒ ê°™ë‹¤. ì™„ì „íˆ ê°™ë‹¤ê³  í•  ìˆ˜ ì—†ëŠ” ì´ìœ ëŠ” <code class="language-plaintext highlighter-rouge">Absolute Position</code> ì •ë³´ê°€ ë¹ ì§„ì±„ë¡œ ë‚´ì í–ˆê¸° ë•Œë¬¸ì´ë‹¤.</p>

<p>ë”°ë¼ì„œ ì—°ì‚°ì˜ ì˜ë¯¸ ì—­ì‹œ ìš°ë¦¬ê°€ ê¸°ì¡´ì— ì•Œê³  ìˆë˜ ë°”ì™€ ë™ì¼í•˜ë‹¤. í˜¹ì‹œ í–‰ë ¬ $Q,K,V$ì™€ <code class="language-plaintext highlighter-rouge">Self-Attention</code>ê°€ ë‚´í¬í•˜ëŠ” ì˜ë¯¸ì— ëŒ€í•´ ìì„¸íˆ ê¶ê¸ˆí•˜ì‹  ë¶„ì´ë¼ë©´ í•„ìê°€ ì‘ì„±í•œ <a href="https://qcqced123.github.io/nlp/transformer">Transformerë…¼ë¬¸ ë¦¬ë·°</a>ë¥¼ ë³´ê³  ì˜¤ì‹œê¸¸ ë°”ë€ë‹¤. ê·¸ë˜ë„ ì–´ì°¨í”¼ ë’¤ì— ë‚¨ì€ ë‘ê°œì˜ í•­ì„ ì„¤ëª…í•˜ë ¤ë©´ ì–´ì°¨í”¼ ì˜ˆì‹œë¥¼ ë“¤ì–´ì•¼ í•˜ê¸° ë•Œë¬¸ì— <code class="language-plaintext highlighter-rouge">c2c</code>í•­ì—ì„œë¶€í„° ì‹œì‘í•´ë³´ë ¤ í•œë‹¤.</p>

<p>ë‹¹ì‹ ì€ ì˜¤ëŠ˜ ì €ë… ë°¥ìœ¼ë¡œ <strong>ì°¨ëŒë°•ì´ ëœì¥ ì°Œê°œ, ì‚¼ê²¹ì‚´ ê·¸ë¦¬ê³  í›„ì‹ìœ¼ë¡œ êµ¬ìš´ ë‹¬ê±€ì„ ë¨¹ê³  ì‹¶ë‹¤.</strong> ì§‘ì— ì¬ë£Œê°€ í•˜ë‚˜ë„ ì—†ì§€ë§Œ ë§ˆíŠ¸ì— ê°€ê¸° ê·€ì°®ìœ¼ë‹ˆ <strong>í•„ìš”í•œ ì‹ìì¬ë¥¼ ë‚¨í¸ì—ê²Œ ì‚¬ì˜¤ë¼ê³  ì‹œí‚¬ ìƒê°ì´ë‹¤.</strong> ë‹¹ì‹ ì€ ê·¸ë˜ì„œ í•„ìš”í•œ ì¬ë£Œ ë¦¬ìŠ¤íŠ¸ë¥¼ ì ê³  ìˆë‹¤. <strong>ê·¸ë ‡ë‹¤ë©´ í•„ìš”í•œ ì¬ë£Œë¥¼ ì–´ë–¤ ì‹ìœ¼ë¡œ í‘œí˜„í•´ì„œ ì ì–´ì¤˜ì•¼ ë‚¨í¸ì´ ê°€ì¥ ë¹ ë¥´ê³  ì •í™•í•˜ê²Œ í•„ìš”í•œ ëª¨ë“  ì‹ìì¬ë¥¼ ì‚¬ì˜¬ ìˆ˜ ìˆì„ê¹Œ??</strong></p>

<p>ì´ê²ƒì„ ê³ ë¯¼í•˜ëŠ”ê²Œ ë°”ë¡œ í–‰ë ¬ $Q^c$ì™€ <code class="language-plaintext highlighter-rouge">linear projector</code> ì¸ $W_{Q^c}$ì˜ ì—­í• ì´ë‹¤.ì˜ˆë¥¼ ë“¤ì–´ ê°™ì€ ì•ë‹¤ë¦¬ì‚´ì´ë¼ë„ êµ¬ì´ìš©ì´ ìˆê³  ì°Œê°œìš©ì´ ìˆë‹¤. ë‹¬ê±€ë„ êµ¬ìš´ ë‹¬ê±€ì´ ìˆê³  ë‚ ë‹¬ê±€ì´ ìˆë‹¤. ì •í™•íˆ ìš©ë„ë¥¼ ì ì–´ì£¼ëŠ”ê²Œ ë‚¨í¸ ì…ì¥ì—ì„œëŠ” ì•„ë‚´ì˜ ì˜ë„ëŒ€ë¡œ ì •í™•í•˜ê²Œ ì¥ì„ ë³´ê¸° í›¨ì”¬ í¸í•  ê²ƒì´ë‹¤.</p>

<p>í•œí¸, ë‚´ì ì€ ë³¸ë˜ íŒŒë¼ë¯¸í„°ê°€ í•„ìš”í•œ ì—°ì‚°ì€ ì•„ë‹ˆë¼ì„œ ì‹¤ì œ ì†ì‹¤í•¨ìˆ˜ ì˜¤ì°¨ ì—­ì „ì„ í†µí•´ ìµœì í™”(í•™ìŠµ)ë˜ëŠ” ëŒ€ìƒì€ ë°”ë¡œ $W_{Q^c}$ê°€ ëœë‹¤. ë‚¨í¸ì´ ì¥ì„ ë¹ ë¥´ê³  ì •í™•í•˜ê²Œ ë³´ëŠ”ë° ê³¼ì—° ë‹¹ì‹ ì´ ì ì–´ì¤€ ë¦¬ìŠ¤íŠ¸ë§Œ ì˜í–¥ì„ ë¯¸ì¹ ê¹Œ??</p>

<p>ì•„ë‹ˆë‹¤. ë‹¹ì‹ ì´ ì–´ë–¤ ìŒì‹ì„ ìœ„í•´ ì–´ë–¤ ì¬ë£Œê°€ í•„ìš”í•œì§€ ê·¸ ì˜ë„ë¥¼ ì˜ ì ì–´ì£¼ëŠ” ê²ƒë„ ì¤‘ìš”í•˜ì§€ë§Œ ì‹¤ì œ ë§ˆíŠ¸ì— ì í˜€ ìˆëŠ” ìƒí’ˆëª…ê³¼ ìƒí’ˆì„¤ëª… ì—­ì‹œ ì¤‘ìš”í•˜ë‹¤. ì¢€ ì–µì§€ìŠ¤ëŸ¬ìš´ ì˜ˆì‹œì²˜ëŸ¼ ë³´ì´ê¸´ í•˜ì§€ë§Œ ë‹¬ê±€ì˜ ê²½ìš° ìœ¡ì•ˆìœ¼ë¡œë§Œ ë³´ë©´ ì´ê²ƒì´ êµ¬ìš´ ë‹¬ê±€ì¸ì§€ ë‚ ë‹¬ê±€ì¸ì§€ êµ¬ë¶„í•  ìˆ˜ ì—†ë‹¤. ê·¸ëŸ°ë° ë§ˆíŠ¸ì— ë³„ë‹¤ë¥¸ ì„¤ëª…ì—†ì´ ìƒí’ˆëª…ìœ¼ë¡œ <code class="language-plaintext highlighter-rouge">â€œë‹¬ê±€â€</code> ì´ë¼ê³ ë§Œ ì í˜€ìˆë‹¤ ìƒê°í•´ë³´ì.</p>

<p>ì•„ë¬´ë¦¬ ë‹¹ì‹ ì´ ì¢‹ì€ í–‰ë ¬ $Q^c$ë¥¼ í‘œí˜„í•´ì¤˜ë„ ë‚¨í¸ì´ ë‚ ë‹¬ê±€ì„ ì‚¬ì˜¬ í™•ë¥ ì´ ê½¤ë‚˜ ë†’ì„ ê²ƒì´ë‹¤. ì´ë ‡ê²Œ ë§ˆíŠ¸ì— ì í˜€ìˆëŠ” ìƒí’ˆëª…ê³¼ ìƒí’ˆì„¤ëª…ì´ ë°”ë¡œ í–‰ë ¬ $K^c$ì— ëŒ€ì‘ëœë‹¤. ê·¸ë¦¬ê³  ë¬¼ê±´ì„ ì‚¬ê¸° ìœ„í•´ ë‹¹ì‹ ì´ ì ì–´ì¤€ ì‹ìì¬ ë¦¬ìŠ¤íŠ¸ì™€ ë§¤ì¥ì— ì íŒ ìƒí’ˆëª…ê³¼ ìƒí’ˆì„¤ëª…ì„ ëŒ€ì¡°í•˜ë©° ì´ê²ƒì´ ì˜ë„ì— ë§ëŠ” ìƒí’ˆì¸ì§€ ë”°ì ¸ë³´ëŠ” ì‘ì—…ì´ ë°”ë¡œ $Q_i^câ€¢K_j^{cT}$, <code class="language-plaintext highlighter-rouge">c2c matrix</code>ê°€ ëœë‹¤.</p>

<p>ë‹¤ë§Œ, ì „ì—­ ì–´í…ì…˜ì„ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì— ë‹¬ê±€ì„ ì‚¬ê¸° ìœ„í•´ ë§¤ì¥ì— ìˆëŠ” ëª¨ë“  ìƒí’ˆê³¼ ëŒ€ì¡°ë¥¼ í•œë‹¤ê³  ìƒê°í•˜ë©´ ëœë‹¤. íŠ¹íˆ ê¸°ì¡´ ì „ì—­ ì–´í…ì…˜ì˜ $Q_i^câ€¢K_j^{cT}$ì˜ ê²½ìš° ëª¨ë“  ìƒí’ˆê³¼ ëŒ€ì¡°í•˜ëŠ” ê³¼ì •ì—ì„œ ëŒ€ì¡°êµ°ì´ ë§¤ì¥ì— ì „ì‹œëœ ìœ„ì¹˜, ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜ìƒ ì–´ëŠ ì½”ë„ˆì— ì†í•˜ëŠ”ì§€ ë“±ì˜ ìœ„ì¹˜ ì •ë³´ë¥¼ í•œêº¼ë²ˆì— ê³ ë ¤í•˜ì§€ë§Œ, ìš°ë¦¬ì˜(<code class="language-plaintext highlighter-rouge">c2c matrix</code>) ê²½ìš° ì—¬ê¸°ì„œ ì´ëŸ° ìœ„ì¹˜ ì •ë³´ë¥¼ ì „í˜€ ê³ ë ¤í•˜ì§€ ì•Šê³  ë’¤ì— ë‘ ê°œì˜ í•­ì—ì„œ ë”°ë¡œ ê³ ë ¤í•œë‹¤.</p>

<p>ì •ë¦¬í•˜ë©´, <code class="language-plaintext highlighter-rouge">c2c</code>ëŠ” ë§¤ì¥ì— ì§„ì—´ëœ ì‹ìì¬ì˜ ìƒí’ˆëª… ë° ì„¤ëª…ë§Œ ê°€ì§€ê³  ë‚´ê°€ ì‚¬ì•¼ í•˜ëŠ” ì‹ì¬ë£Œì¸ì§€ ì•„ë‹Œì§€ íŒë‹¨í•˜ëŠ” ì‘ì—…ì„ ìˆ˜í•™ì ìœ¼ë¡œ ëª¨ë¸ë§ í–ˆë‹¤ê³  ë³¼ ìˆ˜ ìˆê² ë‹¤. ìì—°ì–´ ì²˜ë¦¬ ë§¥ë½ì—ì„œ ë°”ë¼ë³´ë©´, íŠ¹ì • í† í°ì˜ ì˜ë¯¸ë¥¼ ì•Œê¸° ìœ„í•´ì„œ <code class="language-plaintext highlighter-rouge">syntactical</code>í•œ ì •ë³´ì—†ì´ ìˆœìˆ˜í•˜ê²Œ ë‚˜ë¨¸ì§€ ë‹¤ë¥¸ í† í°ë“¤ì˜ ì˜ë¯¸ë¥¼ ê°€ì¤‘í•©ìœ¼ë¡œ ë°˜ì˜í•˜ëŠ” í–‰ìœ„ì— ëŒ€ì‘ëœë‹¤.</p>

<p><strong><code class="language-plaintext highlighter-rouge">ğŸ—‚ï¸Â c2p matrix</code></strong></p>

\[c2p = Q_i^câ€¢K_{âˆ‚(i,j)}^{rT}\]

<p><code class="language-plaintext highlighter-rouge">content2position</code>ì˜ ì•½ìë¡œ ìˆ˜ì‹ ìš°ë³€ì˜ ë‘ë²ˆì§¸ í•­, $Q_i^câ€¢K_{âˆ‚(i,j)}^{rT}$ë¥¼ ê°€ë¦¬í‚¨ë‹¤. <code class="language-plaintext highlighter-rouge">c2c</code>ë•Œì™€ëŠ” ë‹¤ë¥´ê²Œ ì„œë¡œ ì¶œì²˜ê°€ ë‹¤ë¥¸ ë‘ í–‰ë ¬ì„ ì‚¬ìš©í•´ <code class="language-plaintext highlighter-rouge">c2p</code>ë¼ëŠ” ì´ë¦„ì„ ë¶™ì˜€ë‹¤. ë‚´ì  ëŒ€ìƒì˜ ì¿¼ë¦¬ëŠ” <code class="language-plaintext highlighter-rouge">Input Embedding</code>ìœ¼ë¡œë¶€í„° ë§Œë“  í–‰ë ¬ $Q_i^c$, í‚¤ëŠ” <code class="language-plaintext highlighter-rouge">Position Embedding</code>ìœ¼ë¡œë¶€í„° ë§Œë“  í–‰ë ¬ $K_{âˆ‚(i,j)}^{rT}$ ì„ ì‚¬ìš©í–ˆë‹¤. <code class="language-plaintext highlighter-rouge">word context</code>ì™€ <code class="language-plaintext highlighter-rouge">relative position</code>ì„ ì„œë¡œ ëŒ€ì¡°í•œë‹¤ëŠ” ê²ƒì´ ë¬´ìŠ¨ ì˜ë¯¸ë¥¼ ê°–ëŠ”ì§€ ì§ê´€ì ìœ¼ë¡œ ì•Œê¸° í˜ë“œë‹ˆ ì¥ë³´ê¸° ì˜ˆì‹œë¥¼ í†µí•´ ì´í•´í•´ë³´ì.</p>

<p>êµ¬ìš´ ë‹¬ê±€ê³¼ ë‚ ë‹¬ê±€ì˜ ì˜ˆì‹œë¥¼ ë“¤ë©´ì„œ ìƒí’ˆëª…ê³¼ ì„¤ëª…ì´ ì¥ë³´ê¸°ì— ì¤‘ìš”í•œ ì˜í–¥ì„ ë¯¸ì¹œë‹¤ê³  ì–¸ê¸‰í–ˆë‹¤. í•˜ì§€ë§Œ ìƒí’ˆëª…ê³¼ ì„¤ëª…ì´ ì—¬ì „íˆ ë‹¨ìˆœ <code class="language-plaintext highlighter-rouge">â€œë‹¬ê±€â€</code>ìœ¼ë¡œ ì í˜€ ìˆì–´ë„ ìš°ë¦¬ëŠ” ì´ê²ƒì„ êµ¬ë¶„í•´ ë‚¼ ë°©ë²•ì´ ìˆë‹¤. ë°”ë¡œ ì£¼ë³€ì— ì§„ì—´ëœ ìƒí’ˆì´ ë¬´ì—‡ì¸ì§€ ì‚´í´ë³´ëŠ” ê²ƒì´ë‹¤. <code class="language-plaintext highlighter-rouge">â€œë‹¬ê±€â€</code> ë°”ë¡œ ì˜†ì— ìš°ìœ , ì¹˜ì¦ˆ, ìƒì„ , ì •ìœ¡ê³¼ ê°™ì€ ì‹ ì„ ì‹í’ˆë¥˜ê°€ ë°°ì¹˜ë˜ì–´ ìˆë‹¤ê³  ê°€ì •í•´ë³´ì. ìš°ë¦¬ëŠ” ìš°ë¦¬ ëˆˆ ì•ì— ìˆëŠ” <code class="language-plaintext highlighter-rouge">â€œë‹¬ê±€â€</code>ì´ ë‚ ë‹¬ê±€ì´ë¼ê³  ê¸°ëŒ€í•´ ë´„ì§í•˜ë‹¤. ë§Œì•½ <code class="language-plaintext highlighter-rouge">â€œë‹¬ê±€â€</code> ì˜†ì— ì¥í¬, ë§ë¦° ì˜¤ì§•ì–´, ìœ¡í¬, ê³¼ì ê°™ì€ ê°„ì‹ë¥˜ ìƒí’ˆë“¤ì´ ë°°ì¹˜ë˜ì–´ ìˆë‹¤ë©´ ì–´ë–¨ê¹Œ?? ê·¸ëŸ¼ ì´ <code class="language-plaintext highlighter-rouge">â€œë‹¬ê±€â€</code>ì€ ì¶©ë¶„íˆ êµ¬ìš´ ë‹¬ê±€ì´ë¼ê³  í•´ì„í•´ë³¼ ìˆ˜ ìˆë‹¤. ì´ì²˜ëŸ¼ ì£¼ìœ„ì— ì–´ë–¤ ë‹¤ë¥¸ ìƒí’ˆë“¤ì´ ë°°ì¹˜ ë˜ì–´ ìˆëŠ”ê°€ë¥¼ í†µí•´ ìš°ë¦¬ê°€ ì‚¬ë ¤ëŠ” ë¬¼ê±´ì´ ë§ëŠ”ì§€ ëŒ€ì¡°í•´ë³´ëŠ” í–‰ìœ„ê°€ ë°”ë¡œ <code class="language-plaintext highlighter-rouge">c2p</code> ì— ëŒ€ì‘ëœë‹¤. ê·¸ë ‡ë‹¤ë©´ ì£¼ìœ„ì— ì–´ë–¤ ë‹¤ë¥¸ ìƒí’ˆë“¤ì´ ë°°ì¹˜ ë˜ì–´ ìˆëŠ”ê°€ ì •ë³´ë¥¼ ëª¨ì•„ ë†“ì€ ê²ƒì´ ë°”ë¡œ $K_{âˆ‚(i,j)}^{rT}$ê°€ ëœë‹¤.</p>

<p><strong><code class="language-plaintext highlighter-rouge">ğŸ”¬Â p2c matrix</code></strong></p>

\[p2c = K_j^câ€¢Q_{âˆ‚(i,j)}^{rT}\]

<p><code class="language-plaintext highlighter-rouge">Disentangled Self-Attention</code>ì´ ì—¬íƒ€ ë‹¤ë¥¸ ì–´í…ì…˜ ê¸°ë²•ë“¤ê³¼ ê°€ì¥ ì°¨ë³„í™”ë˜ëŠ” ë¶€ë¶„ì´ë‹¤. ì €ìê°€ ë…¼ë¬¸ì—ì„œ ê°€ì¥ ê°•ì¡°í•˜ëŠ” ë¶€ë¶„ì´ê¸°ë„ í•˜ë‹¤. ì‚¬ì‹¤ ê·¸ëŸ° ê²ƒì¹˜ê³ ëŠ” ë…¼ë¬¸ ì† ì„¤ëª…ì´ ìƒë‹¹íˆ ë¶ˆì¹œì ˆí•´ ì´í•´í•˜ê¸° ì°¸ ë‚œí•´í•œ ê°œë…ì´ë‹¤. ì´ê±° ì„¤ëª…í•˜ê³  ì‹¶ì–´ì„œ ì¥ë³´ê¸° ì˜ˆì‹œë¥¼ ìƒê°í•´ë‚´ê²Œ ë˜ì—ˆë‹¤. ë‹¤ì‹œ ë‚¨í¸ì—ê²Œ ì¤„ ì¥ë³´ê¸° ë¦¬ìŠ¤íŠ¸ë¥¼ ì‘ì„±í•˜ë˜ ì‹œì ìœ¼ë¡œ ëŒì•„ê°€ë³´ì.</p>

<p>ì˜¤ëŠ˜ ì €ë… ë©”ë‰´ëŠ” ì°¨ëŒë°•ì´ ëœì¥ì°Œê°œì™€ êµ¬ìš´ ì‚¼ê²¹ì‚´ì´ë‹¤. ë¨¼ì € ì°¨ëŒë°•ì´ ëœì¥ì°Œê°œë¥¼ ë§Œë“¤ë ¤ë©´ ì–´ë–¤ ì¬ë£Œê°€ í•„ìš”í• ê¹Œ?? ì°¨ëŒë°•ì´, ëœì¥, ì²­ì–‘ê³ ì¶”, ì–‘íŒŒ, ë‹¤ì§„ ë§ˆëŠ˜, í˜¸ë°•ê³¼ ê°™ì€ ì‹ìì¬ê°€ í•„ìš”í•  ê²ƒì´ë‹¤. ê·¸ë¦¬ê³  ì‚¼ê²¹ì‚´ì— í•„ìš”í•œ ì¬ë£Œë¥¼ ìƒê°í•´ë³´ì. ìƒì‚¼ê²¹ì‚´ê³¼ ì¡ë‚´ë¥¼ ì—†ì• ëŠ”ë° í•„ìš”í•œ í›„ì¶”ì™€ ì†Œê¸ˆ ê·¸ë¦¬ê³  êµ¬ì›Œ ë¨¹ì„ í†µë§ˆëŠ˜ì´  í•„ìš”í•˜ë‹¤ê³  ë‹¹ì‹ ì€ ìƒê°í–ˆë‹¤. ê·¸ëŸ¼ ì´ì œ ì´ê²ƒì„ ë°”íƒ•ìœ¼ë¡œ ë¦¬ìŠ¤íŠ¸ë¥¼ ì‘ì„±í•  ê²ƒì´ë‹¤. ì–´ë–¤ ì‹ìœ¼ë¡œ ë¦¬ìŠ¤íŠ¸ë¥¼ ì‘ì„±í•˜ëŠ”ê²Œ ê°€ì¥ ìµœì ì¼ê¹Œ??</p>

<p><code class="language-plaintext highlighter-rouge">c2c</code>, <code class="language-plaintext highlighter-rouge">c2p</code> ì˜ˆì‹œì™€ í•¨ê»˜ ìƒê°í•´ë³´ë©´ ì•Œ ìˆ˜ ìˆë‹¤. <code class="language-plaintext highlighter-rouge">c2c</code>ì—ì„œëŠ” ê°™ì€ ì¬ë£Œë¼ë„ ê·¸ ìš©ë„ì— ë”°ë¼ì„œ ì‚¬ì•¼í•  í’ˆëª©ì´ ë‹¬ë¼ì§„ë‹¤ê³  ì–¸ê¸‰í•œ ë°”ìˆë‹¤. <code class="language-plaintext highlighter-rouge">c2p</code> ì—ì„œëŠ” ì •í™•í•œ ì„¤ëª…ì´ ì—†ì–´ë„ ì£¼ë³€ì— ë‚˜ì—´ëœ í’ˆëª©ë“¤ì„ ë³´ë©´ì„œ ì–´ë–¤ ìƒí’ˆì¸ì§€ ìœ ì¶”ê°€ ê°€ëŠ¥í•˜ë‹¤ê³  í–ˆë‹¤. ì´ê²ƒì„ í•©ì³ë³´ì. ë§Œì•½ ë‹¹ì‹ ì´ ì•„ë˜ì™€ ê°™ì€ ìˆœì„œë¡œ ë¦¬ìŠ¤íŠ¸ë¥¼ ì ì—ˆë‹¤ê³  ê°€ì •í•´ë³´ê² ë‹¤.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># ì¥ë³´ê¸° ë¦¬ìŠ¤íŠ¸ ì˜ˆì‹œ1
</span>
<span class="n">ì°¨ëŒë°•ì´</span><span class="p">,</span> <span class="n">ëœì¥</span><span class="p">,</span> <span class="n">ë§ˆëŠ˜</span><span class="p">,</span> <span class="n">ì²­ì–‘ê³ ì¶”</span><span class="p">,</span> <span class="n">ì–‘íŒŒ</span><span class="p">,</span> <span class="n">í˜¸ë°•</span><span class="p">,</span> <span class="n">ì‚¼ê²¹ì‚´</span><span class="p">,</span> <span class="n">í›„ì¶”</span><span class="p">,</span> <span class="n">ì†Œê¸ˆ</span>
</code></pre></div></div>

<p>ì•„ê¹Œ í•„ìš”í•œ í’ˆëª©ì„ ë‚˜ì—´í–ˆì„ ë•Œ ë¶„ëª…íˆ ë‹¤ì§„ ë§ˆëŠ˜ê³¼ í†µë§ˆëŠ˜ì„ ë™ì‹œì— ìƒê°í–ˆì—ˆë‹¤. ê·¼ë° ìœ„ì²˜ëŸ¼ ë¦¬ìŠ¤íŠ¸ë¥¼ ì‘ì„±í•´ì„œ ë‚¨í¸ì—ê²Œ ì¤¬ë‹¤ë©´ ë‚¨í¸ì€ ì–´ë–¤ ë§ˆëŠ˜ì„ ì‚¬ì˜¬ê¹Œ?? ë‹¹ì—°íˆ ì°¨ëŒë°•ì´ì™€ ëœì¥ ê·¸ë¦¬ê³  ì–‘íŒŒ ì‚¬ì´ì— ë§ˆëŠ˜ì´ ìœ„ì¹˜í•œ ê²ƒì„ ë³´ê³  ë‚¨í¸ì€ êµ­ë¬¼ìš© ë§ˆëŠ˜ì´ í•„ìš”í•˜êµ¬ë‚˜ ì‹¶ì–´ì„œ ë‹¤ì§„ ë§ˆëŠ˜ì„ ì‚¬ì˜¬ ê²ƒì´ë‹¤.</p>

<p>ê·¸ë ‡ë‹¤ë©´ ë°˜ëŒ€ë¡œ ë‹¹ì‹ ì´ ì•„ë˜ì²˜ëŸ¼ ë¦¬ìŠ¤íŠ¸ë¥¼ ì‘ì„±í–ˆë‹¤ê³  ìƒê°í•´ë³´ì.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># ì¥ë³´ê¸° ë¦¬ìŠ¤íŠ¸ ì˜ˆì‹œ2
</span>
<span class="n">ì°¨ëŒë°•ì´</span><span class="p">,</span> <span class="n">ëœì¥</span><span class="p">,</span> <span class="n">ì²­ì–‘ê³ ì¶”</span><span class="p">,</span> <span class="n">ì–‘íŒŒ</span><span class="p">,</span> <span class="n">í˜¸ë°•</span><span class="p">,</span> <span class="n">ì‚¼ê²¹ì‚´</span><span class="p">,</span> <span class="n">ë§ˆëŠ˜</span><span class="p">,</span> <span class="n">í›„ì¶”</span><span class="p">,</span> <span class="n">ì†Œê¸ˆ</span>
</code></pre></div></div>

<p>ì´ë²ˆì—ëŠ” ì‚¼ê²¹ì‚´ êµ¬ìš¸ ë•Œ, ê°™ì´ êµ¬ì›Œë¨¹ì„ í†µë§ˆëŠ˜ì´ í•„ìš”í•˜êµ¬ë‚˜ë¥¼ ë‚¨í¸ì´ ëŠë‚„ ìˆ˜ ìˆì„ ê²ƒì´ë‹¤. í•œí¸ ì•„ë˜ì™€ ê°™ì€ ìƒí™©ì´ë¼ë©´ ì–´ë–¨ê¹Œ??</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># ì¥ë³´ê¸° ë¦¬ìŠ¤íŠ¸ ì˜ˆì‹œ3
</span>
<span class="n">ì°¨ëŒë°•ì´</span><span class="p">,</span> <span class="n">ëœì¥</span><span class="p">,</span> <span class="n">ë§ˆëŠ˜</span><span class="p">,</span> <span class="n">ì²­ì–‘ê³ ì¶”</span><span class="p">,</span> <span class="n">ì–‘íŒŒ</span><span class="p">,</span> <span class="n">í˜¸ë°•</span><span class="p">,</span> <span class="n">ì‚¼ê²¹ì‚´</span><span class="p">,</span> <span class="n">ë§ˆëŠ˜</span><span class="p">,</span> <span class="n">í›„ì¶”</span><span class="p">,</span> <span class="n">ì†Œê¸ˆ</span>
</code></pre></div></div>

<p>ì¡°ê¸ˆ ì„¼ìŠ¤ê°€ ìˆëŠ” ë‚¨í¸ì´ë¼ë©´ ëœì¥ì°Œê°œ êµ­ë¬¼ìš© ë‹¤ì§„ë§ˆëŠ˜ê³¼ ì‚¼ê²¹ì‚´ êµ¬ì´ìš© í†µë§ˆëŠ˜ì´ ë™ì‹œì— í•„ìš”í•˜êµ¬ë‚˜ë¼ê³  ìœ ì¶”í•˜ê³  ë§¤ì¥ì—ì„œ ë‹¤ì§„ë§ˆëŠ˜, í†µë§ˆëŠ˜ì´ë¼ ì¨ìˆëŠ” í’ˆëª©ì„ ì°¾ì•„ì„œ ë‘˜ ë‹¤ ì‚¬ì˜¬ ê²ƒì´ë‹¤. ë¬¼ë¡  ì„¼ìŠ¤ìˆëŠ” ì•„ë‚´ë¼ë©´ ì• ì´ˆì— ì €ë ‡ê²Œ ì• ë§¤í•˜ê²Œ <code class="language-plaintext highlighter-rouge">ë§ˆëŠ˜</code>ì´ë¼ê³  2ë²ˆ ì•ˆì ê³  <code class="language-plaintext highlighter-rouge">ë‹¤ì§„ë§ˆëŠ˜</code>, <code class="language-plaintext highlighter-rouge">í†µë§ˆëŠ˜</code>ì´ë¼ê³  ìš©ë„ë¥¼ í•¨ê»˜ ì ì–´ì¤¬ê² ì§€ë§Œ ë§ì´ë‹¤.</p>

<p>ì´ëŸ¬í•œ ì¼ë ¨ì˜ ìƒí™©ì´ ë°”ë¡œ <code class="language-plaintext highlighter-rouge">p2c</code>ì— ëŒ€ì‘ëœë‹¤. ê·¸ë ‡ë‹¤ë©´ ì•„ë‚´ê°€ ì ì–´ì¤€ ë¦¬ìŠ¤íŠ¸ì—ì„œ ì£¼ë³€ì— ìœ„ì¹˜í•œ í’ˆëª©ë“¤ì— ë”°ë¼ì„œ í¬ì°©ë˜ëŠ” ëŒ€ìƒ í’ˆëª©ì˜ ìš©ë„ë‚˜ ì“°ì„ìƒˆ, ì˜ë¯¸ ë“±ì´ ë°”ë¡œ í–‰ë ¬ $Q_{âˆ‚(i,j)}^{rT}$ê°€ ëœë‹¤.</p>

<p><strong><code class="language-plaintext highlighter-rouge">âš’ï¸Â DeBERTa Scale Factor</code></strong><br />
ì²˜ìŒì— ë‚˜ì—´í•œ ìˆ˜ì‹ì„ ë‹¤ì‹œ ë³´ë©´ <code class="language-plaintext highlighter-rouge">DeBERTa</code>ì˜ <code class="language-plaintext highlighter-rouge">scale factor</code>ëŠ” ê¸°ì¡´ <code class="language-plaintext highlighter-rouge">Self-Attention</code> ê³¼ ë‹¤ë¥´ê²Œ $\sqrt{3d_h}$ë¥¼ ì‚¬ìš©í•œë‹¤. ì´ìœ ê°€ ë­˜ê¹Œ?? ê¸°ì¡´ ë°©ì‹ì€ <code class="language-plaintext highlighter-rouge">softmax layer</code>ì— ì „ë‹¬í•˜ëŠ” í–‰ë ¬ì˜ ì¢…ë¥˜ê°€ $Qâ€¢K^T$ í•œ ê°œë‹¤. <code class="language-plaintext highlighter-rouge">DeBERTa</code>ì˜ ê²½ìš°ëŠ” 3ê°œë¥¼ ì „ë‹¬í•˜ê²Œ ëœë‹¤. ê·¸ë˜ì„œ $d_h$ì•ì— 3ì„ ê³±í•´ì¤€ ê²ƒì´ë‹¤. official repoì˜ ì½”ë“œë¥¼ í™•ì¸í•´ë³´ë©´ í™•ì‹¤íˆ ì•Œ ìˆ˜ ìˆëŠ”ë°, ì–´í…ì…˜ì— ì‚¬ìš©í•˜ëŠ” í–‰ë ¬ ì¢…ë¥˜ì˜ ê°œìˆ˜ë¥¼ $d_h$ì•ì— ê³±í•´ì¤€ë‹¤. ì•„ë˜ëŠ” <code class="language-plaintext highlighter-rouge">repo</code>ì— ì˜¬ë¼ì™€ ìˆëŠ” ì½”ë“œì˜ ì¼ë¶€ë¥¼ ë°œì·Œí•œ ê²ƒì´ë‹¤.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># official Disentangled Self-Attention by microsoft from official repo
</span>
<span class="p">...</span><span class="n">ì¤‘ëµ</span><span class="p">...</span>
<span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">,</span> <span class="n">return_att</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">query_states</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">relative_pos</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">rel_embeddings</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">query_states</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">query_states</span> <span class="o">=</span> <span class="n">hidden_states</span>
    <span class="n">query_layer</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">transpose_for_scores</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">query_proj</span><span class="p">(</span><span class="n">query_states</span><span class="p">),</span> <span class="bp">self</span><span class="p">.</span><span class="n">num_attention_heads</span><span class="p">).</span><span class="nb">float</span><span class="p">()</span>
    <span class="n">key_layer</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">transpose_for_scores</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">key_proj</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">),</span> <span class="bp">self</span><span class="p">.</span><span class="n">num_attention_heads</span><span class="p">).</span><span class="nb">float</span><span class="p">()</span>
    <span class="n">value_layer</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">transpose_for_scores</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">value_proj</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">),</span> <span class="bp">self</span><span class="p">.</span><span class="n">num_attention_heads</span><span class="p">)</span>
    
    <span class="n">rel_att</span> <span class="o">=</span> <span class="bp">None</span>
    <span class="c1"># Take the dot product between "query" and "key" to get the raw attention scores.
</span>    <span class="n">scale_factor</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="k">if</span> <span class="s">'c2p'</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">pos_att_type</span><span class="p">:</span>
        <span class="n">scale_factor</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">if</span> <span class="s">'p2c'</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">pos_att_type</span><span class="p">:</span>
        <span class="n">scale_factor</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">if</span> <span class="s">'p2p'</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">pos_att_type</span><span class="p">:</span>
        <span class="n">scale_factor</span> <span class="o">+=</span> <span class="mi">1</span>
</code></pre></div></div>

<p><strong><code class="language-plaintext highlighter-rouge">ğŸ‘©â€ğŸ’»Â Implementation</code></strong><br />
ì´ë ‡ê²Œ <code class="language-plaintext highlighter-rouge">Disentangled Self-Attention</code>ì— ëŒ€í•œ ëª¨ë“  ë‚´ìš©ì„ ì‚´í´ë´¤ë‹¤. ì‹¤ì œ êµ¬í˜„ì€ ì–´ë–»ê²Œ í•´ì•¼ í•˜ëŠ”ì§€ í•„ìê°€ ì‘ì„±í•œ íŒŒì´í† ì¹˜ ì½”ë“œì™€ í•¨ê»˜ ì•Œì•„ë³´ì.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Pytorch Implementation of DeBERTa Disentangled Self-Attention
</span>
<span class="k">def</span> <span class="nf">build_relative_position</span><span class="p">(</span><span class="n">x_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
    <span class="s">"""
    Build Relative Position Matrix for Disentangled Self-Attention in DeBERTa
    Args:
        x_size: sequence length of query matrix
    Reference:
        https://github.com/microsoft/DeBERTa/blob/master/DeBERTa/deberta/da_utils.py#L29
        https://arxiv.org/abs/2006.03654
    """</span>
    <span class="n">x_index</span><span class="p">,</span> <span class="n">y_index</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="n">x_size</span><span class="p">),</span> <span class="n">torch</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="n">x_size</span><span class="p">)</span>  <span class="c1"># same as rel_pos in official repo
</span>    <span class="n">rel_pos</span> <span class="o">=</span> <span class="n">x_index</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="o">-</span> <span class="n">y_index</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">rel_pos</span>

<span class="k">def</span> <span class="nf">disentangled_attention</span><span class="p">(</span><span class="n">q</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">k</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">v</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">qr</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">kr</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">dropout</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Dropout</span><span class="p">,</span> <span class="n">mask</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="bp">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
    <span class="s">"""
    Disentangled Self-Attention for DeBERTa, same role as Module "DisentangledSelfAttention" in official Repo
    Args:
        q: content query matrix, shape (batch_size, seq_len, dim_head)
        k: content key matrix, shape (batch_size, seq_len, dim_head)
        v: content value matrix, shape (batch_size, seq_len, dim_head)
        qr: position query matrix, shape (batch_size, 2*max_relative_position, dim_head), r means relative position
        kr: position key matrix, shape (batch_size, 2*max_relative_position, dim_head), r means relative position
        dropout: dropout for attention matrix, default rate is 0.1 from official paper
        mask: mask for attention matrix, shape (batch_size, seq_len, seq_len), apply before softmax layer
    Math:
        c2c = torch.matmul(q, k.transpose(-1, -2))  # A_c2c
        c2p = torch.gather(torch.matmul(q, kr.transpose(-1, -2)), dim=-1, index=c2p_pos)
        p2c = torch.gather(torch.matmul(qr, k.transpose(-1, -2)), dim=-2, index=c2p_pos)
        Attention Matrix = c2c + c2p + p2c
        A = softmax(Attention Matrix/sqrt(3*D_h)), SA(z) = Av
    Notes:
        dot_scale(range 1 ~ 3): scale factor for Qâ€¢K^T result, sqrt(3*dim_head) from official paper by microsoft,
        3 means that use full attention matrix(c2c, c2p, p2c), same as number of using what kind of matrix
        default 1, c2c is always used and c2p &amp; p2c is optional
    References:
        https://github.com/microsoft/DeBERTa/blob/master/DeBERTa/deberta/disentangled_attention.py
        https://arxiv.org/pdf/1803.02155.pdf
        https://arxiv.org/abs/2006.03654
        https://arxiv.org/abs/2111.09543
        https://arxiv.org/abs/1901.02860
        https://arxiv.org/abs/1906.08237
    """</span>
    <span class="n">scale_factor</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">c2c</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">))</span>  <span class="c1"># A_c2c
</span>
    <span class="n">c2p_att</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">kr</span><span class="p">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">))</span>
    <span class="n">c2p_pos</span> <span class="o">=</span> <span class="n">build_relative_position</span><span class="p">(</span><span class="n">q</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="o">+</span> <span class="n">kr</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">/</span> <span class="mi">2</span>  <span class="c1"># same as rel_pos in official repo
</span>    <span class="n">c2p_pos</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">c2p_pos</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">kr</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span><span class="p">).</span><span class="n">repeat</span><span class="p">(</span><span class="n">q</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">c2p</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">gather</span><span class="p">(</span><span class="n">c2p_att</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">c2p_pos</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">c2p</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">scale_factor</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="n">p2c_att</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">qr</span><span class="p">,</span> <span class="n">k</span><span class="p">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">))</span>
    <span class="n">p2c</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">gather</span><span class="p">(</span><span class="n">p2c_att</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">2</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">c2p_pos</span><span class="p">)</span>  <span class="c1"># same as torch.gather(kâ€¢qr^t, dim=-1, index=c2p_pos)
</span>    <span class="k">if</span> <span class="n">p2c</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">scale_factor</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="n">dot_scale</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">scale_factor</span> <span class="o">*</span> <span class="n">q</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]))</span>  <span class="c1"># from official paper by microsoft
</span>    <span class="n">attention_matrix</span> <span class="o">=</span> <span class="p">(</span><span class="n">c2c</span> <span class="o">+</span> <span class="n">c2p</span> <span class="o">+</span> <span class="n">p2c</span><span class="p">)</span> <span class="o">/</span> <span class="n">dot_scale</span>  <span class="c1"># Attention Matrix = A_c2c + A_c2r + A_r2c
</span>    <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">attention_matrix</span> <span class="o">=</span> <span class="n">attention_matrix</span><span class="p">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">mask</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="s">'-inf'</span><span class="p">))</span>  <span class="c1"># Padding Token Masking
</span>    <span class="n">attention_dist</span> <span class="o">=</span> <span class="n">dropout</span><span class="p">(</span>
        <span class="n">F</span><span class="p">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">attention_matrix</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="p">)</span>
    <span class="n">attention_matrix</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">attention_dist</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">attention_matrix</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">p2c</code> ë¥¼ êµ¬í•˜ëŠ” ê³¼ì •ì˜ ì½”ë“œë¼ì¸ì— ì£¼ëª©í•´ë³´ì. ë…¼ë¬¸ì— ê¸°ì¬ëœ ìˆ˜ì‹($K_j^câ€¢Q_{âˆ‚(i,j)}^{rT}$)ê³¼ ë‹¤ë¥´ê²Œ, ì¿¼ë¦¬ì™€ í‚¤ì˜ ìˆœì„œë¥¼ ë’¤ì§‘ì—ˆë‹¤. ê·¸ë˜ì„œ <code class="language-plaintext highlighter-rouge">torch.gather</code>ì˜ ì°¨ì› ë§¤ê°œë³€ìˆ˜ <code class="language-plaintext highlighter-rouge">dim</code>ë¥¼ <code class="language-plaintext highlighter-rouge">c2p</code>ì˜ ìƒí™©ê³¼ ë‹¤ë¥´ê²Œ -<code class="language-plaintext highlighter-rouge">2</code>ë¡œ ì´ˆê¸°í™”í•˜ê²Œ ë˜ì—ˆë‹¤. ë‚´ì í•˜ëŠ” í•­ì˜ ìˆœì„œë¥¼ ë’¤ì§‘ì€ ê²ƒìœ¼ë¡œ ì¸í•´ ìš°ë¦¬ê°€ ì¶”ì¶œí•˜ê³  ì‹¶ì€ ëŒ€ìƒ ê°’ì¸ ìƒëŒ€ ìœ„ì¹˜ ì„ë² ë”©ì´ <code class="language-plaintext highlighter-rouge">-2</code>ë²ˆì§¸ ì°¨ì›ì— ìœ„ì¹˜ í•˜ê²Œ ë˜ê¸° ë•Œë¬¸ì´ë‹¤.</p>

<h4 id="enhanced-mask-decoder"><strong><code class="language-plaintext highlighter-rouge">ğŸ˜·Â Enhanced Mask Decoder</code></strong></h4>
<p><code class="language-plaintext highlighter-rouge">DeBERTa</code>ì˜ ì„¤ê³„ ëª©ì ì€ 2ê°€ì§€ ìœ„ì¹˜ ì •ë³´ë¥¼ ì ì ˆíˆ ì„ì–´ì„œ ìµœëŒ€í•œ í’ë¶€í•œ ì„ë² ë”©ì„ ë§Œë“œëŠ” ê²ƒì´ë¼ê³  í–ˆë‹¤. ìƒëŒ€ ìœ„ì¹˜ ì„ë² ë”©ì€ <code class="language-plaintext highlighter-rouge">Disentangled Self-Attention</code>ì„ í†µí•´ í¬ì°©í•œë‹¤ëŠ” ê²ƒì„ ì´ì œ ì•Œì•˜ë‹¤. ê·¸ëŸ¼ ì ˆëŒ€ ìœ„ì¹˜ ì„ë² ë”©ì€ ì–´ë–¤ ì‹ìœ¼ë¡œ ëª¨ë¸ë§í•´ì¤˜ì•¼ í• ê¹Œ?? ê·¸ ë¬¼ìŒì— ë‹µì€ ë°”ë¡œ <code class="language-plaintext highlighter-rouge">EMD</code>ë¼ ë¶ˆë¦¬ëŠ” <code class="language-plaintext highlighter-rouge">Enhanced Mask Decoder</code>ì— ìˆë‹¤. <code class="language-plaintext highlighter-rouge">EMD</code>ì˜ ì›ë¦¬ì— ëŒ€í•´ ê³µë¶€í•˜ê¸° ì „ì— ì™œ ì ˆëŒ€ ìœ„ì¹˜ ì„ë² ë”©ì´ <code class="language-plaintext highlighter-rouge">NLU</code>ì— í•„ìš”í•œì§€ ì§šê³  ë„˜ì–´ê°€ì.</p>

<p><strong><center><i>a new <u>"store"</u> opened beside the new <u>"mallâ€</u></i></center></strong></p>

<p>ìœ„ ë¬¸ì¥ì€ ì €ìê°€ ë…¼ë¬¸ì—ì„œ <code class="language-plaintext highlighter-rouge">Absolute Position Embedding</code>ì˜ í•„ìš”ì„±ì„ ì—­ì„¤í•  ë•Œ ì‚¬ìš©í•œ ì˜ˆì‹œ ë¬¸ì¥ì´ë‹¤. ê³¼ì—° ìƒëŒ€ ìœ„ì¹˜ ì„ë² ë”©ë§Œ ì‚¬ìš©í•´ì„œ <strong><code class="language-plaintext highlighter-rouge">store</code></strong>ì™€ <strong><code class="language-plaintext highlighter-rouge">mall</code></strong>ì˜ ì°¨ì´ë¥¼ ì˜ êµ¬ë³„í•  ìˆ˜ ìˆì„ê¹Œ ìƒê°í•´ë³´ì. ì•ì„œ ìš°ë¦¬ëŠ” ìƒëŒ€ ìœ„ì¹˜ ì„ë² ë”©ì„ <strong>ëŒ€ìƒ í† í°ê³¼ ê·¸ ë‚˜ë¨¸ì§€ í† í° ì‚¬ì´ì˜ ìœ„ì¹˜ ë³€í™”ì— ë”°ë¼ ë°œìƒí•˜ëŠ” íŒŒìƒì ì¸ ë§¥ë½ ì •ë³´ë¥¼ ë‹´ì€ í–‰ë ¬</strong>ì´ë¼ê³  ì •ì˜í•œ ë°” ìˆë‹¤. ë‹¤ì‹œ ë§í•´, ëŒ€ìƒ í† í°ì˜ ì˜ë¯¸ë¥¼ ì£¼ë³€ì— ì–´ë–¤ <code class="language-plaintext highlighter-rouge">context</code>ê°€ ìˆëŠ”ì§€ íŒŒì•…í•´ í†µí•´ ì´í•´í•´ë³´ê² ë‹¤ëŠ” ê²ƒì´ë‹¤.</p>

<p>ì˜ˆì‹œ ë¬¸ì¥ì„ ë‹¤ì‹œ ë³´ì. ë‘ ëŒ€ìƒ ë‹¨ì–´ ëª¨ë‘ ì£¼ìœ„ì— ë¹„ìŠ·í•œ ì˜ë¯¸ë¥¼ ê°–ëŠ” ë‹¨ì–´ë“¤ì´ ìœ„ì¹˜í•´ ìˆë‹¤. ì´ëŸ° ê²½ìš° ìƒëŒ€ ìœ„ì¹˜ ì„ë² ë”©ë§Œìœ¼ë¡œëŠ” ì‹œí€€ìŠ¤ ë‚´ë¶€ì—ì„œ <strong><code class="language-plaintext highlighter-rouge">store</code></strong>ì™€ <strong><code class="language-plaintext highlighter-rouge">mall</code></strong>ì˜ ì˜ë¯¸ ì°¨ì´ë¥¼ ëª¨ë¸ì´ ëª…í™•í•˜ê²Œ ì´í•´í•˜ê¸° ë§¤ìš° ì–´ë ¤ìš¸ ê²ƒì´ë‹¤. í˜„ì¬ ìƒí™©ì—ì„œ ë‘ ë‹¨ì–´ì˜ ë‰˜ì•™ìŠ¤ ì°¨ì´ëŠ” ê²°êµ­ ë¬¸ì¥ì˜ ì£¼ì–´ëƒ ëª©ì ì–´ëƒ í•˜ëŠ” <code class="language-plaintext highlighter-rouge">syntactical</code>í•œ ì •ë³´ì— ì˜í•´ì„œ ê²°ì •ëœë‹¤. <code class="language-plaintext highlighter-rouge">syntactical</code>í•œ ì •ë³´ì˜ í•„ìš”ì„±ì€ ë°”ë¡œ ì ˆëŒ€ ìœ„ì¹˜ ì„ë² ë”©ì´ <code class="language-plaintext highlighter-rouge">NLU</code>ì— ê¼­ í•„ìš”í•œ ì´ìœ ì— ëŒ€ì‘ëœë‹¤.</p>

<p align="center">
<img src="/assets/images/deberta/emd_overview.png" alt="Enhanced Mask Decoder Overview" class="align-center image-caption" width="60%&quot;, height=&quot;50%" />
<strong><em><a href="https://arxiv.org/abs/2006.03654">Enhanced Mask Decoder Overview</a></em></strong>
</p>

<p><strong><code class="language-plaintext highlighter-rouge">ğŸ¤”Â why named decoder</code></strong><br />
í•„ìëŠ” ì²˜ìŒ ë…¼ë¬¸ì„ ì½ì—ˆì„ ë•Œ <code class="language-plaintext highlighter-rouge">Decoder</code>ë¼ëŠ” ì´ë¦„ì„ ë³´ë©´ì„œ ì°¸ ì˜ì•„í–ˆë‹¤. ë¶„ëª… <code class="language-plaintext highlighter-rouge">Only-Encoder</code> ëª¨ë¸ë¡œ ì•Œê³  ìˆëŠ”ë° ì–´ì°Œí•˜ì—¬ ì´ë¦„ì— ë””ì½”ë”ê°€ ë¶™ëŠ” ëª¨ë“ˆì´ ìˆëŠ” ê²ƒì¸ê°€. ê·¸ë ‡ë‹¤ê³  ì´ë¦„ì„ ì €ë ‡ê²Œ ë¶™ì¸ ì˜ë„ë¥¼ ì„¤ëª…í•˜ëŠ” ê²ƒë„ ì•„ë‹ˆë‹¤. ê·¸ë˜ì„œ í•„ìê°€ ìŠ¤ìŠ¤ë¡œ ì¶”ì¸¡í•´ë´¤ë‹¤.</p>

<p><code class="language-plaintext highlighter-rouge">DeBERTa</code>ëŠ” <code class="language-plaintext highlighter-rouge">pre-train task</code> ë¡œ <code class="language-plaintext highlighter-rouge">MLM</code>ì„ ì‚¬ìš©í–ˆë‹¤. <code class="language-plaintext highlighter-rouge">MLM</code>ì´ ë¬´ì—‡ì¸ê°€?? ë°”ë¡œ ë§ˆìŠ¤í‚¹ëœ ìë¦¬ì— ì ì ˆí•œ í† í°ì„ ì°¾ëŠ” ë¹ˆì¹¸ ì±„ìš°ê¸° ë¬¸ì œë‹¤. ì˜ë¯¸ê¶Œì—ì„œëŠ” ì´ê²ƒì„ <code class="language-plaintext highlighter-rouge">denoising</code>í•œë‹¤ê³  í‘œí˜„í•˜ê¸°ë„ í•˜ëŠ”ë°, <code class="language-plaintext highlighter-rouge">Absolute Position Embedding</code>ì´ ë°”ë¡œ ì´ <code class="language-plaintext highlighter-rouge">denoising</code>ì— ì§€ëŒ€í•œ ì˜í–¥ë ¥ì„ ë¯¸ì¹œë‹¤ëŠ” ì–¸ê¸‰ì„ ë…¼ë¬¸ì—ì„œ ì°¾ì•„ë³¼ ìˆ˜ ìˆë‹¤. ë”°ë¼ì„œ <code class="language-plaintext highlighter-rouge">denoising</code> ì„±ëŠ¥ì— í° ì˜í–¥ì„ ì£¼ëŠ” <code class="language-plaintext highlighter-rouge">Absolute Position Embedding</code>ì„ í™œìš©í•œë‹¤ê³  í•´ì„œ ì´ë¦„ì— <code class="language-plaintext highlighter-rouge">decoder</code>ë¥¼ ë¶™ì˜€ì§€ ì•Šì•˜ë‚˜ ì˜ˆìƒí•´ë³¸ë‹¤.</p>

<p>ë…¼ë¬¸ì— ê°™ì´ ì‹¤ë¦° ê·¸ë¦¼ì„ í†µí•´ì„œë„ ì¶”ì¸¡ì´ ê°€ëŠ¥í•˜ë‹¤. <code class="language-plaintext highlighter-rouge">EMD</code> ì˜ êµ¬ì¡°ë¥¼ ì„¤ëª…í•˜ë©´ì„œ ì˜†ì— BERTì˜ ëª¨ì‹ë„ë„ í•¨ê»˜ ì œê³µí•˜ëŠ”ë°, <code class="language-plaintext highlighter-rouge">BERT</code>ì—ëŠ” <code class="language-plaintext highlighter-rouge">Decoder</code>ê°€ ì „í˜€ ì—†ë‹¤. ê·¸ëŸ°ë°ë„ ì´ë¦„ì„ <code class="language-plaintext highlighter-rouge">BERT decoding layer</code>ë¼ê³  ë¶€ë¥´ëŠ” ê²ƒë³´ë©´ í•„ìì˜ ì¶”ì¸¡ì— ì¢€ ë” ì •ë‹¹ì„±ì„ ë¶€ì—¬í•˜ëŠ” ê²ƒ ê°™ë‹¤.</p>

<p>(+ ì¶”ê°€) offical repo codeì—ì„œë„ <code class="language-plaintext highlighter-rouge">EMD</code>ê°€ ìš°ë¦¬ê°€ ì•„ëŠ” ê·¸ <code class="language-plaintext highlighter-rouge">Encoder</code>ë¥¼ ì‚¬ìš©í•œë‹¤ëŠ” ì‚¬ì‹¤ì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤.</p>

<p><strong><code class="language-plaintext highlighter-rouge">ğŸ¤·â€â™‚ï¸Â How to add Absolute Position</code></strong></p>

<p align="center">
<img src="/assets/images/deberta/deberta_overview.png" alt="DeBERTa Model Structure" class="align-center image-caption" width="45%&quot;, height=&quot;50%" />
<strong><em><a href="https://www.youtube.com/watch?v=gcMyKUXbY8s&amp;t=838s&amp;ab_channel=%EA%B3%A0%EB%A0%A4%EB%8C%80%ED%95%99%EA%B5%90%EC%82%B0%EC%97%85%EA%B2%BD%EC%98%81%EA%B3%B5%ED%95%99%EB%B6%80DSBA%EC%97%B0%EA%B5%AC%EC%8B%A4">DeBERTa Model Structure</a></em></strong>
</p>

<p>ì´ì œ <code class="language-plaintext highlighter-rouge">EMD</code>ê°€ ë¬´ì—‡ì´ë©°, <code class="language-plaintext highlighter-rouge">Absolute Position</code>ì„ ì–´ë–»ê²Œ ëª¨ë¸ì— ì¶”ê°€í•˜ëŠ”ì§€ ì•Œì•„ë³´ì. <code class="language-plaintext highlighter-rouge">EMD</code>ëŠ” <code class="language-plaintext highlighter-rouge">MLM</code> ì„±ëŠ¥ì„ ë†’ì´ê¸° ìœ„í•´ ê³ ì•ˆëœ êµ¬ì¡°ë‹¤. ê·¸ë˜ì„œ í† í° ì˜ˆì¸¡ì„ ìœ„í•œ <code class="language-plaintext highlighter-rouge">feedforward &amp; softmax</code> ë ˆì´ì–´ ì§ì „ì— ìŒ“ëŠ”ë‹¤. ëª‡ê°œì˜ <code class="language-plaintext highlighter-rouge">EMD</code>ë¥¼ ìŒ“ì„ ê²ƒì¸ì§€ëŠ” í•˜ì´í¼íŒŒë¦¬ë¯¸í„°ì´ë©°, ì €ìì˜ ì‹¤í—˜ ê²°ê³¼ <code class="language-plaintext highlighter-rouge">2</code>ê°œ ì‚¬ìš©í•˜ëŠ”ê²Œ ê°€ì¥ íš¨ìœ¨ì ì´ë¼ê³  í•œë‹¤. ìƒˆë¡­ê²Œ ì¸ì½”ë” ë¸”ëŸ­ì„ ìŒ“ì§€ ì•Šê³  <code class="language-plaintext highlighter-rouge">Disentangled-Attention</code> ë ˆì´ì–´ì˜ ê°€ì¥ ë§ˆì§€ë§‰ ì¸ì½”ë” ë¸”ëŸ­ê³¼ ê°€ì¤‘ì¹˜ë¥¼ ê³µìœ í•˜ëŠ” í˜•íƒœë¡œ êµ¬í˜„í•œë‹¤.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># EMD Implementation Example
</span>
<span class="k">class</span> <span class="nc">EnhancedMaskDecoder</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">encoder</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">nn</span><span class="p">.</span><span class="n">ModuleList</span><span class="p">],</span> <span class="n">dim_model</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1024</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">EnhancedMaskDecoder</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">emd_layers</span> <span class="o">=</span> <span class="n">encoder</span>

<span class="k">class</span> <span class="nc">DeBERTa</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">max_seq</span><span class="p">:</span> <span class="mi">512</span><span class="p">,</span> <span class="n">N</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">24</span><span class="p">,</span> <span class="n">N_EMD</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="n">dim_model</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1024</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">16</span><span class="p">,</span> <span class="n">dim_ffn</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">4096</span><span class="p">,</span> <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
			<span class="c1"># Init Sub-Blocks &amp; Modules
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">DeBERTaEncoder</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">max_seq</span><span class="p">,</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">N</span><span class="p">,</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span><span class="p">,</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">num_heads</span><span class="p">,</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">dim_ffn</span><span class="p">,</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">dropout_prob</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">emd_layers</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="p">.</span><span class="n">encoder</span><span class="p">.</span><span class="n">encoder_layers</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">N_EMD</span><span class="p">)].</span> <span class="c1"># weight share
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">emd_encoder</span> <span class="o">=</span> <span class="n">EnhancedMaskDecoder</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">emd_layers</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span><span class="p">)</span>
</code></pre></div></div>

<p>ë”°ë¼ì„œ <code class="language-plaintext highlighter-rouge">N_EMD=2</code> ë¡œ ì„¤ì •í•œë‹¤ëŠ” ê²ƒì€ ê²°êµ­, <code class="language-plaintext highlighter-rouge">Disentangled-Attention</code> ë ˆì´ì–´ì˜ ê°€ì¥ ë§ˆì§€ë§‰ ì¸ì½”ë” ë¸”ëŸ­ì„ 2ê°œ ë” ìŒ“ëŠ” ê²ƒê³¼ ë™ì¹˜ë‹¤. ëŒ€ì‹  ì¸ì½”ë”ì˜ <code class="language-plaintext highlighter-rouge">linear projection</code> ë ˆì´ì–´ì˜ ì…ë ¥ê°’ì´ ë‹¤ë¥´ë‹¤. <code class="language-plaintext highlighter-rouge">Disentangled-Attention</code> ì˜ í–‰ë ¬ $Q^c, K^c, V^c$ëŠ” ì´ì „ ë¸”ëŸ­ì˜ <code class="language-plaintext highlighter-rouge">hidden_states</code> ê°’ì¸ í–‰ë ¬ $H$ë¥¼ ì…ë ¥ìœ¼ë¡œ, í–‰ë ¬ $Q^r, K^r$ì€ ë ˆì´ì–´ì˜ ìœ„ì¹˜ì— ìƒê´€ì—†ì´ ëª¨ë‘ ê°™ì€ ê°’ì˜ <code class="language-plaintext highlighter-rouge">Relative Position Embedding</code> ì„ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©í•œë‹¤.</p>

<p>ë°˜ë©´, <code class="language-plaintext highlighter-rouge">EMD</code> ë§¨ ì²˜ìŒ ì¸ì½”ë” ë¸”ëŸ­ì˜ í–‰ë ¬ $Q^c$ëŠ” ë°”ë¡œ ì§ì „ ë¸”ëŸ­ì˜ <code class="language-plaintext highlighter-rouge">hidden_states</code> ì— <code class="language-plaintext highlighter-rouge">Absolute Position Embedding</code>ì„ ë”í•œ ê°’ì„ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©í•œë‹¤. ì´í›„ ë‚˜ë¨¸ì§€ ë¸”ëŸ­ì—ëŠ” <code class="language-plaintext highlighter-rouge">Disentangled-Attention</code> ì™€ ë§ˆì°¬ê°€ì§€ë¡œ ì´ì „ ë¸”ëŸ­ì˜ <code class="language-plaintext highlighter-rouge">hidden_states</code> ë¥¼ ì‚¬ìš©í•œë‹¤. í–‰ë ¬ $K^c, V^c$ëŠ” ë¸”ëŸ­ ìˆœì„œì— ìƒê´€ì—†ì´ ì´ì „ ë¸”ëŸ­ì˜ <code class="language-plaintext highlighter-rouge">hidden_states</code> ë§Œ ê°€ì§€ê³  <code class="language-plaintext highlighter-rouge">linear projection</code>ì„ ìˆ˜í–‰í•œë‹¤. ê·¸ë¦¬ê³  í–‰ë ¬ $Q^r, K^r$ ì—­ì‹œ ê°™ì€ ê°’ì˜ <code class="language-plaintext highlighter-rouge">Relative Position Embedding</code> ì„ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©í•œë‹¤.</p>

<p>ì‚¬ì‹¤ í•„ìëŠ” ë…¼ë¬¸ë§Œ ì½ì—ˆì„ ë•Œ <code class="language-plaintext highlighter-rouge">EMD</code>ë„ <code class="language-plaintext highlighter-rouge">Relative Position</code> ì •ë³´ë¥¼ ì£¼ì…í•´ <code class="language-plaintext highlighter-rouge">Disengtanled-Attention</code>ì„ ìˆ˜í–‰í•œë‹¤ê³  ì „í˜€ ìƒê°í•˜ì§€ ëª»í–ˆë‹¤. ì´ëŠ” ë…¼ë¬¸ì˜ ì„¤ëª…ì´ ìƒë‹¹íˆ ë¶ˆì¹œì ˆí•œ ë•ë¶„ì¸ë°, ë…¼ë¬¸ì— ì´ì™€ ê´€ë ¨í•´ì„œ ìì„¸í•œ ì„¤ëª…ë„ ì—†ê³  <code class="language-plaintext highlighter-rouge">Absolute Position Embedding</code>ì„ ì‚¬ìš©í•˜ëŠ” ë ˆì´ì–´ë¼ì„œ ë‹¹ì—°íˆ ì¼ë°˜ì ì¸ <code class="language-plaintext highlighter-rouge">Self-Attention</code>ì„ ì‚¬ìš©í•  ê²ƒì´ë¼ê³  ìƒê°í–ˆë˜ ê²ƒì´ë‹¤.</p>

<p>í•„ìëŠ” ì—¬ê¸°ì„œ <code class="language-plaintext highlighter-rouge">Absolute Position</code>ì´ ì™œ í•„ìš”í•œì§€ë„ ì•Œê² ê³  ê·¸ë˜ì„œ í–‰ë ¬í•©ìœ¼ë¡œ ë”í•´ì„œ ì–´í…ì…˜ì„ ìˆ˜í–‰í•˜ëŠ” ê²ƒë„ ì˜ ì•Œê² ëŠ”ë° ì™œ êµ³ì´ ê°€ì¥ ë§ˆì§€ë§‰ ë ˆì´ì–´ì—ì„œ ì´ê±¸ í• ê¹Œ?? í•˜ëŠ” ì˜ë¬¸ì´ ë“¤ì—ˆë‹¤. ì¼ë°˜ <code class="language-plaintext highlighter-rouge">Self-Attention</code>ì²˜ëŸ¼ ë§¨ ì²˜ìŒì— ë”í•˜ê³  ì‹œì‘í•˜ë©´ ì•ˆë ê¹Œ??</p>

<p>ì €ìì˜ ì‹¤í—˜ì— ë”°ë¥´ë©´ <code class="language-plaintext highlighter-rouge">Absolute Position</code> ì„ ì²˜ìŒì— ì¶”ê°€í•˜ëŠ” ê²ƒë³´ë‹¤ <code class="language-plaintext highlighter-rouge">EMD</code>ì²˜ëŸ¼ ê°€ì¥ ë§ˆì§€ë§‰ì— ë”í•´ì£¼ëŠ”ê²Œ ì„±ëŠ¥ì´ ë” ì¢‹ì•˜ë‹¤ê³  í•œë‹¤. ê·¸ ì´ìœ ë¡œ <code class="language-plaintext highlighter-rouge">Absolute Position</code>ë¥¼ ì´ˆë°˜ì— ì¶”ê°€í•˜ë©´ ëª¨ë¸ì´ <code class="language-plaintext highlighter-rouge">Relative Position</code>ì„ í•™ìŠµí•˜ëŠ”ë° ë°©í•´ê°€ ë˜ëŠ” ê²ƒ ê°™ë‹¤ëŠ” ì¶”ì¸¡ì„ í•¨ê»˜ ì„œìˆ í•˜ê³  ìˆë‹¤. <strong>ê·¸ë ‡ë‹¤ë©´ ì™œ ë°©í•´ê°€ ë˜ëŠ” ê²ƒì¼ê¹Œ??</strong></p>

<p>í•„ìì˜ ë‡Œí”¼ì…œì´ì§€ë§Œ ì´ê²ƒ ì—­ì‹œ <code class="language-plaintext highlighter-rouge">blessing of dimensionality</code> ì—ì„œ íŒŒìƒëœ ë¬¸ì œë¼ê³  ìƒê°í•œë‹¤. ì¼ë‹¨ ìš©ì–´ì˜ ëœ»ë¶€í„° ì•Œì•„ë³´ì. <code class="language-plaintext highlighter-rouge">blessing of dimensionality</code> ë€, ê³ ì°¨ì› ê³µê°„ì—ì„œ ë¬´ì‘ìœ„ë¡œ ì„œë¡œ ë‹¤ë¥¸ ë²¡í„° ë‘ê°œë¥¼ ì„ íƒí•˜ë©´ ë‘ ë²¡í„°ëŠ” ê±°ì˜ ëŒ€ë¶€ë¶„ <code class="language-plaintext highlighter-rouge">approximate orthogonality</code>ë¥¼ ê°–ëŠ” í˜„ìƒì„ ì„¤ëª…í•˜ëŠ” ìš©ì–´ë‹¤. ë¬´ì¡°ê±´ ì„±ë¦½í•˜ëŠ” ì„±ì§ˆì€ ì•„ë‹ˆê³  í™•ë¥ ë¡ ì ì¸ ì ‘ê·¼ì´ë¼ëŠ” ê²ƒì„ ëª…ì‹¬í•˜ì. ì•„ë¬´íŠ¼ ì§êµí•˜ëŠ” ë‘ ë²¡í„°ëŠ” ë‚´ì ê°’ì´ 0ì— ìˆ˜ë ´í•œë‹¤. ì¦‰, ë‘ ë²¡í„°ëŠ” ì„œë¡œì—ê²Œ ì˜í–¥ì„ ë¯¸ì¹˜ì§€ ëª»í•œë‹¤ëŠ” ê²ƒì´ë‹¤.</p>

<p>ì´ê²ƒì€ <code class="language-plaintext highlighter-rouge">Transformer</code>ì—ì„œ <code class="language-plaintext highlighter-rouge">Input Embedding</code>ê³¼ <code class="language-plaintext highlighter-rouge">Absolute Position Embedding</code>ì„ í–‰ë ¬í•©ìœ¼ë¡œ ë”í•´ë„ ì¢‹ì€ í•™ìŠµ ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆëŠ” ì´ìœ ê°€ ëœë‹¤. ë‹¤ì‹œ ë§í•´ì„œ, <code class="language-plaintext highlighter-rouge">hidden states space</code> ì—ì„œ <code class="language-plaintext highlighter-rouge">Input Embedding</code> ê³¼ <code class="language-plaintext highlighter-rouge">Absolute Position Embedding</code> ì—­ì‹œ ê°œë³„ ë²¡í„°ê°€ <code class="language-plaintext highlighter-rouge">span</code> í•˜ëŠ” ë¶€ë¶„ ê³µê°„ ë¼ë¦¬ëŠ” ì„œë¡œ ì§êµí•  ê°€ëŠ¥ì„±ì´ ë§¤ìš° ë†’ë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•œë‹¤. ë”°ë¼ì„œ ì„œë¡œ ë‹¤ë¥¸ ì¶œì²˜ë¥¼ í†µí•´ ë§Œë“¤ì–´ì§„ ë‘ í–‰ë ¬ì„ ë”í•´ë„ ì„œë¡œì—ê²Œ ì˜í–¥ì„ ë¯¸ì¹˜ì§€ ëª»í•  ê²ƒì´ê³  ê·¸ë¡œ ì¸í•´ ëª¨ë¸ì´ <code class="language-plaintext highlighter-rouge">Input</code>ê³¼ <code class="language-plaintext highlighter-rouge">Position</code> ì •ë³´ë¥¼ ë”°ë¡œ ì˜ í•™ìŠµí•  ìˆ˜ ìˆì„ ê²ƒì´ë¼ ê¸°ëŒ€í•´ë³¼ ìˆ˜ ìˆë‹¤.</p>

<p align="center">
<img src="/assets/images/deberta/latent_space.png" alt="hidden states vector space example" class="align-center image-caption" width="60%&quot;, height=&quot;50%" />
<strong><em>hidden states vector space example</em></strong>
</p>

<p>ì´ì œ ë‹¤ì‹œ <code class="language-plaintext highlighter-rouge">DeBERTa</code> ê²½ìš°ë¡œ ëŒì•„ì™€ë³´ì. ìœ„ ê·¸ë¦¼ì˜ íŒŒë€ìƒ‰ ì§ì„ ì„ <code class="language-plaintext highlighter-rouge">Input Embedding</code>, ë¹¨ê°„ìƒ‰ ì§ì„ ì„ <code class="language-plaintext highlighter-rouge">Absolute Position Embedding</code>, ì™¼ìª½ì˜ ë³´ë¼ìƒ‰ ì§ì„ ì„ <code class="language-plaintext highlighter-rouge">Relative Position Embedding</code>ì´ë¼ê³  ê°€ì •í•˜ì. <code class="language-plaintext highlighter-rouge">blessing of dimensionality</code>ì— ì˜í•´ <code class="language-plaintext highlighter-rouge">word con text</code> ì •ë³´(íŒŒë€ ì§ì„ )ì™€ <code class="language-plaintext highlighter-rouge">position</code> ì •ë³´(ë¹¨ê°•, ë³´ë¼ ì§ì„ )ëŠ” ê·¸ë¦¼ì²˜ëŸ¼ ì„œë¡œ ê·¼ì‚¬ ì§êµí•  ê°€ëŠ¥ì„±ì´ ë§¤ìš° ë†’ë‹¤. ì—¬ê¸°ë¶€í„° í•„ìì˜ ë‡Œí”¼ì…œì´ ë“¤ì–´ê°€ëŠ”ë°, ë³´ë¼ìƒ‰ ì§ì„ ê³¼ ë¹¨ê°•ìƒ‰ ì§ì„ ì€ ì„±ê²©ì´ ì¢€ ë‹¤ë¥´ì§€ë§Œ ê²°êµ­ ë‘˜ ë‹¤ ì‹œí€€ìŠ¤ì˜ <code class="language-plaintext highlighter-rouge">position</code> ì •ë³´ë¥¼ ë‚˜íƒ€ë‚¸ë‹¤ëŠ” ì ì—ì„œ ë¿Œë¦¬ëŠ” ê°™ë‹¤ê³  ë³¼ ìˆ˜ ìˆë‹¤. ë”°ë¼ì„œ ì‹¤ì œ <code class="language-plaintext highlighter-rouge">hidden states</code> ê³µê°„ì—ì„œ ì–´ë–¤ ì‹ìœ¼ë¡œ ë§µí•‘ë ì§€ëŠ” ì˜ ëª¨ë¥´ê² ì§€ë§Œ, ì„œë¡œ ì§êµí•˜ëŠ” í˜•íƒœëŠ” ì•„ë‹ ê²ƒì´ë¼ ì¶”ì¸¡í•  ìˆ˜ ìˆë‹¤.</p>

<p>ê·¸ë ‡ë‹¤ë©´ <code class="language-plaintext highlighter-rouge">Absolute Position</code>ì„ ëª¨ë¸ ê·¹ì´ˆë°˜ì— ë”í•´ì¤€ë‹¤ê³  ìƒê°í•´ë³´ì. ì¸ì½”ë”ì— ë“¤ì–´ê°€ëŠ” í–‰ë ¬ì€ ê²°êµ­ ìœ„ ê·¸ë¦¼ì˜ ì´ˆë¡ìƒ‰ ì§ì„ ìœ¼ë¡œ í‘œí˜„ë  ê²ƒì´ë‹¤. íŒŒë€ìƒ‰ ì§ì„ ê³¼ ë¹¨ê°„ìƒ‰ ì§ì„ ì´ ê·¼ì‚¬ ì§êµí•œë‹¤ëŠ” ê°€ì •í•˜ì— ë‘ ë°±í„°ì˜ í•©ì€ ë‘ ë²¡í„°ì˜ 45ë„ ì •ë„ ë˜ëŠ” ê³³ì— ìœ„ì¹˜í•˜ê²Œ(ì´ˆë¡ìƒ‰ ì§ì„ ) ë  ê²ƒì´ë‹¤. ê·¸ë ‡ë‹¤ë©´ ë³´ë¼ìƒ‰ ì§ì„ ê³¼ ì´ˆë¡ìƒ‰ ì§ì„ ì˜ ê´€ê³„ ì—­ì‹œ ê·¼ì‚¬ ì§êµì—ì„œ ì„œë¡œ ê°„ì„­í•˜ëŠ” í˜•íƒœë¡œ ë³€í™”í•œë‹¤. ë”°ì„œ <code class="language-plaintext highlighter-rouge">EMD</code>ë¥¼ ê·¹ì´ˆë°˜ì— ì‚¬ìš©í•˜ë©´ ê°„ì„­ì´ ë°œìƒí•´ ëª¨ë¸ì´ <code class="language-plaintext highlighter-rouge">Relative Position</code> ì •ë³´ë¥¼ ì œëŒ€ë¡œ í•™ìŠµí•˜ì§€ ëª»í•  ê²ƒì´ë‹¤.</p>

<p><strong><code class="language-plaintext highlighter-rouge">ğŸ‘©â€ğŸ’»Â Implementation</code></strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Pytorch Implementation of DeBERTa Enhanced Mask Decoder
</span>
<span class="k">class</span> <span class="nc">EnhancedMaskDecoder</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="s">"""
    Class for Enhanced Mask Decoder module in DeBERTa, which is used for Masked Language Model (Pretrain Task)
    Word 'Decoder' means that denoise masked token by predicting masked token
    In official paper &amp; repo, they might use 2 EMD layers for MLM Task
    And this layer's key &amp; value input is output from last disentangled self-attention encoder layer,
    Also, all of them can share parameters and this layer also do disentangled self-attention
    In official repo, they implement this layer so hard coding that we can't understand directly &amp; easily
    So, we implement this layer with our own style, as closely as possible to paper statement
    Notes:
        Also we temporarily implement only extract token embedding, not calculating logit, loss for MLM Task yet
        MLM Task will be implemented ASAP
    Args:
        encoder: list of nn.ModuleList, which is (N_EMD * last encoder layer) from DeBERTaEncoder
    References:
        https://arxiv.org/abs/2006.03654
        https://arxiv.org/abs/2111.09543
        https://github.com/microsoft/DeBERTa/blob/master/DeBERTa/apps/models/masked_language_model.py
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">encoder</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">nn</span><span class="p">.</span><span class="n">ModuleList</span><span class="p">],</span> <span class="n">dim_model</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1024</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">EnhancedMaskDecoder</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">emd_layers</span> <span class="o">=</span> <span class="n">encoder</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">layer_norm</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">dim_model</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">emd_context_layer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">,</span> <span class="n">abs_pos_emb</span><span class="p">,</span> <span class="n">rel_pos_emb</span><span class="p">,</span> <span class="n">mask</span><span class="p">):</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">query_states</span> <span class="o">=</span> <span class="n">hidden_states</span> <span class="o">+</span> <span class="n">abs_pos_emb</span>  <span class="c1"># "I" in official paper,
</span>        <span class="k">for</span> <span class="n">emd_layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">emd_layers</span><span class="p">:</span>
            <span class="n">query_states</span> <span class="o">=</span> <span class="n">emd_layer</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">pos_x</span><span class="o">=</span><span class="n">rel_pos_emb</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="n">mask</span><span class="p">,</span> <span class="n">emd</span><span class="o">=</span><span class="n">query_states</span><span class="p">)</span>
            <span class="n">outputs</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">query_states</span><span class="p">)</span>
        <span class="n">last_hidden_state</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">layer_norm</span><span class="p">(</span><span class="n">query_states</span><span class="p">)</span>  <span class="c1"># because of applying pre-layer norm
</span>        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">stack</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">).</span><span class="n">to</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">last_hidden_state</span><span class="p">,</span> <span class="n">hidden_states</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">abs_pos_emb</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">rel_pos_emb</span><span class="p">,</span> <span class="n">mask</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
        <span class="s">"""
        hidden_states: output from last disentangled self-attention encoder layer
        abs_pos_emb: absolute position embedding
        rel_pos_emb: relative position embedding
        """</span>
        <span class="n">last_hidden_state</span><span class="p">,</span> <span class="n">emd_hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">emd_context_layer</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">abs_pos_emb</span><span class="p">,</span> <span class="n">rel_pos_emb</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">last_hidden_state</span><span class="p">,</span> <span class="n">emd_hidden_states</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">emd_context_layer</code> ë©”ì„œë“œì—ì„œ <code class="language-plaintext highlighter-rouge">Absolute Position</code> ì •ë³´ë¥¼ ì¶”ê°€í•´ì£¼ëŠ” ë¶€ë¶„ì„ ì œì™¸í•˜ë©´ ì¼ë°˜ <code class="language-plaintext highlighter-rouge">Encoder</code> ê°ì²´ì˜ ë™ì‘ê³¼ ë™ì¼í•˜ë‹¤. ë˜í•œ DeBERTaëŠ” ëª¨ë“  ë ˆì´ì–´ê°€ ê°™ì€ ì‹œì ì˜ forward pass ë•Œ, ë™ì¼í•œ ê°€ì¤‘ì¹˜ì˜ <code class="language-plaintext highlighter-rouge">Relative Position Embedding</code>ì„ ì‚¬ìš©í•´ì•¼ í•˜ëŠ”ë°, <code class="language-plaintext highlighter-rouge">EMD</code> ì—­ì‹œ ì˜ˆì™¸ëŠ” ì•„ë‹ˆê¸° ë•Œë¬¸ì— ë°˜ë“œì‹œ ìµœìƒìœ„ ê°ì²´ì—ì„œ ì´ˆê¸°í™”í•œ <code class="language-plaintext highlighter-rouge">Relative Position Embedding</code>ì„ ë˜‘ê°™ì´ ë§¤ê°œë³€ìˆ˜ë¡œ ì „ë‹¬í•´ì¤˜ì•¼ í•œë‹¤.</p>

<p>ê·¸ë¦¬ê³  ë§ˆì§€ë§‰ìœ¼ë¡œ ê°ì²´ì—ì„œ ì‚¬ìš©í•˜ëŠ” <code class="language-plaintext highlighter-rouge">emd_layers</code> ëŠ” ëª¨ë‘ <code class="language-plaintext highlighter-rouge">Disentangled-Attention</code> ë ˆì´ì–´ì˜ ê°€ì¥ ë§ˆì§€ë§‰ ì¸ì½”ë”ë¼ëŠ” ì‚¬ì‹¤ì„ ìŠì§€ ë§ì.</p>

<h4 id="multi-head-attention"><strong><code class="language-plaintext highlighter-rouge">ğŸ‘©â€ğŸ‘©â€ğŸ‘§â€ğŸ‘¦Â Multi-Head Attention</code></strong></h4>

<p>ì´ì œ ë‚˜ë¨¸ì§€ ë¸”ëŸ­ë“¤ì— ëŒ€í•´ì„œ ì‚´í´ë³´ê² ë‹¤. ì›ë¦¬ë‚˜ ì˜ë¯¸ëŠ” ì´ë¯¸ <code class="language-plaintext highlighter-rouge">Transformer</code> ë¦¬ë·°ì—ì„œ ëª¨ë‘ ì‚´í´ë´¤ê¸° ë•Œë¬¸ì— ìƒëµí•˜ê³ , êµ¬í˜„ìƒ íŠ¹ì´ì ì— ëŒ€í•´ì„œë§Œ ì–¸ê¸‰í•˜ë ¤ê³  í•œë‹¤. ë¨¼ì € <code class="language-plaintext highlighter-rouge">Single-Head Atttention</code> ì½”ë“œë¥¼ ë³´ì.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Pytorch Implementation of Single Attention Head
</span>
<span class="k">class</span> <span class="nc">AttentionHead</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="s">"""
    In this class, we implement workflow of single attention head in DeBERTa-Large
    This class has same role as Module "BertAttention" in official Repo (bert.py)
    Args:
        dim_model: dimension of model's latent vector space, default 1024 from official paper
        dim_head: dimension of each attention head, default 64 from official paper (1024 / 16)
        dropout: dropout rate for attention matrix, default 0.1 from official paper
    Math:
        Attention Matrix = c2c + c2p + p2c
        A = softmax(Attention Matrix/sqrt(3*D_h)), SA(z) = Av
    Reference:
        https://arxiv.org/abs/1706.03762
        https://arxiv.org/abs/2006.03654
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim_model</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1024</span><span class="p">,</span> <span class="n">dim_head</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">64</span><span class="p">,</span> <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">AttentionHead</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span> <span class="o">=</span> <span class="n">dim_model</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dim_head</span> <span class="o">=</span> <span class="n">dim_head</span>  <span class="c1"># 1024 / 16 = 64
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">dot_scale</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">dim_head</span><span class="p">))</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">fc_q</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">dim_head</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">fc_k</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">dim_head</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">fc_v</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">dim_head</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">fc_qr</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">dim_head</span><span class="p">)</span>  <span class="c1"># projector for Relative Position Query matrix
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">fc_kr</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">dim_head</span><span class="p">)</span>  <span class="c1"># projector for Relative Position Key matrix
</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">pos_x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">mask</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">emd</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="bp">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">qr</span><span class="p">,</span> <span class="n">kr</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">fc_q</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="bp">self</span><span class="p">.</span><span class="n">fc_k</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="bp">self</span><span class="p">.</span><span class="n">fc_v</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="bp">self</span><span class="p">.</span><span class="n">fc_qr</span><span class="p">(</span><span class="n">pos_x</span><span class="p">),</span> <span class="bp">self</span><span class="p">.</span><span class="n">fc_kr</span><span class="p">(</span><span class="n">pos_x</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">emd</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">q</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">fc_q</span><span class="p">(</span><span class="n">emd</span><span class="p">)</span>
        <span class="n">attention_matrix</span> <span class="o">=</span> <span class="n">disentangled_attention</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">qr</span><span class="p">,</span> <span class="n">kr</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">dropout</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">attention_matrix</span>
</code></pre></div></div>

<p>ë™ì‘ ìì²´ëŠ” ë™ì¼í•˜ì§€ë§Œ, ìƒëŒ€ ìœ„ì¹˜ ì •ë³´ì— ëŒ€í•œ <code class="language-plaintext highlighter-rouge">linear projection</code> ë ˆì´ì–´ê°€ ì¶”ê°€ ë˜ì—ˆë‹¤. ê·¸ë¦¬ê³  <code class="language-plaintext highlighter-rouge">Enhanced Mask Decoder</code> ë¥¼ ìœ„í•´ <code class="language-plaintext highlighter-rouge">forward</code> ë©”ì„œë“œì— ì¡°ê±´ë¬¸ì„ í™œìš©í•˜ì—¬ <code class="language-plaintext highlighter-rouge">Decoding</code>í•˜ëŠ” ì‹œì ì—ëŠ” <code class="language-plaintext highlighter-rouge">hidden_states + absolute position embedding</code> ìœ¼ë¡œ í–‰ë ¬ $Q^c$ë¥¼ í‘œí˜„í•˜ê²Œ êµ¬í˜„í–ˆë‹¤. ì´ë ‡ê²Œ êµ¬í˜„í•˜ë©´ <code class="language-plaintext highlighter-rouge">EMD</code> ë¥¼ ìœ„í•´ ë”°ë¡œ <code class="language-plaintext highlighter-rouge">AttentionHead</code>ë¥¼ êµ¬í˜„í•  í•„ìš”ê°€ ì—†ì–´ì„œ ì½”ë“œ ê°„ì†Œí™”ê°€ ëœë‹¤.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Pytorch Implementation of Multi-Head Attention
</span>
<span class="k">class</span> <span class="nc">MultiHeadAttention</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="s">"""
    In this class, we implement workflow of Multi-Head Self-Attention for DeBERTa-Large
    This class has same role as Module "BertAttention" in official Repo (bert.py)
    In official repo, they use post-layer norm, but we use pre-layer norm which is more stable &amp; efficient for training
    Args:
        dim_model: dimension of model's latent vector space, default 1024 from official paper
        num_heads: number of heads in MHSA, default 16 from official paper for Transformer
        dim_head: dimension of each attention head, default 64 from official paper (1024 / 16)
        dropout: dropout rate, default 0.1
    Math:
        Attention Matrix = c2c + c2p + p2c
        A = softmax(Attention Matrix/sqrt(3*D_h)), SA(z) = Av
    Reference:
        https://arxiv.org/abs/1706.03762
        https://arxiv.org/abs/2006.03654
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim_model</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1024</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">16</span><span class="p">,</span> <span class="n">dim_head</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">64</span><span class="p">,</span> <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MultiHeadAttention</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span> <span class="o">=</span> <span class="n">dim_model</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dim_head</span> <span class="o">=</span> <span class="n">dim_head</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">dropout</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">attention_heads</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">ModuleList</span><span class="p">(</span>
            <span class="p">[</span><span class="n">AttentionHead</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">dim_head</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">dropout</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">num_heads</span><span class="p">)]</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">fc_concat</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">rel_pos_emb</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">mask</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">emd</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="bp">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="s">""" x is already passed nn.Layernorm """</span>
        <span class="k">assert</span> <span class="n">x</span><span class="p">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">3</span><span class="p">,</span> <span class="sa">f</span><span class="s">'Expected (batch, seq, hidden) got </span><span class="si">{</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="s">'</span>
        <span class="n">attention_output</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">fc_concat</span><span class="p">(</span>
            <span class="n">torch</span><span class="p">.</span><span class="n">cat</span><span class="p">([</span><span class="n">head</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">rel_pos_emb</span><span class="p">,</span> <span class="n">mask</span><span class="p">,</span> <span class="n">emd</span><span class="p">)</span> <span class="k">for</span> <span class="n">head</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">attention_heads</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">attention_output</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">MultiHeadAttention</code> ê°ì²´ëŠ” ë‹¨ì¼ <code class="language-plaintext highlighter-rouge">AttentionHead</code> ê°ì²´ë¥¼ í˜¸ì¶œí•  ë•Œ <code class="language-plaintext highlighter-rouge">rel_pos_emb</code> ë¥¼ ë§¤ê°œë³€ìˆ˜ë¡œ ì „ë‹¬í•´ì•¼ í•œë‹¤ëŠ” ì ë§Œ ê¸°ì–µí•˜ë©´ ëœë‹¤.</p>

<h4 id="feed-forward-network"><strong><code class="language-plaintext highlighter-rouge">ğŸ”¬Â Feed Forward Network</code></strong></h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Pytorch Implementation of FeedForward Network
</span>
<span class="k">class</span> <span class="nc">FeedForward</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="s">"""
    Class for Feed-Forward Network module in transformer
    In official paper, they use ReLU activation function, but GELU is better for now
    We change ReLU to GELU &amp; add dropout layer
    Args:
        dim_model: dimension of model's latent vector space, default 512
        dim_ffn: dimension of FFN's hidden layer, default 2048 from official paper
        dropout: dropout rate, default 0.1
    Math:
        FeedForward(x) = FeedForward(LN(x))+x
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim_model</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">512</span><span class="p">,</span> <span class="n">dim_ffn</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2048</span><span class="p">,</span> <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">FeedForward</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">ffn</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">dim_model</span><span class="p">,</span> <span class="n">dim_ffn</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">GELU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">dropout</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">dim_ffn</span><span class="p">,</span> <span class="n">dim_model</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">dropout</span><span class="p">),</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">ffn</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</code></pre></div></div>

<p>ì—­ì‹œ ê¸°ì¡´ <code class="language-plaintext highlighter-rouge">Transformer</code>, <code class="language-plaintext highlighter-rouge">BERT</code>ì™€ ë‹¤ë¥¸ê²Œ ì—†ë‹¤.</p>

<h4 id="debertaencoderlayer"><strong><code class="language-plaintext highlighter-rouge">ğŸ“˜Â DeBERTaEncoderLayer</code></strong></h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Pytorch Implementation of DeBERTaEncoderLayer(single Disentangled-Attention Encoder Block)
</span>
<span class="k">class</span> <span class="nc">DeBERTaEncoderLayer</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="s">"""
    Class for encoder model module in DeBERTa-Large
    In this class, we stack each encoder_model module (Multi-Head Attention, Residual-Connection, LayerNorm, FFN)
    This class has same role as Module "BertEncoder" in official Repo (bert.py)
    In official repo, they use post-layer norm, but we use pre-layer norm which is more stable &amp; efficient for training
    References:
        https://github.com/microsoft/DeBERTa/blob/master/DeBERTa/deberta/bert.py
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim_model</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1024</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">16</span><span class="p">,</span> <span class="n">dim_ffn</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">4096</span><span class="p">,</span> <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">DeBERTaEncoderLayer</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">self_attention</span> <span class="o">=</span> <span class="n">MultiHeadAttention</span><span class="p">(</span>
            <span class="n">dim_model</span><span class="p">,</span>
            <span class="n">num_heads</span><span class="p">,</span>
            <span class="nb">int</span><span class="p">(</span><span class="n">dim_model</span> <span class="o">/</span> <span class="n">num_heads</span><span class="p">),</span>
            <span class="n">dropout</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">layer_norm1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">dim_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">layer_norm2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">dim_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">dropout</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">ffn</span> <span class="o">=</span> <span class="n">FeedForward</span><span class="p">(</span>
            <span class="n">dim_model</span><span class="p">,</span>
            <span class="n">dim_ffn</span><span class="p">,</span>
            <span class="n">dropout</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">pos_x</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Embedding</span><span class="p">,</span> <span class="n">mask</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">emd</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="bp">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="s">""" rel_pos_emb is fixed for all layer in same forward pass time """</span>
        <span class="n">ln_x</span><span class="p">,</span> <span class="n">ln_pos_x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">layer_norm1</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="bp">self</span><span class="p">.</span><span class="n">layer_norm1</span><span class="p">(</span><span class="n">pos_x</span><span class="p">)</span>  <span class="c1"># pre-layer norm, weight share
</span>        <span class="n">residual_x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">dropout</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">self_attention</span><span class="p">(</span><span class="n">ln_x</span><span class="p">,</span> <span class="n">ln_pos_x</span><span class="p">,</span> <span class="n">mask</span><span class="p">,</span> <span class="n">emd</span><span class="p">))</span> <span class="o">+</span> <span class="n">x</span>

        <span class="n">ln_x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">layer_norm2</span><span class="p">(</span><span class="n">residual_x</span><span class="p">)</span>
        <span class="n">fx</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">ffn</span><span class="p">(</span><span class="n">ln_x</span><span class="p">)</span> <span class="o">+</span> <span class="n">residual_x</span>
        <span class="k">return</span> <span class="n">fx</span>
</code></pre></div></div>

<p>official codeì™€ ë‹¤ë¥´ê²Œ <code class="language-plaintext highlighter-rouge">pre-layernorm</code> ì„ ì‚¬ìš©í•´ êµ¬í˜„í–ˆë‹¤. <code class="language-plaintext highlighter-rouge">pre-layernorm</code>ì— ëŒ€í•´ ê¶ê¸ˆí•˜ë‹¤ë©´ <a href="https://qcqced123.github.io/nlp/transformer#encoderlayer"><strong><em>ì—¬ê¸°</em></strong></a>ë¥¼ í´ë¦­í•´ í™•ì¸í•´ë³´ì.</p>

<h4 id="debertaencoder"><strong><code class="language-plaintext highlighter-rouge">ğŸ“šÂ DeBERTaEncoder</code></strong></h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Pytorch Implementation of DeBERTaEncoderr(N stacked DeBERTaEncoderLayer)
</span>
<span class="k">class</span> <span class="nc">DeBERTaEncoder</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="s">"""
    In this class, 1) encode input sequence, 2) make relative position embedding, 3) stack N DeBERTaEncoderLayer
    This class's forward output is not integrated with EMD Layer's output
    Output have ONLY result of disentangled self-attention
    All of ops order is from official paper &amp; repo by microsoft, but ops operating is slightly different,
    Because they use custom ops, e.g. XDropout, XSoftmax, ..., we just apply pure pytorch ops
    Args:
        max_seq: maximum sequence length, named "max_position_embedding" in official repo, default 512
                 in official paper, this value is called 'k'
        N: number of EncoderLayer, default 24 for large model
    Notes:
        self.rel_pos_emb: P in paper, this matrix is fixed during forward pass in same time,
                          all layer &amp; all module must share this layer from official paper
    References:
        https://github.com/microsoft/DeBERTa/blob/master/DeBERTa/deberta/ops.py
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">max_seq</span><span class="p">:</span> <span class="mi">512</span><span class="p">,</span> <span class="n">N</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">24</span><span class="p">,</span> <span class="n">dim_model</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1024</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">16</span><span class="p">,</span> <span class="n">dim_ffn</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">4096</span><span class="p">,</span> <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">DeBERTaEncoder</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">max_seq</span> <span class="o">=</span> <span class="n">max_seq</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">num_layers</span> <span class="o">=</span> <span class="n">N</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span> <span class="o">=</span> <span class="n">dim_model</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dim_ffn</span> <span class="o">=</span> <span class="n">dim_ffn</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">dropout</span><span class="p">)</span>  <span class="c1"># dropout is not learnable
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">encoder_layers</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">ModuleList</span><span class="p">(</span>
            <span class="p">[</span><span class="n">DeBERTaEncoderLayer</span><span class="p">(</span><span class="n">dim_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">dim_ffn</span><span class="p">,</span> <span class="n">dropout</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">num_layers</span><span class="p">)]</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">layer_norm</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">dim_model</span><span class="p">)</span>  <span class="c1"># for final-Encoder output
</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">rel_pos_emb</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">mask</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
        <span class="s">"""
        inputs: embedding from input sequence
        rel_pos_emb: relative position embedding
        mask: mask for Encoder padded token for speeding up to calculate attention score
        """</span>
        <span class="n">layer_output</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">pos_x</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">rel_pos_emb</span>  <span class="c1"># x is same as word_embeddings or embeddings in official repo
</span>        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">encoder_layers</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">pos_x</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span>
            <span class="n">layer_output</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">last_hidden_state</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">layer_norm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># because of applying pre-layer norm
</span>        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">stack</span><span class="p">(</span><span class="n">layer_output</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">).</span><span class="n">to</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>  <span class="c1"># shape: [N, BS, SEQ_LEN, DIM_Model]
</span>        <span class="k">return</span> <span class="n">last_hidden_state</span><span class="p">,</span> <span class="n">hidden_states</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">EMD</code>ì™€ ë§ˆì°¬ê°€ì§€ë¡œ ë ˆì´ì–´ì˜ ìœ„ì¹˜ì— ìƒê´€ì—†ì´ ê°™ì€ ì‹œì ì—ëŠ” ëª¨ë‘ ë™ì¼í•œ <code class="language-plaintext highlighter-rouge">Relative Position Embedding</code> ì„ ì‚¬ìš©í•´ <code class="language-plaintext highlighter-rouge">linear projection</code> í•˜ë„ë¡ êµ¬í˜„í•´ì£¼ëŠ” ê²ƒì´ ì¤‘ìš” í¬ì¸íŠ¸ë‹¤. <code class="language-plaintext highlighter-rouge">forward</code> ë©”ì„œë“œë¥¼ í™•ì¸í•˜ì!</p>

<h4 id="deberta"><strong><code class="language-plaintext highlighter-rouge">ğŸ¤–Â DeBERTa</code></strong></h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Pytorch Implementation of DeBERTa
</span>
<span class="k">class</span> <span class="nc">DeBERTa</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="s">"""
    Main class for DeBERTa, having all of sub-blocks &amp; modules such as Disentangled Self-Attention, DeBERTaEncoder, EMD
    Init Scale of DeBERTa Hyper-Parameters, Embedding Layer, Encoder Blocks, EMD Blocks
    And then make 3-types of Embedding Layer, Word Embedding, Absolute Position Embedding, Relative Position Embedding
    Args:
        max_seq: maximum sequence length
        N: number of Disentangled-Encoder layers
        N_EMD: number of EMD layers
        dim_model: dimension of model
        num_heads: number of heads in multi-head attention
        dim_ffn: dimension of feed-forward network, same as intermediate size in official repo
        dropout: dropout rate
    Notes:
        MLM Task is not implemented yet, will be implemented ASAP, but you can get token encode output (embedding)
    References:
        https://arxiv.org/abs/2006.03654
        https://arxiv.org/abs/2111.09543
        https://github.com/microsoft/DeBERTa/blob/master/experiments/language_model/deberta_xxlarge.json
        https://github.com/microsoft/DeBERTa/blob/master/DeBERTa/deberta/config.py
        https://github.com/microsoft/DeBERTa/blob/master/DeBERTa/deberta/deberta.py
        https://github.com/microsoft/DeBERTa/blob/master/DeBERTa/deberta/bert.py
        https://github.com/microsoft/DeBERTa/blob/master/DeBERTa/deberta/disentangled_attention.py
        https://github.com/microsoft/DeBERTa/blob/master/DeBERTa/apps/models/masked_language_model.py
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span>
            <span class="n">vocab_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
            <span class="n">max_seq</span><span class="p">:</span> <span class="mi">512</span><span class="p">,</span>
            <span class="n">N</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">24</span><span class="p">,</span>
            <span class="n">N_EMD</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span>
            <span class="n">dim_model</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1024</span><span class="p">,</span>
            <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">16</span><span class="p">,</span>
            <span class="n">dim_ffn</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">4096</span><span class="p">,</span>
            <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">DeBERTa</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="c1"># Init Scale of DeBERTa
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">max_seq</span> <span class="o">=</span> <span class="n">max_seq</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">max_rel_pos</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">max_seq</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">N</span> <span class="o">=</span> <span class="n">N</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">N_EMD</span> <span class="o">=</span> <span class="n">N_EMD</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span> <span class="o">=</span> <span class="n">dim_model</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dim_ffn</span> <span class="o">=</span> <span class="n">dim_ffn</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dropout_prob</span> <span class="o">=</span> <span class="n">dropout</span>

        <span class="c1"># Init Embedding Layer
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">word_embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">dim_model</span><span class="p">)</span>  <span class="c1"># Word Embedding which is not add Absolute Position
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">rel_pos_emb</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Embedding</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">max_rel_pos</span><span class="p">,</span> <span class="n">dim_model</span><span class="p">)</span>  <span class="c1"># Relative Position Embedding
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">abs_pos_emb</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">max_seq</span><span class="p">,</span> <span class="n">dim_model</span><span class="p">)</span>  <span class="c1"># Absolute Position Embedding for EMD Layer
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">layer_norm1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">dim_model</span><span class="p">)</span>  <span class="c1"># for word embedding
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">layer_norm2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">dim_model</span><span class="p">)</span>  <span class="c1"># for rel_pos_emb
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">dropout_prob</span><span class="p">)</span>

        <span class="c1"># Init Sub-Blocks &amp; Modules
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">DeBERTaEncoder</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">max_seq</span><span class="p">,</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">N</span><span class="p">,</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span><span class="p">,</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">num_heads</span><span class="p">,</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">dim_ffn</span><span class="p">,</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">dropout_prob</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">emd_layers</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="p">.</span><span class="n">encoder</span><span class="p">.</span><span class="n">encoder_layers</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">N_EMD</span><span class="p">)]</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">emd_encoder</span> <span class="o">=</span> <span class="n">EnhancedMaskDecoder</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">emd_layers</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">mask</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">):</span>
        <span class="k">assert</span> <span class="n">inputs</span><span class="p">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">3</span><span class="p">,</span> <span class="sa">f</span><span class="s">'Expected (batch, sequence, vocab_size) got </span><span class="si">{</span><span class="n">inputs</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="s">'</span>
        <span class="c1"># Embedding Layer
</span>        <span class="n">word_embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">dropout</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">layer_norm1</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">word_embedding</span><span class="p">(</span><span class="n">inputs</span><span class="p">))</span>
        <span class="p">)</span>
        <span class="n">rel_pos_emb</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">dropout</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">layer_norm2</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">rel_pos_emb</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">max_rel_pos</span><span class="p">).</span><span class="n">repeat</span><span class="p">(</span><span class="n">inputs</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]).</span><span class="n">to</span><span class="p">(</span><span class="n">inputs</span><span class="p">)))</span>
        <span class="p">)</span>
        <span class="n">abs_pos_emb</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">abs_pos_emb</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">max_seq</span><span class="p">).</span><span class="n">repeat</span><span class="p">(</span><span class="n">inputs</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]).</span><span class="n">to</span><span class="p">(</span><span class="n">inputs</span><span class="p">))</span>  <span class="c1"># "I" in paper
</span>
        <span class="c1"># Disentangled Self-Attention Encoder Layer
</span>        <span class="n">last_hidden_state</span><span class="p">,</span> <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">word_embeddings</span><span class="p">,</span> <span class="n">rel_pos_emb</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span>

        <span class="c1"># Enhanced Mask Decoder Layer
</span>        <span class="n">emd_hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span>
        <span class="n">emd_last_hidden_state</span><span class="p">,</span> <span class="n">emd_hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">emd_encoder</span><span class="p">(</span><span class="n">emd_hidden_states</span><span class="p">,</span> <span class="n">abs_pos_emb</span><span class="p">,</span> <span class="n">rel_pos_emb</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">last_hidden_state</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">,</span> <span class="n">emd_last_hidden_state</span><span class="p">,</span> <span class="n">emd_hidden_states</span>
</code></pre></div></div>]]></content><author><name>qcqced</name><email>qcqced123@gmail.com</email></author><category term="NLP" /><category term="Natural Language Process" /><category term="DeBERTa" /><category term="BERT" /><category term="RoBERTa" /><category term="Transformer" /><category term="Self-Attention" /><category term="Disentangled-Attention" /><category term="Relative Position Embedding" /><category term="EMD" /><category term="Encoder" /><summary type="html"><![CDATA[Transformer Official Paper Review with Pytorch Implementation]]></summary></entry><entry><title type="html">ğŸ¤–Â [Transformer] Attention Is All You Need</title><link href="http://localhost:4000/nlp/transformer" rel="alternate" type="text/html" title="ğŸ¤–Â [Transformer] Attention Is All You Need" /><published>2023-08-04T00:00:00+09:00</published><updated>2023-08-04T01:00:00+09:00</updated><id>http://localhost:4000/nlp/Transformer</id><content type="html" xml:base="http://localhost:4000/nlp/transformer"><![CDATA[<h3 id="overview"><code class="language-plaintext highlighter-rouge">ğŸ”­Â Overview</code></h3>

<p><code class="language-plaintext highlighter-rouge">Transformer</code>ëŠ” 2017ë…„ Googleì´ NIPSì—ì„œ ë°œí‘œí•œ ìì—°ì–´ ì²˜ë¦¬ìš© ì‹ ê²½ë§ìœ¼ë¡œ ê¸°ì¡´ <code class="language-plaintext highlighter-rouge">RNN</code> ê³„ì—´(LSTM, GRU) ì‹ ê²½ë§ì´ ê°€ì§„ ë¬¸ì œë¥¼ í•´ê²°í•˜ê³  ìµœëŒ€í•œ ì¸ê°„ì˜ ìì—°ì–´ ì´í•´ ë°©ì‹ì„ ìˆ˜í•™ì ìœ¼ë¡œ ëª¨ë¸ë§ í•˜ë ¤ëŠ” ì˜ë„ë¡œ ì„¤ê³„ ë˜ì—ˆë‹¤. ì´ ëª¨ë¸ì€ ì´ˆê¸° <code class="language-plaintext highlighter-rouge">Encoder-Decoder</code> ë¥¼ ëª¨ë‘ ê°–ì¶˜ <code class="language-plaintext highlighter-rouge">seq2seq</code> í˜•íƒœë¡œ ê³ ì•ˆ ë˜ì—ˆìœ¼ë©°, ë‹¤ì–‘í•œ ë²ˆì—­ í…ŒìŠ¤í¬ì—ì„œ <code class="language-plaintext highlighter-rouge">SOTA</code>ë¥¼ ë‹¬ì„±í•´ ì£¼ëª©ì„ ë°›ì•˜ë‹¤. ì´í›„ì—ëŠ” ì—¬ëŸ¬ë¶„ë„ ì˜ ì•„ì‹œëŠ” ê²ƒì²˜ëŸ¼  <code class="language-plaintext highlighter-rouge">BERT</code>, <code class="language-plaintext highlighter-rouge">GPT</code>, <code class="language-plaintext highlighter-rouge">ViT</code>ì˜ ë² ì´ìŠ¤ ë¼ì¸ìœ¼ë¡œ ì±„íƒ ë˜ë©°, í˜„ëŒ€ ë”¥ëŸ¬ë‹ ì—­ì‚¬ì— í•œ íšì„ ê·¸ì€ ëª¨ë¸ë¡œ í‰ê°€ ë°›ê³  ìˆë‹¤.</p>

<p>í˜„ëŒ€ ë”¥ëŸ¬ë‹ì˜ ì „ì„±ê¸°ë¥¼ ì—´ì–´ì¤€ <code class="language-plaintext highlighter-rouge">Transformer</code>ëŠ” ì–´ë–¤ ì•„ì´ë””ì–´ë¡œ ê¸°ì¡´ <code class="language-plaintext highlighter-rouge">Recurrent</code> ê³„ì—´ì´ ê°€ì¡Œë˜ ë¬¸ì œë“¤ì„ í•´ê²°í–ˆì„ê¹Œ?? ì´ê²ƒì„ ì œëŒ€ë¡œ ì´í•´í•˜ë ¤ë©´ ë¨¼ì € ê¸°ì¡´ ìˆœí™˜ ì‹ ê²½ë§ ëª¨ë¸ë“¤ì´ ê°€ì¡Œë˜ ë¬¸ì œë¶€í„° ì§šê³  ë„˜ì–´ê°ˆ í•„ìš”ê°€ ìˆë‹¤.</p>

<h3 id="limitation-of-recurrent-structure"><strong><code class="language-plaintext highlighter-rouge">ğŸ¤”Â Limitation of Recurrent Structure</code></strong></h3>

<ul>
  <li><strong>1) ì¸ê°„ê³¼ ë‹¤ë¥¸ ë©”ì»¤ë‹ˆì¦˜ì˜ Vanishing Gradient ë°œìƒ (Activation Function with Backward)</strong></li>
  <li><strong>2) ì ì  íë ¤ì§€ëŠ” Inputsì— Attention (Activation Function with Forward)</strong></li>
  <li><strong>3) ë””ì½”ë”ê°€ ê°€ì¥ ë§ˆì§€ë§‰ ë‹¨ì–´ë§Œ ì—´ì‹¬íˆ ë³´ê³  <code class="language-plaintext highlighter-rouge">denoising</code> ìˆ˜í–‰ (Seq2Seq with Bi-Directional RNN)</strong></li>
</ul>

<p><strong><code class="language-plaintext highlighter-rouge">ğŸ“ˆÂ 1) ì¸ê°„ê³¼ ë‹¤ë¥¸ ë©”ì»¤ë‹ˆì¦˜ì˜ Vanishing Gradient ë°œìƒ (Activation Function with Backward)</code></strong></p>

\[h(t) = tanh(x_tW_x + h_{t-1}W_h + b)\]

<p><code class="language-plaintext highlighter-rouge">RNN</code>ì˜ í™œì„± í•¨ìˆ˜ì¸ <code class="language-plaintext highlighter-rouge">Hyperbolic Tangent</code> ëŠ” $y$ê°’ì´ <code class="language-plaintext highlighter-rouge">[-1, 1]</code> ì‚¬ì´ì—ì„œ ì •ì˜ë˜ë©° ê¸°ìš¸ê¸°ì˜ ìµœëŒ€ê°’ì€ 1ì´ë‹¤. ë”°ë¼ì„œ ì´ì „ ì‹œì  ì •ë³´ëŠ” ì‹œì ì´ ì§€ë‚˜ë©´ ì§€ë‚ ìˆ˜ë¡ (ë” ë§ì€ ì…€ì„ í†µê³¼í• ìˆ˜ë¡) ê·¸ë¼ë””ì–¸íŠ¸ ê°’ì´ ì‘ì•„ì ¸ ë¯¸ë˜ ì‹œì ì˜ í•™ìŠµì— ë§¤ìš° ì‘ì€ ì˜í–¥ë ¥ì„ ê°–ê²Œ ëœë‹¤. ì´ê²ƒì´ ë°”ë¡œ ê·¸ ìœ ëª…í•œ <code class="language-plaintext highlighter-rouge">RNN</code>ì˜ <code class="language-plaintext highlighter-rouge">Vanishing Gradient</code> í˜„ìƒì´ë‹¤. ì‚¬ì‹¤ í˜„ìƒì˜ ë°œìƒ ìì²´ëŠ” ê·¸ë ‡ê²Œ í° ë¬¸ì œê°€ ë˜ì§€ ì•ŠëŠ”ë‹¤. <code class="language-plaintext highlighter-rouge">RNN</code>ì—ì„œ ë°œìƒí•˜ëŠ” <code class="language-plaintext highlighter-rouge">Vanishing Gradient</code> ê°€ ë¬¸ì œê°€ ë˜ëŠ” ì´ìœ ëŠ” ë°”ë¡œ ì¸ê°„ì´ ìì—°ì–´ë¥¼ ì´í•´í•˜ëŠ” ë©”ì»¤ë‹ˆì¦˜ê³¼ ë‹¤ë¥¸ ë°©ì‹ìœ¼ë¡œ í˜„ìƒì´ ë°œìƒí•˜ê¸° ë•Œë¬¸ì´ë‹¤. ìš°ë¦¬ê°€ ê¸€ì„ ì½ëŠ” ê³¼ì •ì„ ì˜ ë– ì˜¬ë ¤ ë³´ì. ì–´ë–¤ ë‹¨ì–´ì˜ ì˜ë¯¸ë¥¼ ì•Œê¸° ìœ„í•´ ê°€ê¹Œìš´ ì£¼ë³€ ë‹¨ì–´ì˜ ë¬¸ë§¥ì„ í™œìš©í•  ë•Œë„ ìˆì§€ë§Œ, ì € ë©€ë¦¬ ë–¨ì–´ì§„ ë¬¸ë‹¨ì˜ ë¬¸ë§¥ì„ í™œìš©í•  ë•Œë„ ìˆë‹¤. ì´ì²˜ëŸ¼ ë‹¨ì–´ í˜¹ì€ ì‹œí€€ìŠ¤ë¥¼ êµ¬ì„±í•˜ëŠ” <code class="language-plaintext highlighter-rouge">ì›ì†Œ ì‚¬ì´ì˜ ê´€ê³„ì„±</code>ì´ë‚˜ <code class="language-plaintext highlighter-rouge">ì–´ë–¤ ë‹¤ë¥¸ ì˜ë¯¸ë¡ ì ì¸ ì´ìœ </code>ë¡œ <code class="language-plaintext highlighter-rouge">ë¶ˆê· í˜•</code>í•˜ê²Œ í˜„ì¬ ì‹œì ì˜ í•™ìŠµì— ì˜í–¥ë ¥ì„ ê°–ê²Œ ë˜ëŠ”ê²Œ ì•„ë‹ˆë¼, ë‹¨ìˆœ <code class="language-plaintext highlighter-rouge">ì…ë ¥ ì‹œì </code> ë•Œë¬¸ì— ë¶ˆê· í˜•ì´ ë°œìƒí•˜ê¸° ë•Œë¬¸ì— <code class="language-plaintext highlighter-rouge">RNN</code>ì˜ <code class="language-plaintext highlighter-rouge">Vanishing Gradient</code>ê°€ ë‚®ì€ ì„±ëŠ¥ì˜ ì›ì¸ìœ¼ë¡œ ì§€ëª©ë˜ëŠ” ê²ƒì´ë‹¤.</p>

<p>ë‹¤ì‹œ ë§í•´, ì‹¤ì œ ìì—°ì–´ì˜ ë¬¸ë§¥ì„ íŒŒì•…í•´ ê·¸ë¼ë””ì–¸íŠ¸ì— ë°˜ì˜í•˜ëŠ”ê²Œ ì•„ë‹ˆë¼ ë‹¨ìˆœíˆ ì‹œì ì— ë”°ë¼ì„œ ê·¸ ì˜í–¥ë ¥ì„ ë°˜ì˜í•˜ê²Œ ëœë‹¤ëŠ” ê²ƒì´ë‹¤. ë©€ë¦¬ ë–¨ì–´ì§„ ì‹œí€€ìŠ¤ì˜ ë¬¸ë§¥ì´ í•„ìš”í•œ ê²½ìš°ë¥¼ <code class="language-plaintext highlighter-rouge">Recurrent</code> êµ¬ì¡°ëŠ” ì •í™•íˆ í•™ìŠµí•  ìˆ˜ ì—†ë‹¤.</p>

<p>ê·¸ë ‡ë‹¤ë©´ í™œì„± í•¨ìˆ˜ë¥¼ <code class="language-plaintext highlighter-rouge">relu</code> í˜¹ì€ <code class="language-plaintext highlighter-rouge">gelu</code> ë¥¼ ì‚¬ìš©í•˜ë©´ ìœ„ ë¬¸ì œë¥¼ í•´ê²°í•  ìˆ˜ ìˆì„ê¹Œ? <code class="language-plaintext highlighter-rouge">Vanishing Graident</code> ë¬¸ì œëŠ” í•´ê²°í•  ìˆ˜ë„ ìˆìœ¼ë‚˜ <code class="language-plaintext highlighter-rouge">hidden_state</code> ê°’ì´ ë°œì‚°í•  ê²ƒì´ë‹¤. ê·¸ ì´ìœ ëŠ” ë‘ í™œì„± í•¨ìˆ˜ ëª¨ë‘ ì–‘ìˆ˜ êµ¬ê°„ì—ì„œ ì„ í˜•ì¸ë°, ì´ì „ ì •ë³´ë¥¼ ëˆ„ì í•´ì„œ ê°€ì¤‘ì¹˜ì™€ ê³±í•˜ê³  í˜„ì¬ ì…ë ¥ê°’ì— ë”í•˜ëŠ” <code class="language-plaintext highlighter-rouge">RNN</code>ì˜ êµ¬ì¡°ë¥¼ ìƒê°í•´ë³´ë©´ ë„˜ì–´ì˜¤ëŠ” ì´ì „ ì •ë³´ëŠ” ëˆ„ì ë˜ë©´ì„œ ì ì  ì»¤ì§ˆ ê²ƒì´ê³  ê·¸ëŸ¬ë‹¤ ê²°êµ­ ë°œì‚°í•˜ê²Œ ëœë‹¤.</p>

<p>ê²°ë¡ ì ìœ¼ë¡œ <code class="language-plaintext highlighter-rouge">Vanishing Gradient</code> í˜„ìƒ ìì²´ê°€ ë¬¸ì œëŠ” ì•„ë‹ˆì§€ë§Œ ëª¨ë¸ì´ ìì—°ì–´ì˜ ë¬¸ë§¥ì„ íŒŒì•…í•´ ê·¸ë¼ë””ì–¸íŠ¸ì— ë°˜ì˜í•˜ëŠ”ê²Œ ì•„ë‹ˆë¼ ë‹¨ìˆœíˆ ì‹œì ì— ë”°ë¼ì„œ ë¶ˆê· í˜•í•˜ê²Œ ë°œìƒí•˜ê¸° ë•Œë¬¸ì— ë‚®ì€ ì„±ëŠ¥ì˜ ì›ì¸ìœ¼ë¡œ ì§€ëª© ë°›ëŠ” ê²ƒì´ë‹¤. ì´ê²ƒì„ <code class="language-plaintext highlighter-rouge">long-term dependency</code>ë¼ê³  ë¶€ë¥´ê¸°ë„ í•œë‹¤.</p>

<p><strong><code class="language-plaintext highlighter-rouge">âœï¸Â 2) ì ì  íë ¤ì§€ëŠ” Inputsì— Attention (Activation Function with Forward)</code></strong></p>

<p align="center">
<img src="/assets/images/transformer/tanh.png" alt="tanh function" class="align-center image-caption" width="75%&quot;, height=&quot;50%" />
<strong><em>tanh function</em></strong>
</p>

<p><code class="language-plaintext highlighter-rouge">Hyperbolic Tangent</code> ì€  $y$ê°’ì´ <code class="language-plaintext highlighter-rouge">[-1, 1]</code> ì‚¬ì´ì—ì„œ ì •ì˜ëœë‹¤ê³  í–ˆë‹¤. ë‹¤ì‹œ ë§í•´ ì…€ì˜ ì¶œë ¥ê°’ì´ í•­ìƒ ì¼ì • ë²”ìœ„ê°’( <code class="language-plaintext highlighter-rouge">[-1,1]</code> )ìœ¼ë¡œ ì œí•œ(ê°€ì¤‘ì¹˜, í¸í–¥ ë”í•˜ëŠ” ê²ƒì€ ì¼ë‹¨ ì œì™¸) ëœë‹¤ëŠ” ê²ƒì´ë‹¤. ë”°ë¼ì„œ í•œì •ëœ ì¢ì€ ë²”ìœ„ì— ì¶œë ¥ê°’ë“¤ì´ ë§µí•‘ë˜ëŠ”ë°, ì´ëŠ” ê²°êµ­ ì…ë ¥ê°’ì˜ ì •ë³´ëŠ” ëŒ€ë¶€ë¶„ ì†Œì‹¤ëœ ì±„ ì¼ë¶€ íŠ¹ì§•ë§Œ ì •ì œ ë˜ì–´ ì¶œë ¥ë˜ê³  ë‹¤ìŒ ë ˆì´ì–´ë¡œ <code class="language-plaintext highlighter-rouge">forward</code> ë¨ì„ ì˜ë¯¸í•œë‹¤. ê·¸ë˜í”„ë¥¼ í•œ ë²ˆ ì‚´í´ë³´ì. íŠ¹íˆ <code class="language-plaintext highlighter-rouge">Inputs</code> ê°’ì´ 2.5 ì´ìƒì¸ ê²½ìš°ë¶€í„°ëŠ” ì¶œë ¥ê°’ì´ ê±°ì˜ 1ì— ìˆ˜ë ´í•´ ê·¸ ì°¨ì´ë¥¼ ì§ê´€ì ìœ¼ë¡œ íŒŒì•…í•˜ê¸° í˜ë“¤ë‹¤. ì´ëŸ¬í•œ í™œì„±í•¨ìˆ˜ê°€ ìˆ˜ì‹­ê°œ, ìˆ˜ë°±ê°œ ìŒ“ì¸ë‹¤ë©´ ê²°êµ­ ì›ë³¸ ì •ë³´ëŠ” ë§¤ìš° íë ¤ì§€ê³  ë­‰ê°œì ¸ì„œ ë‹¤ë¥¸ ì¸ìŠ¤í„´ìŠ¤ì™€ êµ¬ë³„ì´ í˜ë“¤ì–´ ì§ˆ ê²ƒì´ë‹¤.</p>

<p><strong><code class="language-plaintext highlighter-rouge">ğŸ”¬Â 3) ë””ì½”ë”ê°€ ê°€ì¥ ë§ˆì§€ë§‰ ë‹¨ì–´ë§Œ ì—´ì‹¬íˆ ë³´ê³  denoising ìˆ˜í–‰ (Seq2Seq with Bi-Directional RNN)</code></strong><br />
<code class="language-plaintext highlighter-rouge">â€œì“°ë‹¤â€</code> ($t_7$)ë¼ëŠ” ë‹¨ì–´ì˜ ëœ»ì„ ì´í•´í•˜ë ¤ë©´ <code class="language-plaintext highlighter-rouge">â€œëˆì„â€</code>, <code class="language-plaintext highlighter-rouge">â€œëª¨ìë¥¼â€</code>, <code class="language-plaintext highlighter-rouge">â€œë§›ì´â€</code>, <code class="language-plaintext highlighter-rouge">â€œê¸€ì„â€</code>($t_1$)ê³¼ ê°™ì´ ë©€ë¦¬ ìˆëŠ” ì• ë‹¨ì–´ë¥¼ ë´ì•¼ ì•Œ ìˆ˜ ìˆëŠ”ë°, $h_7$ ì—ëŠ” $t_1$ì´ íë ¤ì§„ ì±„ë¡œ ë“¤ì–´ê°€ ìˆì–´ì„œ $t_7$ì˜ ì œëŒ€ë¡œ ëœ ì˜ë¯¸ë¥¼ í¬ì°©í•˜ì§€ ëª»í•œë‹¤. ì‹¬ì§€ì–´ ì–¸ì–´ê°€ ì˜ì–´ë¼ë©´ ë’¤ë¥¼ ë´ì•¼ ì •í™•í•œ ë¬¸ë§¥ì„ ì•Œ ìˆ˜ ìˆëŠ”ë° <code class="language-plaintext highlighter-rouge">Vanilla RNN</code>ì€ ë‹¨ë°©í–¥ìœ¼ë¡œë§Œ í•™ìŠµì„ í•˜ê²Œ ë˜ì–´ ë¬¸ì¥ì˜ ë’·ë¶€ë¶„ ë¬¸ë§¥ì€ ë°˜ì˜ì¡°ì°¨(ë’¤ì— ìœ„ì¹˜í•œ ëª©ì ì–´ì— ë”°ë¼ì„œ ì“°ë‹¤ë¼ëŠ” ë‹¨ì–´ì˜ ë‰˜ì•™ìŠ¤ëŠ” ë‹¬ë¼ì§) í•  ìˆ˜ ì—†ë‹¤. ê·¸ë˜ì„œ <code class="language-plaintext highlighter-rouge">Bi-directional RNN</code> ì¨ì•¼í•˜ëŠ”ë°, ì´ê²ƒë„ ì—­ì‹œë„ ì—¬ì „íˆ <code class="language-plaintext highlighter-rouge">â€œê±°ë¦¬â€</code>ì— ì˜í–¥ ë°›ëŠ”ë‹¤ëŠ” ê±´ ë³€í•˜ì§€ ì•Šê¸° ë•Œë¬¸ì— ê·¼ë³¸ì ì¸ í•´ê²°ì±…ì´ë¼ ë³¼ ìˆ˜ ì—†ë‹¤.</p>

<p>í•œí¸, ë””ì½”ë”ì˜ <code class="language-plaintext highlighter-rouge">Next Token Prediction</code> ì„±ëŠ¥ì€ ë¬´ì¡°ê±´ ì¸ì½”ë”ë¡œë¶€í„° ë°›ëŠ” <code class="language-plaintext highlighter-rouge">Context Vector</code>ì˜ í’ˆì§ˆì— ë”°ë¼ ì¢Œì§€ìš°ì§€ ëœë‹¤. ê·¸ëŸ¬ë‚˜ Recurrent êµ¬ì¡°ì˜ ì¸ì½”ë”ë¡œë¶€í„° ë‚˜ì˜¨ Context VectorëŠ” ì•ì„œ ì„œìˆ í•œ ê²ƒì²˜ëŸ¼ ì¢‹ì€ í’ˆì§ˆ(ë’¤ìª½ ë‹¨ì–´ê°€ ìƒëŒ€ì ìœ¼ë¡œ ì„ ëª…í•¨)ì´ ì•„ë‹ˆë‹¤. ë”°ë¼ì„œ ë””ì½”ë”ì˜ ë²ˆì—­(ë‹¤ìŒ ë‹¨ì–´ ì˜ˆì¸¡) ì„±ëŠ¥ ì—­ì‹œ ì¢‹ì„ë¦¬ê°€ ì—†ë‹¤.</p>

<p>ê²°êµ­ <code class="language-plaintext highlighter-rouge">Recurrent</code> êµ¬ì¡° ìì²´ì— ëª…í™•í•œ í•œê³„ê°€ ì¡´ì¬í•˜ì—¬ ì¸ê°„ì´ ìì—°ì–´ë¥¼ ì‚¬ìš©í•˜ê³  ì´í•´í•˜ëŠ” ë§¥ë½ê³¼ ë‹¤ë¥¸ ë°©ì‹ìœ¼ë¡œ ë™ì‘í–ê²Œ ë˜ì—ˆë‹¤. <code class="language-plaintext highlighter-rouge">LSTM</code>, <code class="language-plaintext highlighter-rouge">GRU</code>ì˜ ì œì•ˆìœ¼ë¡œ ì–´ëŠ ì •ë„ ë¬¸ì œë¥¼ ì™„í™” ì‹œì¼°ìœ¼ë‚˜, ì•ì—ì„œ ì„œìˆ í–ˆë“¯ì´ íƒœìƒì´ <code class="language-plaintext highlighter-rouge">Recurrent Structure</code>ì„ ê°€ì§€ê¸° ë•Œë¬¸ì— ê·¼ë³¸ì ì¸ í•´ê²°ì±…ì´ ë˜ì§€ëŠ” ëª»í–ˆë‹¤. ê·¸ë ‡ë‹¤ë©´ ì´ì œ <code class="language-plaintext highlighter-rouge">Transformer</code>ê°€ ì–´ë–»ê²Œ ìœ„ì— ì„œìˆ í•œ 3ê°€ì§€ ë¬¸ì œë¥¼ í•´ê²°í•˜ê³  í˜„ì¬ì˜ ìœ„ìƒì„ ê°–ê²Œ ë˜ì—ˆëŠ”ì§€ ì•Œì•„ë³´ì.</p>

<h3 id="modeling"><strong><code class="language-plaintext highlighter-rouge">ğŸŒŸÂ Modeling</code></strong></h3>

<p align="center">
<img src="/assets/images/transformer/transformer_overview.png" alt="Attention Is All You Need" class="align-center image-caption" width="50%&quot;, height=&quot;50%" />
<strong><em><a href="https://arxiv.org/abs/1706.03762">Attention Is All You Need</a></em></strong>
</p>

<p>ì•ì„œ <code class="language-plaintext highlighter-rouge">Recurrent</code> êµ¬ì¡°ì˜ <code class="language-plaintext highlighter-rouge">Vanishing Gradient</code> ì„ ì„¤ëª…í•˜ë©´ì„œ ì‹œì ì— ë”°ë¼ ì •ë³´ë¥¼ ì†Œì‹¤í•˜ê²Œ ë˜ëŠ” í˜„ìƒì€ ì¸ê°„ì˜ ìì—°ì–´ ì´í•´ ë°©ì‹ì´ ì•„ë‹ˆë¼ëŠ” ì ì„ ì–¸ê¸‰í•œ ì  ìˆë‹¤. ë”°ë¼ì„œ <code class="language-plaintext highlighter-rouge">Transformer</code>ëŠ” ìµœëŒ€í•œ ì¸ê°„ì˜ ìì—°ì–´ ì´í•´ ë°©ì‹ì„ ìˆ˜í•™ì ìœ¼ë¡œ ëª¨ë¸ë§ í•˜ëŠ” ê²ƒì— ì´ˆì ì„ ë§ì·„ë‹¤. ìš°ë¦¬ê°€ ì“°ì—¬ì§„ ê¸€ì„ ì´í•´í•˜ê¸° ìœ„í•´ í•˜ëŠ” í–‰ë™ë“¤ì„ ë– ì˜¬ë ¤ ë³´ì. <strong><code class="language-plaintext highlighter-rouge">â€œAppleâ€</code><u>ì´ë€ ë‹¨ì–´ê°€ ì‚¬ê³¼ë¥¼ ë§í•˜ëŠ” ê²ƒì¸ì§€, ë¸Œëœë“œ ì• í”Œì„ ì§€ì¹­í•˜ëŠ” ê²ƒì¸ì§€ íŒŒì•…í•˜ê¸° ìœ„í•´ ê°™ì€ ë¬¸ì¥ì— ì†í•œ ì£¼ë³€ ë‹¨ì–´ë¥¼ ì‚´í”¼ê¸°ë„ í•˜ê³  ê·¸ë˜ë„ íŒŒì•…í•˜ê¸° í˜ë“¤ë‹¤ë©´ ì•ë’¤ ë¬¸ì¥, ë‚˜ì•„ê°€ ë¬¸ì„œ ì „ì²´ ë ˆë²¨ì—ì„œ ë§¥ë½ì„ íŒŒì•…í•˜ê¸° ìœ„í•´ ë…¸ë ¥í•œë‹¤.</u></strong> <code class="language-plaintext highlighter-rouge">Transformer</code> ì—°êµ¬ì§„ì€ ë°”ë¡œ ì´ ê³¼ì •ì— ì£¼ëª©í–ˆìœ¼ë©° ì´ê²ƒì„ ëª¨ë¸ë§í•˜ì—¬ ê·¸ ìœ ëª…í•œ <code class="language-plaintext highlighter-rouge">Self-Attention</code>ì„ ê³ ì•ˆí•´ë‚¸ë‹¤.</p>

<p align="center">
<img src="/assets/images/transformer/word_embedding.png" alt="Word Embedding Space" class="align-center image-caption" width="50%&quot;, height=&quot;50%" />
<strong><em><a href="https://www.researchgate.net/figure/Visualization-of-the-word-embedding-space_fig4_343595281/download">Word Embedding Space</a></em></strong>
</p>

<p>ë‹¤ì‹œ ë§í•´ <code class="language-plaintext highlighter-rouge">Self-Attention</code>ì€ í† í°ì˜ ì˜ë¯¸ë¥¼ ì´í•´í•˜ê¸° ìœ„í•´ <code class="language-plaintext highlighter-rouge">ì „ì²´ ì…ë ¥ ì‹œí€€ìŠ¤</code> ì¤‘ì—ì„œ ì–´ë–¤ ë‹¨ì–´ì— ì£¼ëª©í•´ì•¼í• ì§€ë¥¼ ìˆ˜í•™ì ìœ¼ë¡œ í‘œí˜„í•œ ê²ƒì´ë¼ ë³¼ ìˆ˜ ìˆë‹¤. <strong><u>ì¢€ ë” êµ¬ì²´ì ìœ¼ë¡œëŠ” ì‹œí€€ìŠ¤ì— ì†í•œ ì—¬ëŸ¬ í† í° ë²¡í„°(í–‰ë°±í„°)ë¥¼ ì„ë² ë”© ê³µê°„ ì–´ë””ì— ë°°ì¹˜í•  ê²ƒì¸ê°€ì— ëŒ€í•´ í›ˆë ¨í•˜ëŠ” í–‰ìœ„ë‹¤.</u></strong></p>

<p align="center">
<img src="/assets/images/transformer/scaled_dot_attention.png" alt="Scaled Dot-Product Attention" class="align-center image-caption" width="75%&quot;, height=&quot;50%" />
<strong><em><a href="https://arxiv.org/abs/1706.03762">Scaled Dot-Product Attention</a></em></strong>
</p>

<p>ê·¸ë ‡ë‹¤ë©´ ì´ì œë¶€í„° <code class="language-plaintext highlighter-rouge">Transformer</code> ê°€ ì–´ë–¤ ì•„ì´ë°ì´ì…˜ì„ í†µí•´ ê¸°ì¡´ ìˆœí™˜ ì‹ ê²½ë§ ëª¨ë¸ì˜ ë‹¨ì ì„ í•´ê²°í•˜ê³  ë”¥ëŸ¬ë‹ê³„ì˜ <code class="language-plaintext highlighter-rouge">G.O.A.T</code> ìë¦¬ë¥¼ ì°¨ì§€í–ˆëŠ”ì§€ ì•Œì•„ë³´ì. ëª¨ë¸ì€ í¬ê²Œ ì¸ì½”ë”ì™€ ë””ì½”ë” ë¶€ë¶„ìœ¼ë¡œ ë‚˜ë‰˜ëŠ”ë°, í•˜ëŠ” ì—­í• ê³¼ ë¯¸ì„¸í•œ êµ¬ì¡°ìƒì˜ ì°¨ì´ë§Œ ìˆì„ë¿ ë‘ ëª¨ë“ˆ ëª¨ë‘ <code class="language-plaintext highlighter-rouge">Self-Attention</code>ì´ ì œì¼ ì¤‘ìš”í•˜ë‹¤ëŠ” ë³¸ì§ˆì€ ë³€í•˜ì§€ ì•ŠëŠ”ë‹¤. ë”°ë¼ì„œ <code class="language-plaintext highlighter-rouge">Input Embedding</code>ë¶€í„° ì°¨ë¡€ëŒ€ë¡œ ì‚´í´ë³´ë˜,  <code class="language-plaintext highlighter-rouge">Self-Attention</code> ì€ íŠ¹ë³„íˆ ì‚¬ìš©ëœ í•˜ìœ„ ë¸”ëŸ­ ë‹¨ìœ„ë¥¼ ë¹ ì§ ì—†ì´, ì„¸ì„¸í•˜ê²Œ ì‚´í´ë³¼ ê²ƒì´ë‹¤.</p>

<p align="center">
<img src="/assets/images/transformer/class_diagram.png" alt="Class Diagram" class="align-center image-caption" width="35%&quot;, height=&quot;50%" />
<strong><em>Class Diagram</em></strong>
</p>

<p>ì´ë ‡ê²Œ í•˜ìœ„ ëª¨ë“ˆì— ëŒ€í•œ ì„¤ëª…ë¶€í„° ìŒ“ì•„ ë‚˜ê°€ ë§ˆì§€ë§‰ì—ëŠ” ì‹¤ì œ êµ¬í˜„ ì½”ë“œì™€ í•¨ê»˜ ì „ì²´ì ì¸ êµ¬ì¡° ì¸¡ë©´ì—ì„œë„ ëª¨ë¸ì„ í•´ì„í•´ë³¼ ê²ƒì´ë‹¤. ëê¹Œì§€ í¬ìŠ¤íŒ…ì„ ì½ì–´ì£¼ì‹œê¸¸ ë°”ë€ë‹¤.</p>

<h4 id="-input-embedding"><code class="language-plaintext highlighter-rouge">ğŸ”¬ Input Embedding</code></h4>

\[X_E \in R^{B * S_E * V_E} \\
X_D \in R^{B * S_D * V_D}\]

<p><code class="language-plaintext highlighter-rouge">Transformer</code>ëŠ” ì¸ì½”ë”ì™€ ë””ì½”ë”ë¡œ ì´ë¤„ì§„ <code class="language-plaintext highlighter-rouge">seq2seq</code> êµ¬ì¡°ë¥¼ ê°€ì§€ê³  ìˆë‹¤. ì¦‰, ëŒ€ìƒ ì–¸ì–´ë¥¼ íƒ€ê²Ÿ ì–¸ì–´ë¡œ ë²ˆì—­í•˜ëŠ”ë° ëª©ì ì„ ë‘ê³  ìˆê¸° ë•Œë¬¸ì— ì…ë ¥ìœ¼ë¡œ ëŒ€ìƒ ì–¸ì–´ ì‹œí€€ìŠ¤ì™€ íƒ€ê²Ÿ ì–¸ì–´ ì‹œí€€ìŠ¤ ëª¨ë‘ í•„ìš”í•˜ë‹¤. $X_E$ëŠ” <code class="language-plaintext highlighter-rouge">ì¸ì½”ë”</code>ì˜ ì…ë ¥ í–‰ë ¬ì„ ë‚˜íƒ€ë‚´ê³ , $X_D$ëŠ” <code class="language-plaintext highlighter-rouge">ë””ì½”ë”</code>ì˜ ì…ë ¥ í–‰ë ¬ì„ ì˜ë¯¸í•œë‹¤. ì´ ë•Œ, $B$ëŠ” <code class="language-plaintext highlighter-rouge">batch size</code>, $S$ëŠ” <code class="language-plaintext highlighter-rouge">max_seq</code>, $V$ëŠ” ê°œë³„ ëª¨ë“ˆì´ ê°€ì§„ <code class="language-plaintext highlighter-rouge">Vocab</code>ì˜ ì‚¬ì´ì¦ˆë¥¼ ê°€ë¦¬í‚¨ë‹¤. ìœ„ ìˆ˜ì‹ì€ ì‚¬ì‹¤ ë…¼ë¬¸ì— ì…ë ¥ì— ëŒ€í•œ ìˆ˜ì‹ì´ ë”°ë¡œ ì„œìˆ  ë˜ì–´ ìˆì§€ ì•Šì•„, í•„ìê°€ ì§ì ‘ ë§Œë“  ê²ƒì´ë‹¤. ì•ìœ¼ë¡œë„ í•´ë‹¹ ê¸°í˜¸ë¥¼ ì´ìš©í•´ ìˆ˜ì‹ì„ í‘œí˜„í•  ì˜ˆì •ì´ë‹ˆ ì°¸ê³  ë°”ë€ë‹¤.</p>

\[W_E \in R^{V_E * d} \\
W_D \in R^{V_D * d} \\\]

<p>ì´ë ‡ê²Œ ì •ì˜ëœ ì…ë ¥ê°’ì„ ê°œë³„ ëª¨ë“ˆì˜ ì„ë² ë”© ë ˆì´ì–´ì— í†µê³¼ ì‹œí‚¨ ê²°ê³¼ë¬¼ì´ ë°”ë¡œ <code class="language-plaintext highlighter-rouge">Input Embedding</code>ì´ ëœë‹¤. $d$ëŠ” <code class="language-plaintext highlighter-rouge">Transformer</code> ëª¨ë¸ì˜ ì€ë‹‰ì¸µì˜ í¬ê¸°ë¥¼ ì˜ë¯¸í•œë‹¤. ë”°ë¼ì„œ <code class="language-plaintext highlighter-rouge">Position Embedding</code> ê³¼ ë”í•´ì§€ê¸° ì „, ì„ë² ë”© ë ˆì´ì–´ë¥¼ í†µê³¼í•œ <code class="language-plaintext highlighter-rouge">Input Embedding</code>ì˜ ëª¨ì–‘ì€ ì•„ë˜ ìˆ˜ì‹ê³¼ ê°™ë‹¤.</p>

\[X_E \in R^{B*S_E*d} \\
X_D \in R^{B*S_D*d} \\\]

<p>ê·¸ë ‡ë‹¤ë©´ ì‹¤ì œ êµ¬í˜„ì€ ì–´ë–»ê²Œ í• ê¹Œ?? <code class="language-plaintext highlighter-rouge">Transformer</code> ì˜ <code class="language-plaintext highlighter-rouge">Input Embedding</code>ì€ <code class="language-plaintext highlighter-rouge">nn.Embedding</code>ìœ¼ë¡œ ë ˆì´ì–´ë¥¼ ì •ì˜í•´ ì‚¬ìš©í•œë‹¤. <code class="language-plaintext highlighter-rouge">nn.Linear</code>ë„ ìˆëŠ”ë° ì™œ êµ³ì´ <code class="language-plaintext highlighter-rouge">nn.Embedding</code>ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì¼ê¹Œ??</p>

<p>ìì—°ì–´ ì²˜ë¦¬ì—ì„œ ì…ë ¥ ì„ë² ë”©ì„ ë§Œë“¤ë•ŒëŠ” ëª¨ë¸ì˜ í† í¬ë‚˜ì´ì €ì— ì˜í•´ ì‚¬ì „ ì •ì˜ëœ <code class="language-plaintext highlighter-rouge">vocab</code>ì˜ ì‚¬ì´ì¦ˆê°€ ì…ë ¥ ì‹œí€€ìŠ¤ì— ì†í•œ í† í° ê°œìˆ˜ë³´ë‹¤ í›¨ì”¬ í¬ê¸° ë•Œë¬¸ì— ë°ì´í„° ë£©ì—… í…Œì´ë¸” ë°©ì‹ì˜Â <code class="language-plaintext highlighter-rouge">nn.Embedding</code>Â ì„ ì‚¬ìš©í•˜ê²Œ ëœë‹¤. ì´ê²Œ ë¬´ìŠ¨ ë§ì´ëƒë©´, í† í¬ë‚˜ì´ì €ì— ì˜í•´ ì‚¬ì „ì— ì •ì˜ëœÂ <code class="language-plaintext highlighter-rouge">vocab</code>Â ì „ì²´ê°€Â <code class="language-plaintext highlighter-rouge">nn.Embedding(vocab_size, dim_model)</code>ë¡œ íˆ¬ì˜ ë˜ì–´ ê°€ë¡œëŠ” <code class="language-plaintext highlighter-rouge">vocab</code> ì‚¬ì´ì¦ˆ, ì„¸ë¡œëŠ” ëª¨ë¸ì˜ ì°¨ì› í¬ê¸°ì— í•´ë‹¹í•˜ëŠ” ë£©ì—… í…Œì´ë¸”ì´ ìƒì„±ë˜ê³ , ë‚´ê°€ ì…ë ¥í•œ í† í°ë“¤ì€ ì „ì²´Â <code class="language-plaintext highlighter-rouge">vocab</code>ì˜ ì¼ë¶€ë¶„ì¼í…Œë‹ˆ ì „ì²´ ì„ë² ë”© ë£©ì—… í…Œì´ë¸”ì—ì„œ ë‚´ê°€ ì„ë² ë”©í•˜ê³  ì‹¶ì€ í† í°ë“¤ì˜ ì¸ë±ìŠ¤ë§Œ ì•Œì•„ë‚¸ë‹¤ëŠ” ê²ƒì´ë‹¤. ê·¸ë˜ì„œÂ <code class="language-plaintext highlighter-rouge">nn.Embedding</code>Â ì€ ë ˆì´ì–´ì— ì •ì˜ëœ ì°¨ì›ê³¼ ì‹¤ì œ ì…ë ¥ ë°ì´í„°ì˜ ì°¨ì›ì´ ë§ì§€ ì•Šì•„ë„ í•¨ìˆ˜ê°€ ë™ì‘í•˜ê²Œ ëœë‹¤. <code class="language-plaintext highlighter-rouge">nn.Linear</code> ì™€ ì…ë ¥ ì°¨ì›ì— ëŒ€í•œ ì¡°ê±´ ë¹¼ê³ ëŠ” ë™ì¼í•œ ë™ì‘ì„ ìˆ˜í–‰í•˜ê¸° ë•Œë¬¸ì— ì‚¬ì „ ì •ì˜ëœ <code class="language-plaintext highlighter-rouge">vocab</code> ì‚¬ì´ì¦ˆì™€ ì…ë ¥ ì‹œí€€ìŠ¤ì˜ í† í° ê°œìˆ˜ê°€ ê°™ë‹¤ë©´ <code class="language-plaintext highlighter-rouge">nn.Linear</code>ë¥¼ ì‚¬ìš©í•´ë„ ë¬´ë°©í•˜ë‹¤.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Input Embedding Example
</span>
<span class="k">class</span> <span class="nc">Transformer</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
	<span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">enc_vocab_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">dec_vocab_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">max_seq</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">512</span><span class="p">,</span>
        <span class="n">enc_N</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">6</span><span class="p">,</span>
        <span class="n">dec_N</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">6</span><span class="p">,</span>
        <span class="n">dim_model</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">512</span><span class="p">,</span> <span class="c1"># latent vector space
</span>        <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span>
        <span class="n">dim_ffn</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2048</span><span class="p">,</span>
        <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Transformer</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">enc_input_embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">enc_vocab_size</span><span class="p">,</span> <span class="n">dim_model</span><span class="p">)</span> <span class="c1"># Encoder Input Embedding Layer
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">dec_input_embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">dec_vocab_size</span><span class="p">,</span> <span class="n">dim_model</span><span class="p">)</span> <span class="c1"># Decoder Input Embedding Layer
</span>	
	<span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">enc_inputs</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">dec_inputs</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">enc_pad_index</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">dec_pad_index</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
            <span class="n">enc_x</span><span class="p">,</span> <span class="n">dec_x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">enc_input_embedding</span><span class="p">(</span><span class="n">enc_inputs</span><span class="p">),</span> <span class="bp">self</span><span class="p">.</span><span class="n">dec_input_embedding</span><span class="p">(</span><span class="n">dec_inputs</span><span class="p">)</span>
</code></pre></div></div>

<p>ìœ„ì˜ ì˜ˆì‹œ ì½”ë“œë¥¼ í•¨ê»˜ ì‚´í´ë³´ì. <code class="language-plaintext highlighter-rouge">__init__</code> ì˜ <code class="language-plaintext highlighter-rouge">self.enc_input_embedding</code>, <code class="language-plaintext highlighter-rouge">self._dec_input_embedding</code>ì´ ë°”ë¡œ $W_E, W_D$ì— ëŒ€ì‘ëœë‹¤. í•œí¸ <code class="language-plaintext highlighter-rouge">forward</code> ë©”ì„œë“œì— ì •ì˜ëœ <code class="language-plaintext highlighter-rouge">enc_x</code>, <code class="language-plaintext highlighter-rouge">dec_x</code> ëŠ” ì„ë² ë”© ë ˆì´ì–´ë¥¼ ê±°ì¹˜ê³  ë‚˜ì˜¨ $X_E, X_D$ì— í•´ë‹¹ëœë‹¤.</p>

<p>í•œí¸, $X_E, X_D$ì€ ê°ê° ì¸ì½”ë”, ë””ì½”ë” ëª¨ë“ˆë¡œ í˜ëŸ¬ ë“¤ì–´ê°€ <code class="language-plaintext highlighter-rouge">Absolute Position Embedding</code>ê³¼ ë”í•´ì§„(í–‰ë ¬ í•©) ë’¤, ê°œë³„ ëª¨ë“ˆì˜ ì…ë ¥ê°’ìœ¼ë¡œ í™œìš©ëœë‹¤.</p>

<p><strong><code class="language-plaintext highlighter-rouge">ğŸ”¢Â Absolute Position Embedding(Encoding)</code></strong><br />
ì…ë ¥ ì‹œí€€ìŠ¤ì— ìœ„ì¹˜ ì •ë³´ë¥¼ ë§µí•‘í•´ì£¼ëŠ” ì—­í• ì„ í•œë‹¤. í•„ìëŠ” ê°œì¸ì ìœ¼ë¡œ <code class="language-plaintext highlighter-rouge">Transformer</code>ì—ì„œ ê°€ì¥ ì¤‘ìš”í•œ ìš”ì†Œë¥¼ ë½‘ìœ¼ë¼ê³  í•˜ë©´ ì„¸ ì†ê°€ë½ ì•ˆì— ë“¤ì–´ê°€ëŠ” íŒŒíŠ¸ë¼ê³  ìƒê°í•œë‹¤. ë‹¤ìŒ íŒŒíŠ¸ì—ì„œ ìì„¸íˆ ê¸°ìˆ í•˜ê² ì§€ë§Œ, <code class="language-plaintext highlighter-rouge">Self-Attention(ë‚´ì )</code>ì€ ì…ë ¥ ì‹œí€€ìŠ¤ë¥¼ ë³‘ë ¬ë¡œ í•œêº¼ë²ˆì— ì²˜ë¦¬í•  ìˆ˜ ìˆë‹¤ëŠ” ì¥ì ì„ ê°–ê³  ìˆì§€ë§Œ, ê·¸ ìì²´ë¡œëŠ” í† í°ì˜ ìœ„ì¹˜ ì •ë³´ë¥¼ ì¸ì½”ë”©í•  ìˆ˜ ì—†ë‹¤. ìš°ë¦¬ê°€ ë”°ë¡œ ìœ„ì¹˜ ì •ë³´ë¥¼ ì•Œë ¤ì£¼ì§€ ì•ŠëŠ” ì´ìƒ ì¿¼ë¦¬ í–‰ë ¬ì˜ 2ë²ˆì§¸ í–‰ë²¡í„°ê°€ ì…ë ¥ ì‹œí€€ìŠ¤ì—ì„œ ëª‡ ë²ˆì§¸ ìœ„ì¹˜í•œ í† í°ì¸ì§€ ëª¨ë¸ì€ ì•Œ ê¸¸ì´ ì—†ë‹¤.</p>

<p>ê·¸ëŸ°ë°, í…ìŠ¤íŠ¸ëŠ” <code class="language-plaintext highlighter-rouge">Permutation Equivariant</code>í•œ <code class="language-plaintext highlighter-rouge">Bias</code> ê°€ ìˆê¸° ë•Œë¬¸ì— í† í°ì˜ ìœ„ì¹˜ ì •ë³´ëŠ” <code class="language-plaintext highlighter-rouge">NLP</code>ì—ì„œ ë§¤ìš° ì¤‘ìš”í•œ ìš”ì†Œë¡œ ê¼½íŒë‹¤. <strong>ì§ê´€ì ìœ¼ë¡œë„ í† í°ì˜ ìˆœì„œëŠ” ì‹œí€€ìŠ¤ê°€ ë‚´í¬í•˜ëŠ” ì˜ë¯¸ì— ì§€ëŒ€í•œ ì˜í–¥ì„ ë¼ì¹œë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤.</strong> ì˜ˆë¥¼ ë“¤ì–´ <code class="language-plaintext highlighter-rouge">â€œì² ìˆ˜ëŠ” ì˜í¬ë¥¼ ì¢‹ì•„í•œë‹¤â€</code>ë¼ëŠ” ë¬¸ì¥ê³¼ <code class="language-plaintext highlighter-rouge">â€œì˜í¬ëŠ” ì² ìˆ˜ë¥¼ ì¢‹ì•„í•œë‹¤â€</code>ë¼ëŠ” ë¬¸ì¥ì˜ ì˜ë¯¸ê°€ ê°™ì€ê°€ ìƒê°í•´ë³´ì. ì£¼ì–´ì™€ ëª©ì ì–´ ìœ„ì¹˜ê°€ ë°”ë€Œë©´ì„œ ì •ë°˜ëŒ€ì˜ ëœ»ì´ ë˜ì–´ë²„ë¦°ë‹¤.</p>

<p align="center">
<img src="/assets/images/transformer/positional_encoding.png" alt="Positional Encoding Example" class="align-center image-caption" width="75%&quot;, height=&quot;50%" />
<strong><em><a href="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/56cf1596-c770-410c-8053-5876c3c66fff/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2022-10-09_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_5.13.48.png">Positional Encoding Example</a></em></strong>
</p>

<p>ë”°ë¼ì„œ ì €ìëŠ” ì…ë ¥ ì…ë² ë”©ì— ìœ„ì¹˜ ì •ë³´ë¥¼ ì¶”ê°€í•˜ê³ ì <code class="language-plaintext highlighter-rouge">Position Encoding</code> ì„ ì œì•ˆí•œë‹¤. ì‚¬ì‹¤ <code class="language-plaintext highlighter-rouge">Position Encoding</code> ì€ ì—¬ëŸ¬ ë‹¨ì  ë•Œë¬¸ì— í›„ëŒ€ <code class="language-plaintext highlighter-rouge">Transformer</code>  íŒŒìƒ ëª¨ë¸ì—ì„œëŠ” ì˜ ì‚¬ìš©ë˜ì§€ ì•ŠëŠ” ì¶”ì„¸ë‹¤. ëŒ€ì‹  ëª¨ë¸ì´ í•™ìŠµì„ í†µí•´ ìµœì ê°’ì„ ì°¾ì•„ì£¼ëŠ” <code class="language-plaintext highlighter-rouge">Position Embedding</code> ë°©ì‹ì„ ëŒ€ë¶€ë¶„ ì°¨ìš©í•˜ê³  ìˆë‹¤. í•„ì ì—­ì‹œ <code class="language-plaintext highlighter-rouge">Position Embedding</code> ì„ ì‚¬ìš©í•´ ìœ„ì¹˜ ì„ë² ë”©ì„ êµ¬í˜„í–ˆê¸° ë•Œë¬¸ì— ì›ë¦¬ì™€ ë‹¨ì ì— ëŒ€í•´ì„œë§Œ ê°„ë‹¨íˆ ì†Œê°œí•˜ê³  ë„˜ì–´ê°€ë ¤ í•œë‹¤. ë˜í•œ ì €ì ì—­ì‹œ ë…¼ë¬¸ì—ì„œ ë‘ ë°©ì‹ ì¤‘ ì–´ëŠ ê²ƒì„ ì¨ë„ ë¹„ìŠ·í•œ ì„±ëŠ¥ì„ ë³´ì—¬ì¤€ë‹¤ê³  ì–¸ê¸‰í•˜ê³  ìˆë‹¤.</p>

\[P_E \in R^{B*S_E*D} \\
 P_D \in R^{B*S_D*D} \\
P(pos, 2i) = sin(pos/\overset{}
  {10000_{}^{2i/dmodel}}) \\
P(pos, 2i+1) = cos(pos/\overset{}
  {10000_{}^{2i/dmodel}})\]

<p><strong>ì›ë¦¬ëŠ” ë§¤ìš° ê°„ë‹¨í•˜ë‹¤. ì‚¬ì¸í•¨ìˆ˜ì™€ ì½”ì‚¬ì¸ í•¨ìˆ˜ì˜ ì£¼ê¸°ì„±ì„ ì´ìš©í•´ ê°œë³„ ì¸ë±ìŠ¤ì˜ í–‰ë²¡í„° ê°’ì„ í‘œí˜„í•˜ëŠ” ê²ƒì´ë‹¤.</strong> í–‰ë²¡í„°ì˜ ì›ì†Œ ì¤‘ì—ì„œ ì§ìˆ˜ë²ˆì§¸ ì¸ë±ìŠ¤ì— ìœ„ì¹˜í•œ ì›ì†ŒëŠ” (ì§ìˆ˜ë²ˆì§¸ ì—´ë²¡í„°) \(sin(pos/\overset{}{10000_{}^{2i/dmodel}})\) ì˜ í•¨ìˆ«ê°’ì„ ì´ìš©í•´ ì±„ì›Œë„£ê³ , í™€ìˆ˜ë²ˆì§¸ ì›ì†ŒëŠ” \(cos(pos/\overset{}{10000_{}^{2i/dmodel}})\)ë¥¼ ì´ìš©í•´ ì±„ì›Œë„£ëŠ”ë‹¤.</p>

<p align="center">
<img src="/assets/images/transformer/sin_cos_graph.png" alt="periodic function graph" class="align-center image-caption" width="75%&quot;, height=&quot;50%" />
<strong><em>periodic function graph</em></strong>
</p>

<p>ì´ˆë¡ìƒ‰ ê·¸ë˜í”„ëŠ” \(sin(pos/\overset{}{10000_{}^{2i/dmodel}})\), ì£¼í™©ìƒ‰ ê·¸ë˜í”„ëŠ” \(cos(pos/\overset{}{10000_{}^{2i/dmodel}})\)ë¥¼ ì‹œê°í™”í–ˆë‹¤. ì§€ë©´ì˜ ì œí•œìœ¼ë¡œ <code class="language-plaintext highlighter-rouge">max_seq=512</code> ë§Œí¼ì˜ ë³€í™”ëŸ‰ì„ ë‹´ì§€ëŠ” ëª»í–ˆì§€ë§Œ, xì¶•ì´ ì»¤ì§ˆìˆ˜ë¡ ë‘ í•¨ìˆ˜ ëª¨ë‘ ì§„ë™ ì£¼ê¸°ê°€ ì¡°ê¸ˆì”© ì»¤ì§€ëŠ” ì–‘ìƒì„ ë³´ì—¬ì¤€ë‹¤. ë”°ë¼ì„œ ê°œë³„ ì¸ë±ìŠ¤(í–‰ë²¡í„°)ë¥¼ ì¤‘ë³µë˜ëŠ” ê°’ ì—†ì´ í‘œí˜„í•˜ëŠ” ê²ƒì´ ê°€ëŠ¥í•˜ë‹¤ê³  ì €ìëŠ” ì£¼ì¥í•œë‹¤.</p>

<p align="center">
<img src="/assets/images/transformer/positional_encoding_result.png" alt="Positional Encoding Result" class="align-center image-caption" width="75%&quot;, height=&quot;50%" />
<strong><em><a href="https://wikidocs.net/162099">Positional Encoding Result</a></em></strong>
</p>

<p>ìœ„ ê·¸ë¦¼ì€ í† í° <code class="language-plaintext highlighter-rouge">256</code>ê°œë¡œ êµ¬ì„±ëœ ì‹œí€€ìŠ¤ì— ëŒ€í•´ <code class="language-plaintext highlighter-rouge">Positional Encoding</code>í•œ ê²°ê³¼ë¥¼ ì‹œê°í™”í•œ ìë£Œë‹¤. ê·¸ë˜í”„ì˜ $x$ì¶•ì€ <code class="language-plaintext highlighter-rouge">í–‰ë²¡í„°ì˜ ì›ì†Œ</code>ì´ì <code class="language-plaintext highlighter-rouge">Transformer</code>ì˜ ì€ë‹‰ ë²¡í„° ì°¨ì›ì„ ê°€ë¦¬í‚¤ê³ , $y$ì¶•ì€ <code class="language-plaintext highlighter-rouge">ì‹œí€€ìŠ¤ì˜ ì¸ë±ìŠ¤</code>(í–‰ë²¡í„°)ë¥¼ ì˜ë¯¸í•œë‹¤. ìœ¡ì•ˆìœ¼ë¡œ ì •í™•í•˜ê²Œ ì°¨ì´ë¥¼ ì¸ì‹í•˜ê¸° ì‰½ì§€ëŠ” ì•Šì§€ë§Œ, í–‰ë²¡í„°ê°€ ëª¨ë‘ ìœ ë‹ˆí¬í•˜ê²Œ í‘œí˜„ëœë‹¤ëŠ” ì‚¬ì‹¤(ì§ì ‘ ì‹¤ìˆ˜ê°’ì„ í™•ì¸í•´ë³´ë©´ ì •ë§ ë¯¸ì„¸í•œ ì°¨ì´ì§€ë§Œ ê°œë³„ í† í°ì˜ í¬ì†Œì„±ì´ ë³´ì¥)ì„ ì•Œ ìˆ˜ ìˆë‹¤. ì‘ì€ ì°¨ì´ë¥¼ ì‹œê°í™” ìë£Œë¡œ íŒŒì•…í•˜ê¸°ëŠ” ì‰½ì§€ ì•Šê¸° ë•Œë¬¸ì— ì§„ì§œ ê·¸ëŸ°ê°€ ê¶ê¸ˆí•˜ì‹  ë¶„ë“¤ì€ ì§ì ‘ ì‹¤ìˆ˜ê°’ì„ êµ¬í•´ë³´ëŠ” ê²ƒì„ ì¶”ì²œë“œë¦°ë‹¤.</p>

<p><strong>ì—¬ê¸°ì„œ í–‰ë²¡í„°ì˜ í¬ì†Œì„±ì´ë€ ê°œë³„ í–‰ë²¡í„° ì›ì†Œì˜ í¬ì†Œì„±ì„ ë§í•˜ëŠ”ê²Œ ì•„ë‹ˆë‹¤.</strong> 0ë²ˆ í† í°, 4ë²ˆ í† í°, 9ë²ˆ í† í°ì˜ í–‰ë²¡í„° 1ë²ˆì§¸ ì›ì†Œì˜ ê°’ì€ ê°™ì„ ìˆ˜ ìˆë‹¤. í•˜ì§€ë§Œ ì§„ë™ ì£¼ê¸°ê°€ ê°ˆìˆ˜ë¡ ì»¤ì§€ëŠ” ì£¼ê¸°í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì— ë‹¤ë¥¸ ì›ì†Œ(ì°¨ì›)ê°’ì€ ë‹¤ë¥¼ ê²ƒì´ë¼ ê¸°ëŒ€í•  ìˆ˜ ìˆëŠ”ë°, <strong>ë°”ë¡œ ì´ê²ƒì„ í–‰ë²¡í„°ì˜ í¬ì†Œì„±ì´ë¼ê³  ì •ì˜í•˜ëŠ” ê²ƒì´ë‹¤.</strong> ë§Œì•½ 1ë²ˆ í† í°ê³¼ 2ë²ˆ í† í°ì˜ ëª¨ë“  í–‰ë²¡í„° ì›ì†Œê°’ì´ ê°™ë‹¤ë©´ ê·¸ê²ƒì€ í¬ì†Œì„± ì›ì¹™ì— ìœ„ë°°ë˜ëŠ” ìƒí™©ì´ë‹¤.</p>

<p align="center">
<img src="/assets/images/transformer/encoding.png" alt="Positional Encoding" class="align-center image-caption" width="75%&quot;, height=&quot;50%" />
</p>

<p align="center">
<img src="/assets/images/transformer/embedding.png" alt="Compare Performance between Encoding and Embedding" class="align-center image-caption" width="75%&quot;, height=&quot;50%" />
<strong><em><a href="https://arxiv.org/abs/1706.03762">Compare Performance between Encoding and Embedding</a></em></strong>
</p>

<p>ë¹„ë¡ ê°œë³„ í–‰ë²¡í„°ì˜ í¬ì†Œì„±ì´ ë³´ì¥ëœë‹¤ê³  í•´ë„ <code class="language-plaintext highlighter-rouge">Position Encoding</code>ì€ <code class="language-plaintext highlighter-rouge">not trainable</code>í•´ì„œ <code class="language-plaintext highlighter-rouge">static</code>í•˜ë‹¤ëŠ” ë‹¨ì ì´ ìˆë‹¤. ëª¨ë“  ë°°ì¹˜ì˜ ì‹œí€€ìŠ¤ê°€ ë™ì¼í•œ ìœ„ì¹˜ ì •ë³´ê°’ì„ ê°–ê²Œ ëœë‹¤ëŠ” ê²ƒì´ë‹¤. <code class="language-plaintext highlighter-rouge">512</code>ê°œì˜ í† í°ìœ¼ë¡œ êµ¬ì„±ëœ ì‹œí€€ìŠ¤ Aì™€ Bê°€ ìˆë‹¤ê³  ê°€ì •í•´ë³´ì. ì´ ë•Œ ì‹œí€€ìŠ¤ AëŠ” ë¬¸ì¥ <code class="language-plaintext highlighter-rouge">5</code>ê°œë¡œ êµ¬ì„± ë˜ì–´ ìˆê³ , BëŠ” ë¬¸ì¥ <code class="language-plaintext highlighter-rouge">12</code>ê°œë¡œ ë§Œë“¤ì–´ì¡Œë‹¤. ë‘ ì‹œí€€ìŠ¤ì˜ <code class="language-plaintext highlighter-rouge">11</code>ë²ˆì§¸ í† í°ì˜ ë¬¸ì¥ ì„±ë¶„ì€ ê³¼ì—° ê°™ì„ê¹Œ?? ì•„ë§ˆë„ ëŒ€ë¶€ë¶„ì˜ ê²½ìš°ì— ë‹¤ë¥¼ ê²ƒì´ë‹¤. í…ìŠ¤íŠ¸ ë°ì´í„°ì—ì„œ ìˆœì„œ ì •ë³´ê°€ ì¤‘ìš”í•œ ì´ìœ  ì¤‘ í•˜ë‚˜ëŠ” ë°”ë¡œ <code class="language-plaintext highlighter-rouge">syntactical</code> í•œ ì •ë³´ë¥¼ í¬ì°©í•˜ê¸° ìœ„í•¨ì´ë‹¤. <code class="language-plaintext highlighter-rouge">Position Encoding</code>ì€ <code class="language-plaintext highlighter-rouge">static</code> í•˜ê¸° ë•Œë¬¸ì— ì´ëŸ¬í•œ íƒ€ì…ì˜ ì •ë³´ë¥¼ ì¸ì½”ë”© í•˜ê¸° ì‰½ì§€ ì•Šë‹¤. ê·¸ë˜ì„œ ì¢€ ë” í’ë¶€í•œ í‘œí˜„ì„ ë‹´ì„ ìˆ˜ ìˆëŠ” <code class="language-plaintext highlighter-rouge">Position Embedding</code>ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ìµœê·¼ ì¶”ì„¸ë‹¤.</p>

<p><strong><code class="language-plaintext highlighter-rouge">âœï¸ Position Embedding</code></strong></p>

<p>ê·¸ë ‡ë‹¤ë©´ ì´ì œ <code class="language-plaintext highlighter-rouge">Position Embedding</code>ì— ëŒ€í•´ ì•Œì•„ë³´ì. <code class="language-plaintext highlighter-rouge">Position Embedding</code> ì€ <code class="language-plaintext highlighter-rouge">Input Embedding</code>ì„ ì •ì˜í•œ ë°©ì‹ê³¼ ê±°ì˜ ìœ ì‚¬í•˜ë‹¤. ë¨¼ì € ì…ë ¥ê°’ê³¼ <code class="language-plaintext highlighter-rouge">weight</code> ì˜ ëª¨ì–‘ë¶€í„° í™•ì¸í•´ë³´ì.</p>

\[P_E \in R^{B*S_E*d} \\
P_D \in R^{B*S_d*d} \\
W_{P_E} \in R^{S_E * d} \\
W_{P_D} \in R^{S_D * d} \\\]

<p>$P_E, P_D$ëŠ” ê°œë³„ ëª¨ë“ˆì˜ ìœ„ì¹˜ ì„ë² ë”© ë ˆì´ì–´ ì…ë ¥ì„ ê°€ë¦¬í‚¤ë©°, $W_{P_E}, W_{P_D}$ê°€ ê°œë³„ ëª¨ë“ˆì˜ ìœ„ì¹˜ ì„ë² ë”© ë ˆì´ì–´ê°€ ëœë‹¤. ì´ì œ ì´ê²ƒì„ ì½”ë“œë¡œ ì–´ë–»ê²Œ êµ¬í˜„í•˜ëŠ”ì§€ ì‚´í´ë³´ì.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Absolute Position Embedding Example
</span>
<span class="k">class</span> <span class="nc">Encoder</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="s">"""
    In this class, encode input sequence and then we stack N EncoderLayer
    First, we define "positional embedding" and then add to input embedding for making "word embedding"
    Second, forward "word embedding" to N EncoderLayer and then get output embedding
    In official paper, they use positional encoding, which is base on sinusoidal function(fixed, not learnable)
    But we use "positional embedding" which is learnable from training
    Args:
        max_seq: maximum sequence length, default 512 from official paper
        N: number of EncoderLayer, default 6 for base model
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">max_seq</span><span class="p">:</span> <span class="mi">512</span><span class="p">,</span> <span class="n">N</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">6</span><span class="p">,</span> <span class="n">dim_model</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">512</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span> <span class="n">dim_ffn</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2048</span><span class="p">,</span> <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Encoder</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">max_seq</span> <span class="o">=</span> <span class="n">max_seq</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">scale</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">dim_model</span><span class="p">))</span>  <span class="c1"># scale factor for input embedding from official paper
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">positional_embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">max_seq</span><span class="p">,</span> <span class="n">dim_model</span><span class="p">)</span>  <span class="c1"># add 1 for cls token
</span>
		<span class="p">...</span> <span class="n">ì¤‘ëµ</span> <span class="p">...</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">mask</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
        <span class="s">"""
        inputs: embedding from input sequence, shape =&gt; [BS, SEQ_LEN, DIM_MODEL]
        mask: mask for Encoder padded token for speeding up to calculate attention score
        """</span>
        <span class="n">layer_output</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">pos_x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">max_seq</span><span class="p">).</span><span class="n">repeat</span><span class="p">(</span><span class="n">inputs</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]).</span><span class="n">to</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">dropout</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">scale</span> <span class="o">*</span> <span class="n">inputs</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">positional_embedding</span><span class="p">(</span><span class="n">pos_x</span><span class="p">)</span>  <span class="c1"># layernorm ì ìš©í•˜ê³ 
</span>        <span class="p">)</span>
		<span class="p">...</span> <span class="n">ì¤‘ëµ</span> <span class="p">...</span> 
</code></pre></div></div>

<p>ìœ„ ì½”ë“œëŠ” <code class="language-plaintext highlighter-rouge">Transformer</code>ì˜ ì¸ì½”ë” ëª¨ë“ˆì„ êµ¬í˜„í•œ ê²ƒì´ë‹¤. ê·¸ë˜ì„œ <code class="language-plaintext highlighter-rouge">forward</code> ë©”ì„œë“œì˜ <code class="language-plaintext highlighter-rouge">pos_x</code> ê°€ ë°”ë¡œ $P_E$ê°€ ë˜ë©°, <code class="language-plaintext highlighter-rouge">__init__</code>ì˜ <code class="language-plaintext highlighter-rouge">self.positional_embedding</code>ì´ ë°”ë¡œ $W_{P_E}$ì— ëŒ€ì‘ëœë‹¤. ì´ë ‡ê²Œ ì •ì˜í•œ <code class="language-plaintext highlighter-rouge">Position Embedding</code>ì€ <code class="language-plaintext highlighter-rouge">Input Embedding</code>ê³¼ ë”í•´ì„œ <code class="language-plaintext highlighter-rouge">Word Embedding</code> ì„ ë§Œë“ ë‹¤. <code class="language-plaintext highlighter-rouge">Word Embedding</code> ì€ ë‹¤ì‹œ ê°œë³„ ëª¨ë“ˆì˜ <code class="language-plaintext highlighter-rouge">linear projection</code> ë ˆì´ì–´ì— ëŒ€í•œ ì…ë ¥ $X$ë¡œ ì‚¬ìš© ëœë‹¤.</p>

<p><strong>í•œí¸,</strong> <code class="language-plaintext highlighter-rouge">Input Embedding</code> <strong>ê³¼</strong> <code class="language-plaintext highlighter-rouge">Position Embedding</code><strong>ì„ ë”í•œë‹¤ëŠ” ê²ƒì— ì£¼ëª©í•´ë³´ì. í•„ìëŠ” ë³¸ ë…¼ë¬¸ì„ ë³´ë©° ê°€ì¥ ì˜ë¬¸ì´ ë“¤ì—ˆë˜ ë¶€ë¶„ì´ë‹¤. ë„ëŒ€ì²´ ì™œ ì™„ì „íˆ ì„œë¡œ ë‹¤ë¥¸ ì¶œì²˜ì—ì„œ ë§Œë“¤ì–´ì§„ í–‰ë ¬ ë‘ê°œë¥¼</strong> <code class="language-plaintext highlighter-rouge">concat</code> <strong>í•˜ì§€ ì•Šê³  ë”í•´ì„œ ì‚¬ìš©í–ˆì„ê¹Œ??</strong> <code class="language-plaintext highlighter-rouge">concat</code><strong>ì„ ì´ìš©í•˜ë©´ <code class="language-plaintext highlighter-rouge">Input</code>ê³¼ <code class="language-plaintext highlighter-rouge">Position</code> ì •ë³´ë¥¼ ì„œë¡œ ë‹¤ë¥¸ ì°¨ì›ì— ë‘ê³  í•™ìŠµí•˜ëŠ”ê²Œ ê°€ëŠ¥í–ˆì„í…ë° ë§ì´ë‹¤.</strong></p>

<p><strong><code class="language-plaintext highlighter-rouge">ğŸ¤”Â Why Sum instead of Concatenate</code></strong><br />
í–‰ë ¬í•©ì„ ì‚¬ìš©í•˜ëŠ” ì´ìœ ì— ëŒ€í•´ ì €ìê°€ íŠ¹ë³„íˆ ì–¸ê¸‰í•˜ì§€ëŠ” ì•Šì•„ì„œ ë•Œë¬¸ì— ì •í™•í•œ ì˜ë„ë¥¼ ì•Œ ìˆ˜ ì—†ì§€ë§Œ, <strong>ì¶”ì¸¡í•˜ê±´ë° <code class="language-plaintext highlighter-rouge">blessing of dimensionality</code> íš¨ê³¼ë¥¼ ì˜ë„í–ˆì§€ ì•Šì•˜ë‚˜ ì‹¶ë‹¤.</strong> <code class="language-plaintext highlighter-rouge">blessing of dimensionality</code> ë€, ê³ ì°¨ì› ê³µê°„ì—ì„œ ë¬´ì‘ìœ„ë¡œ ì„œë¡œ ë‹¤ë¥¸ ë²¡í„° ë‘ê°œë¥¼ ì„ íƒí•˜ë©´ ë‘ ë²¡í„°ëŠ” ê±°ì˜ ëŒ€ë¶€ë¶„ <code class="language-plaintext highlighter-rouge">approximate orthogonality</code>ë¥¼ ê°–ëŠ” í˜„ìƒì„ ì„¤ëª…í•˜ëŠ” ìš©ì–´ë‹¤. ë¬´ì¡°ê±´ ì„±ë¦½í•˜ëŠ” ì„±ì§ˆì€ ì•„ë‹ˆê³  í™•ë¥ ë¡ ì ì¸ ì ‘ê·¼ì´ë¼ëŠ” ê²ƒì„ ëª…ì‹¬í•˜ì. ì•„ë¬´íŠ¼ ì§êµí•˜ëŠ” ë‘ ë²¡í„°ëŠ” ë‚´ì ê°’ì´ 0ì— ìˆ˜ë ´í•œë‹¤. ì¦‰, ë‘ ë²¡í„°ëŠ” ì„œë¡œì—ê²Œ ì˜í–¥ì„ ë¯¸ì¹˜ì§€ ëª»í•œë‹¤ëŠ” ê²ƒì´ë‹¤. ì´ê²ƒì€ ì „ì²´ ëª¨ë¸ì˜ <code class="language-plaintext highlighter-rouge">hidden states space</code> ì—ì„œ <code class="language-plaintext highlighter-rouge">Input Embedding</code> ê³¼ <code class="language-plaintext highlighter-rouge">Position Embedding</code> ì—­ì‹œ ê°œë³„ ë²¡í„°ê°€ <code class="language-plaintext highlighter-rouge">span</code> í•˜ëŠ” ë¶€ë¶„ ê³µê°„ ë¼ë¦¬ëŠ” ì„œë¡œ ì§êµí•  ê°€ëŠ¥ì„±ì´ ë§¤ìš° ë†’ë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•œë‹¤. ë”°ë¼ì„œ ì„œë¡œ ë‹¤ë¥¸ ì¶œì²˜ë¥¼ í†µí•´ ë§Œë“¤ì–´ì§„ ë‘ í–‰ë ¬ì„ ë”í•´ë„ ì„œë¡œì—ê²Œ ì˜í–¥ì„ ë¯¸ì¹˜ì§€ ëª»í•  ê²ƒì´ê³  ê·¸ë¡œ ì¸í•´ ëª¨ë¸ì´ <code class="language-plaintext highlighter-rouge">Input</code>ê³¼ <code class="language-plaintext highlighter-rouge">Position</code> ì •ë³´ë¥¼ ë”°ë¡œ ì˜ í•™ìŠµí•  ìˆ˜ ìˆì„ ê²ƒì´ë¼ ê¸°ëŒ€í•´ë³¼ ìˆ˜ ìˆë‹¤. ê°€ì •ëŒ€ë¡œë§Œ ëœë‹¤ë©´, <code class="language-plaintext highlighter-rouge">concat</code> ì„ ì‚¬ìš©í•´ ëª¨ë¸ì˜ <code class="language-plaintext highlighter-rouge">hidden states space</code> ë¥¼ ëŠ˜ë ¤ <code class="language-plaintext highlighter-rouge">Computational Overhead</code> ë¥¼ ìœ ë°œí•˜ëŠ” ê²ƒë³´ë‹¤ í›¨ì”¬ íš¨ìœ¨ì ì´ë¼ê³  ë³¼ ìˆ˜ ìˆê² ë‹¤.</p>

<p>í•œí¸ <code class="language-plaintext highlighter-rouge">blessing of dimensionality</code>ì— ëŒ€í•œ ì„¤ëª…ê³¼ ì¦ëª…ì€ ê½¤ë‚˜ ë§ì€ ë‚´ìš©ì´ í•„ìš”í•´ ì—¬ê¸°ì„œëŠ” ìì„¸íˆ ë‹¤ë£¨ì§€ ì•Šê³ , ë‹¤ë¥¸ í¬ìŠ¤íŠ¸ì—ì„œ ë”°ë¡œ ë‹¤ë£¨ê² ë‹¤. ê´€ë ¨í•˜ì—¬ ì¢‹ì€ ë‚´ìš©ì„ ë‹´ê³  ìˆëŠ” ê¸€ì˜ ë§í¬ë¥¼ ê°™ì´ ì²¨ë¶€í–ˆìœ¼ë‹ˆ ì½ì–´ë³´ì‹¤ ê²ƒì„ ê¶Œí•œë‹¤(<a href="https://softwaredoug.com/blog/2022/12/26/surpries-at-hi-dimensions-orthoginality.html">ë§í¬1</a>, <a href="https://www.reddit.com/r/MachineLearning/comments/cttefo/comment/exs7d08/">ë§í¬2</a>).</p>

<h4 id="-self-attention-with-linear-projection"><code class="language-plaintext highlighter-rouge">ğŸš€ Self-Attention with linear projection</code></h4>

<p>ì™œ ì´ë¦„ì´ <code class="language-plaintext highlighter-rouge">self-attention</code>ì¼ê¹Œ ë¨¼ì € ê³ ë¯¼í•´ë³´ì. ì‚¬ì‹¤ <code class="language-plaintext highlighter-rouge">attention</code> ê°œë…ì€ ë³¸ ë…¼ë¬¸ì´ ë°œí‘œë˜ê¸° ì´ì „ë¶€í„° ì‚¬ìš©ë˜ë˜ ê°œë…ì´ë‹¤. <code class="language-plaintext highlighter-rouge">attention</code>ì€ <code class="language-plaintext highlighter-rouge">seq2seq</code> êµ¬ì¡°ì—ì„œ ì²˜ìŒ ë‚˜ì™”ëŠ”ë°, <code class="language-plaintext highlighter-rouge">seq2seq</code> ì€ ë²ˆì—­ ì„±ëŠ¥ì„ ë†’ì´ëŠ” ê²ƒì„ ëª©ì ìœ¼ë¡œ ê³ ì•ˆëœ êµ¬ì¡°ë¼ì„œ, ëª©í‘œì¸ ë””ì½”ë”ì˜ <code class="language-plaintext highlighter-rouge">hidden_states</code> ê°’ì„ ì¿¼ë¦¬ë¡œ, ì¸ì½”ë”ì˜ <code class="language-plaintext highlighter-rouge">hidden_states</code>ë¥¼ í‚¤, ë²¨ë¥˜ì˜ ì¶œì²˜ë¡œ ì‚¬ìš©í–ˆë‹¤. ì¦‰, ì„œë¡œ ë‹¤ë¥¸ ì¶œì²˜ì—ì„œ ë‚˜ì˜¨ <code class="language-plaintext highlighter-rouge">hidden_states</code> ì„ ì‚¬ìš©í•´ ë‚´ì  ì—°ì‚°ì„ ìˆ˜í–‰í–ˆë˜ ê²ƒì´ë‹¤. ì´ëŸ° ê°œë…ì— ì´ì œ <code class="language-plaintext highlighter-rouge">â€œself"</code> ë¼ëŠ” ì´ë¦„ì´ ë¶™ì—ˆë‹¤. ê²°êµ­ ê°™ì€ ì¶œì²˜ì—ì„œ ë‚˜ì˜¨ <code class="language-plaintext highlighter-rouge">hidden_states</code> ë¥¼ ë‚´ì í•˜ê² ë‹¤ëŠ” ì˜ë¯¸ë¥¼ ë‚´í¬í•˜ê³  ìˆëŠ” ê²ƒì´ë‹¤. ë‚´ì ì€ ë‘ ë²¡í„°ì˜ <code class="language-plaintext highlighter-rouge">â€œë‹®ì€ ì •ë„â€</code> ë¥¼ ìˆ˜í•™ì ìœ¼ë¡œ ê³„ì‚°í•œë‹¤. ë”°ë¼ì„œ <code class="language-plaintext highlighter-rouge">self-attention</code> ì´ë€ ê°„ë‹¨í•˜ê²Œ, ê°™ì€ ì¶œì²˜ì—ì„œ ë§Œë“¤ì–´ì§„ $Q$(ì¿¼ë¦¬), $K$(í‚¤), $V$(ë²¨ë¥˜)ê°€ <code class="language-plaintext highlighter-rouge">ì„œë¡œ ì–¼ë§ˆë‚˜ ë‹®ì•˜ëŠ”ì§€</code> ê³„ì‚°í•´ë³´ê² ë‹¤ëŠ” ê²ƒì´ë‹¤.</p>

<p align="center">
<img src="/assets/images/transformer/linear_projection.png" alt="self-attention with linear projection" class="align-center image-caption" width="50%&quot;, height=&quot;50%" />
<strong><em><a href="https://jalammar.github.io/illustrated-transformer/">self-attention with linear projection</a></em></strong>
</p>

<p>ê·¸ë ‡ë‹¤ë©´ ì´ì œ $Q$(ì¿¼ë¦¬), $K$(í‚¤), $V$(ë²¨ë¥˜)ì˜ ì •ì²´, ê°™ì€ ì¶œì²˜ì—ì„œ ë‚˜ì™”ë‹¤ëŠ” ë§ì˜ ì˜ë¯¸ ê·¸ë¦¬ê³  ì…ë ¥ í–‰ë ¬ $X$ë¥¼ <code class="language-plaintext highlighter-rouge">linear projection</code> í•˜ì—¬ $Q$(ì¿¼ë¦¬), $K$(í‚¤), $V$(ë²¨ë¥˜) í–‰ë ¬ì„ ë§Œë“œëŠ” ì´ìœ ë¥¼ <strong>êµ¬ì²´ì ì¸ ì˜ˆì‹œë¥¼ í†µí•´ ì´í•´í•´ë³´ì.</strong> ì¶”ê°€ë¡œ $Q$(ì¿¼ë¦¬), $K$(í‚¤), $V$(ë²¨ë¥˜) ê°œë…ì€ <code class="language-plaintext highlighter-rouge">Information Retrieval</code>ì—ì„œ ë¨¼ì € íŒŒìƒëœ ê°œë…ì´ë¼ì„œ ì˜ˆì‹œ ì—­ì‹œ ì •ë³´ ê²€ìƒ‰ê³¼ ê´€ë ¨ëœ ê²ƒìœ¼ë¡œ ì¤€ë¹„í–ˆë‹¤.</p>

<p>ë‹¹ì‹ ì´ ë§Œì•½ <code class="language-plaintext highlighter-rouge">â€œì—ì–´ì»¨ í•„í„° ì²­ì†Œí•˜ëŠ” ë°©ë²•â€</code>ì´ ê¶ê¸ˆí•´ êµ¬ê¸€ì— ê²€ìƒ‰í•˜ëŠ” ìƒí™©ì´ë¼ê³  ê°€ì •í•´ë³´ê² ë‹¤. <strong>ëª©í‘œëŠ” ê°€ì¥ ë¹ ë¥´ê³  ì •í™•í•˜ê²Œ ë‚´ê°€ ì›í•˜ëŠ” í•„í„° ì²­ì†Œ ë°©ë²•ì— ëŒ€í•œ ì§€ì‹ì„ íšë“í•˜ëŠ” ê²ƒì´ë‹¤.</strong> <strong><code class="language-plaintext highlighter-rouge">ê·¸ë ‡ë‹¤ë©´ ë‹¹ì‹ ì€ ë­ë¼ê³  êµ¬ê¸€ ê²€ìƒ‰ì°½ì— ê²€ìƒ‰í•  ê²ƒì¸ê°€??</code></strong> <strong>ì´ê²ƒì´ ë°”ë¡œ</strong> $Q$<strong>(ì¿¼ë¦¬)ì— í•´ë‹¹í•œë‹¤.</strong> ë‹¹ì‹ ì€ ê²€ìƒ‰ì°½ì— <code class="language-plaintext highlighter-rouge">â€œì—ì–´ì»¨ í•„í„° ì²­ì†Œí•˜ëŠ” ë°©ë²•â€</code>ì„ ì…ë ¥í•´ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°˜í™˜ ë°›ì•˜ë‹¤. <strong>ë°˜í™˜ ë°›ì€ ê²°ê³¼ë¬¼ì˜ ì§‘í•©ì´ ë°”ë¡œ</strong> $K$<strong>(í‚¤)ê°€ ëœë‹¤.</strong> ë‹¹ì‹ ì€ ì´ 100ê°œì˜ ë¸”ë¡œê·¸ ê²Œì‹œë¬¼ì„ í‚¤ ê°’ìœ¼ë¡œ ë°›ì•˜ë‹¤. ê·¸ë˜ì„œ ë‹¹ì‹ ì´ ì‚¬ìš©í•˜ëŠ” ì‚¼ì„± ë¬´í’ ì—ì–´ì»¨ì˜ í•„í„° ì²­ì†Œë²•ì´ ì •í™•íˆ ì íŒ ê²Œì‹œë¬¼ì„ ì°¾ê¸° ìœ„í•´ í•˜ë‚˜ í•˜ë‚˜ ë§í¬ë¥¼ íƒ€ê³  ë“¤ì–´ê°€ ë³´ì•˜ë‹¤. í•˜ì§€ë§Œ ì •í™•í•˜ê²Œ ì›í•˜ëŠ” ì •ë³´ê°€ ì—†ì–´ì„œ ê³„ì† ì°¾ë‹¤ë³´ë‹ˆ ê²°êµ­ 4í˜ì´ì§€ ì¯¤ì—ì„œ ì›í•˜ë˜ ì •ë³´ê°€ ë‹´ê¸´ ê²Œì‹œë¬¼ì„ ì°¾ì„ ìˆ˜ ìˆì—ˆë‹¤. <strong>ì´ë ‡ê²Œ ë‚´ê°€ ì›í•˜ëŠ” ì •ë³´ì¸ì§€ ì•„ë‹Œì§€ ëŒ€ì¡°í•˜ëŠ” ê³¼ì •ì´ ë°”ë¡œ</strong> $Q$<strong>(ì¿¼ë¦¬)ì™€</strong> $K$<strong>(í‚¤) í–‰ë ¬ì„</strong> <code class="language-plaintext highlighter-rouge">ë‚´ì </code><strong>í•˜ëŠ” í–‰ìœ„ê°€ ëœë‹¤.</strong> ê³§ë°”ë¡œ ì—ì–´ì»¨ ì²­ì†Œë¥¼ í•˜ë ¤ê³  ë³´ë‹ˆ, ë°©ë²•ì„ ê¹Œë¨¹ì–´ì„œ ë§¤ë…„ ì—¬ë¦„ë§ˆë‹¤ ê²€ìƒ‰ì„ í•´ì•¼í•  ê²ƒ ê°™ì•„ í•´ë‹¹ ê²Œì‹œë¬¼ì„ ë¶ë§ˆí¬ì— ì €ì¥í•´ë‘ì—ˆë‹¤. <strong>ì—¬ê¸°ì„œ ë¶ë§ˆí¬ê°€ ë°”ë¡œ</strong> $V$<strong>(ë²¨ë¥˜) í–‰ë ¬ì´ ëœë‹¤.</strong></p>

<p>ì´ ëª¨ë“  ê³¼ì •ì— 10ë¶„ì´ ê±¸ë ¸ë‹¤. ê²¨ìš° í•„í„° ì²­ì†Œ ë°©ë²•ì„ ì°¾ëŠ”ë° 10ë¶„ì´ë¼ë‹ˆ ë‹¹ì‹ ì€ ìì¡´ì‹¬ì´ ìƒí–ˆë‹¤. <code class="language-plaintext highlighter-rouge">ë” ë¹¨ë¦¬ ì›í•˜ëŠ” ì •ë³´(ì†ì‹¤ í•¨ìˆ˜ ìµœì í™”)</code>ë¥¼ ì°¾ì„ ìˆ˜ ìˆëŠ” ë°©ë²•ì´ ì—†ì„ê¹Œ ê³ ë¯¼í•´ë³´ë‹¤ê°€ <code class="language-plaintext highlighter-rouge">ë‹¹ì‹ ì´ ì‚¬ìš©í•˜ëŠ” ì—ì–´ì»¨ ë¸Œëœë“œëª…(ì‚¼ì„± Bespoke ì—ì–´ì»¨)ì„ ê²€ìƒ‰ì–´ì— ì¶”ê°€í•˜ê¸°ë¡œ í–ˆë‹¤</code>. ê·¸ë¬ë”ë‹ˆ 1í˜ì´ì§€ ìµœí•˜ë‹¨ì—ì„œ ì•„ê¹Œ 4í˜ì´ì§€ì—ì„œ ì°¾ì€ ì •ë³´ë¥¼ ê³§ë°”ë¡œ ì°¾ì„ ìˆ˜ ìˆì—ˆë‹¤. ê·¸ ë•ë¶„ì— ì‹œê°„ì„ <code class="language-plaintext highlighter-rouge">10ë¶„</code>ì—ì„œ <code class="language-plaintext highlighter-rouge">1ë¶„ 30ì´ˆ</code>ë¡œ ë‹¨ì¶•ì‹œí‚¬ ìˆ˜ ìˆì—ˆë‹¤. <strong>ì´ë ‡ê²Œ ê²€ìƒ‰ ì‹œê°„ì„ ë‹¨ì¶•(ì†ì‹¤ ì¤„ì´ê¸°)í•˜ê¸° ìœ„í•´ ë” ë‚˜ì€ ê²€ìƒ‰ í‘œí˜„ì„ ê³ ë¯¼í•˜ê³  ìˆ˜ì •í•˜ëŠ” í–‰ìœ„ê°€ ë°”ë¡œ ì…ë ¥</strong> $X$ì— $W_{Q}$<strong>ë¥¼ ê³±í•´ í–‰ë ¬</strong> $Q$ <strong>ì„ ë§Œë“œëŠ” ìˆ˜ì‹ìœ¼ë¡œ í‘œí˜„ëœë‹¤.</strong></p>

<p>1ë…„ ë’¤ ì—¬ë¦„, ë‹¹ì‹ ì€ ë¸Œë¼ìš°ì €ë¥¼ ë°”ê¾¼ íƒ“ì— ë¶ë§ˆí¬ê°€ ì´ˆê¸°í™” ë˜ì–´ ë‹¤ì‹œ í•œ ë²ˆ ê²€ìƒ‰ì„ í•´ì•¼ í–ˆë‹¤. í•˜ì§€ë§Œ ì—¬ì „íˆ ê²€ìƒ‰ì–´ëŠ” ê¸°ì–µí•˜ê³  ìˆì–´ì„œ, 1ë…„ì „ ìµœì ì˜ ê²°ê³¼ë¥¼ ì–»ì—ˆë˜ ê·¸ëŒ€ë¡œ ë‹¤ì‹œ ê²€ìƒ‰ì„ í–ˆë‹¤. ë¶„ëª… ë˜‘ê°™ì´ ê²€ìƒ‰ì„ í–ˆëŠ”ë° ê°™ì€ ê²°ê³¼ê°€ 1í˜ì´ì§€ ìµœìƒë‹¨ì—ì„œ ë°˜í™˜ë˜ê³  ìˆì—ˆë‹¤. ë‹¹ì‹ ì€ ì´ê²Œ ì–´ë–»ê²Œ ëœ ì¼ì¸ì§€ ê¶ê¸ˆí•´ í¬ìŠ¤íŠ¸ë¥¼ ì²œì²œíˆ ë³´ë˜ ì¤‘, ì œëª©ì— 1ë…„ì „ì—ëŠ” ì—†ë˜ <code class="language-plaintext highlighter-rouge">ì‚¼ì„± Bespoke ì—ì–´ì»¨</code> ì´ë¼ëŠ” í‚¤ì›Œë“œê°€ í¬í•¨ ë˜ì–´ ìˆì—ˆë‹¤. ê²Œì‹œë¬¼ì˜ ì£¼ì¸ì¥ì´ <code class="language-plaintext highlighter-rouge">SEO ìµœì í™”</code>ë¥¼ ìœ„í•´ ì¶”ê°€í–ˆë˜ ê²ƒì´ì—ˆë‹¤. ë•ë¶„ì— ë‹¹ì‹ ì€ ì†Œìš” ì‹œê°„ì„ <code class="language-plaintext highlighter-rouge">1ë¶„ 30ì´ˆ</code>ì—ì„œ <code class="language-plaintext highlighter-rouge">20ì´ˆ</code>ë¡œ ì¤„ì¼ ìˆ˜ ìˆì—ˆë‹¤. <strong>ì´ëŸ° ìƒí™©ì´ ë°”ë¡œ ì…ë ¥</strong> $X$ì— $W_{K}$<strong>ë¥¼ ê³±í•´ í–‰ë ¬</strong> $K$ <strong>ë¥¼ ë§Œë“œëŠ” ìˆ˜ì‹ì— ëŒ€ì‘ëœë‹¤.</strong></p>

<p>ìš°ë¦¬ëŠ” ìœ„ ì˜ˆì‹œë¥¼ í†µí•´ ì›í•˜ëŠ” ì •ë³´ë¥¼ ë¹ ë¥´ê³  ì •í™•í•˜ê²Œ ì°¾ëŠ” í–‰ìœ„ë€, ë‹µë³€ìê°€ ì´í•´í•˜ê¸° ì¢‹ì€ ì§ˆë¬¸ê³¼ ì§ˆë¬¸ìì˜ ì§ˆë¬¸ ì˜ë„ì— ë¶€í•©í•˜ëŠ” ì¢‹ì€ ë‹µë³€ìœ¼ë¡œ ì™„ì„±ëœë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆì—ˆë‹¤. ë¿ë§Œ ì•„ë‹ˆë¼, ì¢‹ì€ ì§ˆë¬¸ê³¼ ì¢‹ì€ ë‹µë³€ì´ë¼ëŠ” ê²ƒì€ ì²˜ìŒë¶€í„° ì™„ì„±ë˜ëŠ”ê²Œ ì•„ë‹ˆë¼ <strong>ê²€ìƒ‰ ì‹œê°„ì„ ë‹¨ì¶•í•˜ë ¤ëŠ” ëŠì„ì—†ëŠ” ë…¸ë ¥</strong>ì„ í†µí•´ ì„±ì·¨ëœë‹¤ëŠ” ê²ƒ ì—­ì‹œ ê¹¨ìš°ì³¤ë‹¤. ë‘ê°€ì§€ ì¸ì‚¬ì´íŠ¸ê°€ ë°”ë¡œ <code class="language-plaintext highlighter-rouge">linear projection</code>ìœ¼ë¡œ í–‰ë ¬ $Q, K,V$ì„ ì •ì˜í•œ ì´ìœ ë‹¤. <strong>ë‚´ê°€ ì›í•˜ëŠ” ì •ë³´ì¸ì§€ ì•„ë‹Œì§€ ëŒ€ì¡°í•˜ëŠ” ë‚´ì  ì—°ì‚°ì€ ìˆ˜í–‰í•˜ëŠ”ë° ê°€ì¤‘ì¹˜ í–‰ë ¬ì´ í•„ìš” ì—†ê¸° ë•Œë¬¸ì— ì†ì‹¤í•¨ìˆ˜ì˜ ì˜¤ì°¨ ì—­ì „ì„ í™œìš©í•œ ìˆ˜ì¹˜ ìµœì í™”ë¥¼ ìˆ˜í–‰í•  ìˆ˜ ì—†ë‹¤.</strong> ê·¸ë˜ì„œ ì†ì‹¤í•¨ìˆ˜ ë¯¸ë¶„ì— ì˜í•œ ìµœì í™”ê°€ ê°€ëŠ¥í•˜ë„ë¡  <code class="language-plaintext highlighter-rouge">linear projection matrix</code>ë¥¼ í™œìš©í•´ í–‰ë ¬ $Q, K,V$ë¥¼ ì •ì˜í•´ì¤€ ê²ƒì´ë‹¤. <strong>ì´ë ‡ê²Œ í•˜ë©´ ëª¨ë¸ì´ ìš°ë¦¬ì˜ ëª©ì ì— ê°€ì¥ ì í•©í•œ ì§ˆë¬¸ê³¼ ë‹µë³€ì„ ì•Œì•„ì„œ í‘œí˜„ í•´ì¤„ ê²ƒì´ë¼ ê¸°ëŒ€í•  ìˆ˜ ìˆê²Œ ëœë‹¤.</strong> í•œí¸, ê°™ì€ ì¶œì²˜ì—ì„œ ë‚˜ì™”ë‹¤ëŠ” ë§ì€ ë°©ê¸ˆ ì˜ˆì‹œì—ì„œ í–‰ë ¬ $Q, K,V$ë¥¼ ë§Œë“œëŠ”ë° ë™ì¼í•˜ê²Œ ì…ë ¥ $X$ë¥¼ ì‚¬ìš© ê²ƒê³¼ ê°™ì€ ìƒí™©ì„ ì˜ë¯¸í•œë‹¤.</p>

<p>ì´ì œ ë‹¤ì‹œ ìì—°ì–´ ì²˜ë¦¬ ë§¥ë½ìœ¼ë¡œ ëŒì•„ì™€ë³´ì. <code class="language-plaintext highlighter-rouge">Transformer</code> ëŠ” ì¢‹ì€ ë²ˆì—­ê¸°ë¥¼ ë§Œë“¤ê¸° ìœ„í•´ ê³ ì•ˆëœ <code class="language-plaintext highlighter-rouge">seq2seq</code> êµ¬ì¡°ì˜ ëª¨ë¸ì´ë‹¤. ì¦‰, ë¹ ë¥´ê³  ì •í™•í•˜ê²Œ ëŒ€ìƒ ì–¸ì–´ì—ì„œ íƒ€ê²Ÿ ì–¸ì–´ë¡œ ë²ˆì—­í•˜ëŠ” ê²ƒì— ëª©í‘œë¥¼ ë‘ê³  ë§Œë“¤ì–´ì¡Œë‹¤ëŠ” ê²ƒì´ë‹¤. ë²ˆì—­ì„ ì˜í•˜ê¸° ìœ„í•´ì„œëŠ” ì–´ë–»ê²Œ í•´ì•¼ í• ê¹Œ?? <strong>1) ëŒ€ìƒ ì–¸ì–´ë¡œ ì“°ì¸ ì‹œí€€ìŠ¤ì˜ ì˜ë¯¸ë¥¼ ì •í™•í•˜ê²Œ íŒŒì•…í•´ì•¼ í•˜ê³ , 2) íŒŒì•…í•œ ì˜ë¯¸ì™€ ê°€ì¥ ìœ ì‚¬í•œ ì‹œí€€ìŠ¤ë¥¼ íƒ€ê²Ÿ ì–¸ì–´ë¡œ ë§Œë“¤ì–´ ë‚´ì•¼ í•œë‹¤.</strong> <code class="language-plaintext highlighter-rouge">ê·¸ë˜ì„œ 1ë²ˆì˜ ì—­í• ì€ Encoderê°€ ê·¸ë¦¬ê³  2ë²ˆì€ Decoderê°€ ë§¡ê²Œ ëœë‹¤</code>. ì¸ì½”ë”ëŠ” ê²°êµ­ (ë²ˆì—­í•˜ëŠ”ë° ì í•©í•œ í˜•íƒœë¡œ) ëŒ€ìƒ ì–¸ì–´ ì‹œí€€ìŠ¤ì˜ ì˜ë¯¸ë¥¼ ì •í™•íˆ ì´í•´í•˜ëŠ” ë°©í–¥(ìˆ«ìë¡œ í‘œí˜„, ì„ë² ë”© ì¶”ì¶œ)ìœ¼ë¡œ í•™ìŠµì„ ìˆ˜í–‰í•˜ê²Œ ë˜ë©°, ë””ì½”ë”ëŠ” ì¸ì½”ë”ì˜ í•™ìŠµ ê²°ê³¼ì™€ ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì¥ì„ íƒ€ê²Ÿ ì–¸ì–´ë¡œ ìƒì„±í•´ë‚´ëŠ” ê³¼ì •ì„ ë°°ìš°ê²Œ ëœë‹¤. ë”°ë¼ì„œ ì¸ì½”ë”ëŠ” ëŒ€ìƒ ì–¸ì–´ë¥¼ ì¶œì²˜ë¡œ, ë””ì½”ë”ëŠ” íƒ€ê²Ÿ ì–¸ì–´ë¥¼ ì¶œì²˜ë¡œ í–‰ë ¬ $Q, K,V$ë¥¼ ë§Œë“ ë‹¤. ì •í™•íˆ <code class="language-plaintext highlighter-rouge">self</code> ë¼ëŠ” ë‹¨ì–´ë¥¼ ì´ë¦„ì— ê°–ë‹¤ ë¶™ì¸ ì˜ë„ì™€ ì¼ë§¥ìƒí†µí•˜ëŠ” ëª¨ìŠµì´ë‹¤.</p>

<p><strong>ê²°êµ­</strong> <code class="language-plaintext highlighter-rouge">Transformer</code> <strong>ì˜ ì„±ëŠ¥ì„ ì¢Œì§€ìš°ì§€ í•˜ëŠ” ê²ƒì€ ëˆ„ê°€ ì–¼ë§ˆë‚˜ ë”</strong> <code class="language-plaintext highlighter-rouge">linear projection weight</code><strong>ì„ ì˜ ìµœì í™” í•˜ëŠ”ê°€ì— ë‹¬ë ¸ë‹¤ê³  ë³¼ ìˆ˜ ìˆë‹¤.</strong></p>

<p><strong>í•œí¸ í•„ìëŠ” ì²˜ìŒ ì´ ë…¼ë¬¸ì„ ì½ì—ˆì„ ë•Œ</strong> <code class="language-plaintext highlighter-rouge">linear projection</code> <strong>ìì²´ì˜ í•„ìš”ì„±ì€ ê³µê°í–ˆìœ¼ë‚˜, êµ³ì´ 3ê°œì˜ í–‰ë ¬ë¡œ ë‚˜ëˆ ì„œ</strong> <code class="language-plaintext highlighter-rouge">train</code> <strong>ì‹œì¼œì•¼ í•˜ëŠ”</strong> <code class="language-plaintext highlighter-rouge">param</code> <strong>ìˆ«ìë¥¼ ëŠ˜ë¦¬ëŠ” ê²ƒë³´ë‹¤ëŠ”</strong> <code class="language-plaintext highlighter-rouge">weight share</code> <strong>í•˜ëŠ” í˜•íƒœë¡œ ë§Œë“œëŠ”ê²Œ ë” íš¨ìœ¨ì ì¼ ê²ƒ ê°™ë‹¤ëŠ” ì¶”ì¸¡ì„ í–ˆì—ˆë‹¤.</strong></p>

<p>ê·¸ëŸ¬ë‚˜ ì´ë²ˆ ë¦¬ë·°ë¥¼ ìœ„í•´ ë‹¤ì‹œ ë…¼ë¬¸ì„ ì½ë˜ ì¤‘, ì¢‹ì€ ì§ˆë¬¸ì„ í•˜ê¸° ìœ„í•œ ë…¸ë ¥ê³¼ ì¢‹ì€ ë‹µë³€ì„ í•˜ê¸° ìœ„í•œ ë…¸ë ¥, ê·¸ë¦¬ê³  í•„ìš”í•œ ì •ë³´ë¥¼ ì •í™•íˆ ì¶”ì¶œí•´ë‚´ëŠ” í–‰ìœ„ë¥¼ ê°ê° ì„œë¡œ ë‹¤ë¥¸ 3ê°œì˜ ë²¡í„°ë¡œ í‘œí˜„í–ˆì„ ë•Œ <strong>ë²¡í„°ë“¤ì´ ê°€ì§€ëŠ” ë°©í–¥ì„±ì´ ì„œë¡œ ë‹¤ë¥¼í…ë°</strong> ê·¸ê²ƒì„ í•˜ë‚˜ì˜ ë²¡í„°ë¡œ í‘œí˜„í•˜ë ¤ë©´ ëª¨ë¸ì´ í•™ìŠµì„ í•˜ê¸° í˜ë“¤ ê²ƒ ê°™ë‹¤ëŠ” ìƒê°ì´ ë“¤ì—ˆë‹¤. ë°©ê¸ˆ ìœ„ì—ì„œ ë“  ì˜ˆì‹œë§Œ ë´ë„ ê·¸ë ‡ë‹¤. ì„œë¡œ ë‹¤ë¥¸ 3ê°œì˜ í–‰ìœ„ ì‚¬ì´ì˜ ìµœì  ì§€ì ì„ ì°¾ìœ¼ë¼ëŠ” ê²ƒê³¼ ë§ˆì°¬ê°€ì§„ë° ê·¸ëŸ° ìŠ¤íŒŸì´ ìˆë‹¤ê³  í•´ë„ ì–¸ì–´ ëª¨ë¸ì´ ì˜ ì°¾ì„ ìˆ˜ ìˆì„ê¹Œ?? ì¸ê°„ë„ ì°¾ê¸° í˜ë“  ê²ƒì„ ëª¨ë¸ì´ ì˜ ì°¾ì„ë¦¬ê°€ ì—†ë‹¤.</p>

<h4 id="scaled-dot-product-attention"><strong><code class="language-plaintext highlighter-rouge">ğŸ“Â Scaled Dot-Product Attention</code></strong></h4>

\[Attention(Q,K,V) = softmax(\frac{QÂ·K^T}{\sqrt{d_k}})V\]

<p>ì´ë²ˆì—ëŠ” <code class="language-plaintext highlighter-rouge">Self-Attention</code> ì˜ ë‘ ë²ˆì§¸ í•˜ìœ„ ë¸”ëŸ­ì¸ <code class="language-plaintext highlighter-rouge">Scaled Dot-Product Attention</code> ì°¨ë¡€ë‹¤. ì‚¬ì‹¤ ìš°ë¦¬ëŠ” <code class="language-plaintext highlighter-rouge">Linear Projection</code> íŒŒíŠ¸ì—ì„œ ì´ë¯¸ ìš°ë¦¬ë„ ëª¨ë¥´ê²Œ <code class="language-plaintext highlighter-rouge">Scaled Dot-Product Attention</code> ì— ëŒ€í•´ ê³µë¶€í–ˆë‹¤. ì˜ˆì‹œë¥¼ ë‹¤ì‹œ í•œ ë²ˆ ìƒê¸°ì‹œì¼œë³´ì. ì§ˆì˜ë¥¼ í†µí•´ ì–»ì€ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸(í‚¤)ì—ì„œ ë‚´ê°€ ì›í•˜ëŠ” ì •ë³´ë¥¼ ì°¾ê¸° ìœ„í•´ ì¿¼ë¦¬ì™€ í‚¤ë¥¼ ëŒ€ì¡°í•œë‹¤ê³  í–ˆë˜ ê²ƒ ê¸°ì–µë‚˜ëŠ”ê°€?? ë°”ë¡œ ê·¸ ëŒ€ì¡°í•˜ëŠ” í–‰ìœ„ë¥¼ ìˆ˜í•™ì ìœ¼ë¡œ ëª¨ë¸ë§í•œ ê²ƒì´ ë°”ë¡œ <code class="language-plaintext highlighter-rouge">Scaled Dot-Product Attention</code> ì— í•´ë‹¹í•œë‹¤.</p>

<p align="center">
<img src="/assets/images/transformer/dot_attention.png" alt="Attention is All You Need" class="align-center image-caption" width="50%&quot;, height=&quot;50%" />
<strong><em><a href="https://arxiv.org/abs/1706.03762">Attention is All You Need</a></em></strong>
</p>

<p><code class="language-plaintext highlighter-rouge">Scaled Dot-Product Attention</code> ì€ ì´ 5ë‹¨ê³„ë¥¼ ê±°ì³ ì™„ì„±ëœë‹¤. ë‹¨ê³„ë§ˆë‹¤ ì–´ë–¤ ì—°ì‚°ì„ ì™œ í•˜ëŠ”ì§€ ê·¸ë¦¬ê³  ë¬´ìŠ¨ ì¸ì‚¬ì´íŠ¸ê°€ ë‹´ê²¨ ìˆëŠ”ì§€ ì•Œì•„ë³´ì. ì´ ì¤‘ì—ì„œ ë§ˆìŠ¤í‚¹ ë‹¨ê³„ëŠ” ì¸ì½”ë”ì™€ ë””ì½”ë”ì˜ ë™ì‘ì„ ìì„¸íˆ ì•Œì•„ì•¼í•˜ê¸° ë•Œë¬¸ì— ì „ì²´ì ì¸ êµ¬ì¡° ê´€ì ì—ì„œ ëª¨ë¸ì„ ë°”ë¼ë³¼ ë•Œ í•¨ê»˜ ì„¤ëª…í•˜ë„ë¡ í•˜ê² ë‹¤.</p>

<p><strong><code class="language-plaintext highlighter-rouge">âœ–ï¸Â Stage 1. Qâ€¢K^T Dot-Product</code></strong></p>

\[Qâ€¢K^T\]

<p>ì¸ê°„ì€ ë¬¸ì¥ì´ë‚˜ ì–´ë–¤ í‘œí˜„ì˜ ì˜ë¯¸ë¥¼ íŒŒì•…í•˜ëŠ”ë° ë°”ë¡œ ì£¼ë³€ ë§¥ë½ì„ ì°¸ê³ í•˜ê±°ë‚˜, ë” ë©€ë¦¬ ë–¨ì–´ì§„ ê³³ì˜ ë‹¨ì–´â€¢ì‹œí€€ìŠ¤ë¥¼ ì´ìš©í•˜ê¸°ë„ í•œë‹¤. <strong>ì¦‰, ì£¼ì–´ì§„ ì‹œí€€ìŠ¤ ë‚´ë¶€ì˜ ëª¨ë“  ë§¥ë½ì„ ì´ìš©í•´ íŠ¹ì • ë¶€ë¶„ì˜ ì˜ë¯¸ë¥¼ ì´í•´í•œë‹¤ëŠ” ê²ƒì´ë‹¤.</strong> ê·¸ë ‡ë‹¤ê³  ëª¨ë“  ì •ë³´ê°€ ë™ì¼í•˜ê²Œ íŠ¹ì • í‘œí˜„ì˜ ì˜ë¯¸ì— ì˜í–¥ì„ ë¯¸ì¹˜ëŠ” ê²ƒì€ ë˜ ì•„ë‹Œë°, ìˆ˜ëŠ¥ ì˜ì–´ì— í‚¬ëŸ¬ ë¬¸í•­ìœ¼ë¡œ ë“±ì¥í•˜ëŠ” ë¹ˆì¹¸ ì±„ìš°ê¸° ë¬¸ì œë¥¼ ì–´ë–»ê²Œ í’€ì—ˆë‚˜ ë– ì˜¬ë ¤ë³´ì. ë””í…Œì¼í•œ í’€ì´ ë°©ì‹ì—ëŠ” ì‚¬ëŒë§ˆë‹¤ ì°¨ì´ê°€ ìˆê² ì§€ë§Œ, ì¼ë°˜ì ìœ¼ë¡œ ì§€ë¬¸ì€ ëª¨ë‘ í›‘ì–´ ë³´ë˜ ë¹ˆì¹¸ì— ë“¤ì–´ê°ˆ ì •ë‹µì˜ ê·¼ê±°ê°€ ë˜ëŠ” íŠ¹ì • ë¬¸ì¥ í˜¹ì€ í‘œí˜„ 1~2ê°œë¥¼ ì°¾ì•„ë‚´ì–´ ë¹„ìŠ·í•œ ì˜ë¯¸ë¥¼ ì§€ë‹Œ ì„ ì§€ë¥¼ ê³¨ë¼ ë‚´ëŠ” ë°©ì‹ì„ ì‚¬ìš©í•œë‹¤. <strong>ë‹¤ì‹œ ë§í•´, ì£¼ì–´ì§„ ì „ì²´ ë‹¨ë½ì—ì„œ ì˜ë¯¸ë¥¼ ì´í•´í•˜ëŠ”ë° ì¤‘ìš”í•œ ì—­í• ì„ í•˜ëŠ” í‘œí˜„ì´ë‚˜ ë¬¸ì¥ì„ ê³¨ë¼ë‚´ì–´ <code class="language-plaintext highlighter-rouge">ì¤‘ìš”ë„</code> ë§Œí¼ <code class="language-plaintext highlighter-rouge">ê°€ì¤‘ì¹˜</code> ë¥¼ ì£¼ê² ë‹¤ëŠ” ê²ƒì´ë‹¤.</strong></p>

<p align="center">
<img src="/assets/images/transformer/attention_visualization.png" alt="Qâ€¢K^T Dot Product Visualization" class="align-center image-caption" width="50%&quot;, height=&quot;50%" />
<strong><em><a href="https://jalammar.github.io/illustrated-transformer/">Qâ€¢K^T Dot Product Visualization</a></em></strong>
</p>

<p>ê·¸ë ‡ë‹¤ë©´ ì´ê²ƒì„ ì–´ë–»ê²Œ ìˆ˜í•™ì ìœ¼ë¡œ ëª¨ë¸ë§í–ˆì„ê¹Œ?? ë°”ë¡œ í–‰ë ¬ $Q$ì™€ $K^T$ì˜ <code class="language-plaintext highlighter-rouge">ë‚´ì </code>ì„ í™œìš©í•œë‹¤. í–‰ë ¬ $Q$ëŠ” ëª¨ë¸ì´ ì˜ë¯¸ë¥¼ íŒŒì•…í•´ì•¼ í•˜ëŠ” ëŒ€ìƒì´ ë‹´ê²¨ ìˆê³ , í–‰ë ¬ $K$ì—ëŠ” ì˜ë¯¸ íŒŒì•…ì— í•„ìš”í•œ ë‹¨ì„œë“¤ì´ ë‹´ê²¨ìˆë‹¤. ë‚´ì ì€ ë‘ ë²¡í„°ì˜ ì„œë¡œ <code class="language-plaintext highlighter-rouge">â€œë‹®ì€ ì •ë„â€</code> ë¥¼ ì˜ë¯¸í•œë‹¤ê³  í–ˆë‹¤. <code class="language-plaintext highlighter-rouge">â€œë‹®ì€ ì •ë„â€</code> ê°€ ë°”ë¡œ <code class="language-plaintext highlighter-rouge">ì¤‘ìš”ë„â€¢ê°€ì¤‘ì¹˜</code>ì— ëŒ€ì‘ëœë‹¤. ë”°ë¼ì„œ ì—°ì‚° ê²°ê³¼ëŠ” ì „ì²´ ì‹œí€€ìŠ¤ì— ì†í•œ í† í°ë“¤ ì‚¬ì´ì˜ <code class="language-plaintext highlighter-rouge">â€œë‹®ì€ ì •ë„â€</code> ê°€ ìˆ˜ì¹˜ë¡œ ë³€í™˜ë˜ì–´ í–‰ë ¬ì— ë‹´ê¸´ë‹¤.</p>

<p>ì™œ <code class="language-plaintext highlighter-rouge">ë‚´ì  ê²°ê³¼</code>ê°€ <code class="language-plaintext highlighter-rouge">ì¤‘ìš”ë„</code>ì™€ ê°™ì€ ì˜ë¯¸ë¥¼ ê°–ê²Œ ë˜ëŠ” ê²ƒì¼ê¹Œ?? ì•„ê¹Œ <code class="language-plaintext highlighter-rouge">Input Embedding</code>ê³¼ <code class="language-plaintext highlighter-rouge">Position Embedding</code>ì„ í–‰ë ¬í•© í•˜ëŠ” ê²ƒì— ëŒ€í•œ ë‹¹ìœ„ì„±ì„ ì„¤ëª…í•˜ë©´ì„œ ê³ ì°¨ì›ìœ¼ë¡œ ê°ˆìˆ˜ë¡ ëŒ€ë¶€ë¶„ì˜ ë²¡í„° ìŒì€ <code class="language-plaintext highlighter-rouge">ì§êµì„±</code>ì„ ê°–ê²Œ ëœë‹¤ê³  ì–¸ê¸‰í•œ ë°” ìˆë‹¤. ê·¸ë˜ì„œ ë‘ ë²¡í„°ê°€ ë¹„ìŠ·í•œ ë°©í–¥ì„±ì„ ê°–ëŠ”ë‹¤ëŠ” ê²ƒ ìì²´ê°€ ë§¤ìš° ë“œë¬¸ì¼ì´ë‹¤. í¬ê·€í•˜ê³  ë“œë¬¸ ì‚¬ê±´ì€ ê·¸ë§Œí¼ ì¤‘ìš”í•˜ë‹¤ê³  ë§í•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì— <code class="language-plaintext highlighter-rouge">ë‚´ì  ê²°ê³¼</code>ë¥¼ <code class="language-plaintext highlighter-rouge">ì¤‘ìš”ë„</code>ì— ë§µí•‘í•˜ëŠ” ê²ƒì´ë‹¤.</p>

<p>í•œí¸, í–‰ë ¬ $Q,K$ ëª¨ë‘ ì°¨ì›ì´ <code class="language-plaintext highlighter-rouge">[Batch, Max_Seq, Dim_Head]</code> ì¸ í…ì„œë¼ì„œ ë‚´ì í•œ ê²°ê³¼ì˜ ëª¨ì–‘ì€ <code class="language-plaintext highlighter-rouge">[Batch, Max_Seq, Max_Seq]</code> ì´ ë  ê²ƒì´ë‹¤.</p>

<p><strong><code class="language-plaintext highlighter-rouge">ğŸ”­Â Stage 2. Scale</code></strong></p>

\[Qâ€¢K^T = \begin{bmatrix}
56.8 &amp; 12.1 &amp; 43.5 \\
30.4 &amp; 100.8 &amp; 24.2 \\
11.11 &amp; 7.34 &amp; 20.23 \\
\end{bmatrix}\]

<p><code class="language-plaintext highlighter-rouge">â€œI am dogâ€</code> ë¼ëŠ” ë¬¸ì¥ì„ $Qâ€¢K^T$í•˜ë©´ ìœ„ì™€ ê°™ì€ <code class="language-plaintext highlighter-rouge">3x3</code> ì§œë¦¬ í–‰ë ¬ì´ ë‚˜ì˜¬ ê²ƒì´ë‹¤. í–‰ë ¬ì„ í–‰ë²¡í„°ë¡œ ë°”ë¼ë³´ì. <strong>í–‰ ì‚¬ì´ì˜ ê°’ì˜ ë¶„í¬ê°€ ê³ ë¥´ì§€ ëª»í•˜ë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤.</strong> ì´ë ‡ê²Œ ë¶„ì‚°ì´ í° ìƒíƒœë¡œ <code class="language-plaintext highlighter-rouge">softmax</code> ì— í†µê³¼ì‹œí‚¤ê²Œ ë˜ë©´ ì—­ì „íŒŒ ê³¼ì •ì—ì„œ <code class="language-plaintext highlighter-rouge">softmax</code> ì˜ ë¯¸ë¶„ê°’ì´ ì¤„ì–´ ë“¤ì–´ í•™ìŠµ ì†ë„ê°€ ëŠë ¤ì§€ê³  ë‚˜ì•„ê°€ <code class="language-plaintext highlighter-rouge">vanishing gradient</code> í˜„ìƒì´ ë°œìƒí•  ìˆ˜ ìˆë‹¤. ë”°ë¼ì„œ í–‰ë²¡í„° ì‚¬ì´ì˜ ë¶„ì‚°ì„ ì¤„ì—¬ì£¼ê¸° ìœ„í•´ì„œ <code class="language-plaintext highlighter-rouge">Scale Factor</code> ë¥¼ ì •ì˜í•˜ê²Œ ëœë‹¤. ê·¸ë ‡ë‹¤ë©´ ì–´ë–¤ <code class="language-plaintext highlighter-rouge">Scale Factor</code> ë¥¼ ì¨ì•¼í• ê¹Œ??</p>

\[\frac{Qâ€¢K^T}{\sqrt{d_h}}\]

<p>ì• ì´ˆì— <code class="language-plaintext highlighter-rouge">Dim Head</code> ì°¨ì›ì— ì†í•œ ê°’ë“¤ì˜ ë¶„ì‚°ì´ í° ê²ƒë„ ë¬¸ì œê°€ ë˜ì§€ë§Œ ì´ê²ƒì€ <code class="language-plaintext highlighter-rouge">Input Embedding</code>ì´ë‚˜ <code class="language-plaintext highlighter-rouge">Position Embedding</code>ì— <code class="language-plaintext highlighter-rouge">layernorm</code> ì„ ì ìš©í•˜ë©´ í•´ê²°í•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì— ë…¼ì˜ ëŒ€ìƒì´ ì•„ë‹ˆë‹¤. ê·¸ê²ƒë³´ë‹¤ëŠ” ë‚´ì  ê³¼ì •ì— ì£¼ëª©í•´ë³´ì. ìš°ë¦¬ëŠ” ë‚´ì ì„ í•˜ë‹¤ë³´ë©´ <code class="language-plaintext highlighter-rouge">Dim Head</code>ì˜ ì°¨ì›ì´ ì»¤ì§ˆìˆ˜ë¡ ë”í•´ì¤˜ì•¼ í•˜ëŠ” ìŠ¤ì¹¼ë¼ ê°’ì˜ ê°œìˆ˜ê°€ ëŠ˜ì–´ë‚˜ê²Œ ëœë‹¤ëŠ” ì‚¬ì‹¤ì„ ì•Œ ìˆ˜ ìˆë‹¤. ë§Œì•½ ìœ„ì—ì„œ ì˜ˆì‹œë¡œ ë“  ìˆ˜ì‹ì˜ <code class="language-plaintext highlighter-rouge">Dim Head</code>ê°€ 64ë¼ê³  ê°€ì •í•´ë³´ì. ê·¸ëŸ¼ ìš°ë¦¬ëŠ” 1í–‰ 1ì—´ì˜ ê°’ì„ ì–»ê¸° ìœ„í•´ 64ê°œì˜ ìŠ¤ì¹¼ë¼ ê°’ì„ ë”í•´ì¤˜ì•¼ í•œë‹¤. ë§Œì•½ <code class="language-plaintext highlighter-rouge">512</code>ì°¨ì›ì´ë¼ë©´ <code class="language-plaintext highlighter-rouge">512</code>ê°œë¡œ ë¶ˆì–´ë‚œë‹¤. <strong>ë”í•´ì¤˜ì•¼ í•˜ëŠ” ìŠ¤ì¹¼ë¼ ê°’ì´ ë§ì•„ì§„ë‹¤ë©´ í–‰ë²¡í„° ë¼ë¦¬ì˜ ë¶„ì‚°ì´ ì»¤ì§ˆ ìš°ë ¤ê°€ ìˆë‹¤.</strong> ë”°ë¼ì„œ ì°¨ì› í¬ê¸°ì˜ ìŠ¤ì¼€ì¼ì— ë”°ë¼ <code class="language-plaintext highlighter-rouge">softmax</code>ì˜ ë¯¸ë¶„ê°’ì´ ì¤„ì–´ë“œëŠ” ê²ƒì„ ë°©ì§€í•˜ê¸° ìœ„í•´ $Qâ€¢K^T$ê²°ê³¼ì— $\sqrt{d_h}$ë¥¼ ë‚˜ëˆ  ì¤€ë‹¤.</p>

<p>ì—¬ë‹´ìœ¼ë¡œ ì´ëŸ¬í•œ <code class="language-plaintext highlighter-rouge">scale factor</code> ì˜ ì¡´ì¬ ë•Œë¬¸ì— <code class="language-plaintext highlighter-rouge">Self-Attention</code>ì„ <code class="language-plaintext highlighter-rouge">Scaled Dot-Product Attention</code> ì´ë¼ê³  ë¶€ë¥´ê¸°ë„ í•œë‹¤.</p>

<p><strong><code class="language-plaintext highlighter-rouge">ğŸ­Â Stage 3. masking</code></strong><br />
ë§ˆìŠ¤í‚¹ì€ ì¸ì½”ë” <code class="language-plaintext highlighter-rouge">Input Padding</code>, ë””ì½”ë” <code class="language-plaintext highlighter-rouge">Masked Multi-Head Attention</code>, ì¸ì½”ë”-ë””ì½”ë” <code class="language-plaintext highlighter-rouge">Self-Attention</code> ì„ ìœ„í•´ í•„ìš”í•œ ê³„ì¸µì´ë‹¤. ë’¤ì— ë‘ê°œëŠ” ë””ì½”ë”ì˜ ë™ì‘ì„ ì•Œì•„ì•¼ ì´í•´ê°€ ê°€ëŠ¥í•˜ê¸° ë•Œë¬¸ì— ì—¬ê¸°ì„œëŠ” ì¸ì½”ë”ì˜ ë§ˆìŠ¤í‚¹ì— ëŒ€í•´ì„œë§Œ ì•Œì•„ë³´ì.</p>

<p align="center">
<img src="/assets/images/transformer/encoder_mask.png" alt="Encoder Padding Mask" class="align-center image-caption" width="50%&quot;, height=&quot;50%" />
<strong><em><a href="https://paul-hyun.github.io/transformer-02/">Encoder Padding Mask</a></em></strong>
</p>

<p>ì‹¤ì œ í…ìŠ¤íŠ¸ ë°ì´í„°ëŠ” ë°°ì¹˜ëœ ì‹œí€€ìŠ¤ë§ˆë‹¤ ê·¸ ê¸¸ì´ê°€ ì œê°ê°ì´ë‹¤. íš¨ìœ¨ì„±ì„ ìœ„í•´ í–‰ë ¬ì„ ì‚¬ìš©í•˜ëŠ” ì»´í“¨í„° ì—°ì‚° íŠ¹ì„±ìƒ ë°°ì¹˜ëœ ì‹œí€€ìŠ¤ì˜ ê¸¸ì´ê°€ ëª¨ë‘ ë‹¤ë¥´ë‹¤ë©´ ì—°ì‚°ì„ ì§„í–‰í•  ìˆ˜ê°€ ì—†ë‹¤. ë”°ë¼ì„œ ë°°ì¹˜ ë‚´ë¶€ì˜ ëª¨ë“  ì‹œí€€ìŠ¤ì˜ ê¸¸ì´ë¥¼ í†µì¼í•´ì£¼ëŠ” ì‘ì—…ì„ í•˜ê²Œ ë˜ëŠ”ë°, ì´ ë•Œ ê¸°ì¤€ ê¸¸ì´ë³´ë‹¤ ì§§ì€ ì‹œí€€ìŠ¤ì— ëŒ€í•´ì„œëŠ” <code class="language-plaintext highlighter-rouge">0</code>ê°’ì„ ì±„ì›Œë„£ëŠ” <code class="language-plaintext highlighter-rouge">padding</code> ì‘ì—…ì„ í•œë‹¤.  í–‰ë ¬ ì—°ì‚°ì—ëŠ” ê¼­ í•„ìš”í–ˆë˜ <code class="language-plaintext highlighter-rouge">padding</code>ì€ ì˜¤íˆë ¤ <code class="language-plaintext highlighter-rouge">softmax</code> ë ˆì´ì–´ë¥¼ ê³„ì‚°í•  ë•Œ ë°©í•´ê°€ ëœë‹¤. ë”°ë¼ì„œ ëª¨ë“  <code class="language-plaintext highlighter-rouge">padding</code> ê°’ì„ <code class="language-plaintext highlighter-rouge">softmax</code>ì˜ í™•ë¥  ê³„ì‚°ì—ì„œ ì™„ì „íˆ ì œì™¸ì‹œí‚¤ê¸° ìœ„í•´ <code class="language-plaintext highlighter-rouge">Input Embedding</code>ì—ì„œ <code class="language-plaintext highlighter-rouge">padding token</code>ì˜ ì¸ë±ìŠ¤ë¥¼ ì €ì¥í•˜ê³  í•´ë‹¹ë˜ëŠ” ëª¨ë“  ì›ì†Œë¥¼ <code class="language-plaintext highlighter-rouge">-âˆ</code> ë¡œ ë§ˆìŠ¤í‚¹í•˜ëŠ” ê³¼ì •ì´ í•„ìš”í•˜ë‹¤.</p>

<p>ì´ ë•Œ ë§ˆìŠ¤í‚¹ ì²˜ë¦¬ëŠ” ì—´ë²¡í„°ì—ë§Œ ì ìš©í•œë‹¤. ê·¸ ì´ìœ ëŠ” ë°”ë¡œ <code class="language-plaintext highlighter-rouge">softmax</code> ê³„ì‚°ì„ ì–´ì°¨í”¼ í–‰ë²¡í„° ë°©í–¥ìœ¼ë¡œë§Œ í•  ê²ƒì´ê¸° ë•Œë¬¸ì´ë‹¤. í–‰ë²¡í„° ë°©í–¥ì˜ <code class="language-plaintext highlighter-rouge">padding token</code>ì—ë„ ë™ì¼í•˜ê²Œ ë§ˆìŠ¤í‚¹ ì ìš©í•˜ëŠ” ê²ƒì€ ìƒê´€ ì—†ìœ¼ë‚˜ ì—´ë²¡í„°ì™€ í–‰ë²¡í„° ë™ì‹œì— ë§ˆìŠ¤í‚¹ ì ìš©í•˜ëŠ” ë™ì‘ì„ êµ¬í˜„í•˜ëŠ” ê²ƒì€ ìƒê°ë³´ë‹¤ ë§ì´ ê¹Œë‹¤ë¡œìš°ë©°, ë‚˜ì¤‘ì— ì†ì‹¤ê°’ ê³„ì‚°í•˜ëŠ” ë‹¨ê³„ì—ì„œ <code class="language-plaintext highlighter-rouge">ignore_index</code> ì˜µì…˜ì„ ì‚¬ìš©í•´ í–‰ë²¡í„°ì˜ <code class="language-plaintext highlighter-rouge">padding token</code>ì„ ë¬´ì‹œí•˜ëŠ” ê²ƒì´ í›¨ì”¬ íš¨ìœ¨ì ì´ë‹¤. í•œí¸, <code class="language-plaintext highlighter-rouge">ignore_index</code> ì˜µì…˜ì€ <code class="language-plaintext highlighter-rouge">nn.CrossEntropyLoss</code> ì— ë§¤ê°œë³€ìˆ˜ë¡œ êµ¬í˜„ ë˜ì–´ ìˆë‹¤.</p>

<p><strong><code class="language-plaintext highlighter-rouge">ğŸ“ˆÂ Stage 4. Softmax &amp; Scoreâ€¢V</code></strong></p>

\[Score = \begin{bmatrix}
  0.90 &amp; 0.07 &amp; 0.03 \\
  0.025 &amp; 0.95 &amp; 0.025 \\
  0.21 &amp; 0.03 &amp; 0.76 
\end{bmatrix}, \ \  V=\begin{bmatrix}
  67.85 &amp; 90 &amp; 91 &amp; ..... \\
  62 &amp; 40 &amp; 50 &amp; ..... \\
  37 &amp; 41 &amp; 20 &amp; .....
\end{bmatrix},\ \  Z = score \ â€¢ \ V\]

\[{\overset{}{z_{1}^{}}} = {\overset{}{Score_{11}^{}}}({\overset{}{V_{11}^{}}}\ + \ {\overset{}{V_{12}^{}}}\ + \ ...) \ + \ {\overset{}{Score_{12}^{}}}({\overset{}{V_{21}^{}}}\ + \ {\overset{}{V_{22}^{}}}\ + \ ...)\ + \ ....... \\
{\overset{}{z_{2}^{}}} = {\overset{}{Score_{21}^{}}}({\overset{}{V_{11}^{}}}\ + \ {\overset{}{V_{12}^{}}}\ + \ ...) \ + \ {\overset{}{Score_{22}^{}}}({\overset{}{V_{21}^{}}}\ + \ {\overset{}{V_{22}^{}}}\ + \ ...)\ + \ ....... \\
{\overset{}{z_{3}^{}}} = {\overset{}{Score_{31}^{}}}({\overset{}{V_{11}^{}}}\ + \ {\overset{}{V_{12}^{}}}\ + \ ...) \ + \ {\overset{}{Score_{32}^{}}}({\overset{}{V_{21}^{}}}\ + \ {\overset{}{V_{22}^{}}}\ + \ ...)\ + \ ....... \\\]

<p>ê³„ì‚°ëœ <code class="language-plaintext highlighter-rouge">ìœ ì‚¬ë„(ë‚´ì  ê²°ê³¼, ì¤‘ìš”ë„, ê°€ì¤‘ì¹˜)</code>, $\frac{Qâ€¢K^T}{\sqrt{d_h}}$ëŠ” ì´í›„ì— í–‰ë ¬ $V$ì™€ ë‹¤ì‹œ ê³±í•´ì ¸ í–‰ë²¡í„° $Z_n$(në²ˆì§¸ í† í°)ì—ì„œ í† í°ì— ëŒ€í•œ ì–´í…ì…˜ ì •ë„ë¥¼ ë‚˜íƒ€ë‚´ëŠ” <code class="language-plaintext highlighter-rouge">ê°€ì¤‘ì¹˜</code>ì˜ ì—­í• ì„ í•˜ê²Œ ëœë‹¤. ê·¸ëŸ¬ë‚˜ ê³„ì‚°ëœ ìœ ì‚¬ë„ëŠ” ë¹„ì •ê·œí™”ëœ í˜•íƒœë‹¤. ìˆ˜ì‹ì—ëŠ” í¸ì˜ìƒ ì´ë¯¸ <code class="language-plaintext highlighter-rouge">softmax</code>ë¥¼ ì ìš©í•œ í˜•íƒœì˜ í–‰ë ¬ì„ ì ì—ˆì§€ë§Œ, ì‹¤ì œë¡œëŠ” ì›ì†Œê°’ì˜ ë¶„ì‚°ì´ ë„ˆë¬´ ì»¤ì„œ ê°€ì¤‘ì¹˜ë¡œëŠ” ì“°ê¸° í˜ë“  ìˆ˜ì¤€ì´ë‹¤. ë”°ë¼ì„œ í–‰ë²¡í„° ë‹¨ìœ„ë¡œ <code class="language-plaintext highlighter-rouge">softmax</code>ì— í†µê³¼ì‹œì¼œ ê²°ê³¼ì˜ í•©ì´ 1ì¸ í™•ë¥ ê°’ìœ¼ë¡œ <code class="language-plaintext highlighter-rouge">ë³€í™˜(ì •ê·œí™”)</code>í•´ í–‰ë ¬ $V$ì˜ ê°€ì¤‘ì¹˜ë¡œ ì‚¬ìš©í•œë‹¤.</p>

<p>ì´ì œ ë‘ë²ˆì§¸ ìˆ˜ì‹ì„ ë³´ì. $Score_{11}$ì— í•´ë‹¹í•˜ëŠ” <code class="language-plaintext highlighter-rouge">0.90</code>ê°€ í–‰ë ¬ $V$ì˜ ì²«ë²ˆì§¸ í–‰ë²¡í„°ì™€ ê³±í•´ì§€ê³  ìˆë‹¤. í–‰ë ¬ $V$ì˜ ì²«ë²ˆì§¸ í–‰ë²¡í„°ëŠ” í† í° <code class="language-plaintext highlighter-rouge">â€œIâ€</code> ë¥¼ <code class="language-plaintext highlighter-rouge">512</code>ì°¨ì›ìœ¼ë¡œ í‘œí˜„í•œ ê²ƒì´ë‹¤. ê·¸ ë‹¤ìŒ $Score_{12}$ëŠ” í–‰ë ¬ $V$ì˜ ë‘ë²ˆì§¸ í–‰ë²¡í„°ì™€, $Score_{13}$ì€ í–‰ë ¬ $V$ì˜ ì„¸ë²ˆì§¸ í–‰ë²¡í„°ì™€ ê°ê° ê³±í•´ì§„ë‹¤.</p>

<p>ì´ í–‰ìœ„ì˜ ì˜ë¯¸ëŠ” ë¬´ì—‡ì¼ê¹Œ?? $Score_{11}$, $Score_{12}$, $Score_{13}$ì€ ëª¨ë‘ ì²«ë²ˆì§¸ í† í°ì¸ <code class="language-plaintext highlighter-rouge">â€œIâ€</code>ì— ì˜ë¯¸ë¥¼ íŒŒì•…í•˜ëŠ”ë° <code class="language-plaintext highlighter-rouge">â€œIâ€</code>, <code class="language-plaintext highlighter-rouge">â€œamâ€</code>, <code class="language-plaintext highlighter-rouge">â€œdogâ€</code>ë¥¼ ì–´ëŠ ì •ë„ë¡œ ì–´í…ì…˜í•´ì•¼ í•˜ëŠ”ì§€, ì¦‰ <code class="language-plaintext highlighter-rouge">â€œIâ€</code>ì˜ ì˜ë¯¸ë¥¼ í‘œí˜„í•˜ëŠ”ë° ì„¸ í† í°ì˜ ì˜ë¯¸ë¥¼ ì–´ëŠ ì •ë„ ë°˜ì˜í• ì§€ ìˆ˜ì¹˜ë¡œ í‘œí˜„í•œ ê²ƒì´ë‹¤. ë‹¹ì—°íˆ ìê¸° ìì‹ ì¸ <code class="language-plaintext highlighter-rouge">â€œIâ€</code>ì™€ <code class="language-plaintext highlighter-rouge">ê°€ì¤‘ì¹˜(ìœ ì‚¬ë„, ì¤‘ìš”ë„)</code>ê°€ ê°€ì¥ ë†’ê¸° ë•Œë¬¸ì— í–‰ë ¬ $V$ì—ì„œ <code class="language-plaintext highlighter-rouge">â€œIâ€</code> ì— í•´ë‹¹í•˜ëŠ” í–‰ë²¡í„° ê°€ì¤‘ì¹˜ì— ê°€ì¥ í° ê°’ì´ ë“¤ì–´ê°„ë‹¤ê³  ìƒê°í•´ë³¼ ìˆ˜ ìˆë‹¤. ì´ë ‡ê²Œ ê° í† í°ë§ˆë‹¤ ê°€ì¤‘í•©ì„ ë°˜ë³µí•´ì£¼ë©´ ìµœì¢…ì ìœ¼ë¡œ <code class="language-plaintext highlighter-rouge">â€œIâ€</code>, <code class="language-plaintext highlighter-rouge">â€œamâ€</code>, <code class="language-plaintext highlighter-rouge">â€œdogâ€</code> ì„ ì¸ì½”ë”©í•œ $Z_1, \ Z_2, \  Z_3$ ê°’ì„ ì–»ì„ ìˆ˜ ìˆë‹¤.</p>

<p><strong><code class="language-plaintext highlighter-rouge">ğŸ‘©â€ğŸ’»Â Implementation</code></strong></p>

<p>ì´ë ‡ê²Œ <code class="language-plaintext highlighter-rouge">Scaled Dot-Product Attention</code> ì„ ëª¨ë‘ ì‚´í´ë³´ì•˜ë‹¤. í•´ë‹¹ ë ˆì´ì–´ëŠ” ëª¨ë¸ì´ ì†ì‹¤ê°’ì´ ê°€ì¥ ì‘ì•„ì§€ëŠ” ë°©í–¥ìœ¼ë¡œ ìµœì í™”í•œ í–‰ë ¬ $Q, K, V$ ì„ ì´ìš©í•´, í† í°ì˜ ì˜ë¯¸ë¥¼ ì´í•´í•˜ëŠ”ë° ì–´ë–¤ ë§¥ë½ê³¼ í‘œí˜„ì— ì¢€ ë” ì§‘ì¤‘í•˜ê³  ëœ ì§‘ì¤‘í•´ì•¼ í•˜ëŠ”ì§€ë¥¼ ìœ ì‚¬ë„ë¥¼ ê¸°ì¤€ìœ¼ë¡œ íŒë‹¨í•œë‹¤ëŠ” ê²ƒì„ ê¼­ ê¸°ì–µí•˜ì. ê·¸ë ‡ë‹¤ë©´ ì‹¤ì œ ì½”ë“œëŠ” ì–´ë–»ê²Œ ì‘ì„± í•´ì•¼í•˜ëŠ”ì§€ í•¨ê»˜ ì•Œì•„ë³´ì. ìƒë‹¨ì˜ <code class="language-plaintext highlighter-rouge">class diagram</code> ì„ ë‹¤ì‹œ í•œ ë²ˆ ë³´ê³  ëŒì•„ì˜¤ì.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Pytorch Implementation of Scaled Dot-Product Self-Attention
</span>
<span class="k">def</span> <span class="nf">scaled_dot_product_attention</span><span class="p">(</span><span class="n">q</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">k</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">v</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">dot_scale</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">mask</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="bp">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
    <span class="s">"""
    Scaled Dot-Product Attention with Masking for Decoder
    Args:
        q: query matrix, shape (batch_size, seq_len, dim_head)
        k: key matrix, shape (batch_size, seq_len, dim_head)
        v: value matrix, shape (batch_size, seq_len, dim_head)
        dot_scale: scale factor for Qâ€¢K^T result
        mask: there are three types of mask, mask matrix shape must be same as single attention head
              1) Encoder padded token
              2) Decoder Masked-Self-Attention
              3) Decoder's Encoder-Decoder Attention
    Math:
        A = softmax(qâ€¢k^t/sqrt(D_h)), SA(z) = Av
    """</span>
    <span class="n">attention_matrix</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">))</span> <span class="o">/</span> <span class="n">dot_scale</span>
    <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">attention_matrix</span> <span class="o">=</span> <span class="n">attention_matrix</span><span class="p">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">mask</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="s">'-inf'</span><span class="p">))</span>
    <span class="n">attention_dist</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">attention_matrix</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">attention_matrix</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">attention_dist</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">attention_matrix</span>
</code></pre></div></div>

<p>ë§ˆìŠ¤í‚¹ ì˜µì…˜ì˜ ê²½ìš° ì£¼ì„ì— ì •ë¦¬ëœ 3ê°€ì§€ ìƒí™© ì¤‘ì—ì„œ í•œ ê°œ ì´ìƒì— í•´ë‹¹ë˜ë©´ ì‹¤í–‰ë˜ë„ë¡ ì½”ë“œë¥¼ ì‘ì„±í–ˆë‹¤. 3ê°€ì§€  ìƒí™©ê³¼ êµ¬ì²´ì ì¸ ë§ˆìŠ¤í‚¹ ë°©ë²•ì— ëŒ€í•´ì„œëŠ” ì „ì²´ ëª¨ë¸ êµ¬ì¡°ë¥¼ ë³´ëŠ” ë•Œ ì†Œê°œí•˜ë„ë¡ í•˜ê² ë‹¤.</p>

<p>í•œí¸, ì¸ì½”ë”ë‚˜ ë””ì½”ë”ë‚˜ ëª¨ë‘ ì‚¬ìš©í•˜ëŠ” ì…ë ¥ê³¼ ë§ˆìŠ¤í‚¹ ë°©ì‹ì— ì°¨ì´ëŠ” ìˆì§€ë§Œ, <code class="language-plaintext highlighter-rouge">Scaled Dot-Product Attention</code> ì—°ì‚° ìì²´ëŠ” ë™ì¼í•œ ê²ƒì„ ì‚¬ìš©í•œë‹¤. ë”°ë¼ì„œ ì—¬ëŸ¬ê°œì˜ ì¸ì½”ë”ë‚˜ ë””ì½”ë” ê°ì²´ë“¤ í˜¹ì€ ì–´í…ì…˜ í•´ë“œ ê°ì²´ë“¤ì´ ëª¨ë‘ ì‰½ê²Œ ì—°ì‚°ì— ì ‘ê·¼í•  ìˆ˜ ìˆê²Œ í´ë˜ìŠ¤ ì™¸ë¶€ì— ë©”ì„œë“œ í˜•íƒœë¡œ êµ¬í˜„í•˜ê²Œ ë˜ì—ˆë‹¤.</p>

<h4 id="multi-head-attention-block"><strong><code class="language-plaintext highlighter-rouge">ğŸ‘©â€ğŸ‘©â€ğŸ‘§â€ğŸ‘¦Â Multi-Head Attention Block</code></strong></h4>

<p>ì§€ê¸ˆê¹Œì§€ ì‚´í´ë³¸ <code class="language-plaintext highlighter-rouge">Self-Attention</code>ì˜ ë™ì‘ì€ ëª¨ë‘ í•œ ê°œì˜ <code class="language-plaintext highlighter-rouge">Attention-Head</code>ì—ì„œ ì¼ì–´ë‚˜ëŠ” ì¼ì„ ì„œìˆ í•œ ê²ƒì´ë‹¤. ì‚¬ì‹¤ ì‹¤ì œ ëª¨ë¸ì—ì„œëŠ” ê°™ì€ ë™ì‘ì´ <code class="language-plaintext highlighter-rouge">N-1</code>ê°œì˜ ë‹¤ë¥¸ í•´ë“œì—ì„œ ë™ì‹œì— ì¼ì–´ë‚˜ëŠ”ë°, ì´ê²ƒì´ ë°”ë¡œ <code class="language-plaintext highlighter-rouge">Multi-Head Attention</code>ì´ë‹¤.</p>

<p><code class="language-plaintext highlighter-rouge">Official Paper</code> ê¸°ì¤€ìœ¼ë¡œ <code class="language-plaintext highlighter-rouge">Transformer-base</code>ì˜ <code class="language-plaintext highlighter-rouge">hidden states</code> ì°¨ì›ì€ <code class="language-plaintext highlighter-rouge">512</code>ì´ë‹¤. ì´ê²ƒì„ ê°œë‹¹ <code class="language-plaintext highlighter-rouge">64</code>ì°¨ì›ì„ ê°–ëŠ” <code class="language-plaintext highlighter-rouge">8</code>ê°œì˜ <code class="language-plaintext highlighter-rouge">Attention-Head</code> ë¡œ ìª¼ê°  ë’¤, 8ê°œì˜ <code class="language-plaintext highlighter-rouge">Attention-Head</code> ì—ì„œ ë™ì‹œì— <code class="language-plaintext highlighter-rouge">Self-Attention</code> ì„ ìˆ˜í–‰í•œë‹¤. ì´í›„ ê²°ê³¼ë¥¼ <code class="language-plaintext highlighter-rouge">concat</code>í•˜ì—¬ ë‹¤ì‹œ <code class="language-plaintext highlighter-rouge">hidden states</code> ë¥¼ <code class="language-plaintext highlighter-rouge">512</code> ë¡œ ë§Œë“  ë’¤, ì—¬ëŸ¬ í•´ë“œì—ì„œ ë§Œë“  ê²°ê³¼ë¥¼ ì—°ê²°í•˜ê³  ì„ì–´ì£¼ê¸° ìœ„í•´ ì…ì¶œë ¥ ì°¨ì›ì´ <code class="language-plaintext highlighter-rouge">hidden states</code>ì™€ ë™ì¼í•œ <code class="language-plaintext highlighter-rouge">linear projection layer</code>ì— í†µê³¼ì‹œí‚¨ë‹¤. ì´ê²ƒì´ ì¸ì½”ë”(í˜¹ì€ ë””ì½”ë”) ë¸”ëŸ­ í•œ ê°œì˜ ìµœì¢… <code class="language-plaintext highlighter-rouge">Self-Attention</code> ê²°ê³¼ê°€ ëœë‹¤.</p>

<p align="center">
<img src="/assets/images/transformer/multi_head_result.png" alt="Multi-Head Attention Result Visualization" class="align-center image-caption" width="50%&quot;, height=&quot;50%" />
<strong><em><a href="https://arxiv.org/abs/1706.03762">Multi-Head Attention Result Visualization</a></em></strong>
</p>

<p><strong>ê·¸ëŸ¼ ì™œ ì´ë ‡ê²Œ ì—¬ëŸ¬ í•´ë“œë¥¼ ì‚¬ìš©í–ˆì„ê¹Œ?? ë°”ë¡œ ì§‘ë‹¨ì§€ì„±ì˜ íš¨ê³¼ë¥¼ ëˆ„ë¦¬ê¸° ìœ„í•¨ì´ë‹¤.</strong> ìƒê°í•´ë³´ì. ì±… í•˜ë‚˜ë¥¼ ì½ì–´ë„ ì‚¬ëŒë§ˆë‹¤ ì •ë§ ë‹¤ì–‘í•œ í•´ì„ì´ ë‚˜ì˜¨ë‹¤. ëª¨ë¸ë„ ë§ˆì°¬ê°€ì§€ë‹¤. ì—¬ëŸ¬ í•´ë“œë¥¼ ì‚¬ìš©í•´ì„œ ì¢€ ë” ë‹¤ì–‘í•˜ê³  í’ë¶€í•œ ì˜ë¯¸ë¥¼ ì„ë² ë”©ì— ë‹´ê³  ì‹¶ì—ˆë˜ ê²ƒì´ë‹¤. Kaggleì„ í•´ë³´ì‹  ë…ìë¼ë©´, ì—¬ëŸ¬ ì „ëµì„ ì‚¬ìš©í•´ ì—¬ëŸ¬ ê°œì˜ ê²°ê³¼ë¥¼ ë„ì¶œí•œ ë’¤, ë§ˆì§€ë§‰ì— ëª¨ë‘ ì•™ìƒë¸”í•˜ë©´ ì „ëµ í•˜ë‚˜ í•˜ë‚˜ì˜ ê²°ê³¼ë³´ë‹¤ ë” ë†’ì€ ì„±ì ì„ ì–»ì–´ë³¸ ê²½í—˜ì´ ìˆì„ ê²ƒì´ë‹¤. ì´ê²ƒë„ ë¹„ìŠ·í•œ íš¨ê³¼ë¥¼ ì˜ë„í–ˆë‹¤ê³  ìƒê°í•œë‹¤. Visionì—ì„œ Conv Filterë¥¼ ì—¬ëŸ¬ ì¢…ë¥˜ ì‚¬ìš©í•´ ë‹¤ì–‘í•œ Feature Mapì„ ì¶”ì¶œí•˜ëŠ” ê²ƒë„ ë¹„ìŠ·í•œ í˜„ìƒì´ë¼ ë³¼ ìˆ˜ ìˆê² ë‹¤.</p>

<p>ìœ„ ê·¸ë¦¼ì€ ì €ìê°€ ì œì‹œí•œ <code class="language-plaintext highlighter-rouge">Multi-Head Attention</code>ì˜ ì‹œê°í™” ê²°ê³¼ë‹¤. ì¤‘ê°„ì— ìˆëŠ” ì—¬ëŸ¬ ìƒ‰ê¹”ì˜ ë ëŠ” ê°œë³„ í•´ë“œê°€ ì–´í…ì…˜í•˜ëŠ” ë°©í–¥ì„ ê°€ë¦¬í‚¨ë‹¤. í† í° <code class="language-plaintext highlighter-rouge">â€œmakingâ€</code> ì— ëŒ€í•´ì„œ í•´ë“œë“¤ì´ ì„œë¡œ ë‹¤ë¥¸ í† í°ì— ì–´í…ì…˜í•˜ê³  ìˆë‹¤.</p>

<p align="center">
<img src="/assets/images/transformer/vit_multi_head_result.png" alt="ViT Multi-Head Attention Result Visualization" class="align-center image-caption" width="50%&quot;, height=&quot;50%" />
<strong><em><a href="https://arxiv.org/abs/2010.11929">ViT Multi-Head Attention Result Visualization</a></em></strong>
</p>

<p>ìœ„ ê·¸ë¦¼ì€ Vision Transformer ë…¼ë¬¸ì—ì„œ ë°œì·Œí•œ ê·¸ë¦¼<a href="https://qcqced123.github.io/cv/vit">(ê·¸ë¦¼ì˜ ìì„¸í•œ ì˜ë¯¸ëŠ” ì—¬ê¸°ì„œ)</a>ì´ë‹¤. ì—­ì‹œ ë§ˆì°¬ê°€ì§€ë¡œ ëª¨ë¸ì˜ ì´ˆë°˜ë¶€ ì¸ì½”ë”ì— ì†í•œ Multi-Headë“¤ì´ ì„œë¡œ ë‹¤ì–‘í•œ í† í°ì— ì–´í…ì…˜ì„ í•˜ê³  ìˆìŒì„ ì•Œ ìˆ˜ ìˆë‹¤. ì¶”ê°€ë¡œ í›„ë°˜ìœ¼ë¡œ ê°ˆìˆ˜ë¡ ì ì  <code class="language-plaintext highlighter-rouge">Attention Distance</code> ê°€ ì¼ì •í•œ ìˆ˜ì¤€ì— ìˆ˜ë ´í•˜ëŠ” ëª¨ìŠµì„ ë³¼ ìˆ˜ ìˆëŠ”ë°, ì´ê²ƒì„ ë ˆì´ì–´ë¥¼ í†µê³¼í• ìˆ˜ë¡ ê°œë³„ í•´ë“œê°€ ìì‹ ì´ ì–´ë–¤ í† í°ì— ì£¼ì˜ë¥¼ ê¸°ìš¸ì—¬ì•¼í• ì§€ êµ¬ì²´ì ìœ¼ë¡œ ì•Œì•„ê°€ëŠ” ê³¼ì •ì´ë¼ê³  í•´ì„í•  ìˆ˜ ìˆë‹¤. ì´ˆë°˜ë¶€ì—ëŠ” ì–´ì°Œí•  ë°”ë¥¼ ëª°ë¼ì„œ ì´í† í° ì €í† í°ì— ì£„ë‹¤ ì–´í…ì…˜í•˜ëŠ” ê²ƒì´ë‹¤.</p>

<p>ê·¸ë˜ì„œ <code class="language-plaintext highlighter-rouge">Transformer</code>ëŠ” <code class="language-plaintext highlighter-rouge">Bottom Layer</code>ì—ì„œëŠ” <code class="language-plaintext highlighter-rouge">Global</code>í•˜ê³  <code class="language-plaintext highlighter-rouge">General</code>í•œ ì •ë³´ë¥¼ í¬ì°©í•˜ê³ , <code class="language-plaintext highlighter-rouge">Output</code>ê³¼ ê°€ê¹Œìš´ <code class="language-plaintext highlighter-rouge">Top Layer</code>ì—ì„œëŠ” <code class="language-plaintext highlighter-rouge">Local</code>í•˜ê³  <code class="language-plaintext highlighter-rouge">Specific</code>í•œ ì •ë³´ë¥¼ í¬ì°©í•œë‹¤.</p>

<p><strong><code class="language-plaintext highlighter-rouge">ğŸ‘©â€ğŸ’»Â Implementation</code></strong></p>

<p>ì´ì œ êµ¬í˜„ì„ ì‹¤ì œë¡œ êµ¬í˜„ì„ í•´ë³´ì. ì—­ì‹œ êµ¬í˜„ì€ íŒŒì´í† ì¹˜ë¡œ ì§„í–‰í–ˆë‹¤.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Pytorch Implemenation of Single Attention Head
</span>
<span class="k">class</span> <span class="nc">AttentionHead</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="s">"""
    In this class, we implement workflow of single attention head
    Args:
        dim_model: dimension of model's latent vector space, default 512 from official paper
        dim_head: dimension of each attention head, default 64 from official paper (512 / 8)
        dropout: dropout rate, default 0.1
    Math:
        [q,k,v]=zâ€¢U_qkv, A = softmax(qâ€¢k^t/sqrt(D_h)), SA(z) = Av
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim_model</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">512</span><span class="p">,</span> <span class="n">dim_head</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">64</span><span class="p">,</span> <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">AttentionHead</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span> <span class="o">=</span> <span class="n">dim_model</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dim_head</span> <span class="o">=</span> <span class="n">dim_head</span>  <span class="c1"># 512 / 8 = 64
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">dropout</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dot_scale</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">dim_head</span><span class="p">))</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">fc_q</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">dim_head</span><span class="p">)</span>  <span class="c1"># Linear Projection for Query Matrix
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">fc_k</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">dim_head</span><span class="p">)</span>  <span class="c1"># Linear Projection for Key Matrix
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">fc_v</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">dim_head</span><span class="p">)</span>  <span class="c1"># Linear Projection for Value Matrix
</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">mask</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">enc_output</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="bp">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">fc_q</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="bp">self</span><span class="p">.</span><span class="n">fc_k</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="bp">self</span><span class="p">.</span><span class="n">fc_v</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># x is previous layer's output
</span>        <span class="k">if</span> <span class="n">enc_output</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="s">""" For encoder-decoder self-attention """</span>
            <span class="n">k</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">fc_k</span><span class="p">(</span><span class="n">enc_output</span><span class="p">)</span>
            <span class="n">v</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">fc_v</span><span class="p">(</span><span class="n">enc_output</span><span class="p">)</span>
        <span class="n">attention_matrix</span> <span class="o">=</span> <span class="n">scaled_dot_product_attention</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">dot_scale</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="n">mask</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">attention_matrix</span>
</code></pre></div></div>

<p>ë˜‘ê°™ì€ <code class="language-plaintext highlighter-rouge">Attention-Head</code>ë¥¼ <code class="language-plaintext highlighter-rouge">N</code>ê°œ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì— ë¨¼ì € <code class="language-plaintext highlighter-rouge">Single Attention Head</code>ì˜ ë™ì‘ì„ ë”°ë¡œ ê°ì²´ë¡œ ë§Œë“¤ì—ˆë‹¤. ì´ë ‡ê²Œ í•˜ë©´ <code class="language-plaintext highlighter-rouge">MultiHeadAttention</code> ê°ì²´ì—ì„œ <code class="language-plaintext highlighter-rouge">nn.ModuleList</code> ë¥¼ ì‚¬ìš©í•´ <code class="language-plaintext highlighter-rouge">N</code>ê°œì˜ í•´ë“œë¥¼ ì´ì–´ë¶™ì¼ ìˆ˜ ìˆì–´ì„œ êµ¬í˜„ì´ í›¨ì”¬ ê°„í¸í•´ì§€ê¸° ë•Œë¬¸ì´ë‹¤. <code class="language-plaintext highlighter-rouge">Single Attention Head</code> ê°ì²´ê°€ í•˜ëŠ” ì¼ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.</p>

<ul>
  <li><strong>1) Linear Projection by Dimension of Single Attention Head</strong></li>
  <li><strong>2) Maksing</strong></li>
  <li><strong>3) Scaled Dot-Product Attention</strong></li>
</ul>

<p>í•œí¸, ì—¬ëŸ¬ <code class="language-plaintext highlighter-rouge">Transformer</code> êµ¬í˜„ Git Repoë¥¼ ì‚´í´ë³´ë©´ êµ¬í˜„ ë°©ë²•ì€ í¬ê²Œ í•„ìì²˜ëŸ¼ <code class="language-plaintext highlighter-rouge">Single Attention Head</code>ë¥¼ ì¶”ìƒí™”í•˜ê±°ë‚˜ <code class="language-plaintext highlighter-rouge">MultiHeadAttention</code> ê°ì²´ í•˜ë‚˜ì— ëª¨ë“  ë™ì‘ì„ ë•Œë ¤ë„£ëŠ” ë°©ì‹ìœ¼ë¡œ ë‚˜ë‰˜ëŠ” ê²ƒ ê°™ë‹¤. ì‚¬ì‹¤ êµ¬í˜„ì— ì •ë‹µì€ ì—†ì§€ë§Œ ê°œì¸ì ìœ¼ë¡œ í›„ìì˜ ë°©ì‹ì€ ë¹„íš¨ìœ¨ì ì´ë¼ ìƒê°í•œë‹¤. ì €ë ‡ê²Œ êµ¬í˜„í•˜ë©´ <code class="language-plaintext highlighter-rouge">3*N</code>ê°œì˜ <code class="language-plaintext highlighter-rouge">linear projector</code>ë¥¼ í´ë˜ìŠ¤ <code class="language-plaintext highlighter-rouge">__init__</code> ì— ë§Œë“¤ê³  ê´€ë¦¬í•´ì¤˜ì•¼ í•˜ëŠ”ë° ì‰½ì§€ ì•Šì„ ê²ƒì´ë‹¤. ë¬¼ë¡  <code class="language-plaintext highlighter-rouge">3</code>ê°œì˜ <code class="language-plaintext highlighter-rouge">linear projector</code> ë§Œ ì´ˆê¸°í™”í•´ì„œ ì‚¬ìš©í•˜ê³  ëŒ€ì‹  ì¶œë ¥ ì°¨ì›ì„ <code class="language-plaintext highlighter-rouge">Dim_Head</code>ê°€ ì•„ë‹Œ <code class="language-plaintext highlighter-rouge">Dim_Model</code>ë¡œ êµ¬í˜„í•œ ë’¤, <code class="language-plaintext highlighter-rouge">N</code>ê°œë¡œ ì°¨ì›ì„ ë¶„í• í•˜ëŠ” ë°©ë²•ë„ ìˆë‹¤. í•˜ì§€ë§Œ ì°¨ì›ì„ ìª¼ê°œëŠ” ë™ì‘ì„ êµ¬í˜„í•˜ëŠ” ê²ƒë„ ì‚¬ì‹¤ ì‰½ì§€ ì•Šë‹¤. ê·¸ë˜ì„œ í•„ìëŠ” ì „ìì˜ ë°©ì‹ì„ ì¶”ì²œí•œë‹¤.</p>

<p>í•œí¸, <code class="language-plaintext highlighter-rouge">forward</code> ë©”ì„œë“œì—  <code class="language-plaintext highlighter-rouge">if enc_output is not None:</code> ë¶€ë¶„ì€ ì¶”í›„ì— ë””ì½”ë”ì—ì„œ <code class="language-plaintext highlighter-rouge">Multi-Head Attention</code>ì„ êµ¬í˜„í•˜ê¸° ìœ„í•´ ì¶”ê°€í•œ ì½”ë“œë‹¤. ë””ì½”ë”ëŠ” ì¸ì½”ë”ì™€ ë‹¤ë¥´ê²Œ í•˜ë‚˜ì˜ ë””ì½”ë” ë¸”ëŸ­ì—ì„œ <code class="language-plaintext highlighter-rouge">Self-Attention</code>ë™ì‘ì„ ë‘ë²ˆí•˜ëŠ”ë°, ë‘ë²ˆì§¸ ë™ì‘ì€ ì„œë¡œ ë‹¤ë¥¸ ì¶œì²˜ì˜ ê°’ì„ ì´ìš©í•´ <code class="language-plaintext highlighter-rouge">linear projection</code>ì„ ìˆ˜í–‰í•œë‹¤. ë”°ë¼ì„œ ê·¸ ê²½ìš°ë¥¼ ì²˜ë¦¬í•´ì£¼ê¸° ìœ„í•´ êµ¬í˜„í•˜ê²Œ ë˜ì—ˆë‹¤.</p>

<p>ì•„ë˜ëŠ” <code class="language-plaintext highlighter-rouge">MultiHeadAttention</code> ì„ êµ¬í˜„í•œ íŒŒì´í† ì¹˜ ì½”ë“œë‹¤.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Pytorch Implemenation of Single Attention Head
</span>
<span class="k">class</span> <span class="nc">MultiHeadAttention</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="s">"""
    In this class, we implement workflow of Multi-Head Self-Attention
    Args:
        dim_model: dimension of model's latent vector space, default 512 from official paper
        num_heads: number of heads in MHSA, default 8 from official paper for Transformer
        dim_head: dimension of each attention head, default 64 from official paper (512 / 8)
        dropout: dropout rate, default 0.1
    Math:
        MSA(z) = [SA1(z); SA2(z); Â· Â· Â· ; SAk(z)]â€¢Umsa
    Reference:
        https://arxiv.org/abs/1706.03762
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim_model</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">512</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span> <span class="n">dim_head</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">64</span><span class="p">,</span> <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MultiHeadAttention</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span> <span class="o">=</span> <span class="n">dim_model</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dim_head</span> <span class="o">=</span> <span class="n">dim_head</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">dropout</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">attention_heads</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">ModuleList</span><span class="p">(</span>
            <span class="p">[</span><span class="n">AttentionHead</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">dim_head</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">dropout</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">num_heads</span><span class="p">)]</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">fc_concat</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">mask</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">enc_output</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="bp">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="s">""" x is already passed nn.Layernorm """</span>
        <span class="k">assert</span> <span class="n">x</span><span class="p">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">3</span><span class="p">,</span> <span class="sa">f</span><span class="s">'Expected (batch, seq, hidden) got </span><span class="si">{</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="s">'</span>
        <span class="n">attention_output</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">fc_concat</span><span class="p">(</span>
            <span class="n">torch</span><span class="p">.</span><span class="n">cat</span><span class="p">([</span><span class="n">head</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">mask</span><span class="p">,</span> <span class="n">enc_output</span><span class="p">)</span> <span class="k">for</span> <span class="n">head</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">attention_heads</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">attention_output</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">MultiHeadAttention</code> ê°ì²´ëŠ” ê°œë³„ í•´ë“œë“¤ì´ ë„ì¶œí•œ ì–´í…ì…˜ ê²°ê³¼ë¥¼ <code class="language-plaintext highlighter-rouge">concat</code>í•˜ê³  ê·¸ê²ƒì„ <code class="language-plaintext highlighter-rouge">connect &amp; mix</code>í•˜ë ¤ê³  <code class="language-plaintext highlighter-rouge">linear projection</code>ì„ ìˆ˜í–‰í•œë‹¤.</p>

<h4 id="-feed-forward-network"><strong><code class="language-plaintext highlighter-rouge">ğŸ”¬ Feed Forward Network</code></strong></h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Pytorch Implementation of FeedForward Network
</span>
<span class="k">class</span> <span class="nc">FeedForward</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="s">"""
    Class for Feed-Forward Network module in transformer
    In official paper, they use ReLU activation function, but GELU is better for now
    We change ReLU to GELU &amp; add dropout layer
    Args:
        dim_model: dimension of model's latent vector space, default 512
        dim_ffn: dimension of FFN's hidden layer, default 2048 from official paper
        dropout: dropout rate, default 0.1
    Math:
        FeedForward(x) = FeedForward(LN(x))+x
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim_model</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">512</span><span class="p">,</span> <span class="n">dim_ffn</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2048</span><span class="p">,</span> <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">FeedForward</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">ffn</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">dim_model</span><span class="p">,</span> <span class="n">dim_ffn</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">GELU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">dropout</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">dim_ffn</span><span class="p">,</span> <span class="n">dim_model</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">dropout</span><span class="p">),</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">ffn</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</code></pre></div></div>

<p>í”¼ë“œ í¬ì›Œë“œëŠ” ëª¨ë¸ì— <code class="language-plaintext highlighter-rouge">non-linearity</code>ë¥¼ ì¶”ê°€í•˜ê¸° ìœ„í•´ ì‚¬ìš©í•˜ëŠ” ë ˆì´ì–´ë‹¤. ì›ë³¸ ëª¨ë¸ì€ <code class="language-plaintext highlighter-rouge">ReLU</code> ë¥¼ ì‚¬ìš©í•˜ì§€ë§Œ ìµœê·¼ <code class="language-plaintext highlighter-rouge">Transformer</code>ë¥˜ ëª¨ë¸ì—ëŠ” <code class="language-plaintext highlighter-rouge">GeLU</code>ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì¢€ ë” ì•ˆì •ì ì¸ í•™ìŠµì„ í•˜ëŠ”ë° ë„ì›€ì´ ëœë‹¤ê³  ë°í˜€ì ¸, í•„ì ì—­ì‹œ <code class="language-plaintext highlighter-rouge">GeLU</code>ë¥¼ ì‚¬ìš©í•´ êµ¬í˜„í–ˆë‹¤. ë˜í•œ ë…¼ë¬¸ì—ëŠ” <code class="language-plaintext highlighter-rouge">dropout</code>ì— ëŒ€í•œ ì–¸ê¸‰ì´ ì „í˜€ ì—†ëŠ”ë°, ì€ë‹‰ì¸µì˜ ì°¨ì›ì„ ì €ë ‡ê²Œ í¬ê²Œ í‚¤ì› ë‹¤ ì¤„ì´ëŠ”ë° <code class="language-plaintext highlighter-rouge">overfitting</code> ì´ìŠˆê°€ ìˆì„ ê²ƒ ê°™ì•„ì„œ <code class="language-plaintext highlighter-rouge">ViT</code> ë…¼ë¬¸ì„ ì°¸ê³ í•´ ë”°ë¡œ ì¶”ê°€í•´ì¤¬ë‹¤.</p>

<h4 id="add--norm"><strong><code class="language-plaintext highlighter-rouge">â•Â Add &amp; Norm</code></strong></h4>

<p><code class="language-plaintext highlighter-rouge">Residual Connection</code>ê³¼ <code class="language-plaintext highlighter-rouge">Layernorm</code>ì„ ì˜ë¯¸í•œë‹¤.  ë”°ë¡œ ê°ì²´ë¥¼ ë§Œë“¤ì–´ì„œ ì‚¬ìš©í•˜ì§€ëŠ” ì•Šê³ , <code class="language-plaintext highlighter-rouge">EncoderLayer</code> ê°ì²´ì— ë¼ì¸ìœ¼ë¡œ ì¶”ê°€í•´ êµ¬í˜„í•˜ê¸° ë•Œë¬¸ì— ì—¬ê¸°ì„œëŠ” ì—­í• ê³¼ ì˜ë¯¸ë§Œ ì„¤ëª…í•˜ê³  ë„˜ì–´ê°€ê² ë‹¤.</p>

<p>ë¨¼ì € <code class="language-plaintext highlighter-rouge">Skip-Connection</code>ìœ¼ë¡œë„ ë¶ˆë¦¬ëŠ” <code class="language-plaintext highlighter-rouge">Residual Connection</code>ì€ ì–´ë–¤ ë ˆì´ì–´ë¥¼ í†µê³¼í•˜ê¸° ì „, ì…ë ¥ $x$ ë¥¼ ë ˆì´ì–´ë¥¼ í†µê³¼í•˜ê³  ë‚˜ì˜¨ ê²°ê³¼ê°’ $fx$ ì— ë”í•´ì¤€ë‹¤. ë”°ë¼ì„œ ë‹¤ìŒ ë ˆì´ì–´ì— í†µê³¼ë˜ëŠ” ì…ë ¥ê°’ì€ $x+fx$ ê°€ ëœë‹¤. ì™œ ì´ë ‡ê²Œ ë”í•´ì¤„ê¹Œ?? ë°”ë¡œ ëª¨ë¸ì˜ ì•ˆì •ì ì¸ í•™ìŠµì„ ìœ„í•´ì„œë‹¤. ì¼ë‹¨ ê·¸ì „ì— ëª…ì‹¬í•˜ê³  ê°€ì•¼í•  ì „ì œê°€ í•˜ë‚˜ ìˆë‹¤. ëª¨ë¸ì˜ ë ˆì´ì–´ê°€ ê¹Šì–´ì§ˆìˆ˜ë¡ ë ˆì´ì–´ë§ˆë‹¤ ê°’ì„ ì¡°ê¸ˆì”© ë°”ê¿”ë‚˜ê°€ëŠ” ê²ƒì´ <code class="language-plaintext highlighter-rouge">Robust</code>í•˜ê³  <code class="language-plaintext highlighter-rouge">Stable</code>í•œ ê²°ê³¼ë¥¼ ë„ì¶œí•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì´ë‹¤. ì§ê´€ì ìœ¼ë¡œ ë ˆì´ì–´ë§ˆë‹¤ ê²°ê³¼ê°€ ë„ë›°ê¸°í•˜ëŠ” ëª¨ë¸ë³´ë‹¤ ì•ˆì •ì ìœ¼ë¡œ ì°¨ê·¼ì°¨ê·¼ í•™ìŠµí•´ë‚˜ê°€ëŠ” ëª¨ë¸ì˜ ì¼ë°˜í™” ì„±ëŠ¥ì´ ë” ì¢‹ì„ ê²ƒì´ë¼ê³  ì¶”ì¸¡í•´ë³¼ ìˆ˜ ìˆë‹¤. ê·¸ë˜ì„œ <code class="language-plaintext highlighter-rouge">Residual Connection</code> ì€ ì…ë ¥ $x$ ì™€ ë ˆì´ì–´ì˜ ì´ìƒì ì¸ ì¶œë ¥ê°’ $H(x)$ ì˜ ì°¨ì´ê°€ í¬ì§€ ì•ŠìŒì„ ê°€ì •í•œë‹¤. ë§Œì•½, ì…ë ¥ $X$ ë¥¼ <code class="language-plaintext highlighter-rouge">10.0</code> , $H(x)$ ë¥¼ <code class="language-plaintext highlighter-rouge">10.4</code> ë¼ê³  í•´ë³´ì. ê·¸ëŸ¼ <code class="language-plaintext highlighter-rouge">Residual Connection</code> ì„ ì‚¬ìš©í•˜ëŠ” ëª¨ë¸ì€ <code class="language-plaintext highlighter-rouge">0.4</code>ì— ëŒ€í•´ì„œë§Œ í•™ìŠµì„ í•˜ë©´ ëœë‹¤. í•œí¸ ì´ê²ƒì„ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ëª¨ë¸ì€ 0ì—ì„œë¶€í„° ì‹œì‘í•´ ë¬´ë ¤ <code class="language-plaintext highlighter-rouge">10.4</code>ë¥¼ í•™ìŠµí•´ì•¼ í•œë‹¤. ì–´ë–¤ ëª¨ë¸ì´ í•™ìŠµí•˜ê¸° ì‰¬ìš¸ê¹Œ?? ë‹¹ì—°íˆ ì „ìì¼ ê²ƒì´ë‹¤. ì´ë ‡ê²Œ ëª¨ë¸ì´ ì´ìƒì ì¸ ê°’ê³¼ ì…ë ¥ì˜ ì°¨ì´ë§Œ í•™ìŠµí•˜ë©´ ë˜ê¸° ë•Œë¬¸ì— ì´ê²ƒì„ <code class="language-plaintext highlighter-rouge">ì”ì°¨ í•™ìŠµ</code>ì´ë¼ê³  ë¶€ë¥´ëŠ” ê²ƒì´ë‹¤.</p>

<p align="center">
<img src="/assets/images/transformer/layernorm.png" alt="Layernorm vs Batchnorm" class="align-center image-caption" width="50%&quot;, height=&quot;50%" />
<strong><em><a href="https://paperswithcode.com/method/layer-normalization">Layernorm vs Batchnorm</a></em></strong>
</p>

<p><code class="language-plaintext highlighter-rouge">Batchnorm</code>ì€ <code class="language-plaintext highlighter-rouge">â€œMini-Batchâ€</code> ë‹¨ìœ„ë¥¼ <code class="language-plaintext highlighter-rouge">Channel(Feature)</code>ë³„ë¡œ í‰ê· ê³¼ í‘œì¤€í¸ì°¨ë¥¼ êµ¬í•œë‹¤ë©´, <code class="language-plaintext highlighter-rouge">Layernorm</code>ì€  <code class="language-plaintext highlighter-rouge">Channel(Feature)</code> ë‹¨ìœ„ë¥¼ <code class="language-plaintext highlighter-rouge">ê°œë³„ ì¸ìŠ¤í„´ìŠ¤</code>ë³„ë¡œ í‰ê· ê³¼ í‘œì¤€í¸ì°¨ë¥¼ êµ¬í•˜ì—¬ ì •ê·œí™”í•˜ëŠ” ë°©ì‹ì´ë‹¤.</p>

<p>ì˜ˆë¥¼ ë“¤ì–´ ë°°ì¹˜ë¡œ 4ê°œì˜ ë¬¸ì¥ì„ ì€ë‹‰ì¸µì˜ ì‚¬ì´ì¦ˆê°€ <code class="language-plaintext highlighter-rouge">512</code>ì¸ ëª¨ë¸ì— ì…ë ¥í•´ì¤¬ë‹¤ê³  ìƒê°í•´ë³´ì. ê·¸ëŸ¼ 4ê°œì˜ ë¬¸ì¥ì€ ê°ê° <code class="language-plaintext highlighter-rouge">512</code>ê°œì˜ ì›ì†Œë¥¼ ê°–ê²Œ ë˜ëŠ”ë°, ì´ê²ƒì— ëŒ€í•œ í‰ê· ê³¼ í‘œì¤€í¸ì°¨ë¥¼ êµ¬í•œë‹¤ëŠ” ê²ƒì´ë‹¤. í•œ ê°œì˜ ë¬¸ì¥ë‹¹ í‰ê· ê³¼ í‘œì¤€í¸ì°¨ë¥¼ 1ê°œì”© êµ¬í•´ì„œ, 4ê°œì˜ ë¬¸ì¥ì´ë‹ˆê¹Œ ì´ 8ê°œê°€ ë‚˜ì˜¤ê² ë‹¤.</p>

<p>ê·¸ë ‡ë‹¤ë©´ ì™œ <code class="language-plaintext highlighter-rouge">Transformer</code>ëŠ” <code class="language-plaintext highlighter-rouge">Layernorm</code>ì„ ì‚¬ìš©í–ˆì„ê¹Œ?? ìì—°ì–´ ì²˜ë¦¬ëŠ” ë°°ì¹˜ë§ˆë‹¤ ì‹œí€€ìŠ¤ì˜ ê¸¸ì´ê°€ ê³ ì •ë˜ì–´ ìˆì§€ ì•Šì•„ íŒ¨ë”©ì´ë‚˜ ì ˆì‚­ì„ ìˆ˜í–‰í•œë‹¤. ì ˆì‚­ë³´ë‹¤ëŠ” íŒ¨ë”©ì´ ë¬¸ì œê°€ ëœë‹¤. íŒ¨ë”©ì€ ì¼ë°˜ì ìœ¼ë¡œ ë¬¸ì¥ì˜ ëë¶€ë¶„ì— í•´ì¤€ë‹¤. ì—¬ê¸°ì„œ <code class="language-plaintext highlighter-rouge">Batchnorm</code>ì„ ì‚¬ìš©í•˜ë©´ ëìª½ì— ìœ„ì¹˜í•œ ë‹¤ë¥¸ ì‹œí€€ìŠ¤ì— ì†í•œ ì •ìƒì ì¸ í† í°ë“¤ì€ íŒ¨ë”©ì— ì˜í•´ ê°’ì´ ì™œê³¡ë  ê°€ëŠ¥ì„±ì´ ìˆë‹¤. ê·¸ë˜ì„œ <code class="language-plaintext highlighter-rouge">Batchnorm</code> ëŒ€ì‹  <code class="language-plaintext highlighter-rouge">Layernorm</code>ì„ ì‚¬ìš©í•œë‹¤. ë˜í•œ <code class="language-plaintext highlighter-rouge">Batchnorm</code> ì€ ë°°ì¹˜ í¬ê¸°ì— ì¢…ì†ì ì´ë¼ì„œ í…ŒìŠ¤íŠ¸ ìƒí™©ì—ì„œëŠ” ê·¸ëŒ€ë¡œ ì‚¬ìš©í•  ìˆ˜ ì—†ë‹¤. ë”°ë¼ì„œ ë°°ì¹˜ ì‚¬ì´ì¦ˆì— ë…ë¦½ì ì¸ <code class="language-plaintext highlighter-rouge">Layernorm</code>ì„ ì‚¬ìš©í•˜ê¸°ë„ í•œë‹¤.</p>

<p>í•œí¸ ì´ëŸ¬í•œ ì •ê·œí™”ë¥¼ ì™œ ì‚¬ìš©í•˜ëŠ”ì§€ ê¶ê¸ˆí•˜ì‹œë‹¤ë©´ ë‹¤ë¥¸ í¬ìŠ¤íŠ¸ì— ì •ë¦¬ë¥¼ í•´ë’€ìœ¼ë‹ˆ ì°¸ê³ í•˜ì‹œê¸¸ ë°”ë€ë‹¤. <strong>ê°„ë‹¨í•˜ê²Œë§Œ ì–¸ê¸‰í•˜ë©´,</strong> <code class="language-plaintext highlighter-rouge">ëª¨ë¸ì˜ ë¹„ì„ í˜•ì„±</code><strong>ê³¼ ê·¸ë¼ë””ì–¸íŠ¸ í¬ê¸° ì‚¬ì´ì˜ ìµœì ì˜</strong> <code class="language-plaintext highlighter-rouge">Trade-Off</code><strong>ë¥¼ ì¸ê°„ì´ ì•„ë‹Œ ëª¨ë¸ë³´ê³  ì°¾ê²Œ ë§Œë“œëŠ”ê²Œ ëª©ì ì´ë¼ ë³¼ ìˆ˜ ìˆë‹¤.</strong></p>

<h4 id="encoderlayer"><strong><code class="language-plaintext highlighter-rouge">ğŸ“˜Â EncoderLayer</code></strong></h4>
<p>ì´ì œ <code class="language-plaintext highlighter-rouge">Single Encoder Block</code>ì„ ì •ì˜í•˜ê¸°ì— í•„ìš”í•œ ëª¨ë“  ì¬ë£Œë¥¼ ì‚´í´ë´¤ë‹¤. ì§€ê¸ˆê¹Œì§€ì˜ ë‚´ìš©ì„ ì¢…í•©í•´ í•œ ê°œì˜ ì¸ì½”ë” ë¸”ëŸ­ì„ ë§Œë“¤ì–´ë³´ì.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Pytorch Implementation of Single Encoder Block
</span>
<span class="k">class</span> <span class="nc">EncoderLayer</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="s">"""
    Class for encoder model module in Transformer
    In this class, we stack each encoder_model module (Multi-Head Attention, Residual-Connection, LayerNorm, FFN)
    We apply pre-layernorm, which is different from original paper
    In common sense, pre-layernorm are more effective &amp; stable than post-layernorm
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim_model</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">512</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span> <span class="n">dim_ffn</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2048</span><span class="p">,</span> <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">EncoderLayer</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">self_attention</span> <span class="o">=</span> <span class="n">MultiHeadAttention</span><span class="p">(</span>
            <span class="n">dim_model</span><span class="p">,</span>
            <span class="n">num_heads</span><span class="p">,</span>
            <span class="nb">int</span><span class="p">(</span><span class="n">dim_model</span> <span class="o">/</span> <span class="n">num_heads</span><span class="p">),</span>
            <span class="n">dropout</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">layer_norm1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">dim_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">layer_norm2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">dim_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">dropout</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">ffn</span> <span class="o">=</span> <span class="n">FeedForward</span><span class="p">(</span>
            <span class="n">dim_model</span><span class="p">,</span>
            <span class="n">dim_ffn</span><span class="p">,</span>
            <span class="n">dropout</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">mask</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="n">ln_x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">layer_norm1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">residual_x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">dropout</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">self_attention</span><span class="p">(</span><span class="n">ln_x</span><span class="p">,</span> <span class="n">mask</span><span class="p">))</span> <span class="o">+</span> <span class="n">x</span>

        <span class="n">ln_x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">layer_norm2</span><span class="p">(</span><span class="n">residual_x</span><span class="p">)</span>
        <span class="n">fx</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">ffn</span><span class="p">(</span><span class="n">ln_x</span><span class="p">)</span> <span class="o">+</span> <span class="n">residual_x</span>
        <span class="k">return</span> <span class="n">fx</span>
</code></pre></div></div>

<p>ì§€ê¸ˆê¹Œì§€ì˜ ë‚´ìš©ì„ ê°ì²´ í•˜ë‚˜ì— ëª¨ì•„ë‘”ê±°ë¼ íŠ¹ë³„íˆ ì„¤ëª…ì´ í•„ìš”í•œ ë¶€ë¶„ì€ ì—†ì§€ë§Œ, í•„ìê°€ <code class="language-plaintext highlighter-rouge">add &amp; norm</code>ì„ ì–¸ì œ ì‚¬ìš©í–ˆëŠ”ì§€ ì£¼ëª©í•´ë³´ì. ì›ë³¸ ë…¼ë¬¸ì€ <code class="language-plaintext highlighter-rouge">Multi-Head Attention</code>ê³¼ <code class="language-plaintext highlighter-rouge">FeedForward</code> <code class="language-plaintext highlighter-rouge">Layer</code>ë¥¼ í†µê³¼í•œ ì´í›„ì— <code class="language-plaintext highlighter-rouge">add &amp; norm</code>ì„ í•˜ëŠ” <code class="language-plaintext highlighter-rouge">post-layernorm</code> ë°©ì‹ì„ ì ìš©í–ˆë‹¤. í•˜ì§€ë§Œ í•„ìëŠ” ë‘ ë ˆì´ì–´ í†µê³¼ ì´ì „ì— ë¯¸ë¦¬ <code class="language-plaintext highlighter-rouge">add &amp; norm</code> ì„ í•´ì£¼ëŠ” <code class="language-plaintext highlighter-rouge">pre-layernorm</code> ë°©ì‹ì„ ì±„íƒí–ˆë‹¤.</p>

<p align="center">
<img src="/assets/images/transformer/prelayernorm.png" alt="pre-layernorm vs post-layernorm" class="align-center image-caption" width="50%&quot;, height=&quot;50%" />
<strong><em><a href="https://github.com/rickiepark/nlp-with-transformers/blob/main/images/chapter03_layer-norm.png">pre-layernorm vs post-layernorm</a></em></strong>
</p>

<p>ìµœê·¼ <code class="language-plaintext highlighter-rouge">Transformer</code>ë¥˜ì˜ ëª¨ë¸ì— <code class="language-plaintext highlighter-rouge">pre-layernorm</code>ì„ ì ìš©í•˜ëŠ” ê²ƒì´ ì¢€ ë” ì•ˆì •ì ì´ê³  íš¨ìœ¨ì ì¸ í•™ìŠµì„ ìœ ë„í•  ìˆ˜ ìˆë‹¤ê³  ì‹¤í—˜ì„ í†µí•´ ë°í˜€ì§€ê³  ìˆë‹¤. <code class="language-plaintext highlighter-rouge">pre-layernorm</code> ì„ ì‚¬ìš©í•˜ë©´ ë³„ë‹¤ë¥¸ <code class="language-plaintext highlighter-rouge">Gradient Explode</code> í˜„ìƒì´ í˜„ì €íˆ ì¤„ì–´ë“¤ì–´ ë³µì¡í•œ ìŠ¤ì¼€ì¤„ëŸ¬(<code class="language-plaintext highlighter-rouge">warmup</code> ê¸°ëŠ¥ì´ ìˆëŠ” ìŠ¤ì¼€ì¤„ëŸ¬)ë¥¼ ì‚¬ìš©í•  í•„ìš”ê°€ ì—†ì–´ì§„ë‹¤ê³  í•˜ë‹ˆ ì°¸ê³ í•˜ì.</p>

<p>ì´ë ‡ê²Œ êµ¬í˜„í•œ <code class="language-plaintext highlighter-rouge">Single Encoder Block</code>ì„ ì´ì œ Nê°œ ìŒ“ê¸°ë§Œ í•˜ë©´ ë“œë””ì–´ ì¸ì½”ë”ë¥¼ ì™„ì„±í•  ìˆ˜ ìˆê²Œ ëœë‹¤.</p>

<h4 id="-encoder"><strong><code class="language-plaintext highlighter-rouge">ğŸ“š Encoder</code></strong></h4>

<p>ë“œë””ì–´ ëŒ€ë§ì˜ <code class="language-plaintext highlighter-rouge">Encoder</code> ê°ì²´ êµ¬í˜„ì„ ì‚´í´ë³¼ ì‹œê°„ì´ë‹¤.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Pytorch Implementation of Encoder(Stacked N EncoderLayer)
</span>
<span class="k">class</span> <span class="nc">Encoder</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="s">"""
    In this class, encode input sequence and then we stack N EncoderLayer
    First, we define "positional embedding" and then add to input embedding for making "word embedding"
    Second, forward "word embedding" to N EncoderLayer and then get output embedding
    In official paper, they use positional encoding, which is base on sinusoidal function(fixed, not learnable)
    But we use "positional embedding" which is learnable from training
    Args:
        max_seq: maximum sequence length, default 512 from official paper
        N: number of EncoderLayer, default 6 for base model
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">max_seq</span><span class="p">:</span> <span class="mi">512</span><span class="p">,</span> <span class="n">N</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">6</span><span class="p">,</span> <span class="n">dim_model</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">512</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span> <span class="n">dim_ffn</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2048</span><span class="p">,</span> <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Encoder</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">max_seq</span> <span class="o">=</span> <span class="n">max_seq</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">scale</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">dim_model</span><span class="p">))</span>  <span class="c1"># scale factor for input embedding from official paper
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">positional_embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">max_seq</span><span class="p">,</span> <span class="n">dim_model</span><span class="p">)</span>  <span class="c1"># add 1 for cls token
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">num_layers</span> <span class="o">=</span> <span class="n">N</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span> <span class="o">=</span> <span class="n">dim_model</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dim_ffn</span> <span class="o">=</span> <span class="n">dim_ffn</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">dropout</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">encoder_layers</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">ModuleList</span><span class="p">(</span>
            <span class="p">[</span><span class="n">EncoderLayer</span><span class="p">(</span><span class="n">dim_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">dim_ffn</span><span class="p">,</span> <span class="n">dropout</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">num_layers</span><span class="p">)]</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">layer_norm</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">dim_model</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">mask</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
        <span class="s">"""
        inputs: embedding from input sequence, shape =&gt; [BS, SEQ_LEN, DIM_MODEL]
        mask: mask for Encoder padded token for speeding up to calculate attention score
        """</span>
        <span class="n">layer_output</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">pos_x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">max_seq</span><span class="p">).</span><span class="n">repeat</span><span class="p">(</span><span class="n">inputs</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]).</span><span class="n">to</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">dropout</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">scale</span> <span class="o">*</span> <span class="n">inputs</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">positional_embedding</span><span class="p">(</span><span class="n">pos_x</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">encoder_layers</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span>
            <span class="n">layer_output</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">encoded_x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">layer_norm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># from official paper &amp; code by Google Research
</span>        <span class="n">layer_output</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">stack</span><span class="p">(</span><span class="n">layer_output</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">).</span><span class="n">to</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>  <span class="c1"># For Weighted Layer Pool: [N, BS, SEQ_LEN, DIM]
</span>        <span class="k">return</span> <span class="n">encoded_x</span><span class="p">,</span> <span class="n">layer_output</span>
</code></pre></div></div>

<p>ì—­ì‹œ ì§€ê¸ˆê¹Œì§€ ë‚´ìš©ì„ ì¢…í•©í•œ ê²ƒë¿ì´ë¼ì„œ í¬ê²Œ íŠ¹ì´í•œ ë‚´ìš©ì€ ì—†ê³ , êµ¬í˜„ìƒ ë†“ì¹˜ê¸° ì‰¬ìš´ ë¶€ë¶„ë§Œ ì•Œê³  ë„˜ì–´ê°€ë©´ ëœë‹¤. <code class="language-plaintext highlighter-rouge">forward</code> ë©”ì„œë“œì˜ ë³€ìˆ˜ <code class="language-plaintext highlighter-rouge">x</code>ë¥¼ ì´ˆê¸°í™”í•˜ëŠ” ì½”ë“œ ë¼ì¸ì„ ì£¼ëª©í•´ë³´ì. ì´ê²ƒì´ ë°”ë¡œ <code class="language-plaintext highlighter-rouge">Input Embedding</code>ê³¼ <code class="language-plaintext highlighter-rouge">Position Embedding</code>ì„ ë”í•˜ëŠ”(í–‰ë ¬ í•©) ì—°ì‚°ì„ êµ¬í˜„í•œ ê²ƒì´ë‹¤. ì´ ë•Œ ë†“ì¹˜ê¸° ì‰¬ìš´ ë¶€ë¶„ì´ ë°”ë¡œ <code class="language-plaintext highlighter-rouge">Input Embedding</code>ì— <code class="language-plaintext highlighter-rouge">scale factor</code>ë¥¼ ê³±í•´ì¤€ë‹¤ëŠ” ê²ƒì´ë‹¤. ì €ìì˜ ì£¼ì¥ì— ë”°ë¥´ë©´ <code class="language-plaintext highlighter-rouge">Input Embedding</code>ì—ë§Œ <code class="language-plaintext highlighter-rouge">scale factor</code>ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì•ˆì •ì ì¸ í•™ìŠµì— ë„ì›€ì´ ëœë‹¤ê³  í•˜ë‹ˆ ì°¸ê³ í•˜ì.</p>

<p>í•œí¸, ë§ˆì§€ë§‰ ì¸ì½”ë” ë¸”ëŸ­ì—ì„œ ë‚˜ì˜¨ ì„ë² ë”©ì„ ë‹¤ì‹œ í•œ ë²ˆ <code class="language-plaintext highlighter-rouge">layernorm</code>ì— í†µê³¼í•˜ë„ë¡ êµ¬í˜„í–ˆë‹¤. ì´ ë¶€ë¶„ë„ ì›ë³¸ ë…¼ë¬¸ì— ìˆëŠ” ë‚´ìš©ì€ ì•„ë‹ˆê³   <code class="language-plaintext highlighter-rouge">ViT</code>ì˜ ë…¼ë¬¸ ë‚´ìš©ì„ ì°¸ê³ í•´ ì¶”ê°€í–ˆë‹¤.</p>

<h4 id="decoderlayer"><strong><code class="language-plaintext highlighter-rouge">ğŸ“˜Â DecoderLayer</code></strong></h4>

<p>ì´ë²ˆì—ëŠ” ë””ì½”ë”ì— ì‚¬ìš©ëœ ë¸”ëŸ­ì˜ ë™ì‘ ë°©ì‹ê³¼ ì˜ë¯¸ ê·¸ë¦¬ê³  êµ¬í˜„ê¹Œì§€ ì•Œì•„ë³´ì. ì‚¬ì‹¤ ë””ì½”ë”ë„ ì§€ê¸ˆê¹Œì§€ ê³µë¶€í•œ ë‚´ìš©ê³¼ í¬ê²Œ ë‹¤ë¥¸ê²Œ ì—†ë‹¤. ë‹¤ë§Œ ì¸ì½”ë”ì™€ëŠ” ëª©ì ì´ ë‹¤ë¥´ê¸° ë•Œë¬¸ì— ë°œìƒí•˜ëŠ” ë¯¸ì„¸í•œ ë™ì‘ì˜ ì°¨ì´ì— ì£¼ëª©í•´ë³´ì. ë¨¼ì € <code class="language-plaintext highlighter-rouge">Single Decoder Block</code>ì€ <code class="language-plaintext highlighter-rouge">Single Encoder Block</code>ê³¼ ë‹¤ë¥´ê²Œ <code class="language-plaintext highlighter-rouge">Self-Attention</code>ì„ ë‘ ë²ˆ ìˆ˜í–‰í•œë‹¤. ì§€ê²¹ê² ì§€ë§Œ ë‹¤ì‹œ í•œ ë²ˆ Transformerì˜ ëª©ì ì„ ìƒê¸°ì‹œì¼œë³´ì. ë°”ë¡œ ëŒ€ìƒ ì–¸ì–´ë¥¼ íƒ€ê²Ÿ ì–¸ì–´ë¡œ ì˜ ë²ˆì—­í•˜ëŠ” ê²ƒì´ì—ˆë‹¤. ì¼ë‹¨ ì¸ì½”ë”ë¥¼ í†µí•´ ëŒ€ìƒ ì–¸ì–´ëŠ” ì˜ ì´í•´í•˜ê²Œ ë˜ì—ˆë‹¤. ê·¸ëŸ¼ ì´ì œ íƒ€ê²Ÿ ì–¸ì–´ë„ ì˜ ì´í•´í•´ì•¼í•˜ì§€ ì•Šì€ê°€?? ê·¸ë˜ì„œ íƒ€ê²Ÿ ì–¸ì–´ë¥¼ ì´í•´í•˜ê¸° ìœ„í•´ <code class="language-plaintext highlighter-rouge">Self-Attention</code>ì„ í•œ ë²ˆ, ê·¸ë¦¬ê³  ëŒ€ìƒ ì–¸ì–´ë¥¼ íƒ€ê²Ÿ ì–¸ì–´ë¡œ ë²ˆì—­í•˜ê¸° ìœ„í•´ <code class="language-plaintext highlighter-rouge">Self-Attention</code>ì„ í•œ ë²ˆ, ì´ 2ë²ˆ ìˆ˜í–‰í•˜ëŠ” ê²ƒì´ë‹¤. ì²«ë²ˆì§¸  <code class="language-plaintext highlighter-rouge">Self-Attention</code> ì„ <code class="language-plaintext highlighter-rouge">Masked Multi-Head Attention</code>, ë‘ë²ˆì§¸ë¥¼ <code class="language-plaintext highlighter-rouge">Encoder-Decoder Multi-Head Attention</code>ì´ë¼ê³  ë¶€ë¥¸ë‹¤.</p>

<p><strong><code class="language-plaintext highlighter-rouge">ğŸ­ Masked Multi-Head Attention</code></strong><br />
ì¸ì½”ë”ì˜ <code class="language-plaintext highlighter-rouge">Multi-Head Attentionì™€</code> í–‰ë ¬ $Q,K,V$ ì˜ ì¶œì²˜ê°€ ë‹¤ë¥´ë‹¤. ë””ì½”ë”ëŠ” ì¶œì²˜ê°€ íƒ€ê²Ÿ ì–¸ì–´ì¸ <code class="language-plaintext highlighter-rouge">linear projection matrix</code>ë¥¼ ì‚¬ìš©í•œë‹¤. ë˜í•œ ì¸ì½”ë”ì™€ ë‹¤ë¥´ê²Œ ê°œë³„ ì‹œì ì— ë§ëŠ” ë§ˆìŠ¤í‚¹ í–‰ë ¬ì´ í•„ìš”í•˜ë‹¤. ë””ì½”ë”ì˜ ê³¼ì—…ì€ ê²°êµ­ ëŒ€ìƒ ì–¸ì–´ë¥¼ ì˜ ì´í•´í•˜ê³  ê·¸ê²ƒì— ê°€ì¥ ì˜ ë“¤ì–´ë§ëŠ” íƒ€ê²Ÿ ì–¸ì–´ ì‹œí€€ìŠ¤ë¥¼ <code class="language-plaintext highlighter-rouge">generate</code>í•˜ëŠ” ê²ƒì´ë‹¤. ì¦‰, <code class="language-plaintext highlighter-rouge">Next Token Prediction</code>ì„ í†µí•´ ì‹œí€€ìŠ¤ë¥¼ ë§Œë“¤ì–´ë‚´ì•¼ í•œë‹¤. ê·¸ëŸ°ë° í˜„ì¬ ì‹œì ì—ì„œ ë¯¸ë˜ ì‹œì ì— ë””ì½”ë”ê°€ ì˜ˆì¸¡í•´ì•¼í•  í† í°ì„ ë¯¸ë¦¬ ì•Œê³  ìˆìœ¼ë©´ ê·¸ê²ƒì„ ì˜ˆì¸¡ì´ë¼ê³  í•  ìˆ˜ ìˆì„ê¹Œ?? ë””ì½”ë”ê°€ í˜„ì¬ ì‹œì ì˜ í† í°ì„ ì˜ˆì¸¡í•˜ëŠ”ë° ë¯¸ë˜ ì‹œì ì˜ <code class="language-plaintext highlighter-rouge">Context</code>ë¥¼ ë°˜ì˜í•˜ì§€ ëª»í•˜ë„ë¡ ë§‰ê¸° ìœ„í•´ ë¯¸ë¦¬ ë§ˆìŠ¤í‚¹ í–‰ë ¬ì„ ì •ì˜í•´ <code class="language-plaintext highlighter-rouge">Word_Embedding</code>ì— ì ìš©í•´ì¤€ë‹¤. ì´ë ‡ê²Œ ë§ˆìŠ¤í‚¹ì´ ì ìš©ëœ ì„ë² ë”© í–‰ë ¬ì„ ê°€ì§€ê³  <code class="language-plaintext highlighter-rouge">linear projection &amp; self-attention</code>ì„ ìˆ˜í–‰í•˜ê¸° ë•Œë¬¸ì— ì´ë¦„ ì•ì— <code class="language-plaintext highlighter-rouge">masked</code>ë¥¼ ë¶™ì´ê²Œ ë˜ì—ˆë‹¤.</p>

<p align="center">
<img src="/assets/images/transformer/decoder_mask.png" alt="Decoder Language Modeling Mask" class="align-center image-caption" width="50%&quot;, height=&quot;50%" />
<strong><em><a href="https://paul-hyun.github.io/transformer-02/">Decoder Language Modeling Mask</a></em></strong>
</p>

<p>ìœ„ ê·¸ë¦¼ì€ ë§ˆìŠ¤í‚¹ì„ ì ìš©í•œ <code class="language-plaintext highlighter-rouge">Word_Embedding</code>ì˜ ëª¨ìŠµì´ë‹¤. ì²« ë²ˆì§¸ ì‹œì ì—ì„œ ëª¨ë¸ì€ ìê¸° ìì‹ ì„ ì œì™¸í•œ ë‚˜ë¨¸ì§€ <code class="language-plaintext highlighter-rouge">Context</code>ë¥¼ ì˜ˆì¸¡ì— í™œìš©í•  ìˆ˜ ì—†ë‹¤. ê·¸ë˜ì„œ ì´í•˜ ë‚˜ë¨¸ì§€ í† í°ì„ ì „ë¶€ ë§ˆìŠ¤í‚¹ ì²˜ë¦¬í•´ì¤¬ë‹¤. ë‘ë²ˆì§¸ ì‹œì ì—ì„œëŠ” ì§ì „ ì‹œì ì¸ ì²«ë²ˆì§¸ í† í°ê³¼ ìê¸° ìì‹ ë§Œ ì°¸ê³ í•  ìˆ˜ ìˆë‹¤. í•œí¸, ì´ë ‡ê²Œ ì§ì „ <code class="language-plaintext highlighter-rouge">Context</code>ë§Œ ê°€ì§€ê³  í˜„ì¬ í† í°ì„ ì¶”ë¡ í•˜ëŠ” ê²ƒì„ <code class="language-plaintext highlighter-rouge">Language Modeling</code>ì´ë¼ ë¶€ë¥¸ë‹¤. ê·¸ë¦¬ê³  ë§ˆì°¬ê°€ì§€ë¡œ ë””ì½”ë” ì—­ì‹œ ì‹œí€€ìŠ¤ì— íŒ¨ë”© ì²˜ë¦¬ë¥¼ í•´ì£¼ê¸° ë•Œë¬¸ì— ì¸ì½”ë”ì™€ ë™ì¼í•œ ì›ë¦¬ë¡œ ë§Œë“  <code class="language-plaintext highlighter-rouge">decoder padding mask</code> ì—­ì‹œ í•„ìš”í•˜ë‹¤.</p>

<p>ë§ˆìŠ¤í‚¹ í–‰ë ¬ êµ¬í˜„ì€ ìµœìƒìœ„ ê°ì²´ì¸ <code class="language-plaintext highlighter-rouge">Transformer</code>ì˜ ë‚´ë¶€ ë©”ì„œë“œë¡œ ë§Œë“¤ì—ˆìœ¼ë‹ˆ, ê·¸ ë•Œ ìì„¸íˆ ì„¤ëª…í•˜ê² ë‹¤. ì´í•˜ ë‚˜ë¨¸ì§€ ë””í…Œì¼ì€ ì¸ì½”ë”ì˜ ê²ƒê³¼ ë™ì¼í•˜ë‹¤.</p>

<p><strong><code class="language-plaintext highlighter-rouge">ğŸª¢ Encoder-Decoder Multi-Head Attention</code></strong><br />
ì¸ì½”ë”ë¥¼ í†µí•´ ì´í•´í•œ ëŒ€ìƒ ì–¸ì–´ ì‹œí€€ìŠ¤ì™€ ë°”ë¡œ ì§ì „ <code class="language-plaintext highlighter-rouge">Self-Attention</code>ì„ í†µí•´ ì´í•´í•œ íƒ€ê²Ÿ ì–¸ì–´ ì‹œí€€ìŠ¤ë¥¼ ì„œë¡œ ëŒ€ì¡°í•˜ëŠ” ë ˆì´ì–´ë‹¤. ìš°ë¦¬ì˜ ì§€ê¸ˆ ëª©ì ì€ <code class="language-plaintext highlighter-rouge">íƒ€ê²Ÿ ì–¸ì–´</code>ì™€ ê°€ì¥ ìœ ì‚¬í•œ <code class="language-plaintext highlighter-rouge">ëŒ€ìƒ ì–¸ì–´</code>ë¥¼ ì°¾ì•„ ë¬¸ì¥ì„ ì™„ì„±í•˜ëŠ” ê²ƒì´ë‹¤. ë”°ë¼ì„œ ì–´í…ì…˜ ê³„ì‚°ì— ì‚¬ìš©ë  í–‰ë ¬ $Q$ ì˜ ì¶œì²˜ëŠ” ì§ì „ ë ˆì´ì–´ì¸ <code class="language-plaintext highlighter-rouge">Masked Multi-Head Attention</code> ì˜ ë°˜í™˜ê°’ì„ ì‚¬ìš©í•˜ê³ , í–‰ë ¬ $K,V$ ëŠ” ì¸ì½”ë”ì˜ ìµœì¢… ë°˜í™˜ê°’ì„ ì‚¬ìš©í•œë‹¤.</p>

<p>í•œí¸, ì—¬ê¸° ë ˆì´ì–´ì—ëŠ” ë§ˆìŠ¤í‚¹ í–‰ë ¬ì´ ì„¸ ì¢…ë¥˜ë‚˜ í•„ìš”í•˜ë‹¤. ê·¸ ì´ìœ ëŠ” ì„œë¡œ ì¶œì²˜ê°€ ë‹¤ë¥¸ ë‘ê°€ì§€ í–‰ë ¬ì„ ê³„ì‚°ì— ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì´ë‹¤. ì§€ê¸ˆì€ ì—¬ì „íˆ ë””ì½”ë”ì˜ ì—­í• ì„ ìˆ˜í–‰í•˜ëŠ” ê²ƒì´ê¸° ë•Œë¬¸ì— ì§ì „ ë ˆì´ì–´ì—ì„œ ì‚¬ìš©í•œ 2ê°œì˜ ë§ˆìŠ¤í‚¹ í–‰ë ¬ì´ ê·¸ëŒ€ë¡œ í•„ìš”í•˜ë‹¤. ê·¸ë¦¬ê³  ì¸ì½”ë”ì—ì„œ ë„˜ì–´ì˜¨ ê°’ì„ ì‚¬ìš©í•œë‹¤ëŠ” ê²ƒì€ ì¸ì½”ë”ì˜ íŒ¨ë”© ì—­ì‹œ ì²˜ë¦¬ê°€ í•„ìš”í•˜ë‹¤ëŠ” ì˜ë¯¸ë‹¤. ë”°ë¼ì„œ <code class="language-plaintext highlighter-rouge">lm_mask</code>, <code class="language-plaintext highlighter-rouge">dec_pad_mask</code>, <code class="language-plaintext highlighter-rouge">enc_pad_mask</code>ê°€ í•„ìš”í•˜ë‹¤. ì—­ì‹œ ë§ˆìŠ¤í‚¹ êµ¬í˜„ì€ ìµœìƒìœ„ ê°ì²´ ì„¤ëª… ë•Œ í•¨ê»˜ ì‚´í´ë³´ê² ë‹¤.</p>

<p><strong><code class="language-plaintext highlighter-rouge">ğŸ‘©â€ğŸ’»Â Implementation</code></strong><br />
ì´ì œ <code class="language-plaintext highlighter-rouge">Single Decoder Block</code>ì˜ êµ¬í˜„ì„ ì‚´í´ë³´ì. ì—­ì‹œ íŒŒì´í† ì¹˜ë¡œ êµ¬í˜„í–ˆë‹¤.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Pytorch Implementation of Single Decoder Block
</span>
<span class="k">class</span> <span class="nc">DecoderLayer</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="s">"""
    Class for decoder model module in Transformer
    In this class, we stack each decoder_model module (Masked Multi-Head Attention, Residual-Connection, LayerNorm, FFN)
    We apply pre-layernorm, which is different from original paper
    References:
        https://arxiv.org/abs/1706.03762
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim_model</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">512</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span> <span class="n">dim_ffn</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2048</span><span class="p">,</span> <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">DecoderLayer</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">masked_attention</span> <span class="o">=</span> <span class="n">MultiHeadAttention</span><span class="p">(</span>
            <span class="n">dim_model</span><span class="p">,</span>
            <span class="n">num_heads</span><span class="p">,</span>
            <span class="nb">int</span><span class="p">(</span><span class="n">dim_model</span> <span class="o">/</span> <span class="n">num_heads</span><span class="p">),</span>
            <span class="n">dropout</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">enc_dec_attention</span> <span class="o">=</span> <span class="n">MultiHeadAttention</span><span class="p">(</span>
            <span class="n">dim_model</span><span class="p">,</span>
            <span class="n">num_heads</span><span class="p">,</span>
            <span class="nb">int</span><span class="p">(</span><span class="n">dim_model</span> <span class="o">/</span> <span class="n">num_heads</span><span class="p">),</span>
            <span class="n">dropout</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">layer_norm1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">dim_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">layer_norm2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">dim_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">layer_norm3</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">dim_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">dropout</span><span class="p">)</span>  <span class="c1"># dropout is not learnable layer
</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">ffn</span> <span class="o">=</span> <span class="n">FeedForward</span><span class="p">(</span>
            <span class="n">dim_model</span><span class="p">,</span>
            <span class="n">dim_ffn</span><span class="p">,</span>
            <span class="n">dropout</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">dec_mask</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">enc_dec_mask</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">enc_output</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="n">ln_x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">layer_norm1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">residual_x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">dropout</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">masked_attention</span><span class="p">(</span><span class="n">ln_x</span><span class="p">,</span> <span class="n">dec_mask</span><span class="p">))</span> <span class="o">+</span> <span class="n">x</span>

        <span class="n">ln_x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">layer_norm2</span><span class="p">(</span><span class="n">residual_x</span><span class="p">)</span>
        <span class="n">residual_x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">dropout</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">enc_dec_attention</span><span class="p">(</span><span class="n">ln_x</span><span class="p">,</span> <span class="n">enc_dec_mask</span><span class="p">,</span> <span class="n">enc_output</span><span class="p">))</span> <span class="o">+</span> <span class="n">x</span>  <span class="c1"># for enc_dec self-attention
</span>
        <span class="n">ln_x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">layer_norm3</span><span class="p">(</span><span class="n">residual_x</span><span class="p">)</span>
        <span class="n">fx</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">ffn</span><span class="p">(</span><span class="n">ln_x</span><span class="p">)</span> <span class="o">+</span> <span class="n">residual_x</span>
        <span class="k">return</span> <span class="n">fx</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">Self-Attention</code> ë ˆì´ì–´ê°€ ì¸ì½”ë”ë³´ë‹¤ í•˜ë‚˜ ë” ì¶”ê°€ë˜ì–´ <code class="language-plaintext highlighter-rouge">add &amp; norm</code> ì„ ì´ 3ë²ˆ í•´ì¤˜ì•¼ í•œë‹¤ëŠ” ê²ƒì„ ì œì™¸í•˜ê³ ëŠ” í¬ê²Œ êµ¬í˜„ìƒì˜ íŠ¹ì´ì ì€ ì—†ë‹¤. ê·¸ì € ì§€ê¸ˆê¹Œì§€ ì‚´í´ë³¸ ë¸”ëŸ­ì„ ìš”ë¦¬ì¡°ë¦¬ ë‹¤ì‹œ ìŒ“ìœ¼ë©´ ëœë‹¤.</p>

<h4 id="decoder"><strong><code class="language-plaintext highlighter-rouge">ğŸ“šÂ Decoder</code></strong></h4>

<p><code class="language-plaintext highlighter-rouge">Single Decoder Block</code>ì„ <code class="language-plaintext highlighter-rouge">N</code>ê°œ ìŒ“ê³  ì „ì²´ ë””ì½”ë” ë™ì‘ì„ ìˆ˜í–‰í•˜ëŠ” <code class="language-plaintext highlighter-rouge">Decoder</code> ê°ì²´ì˜ êµ¬í˜„ì„ ì•Œì•„ë³´ì.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Pytorch Implementation of Decoder(N Stacked Single Decoder Block)
</span>
<span class="k">class</span> <span class="nc">Decoder</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="s">"""
    In this class, decode encoded embedding from encoder by outputs (target language, Decoder's Input Sequence)
    First, we define "positional embedding" for Decoder's Input Sequence,
    and then add them to Decoder's Input Sequence for making "decoder word embedding"
    Second, forward "decoder word embedding" to N DecoderLayer and then pass to linear &amp; softmax for OutPut Probability
    Args:
        vocab_size: size of vocabulary for output probability
        max_seq: maximum sequence length, default 512 from official paper
        N: number of EncoderLayer, default 6 for base model
    References:
        https://arxiv.org/abs/1706.03762
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">vocab_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">max_seq</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">512</span><span class="p">,</span>
        <span class="n">N</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">6</span><span class="p">,</span>
        <span class="n">dim_model</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">512</span><span class="p">,</span>
        <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span>
        <span class="n">dim_ffn</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2048</span><span class="p">,</span>
        <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Decoder</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">max_seq</span> <span class="o">=</span> <span class="n">max_seq</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">scale</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">dim_model</span><span class="p">))</span>  <span class="c1"># scale factor for input embedding from official paper
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">positional_embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">max_seq</span><span class="p">,</span> <span class="n">dim_model</span><span class="p">)</span>  <span class="c1"># add 1 for cls token
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">num_layers</span> <span class="o">=</span> <span class="n">N</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span> <span class="o">=</span> <span class="n">dim_model</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dim_ffn</span> <span class="o">=</span> <span class="n">dim_ffn</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">dropout</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">decoder_layers</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">ModuleList</span><span class="p">(</span>
            <span class="p">[</span><span class="n">DecoderLayer</span><span class="p">(</span><span class="n">dim_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">dim_ffn</span><span class="p">,</span> <span class="n">dropout</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">num_layers</span><span class="p">)]</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">layer_norm</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">dim_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">fc_out</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">dim_model</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">)</span>  <span class="c1"># In Pytorch, nn.CrossEntropyLoss already has softmax function
</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">dec_mask</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">enc_dec_mask</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">enc_output</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
        <span class="s">"""
        inputs: embedding from input sequence, shape =&gt; [BS, SEQ_LEN, DIM_MODEL]
        dec_mask: mask for Decoder padded token for Language Modeling
        enc_dec_mask: mask for Encoder-Decoder Self-Attention, from encoder padded token
        """</span>
        <span class="n">layer_output</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">pos_x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">max_seq</span><span class="p">).</span><span class="n">repeat</span><span class="p">(</span><span class="n">inputs</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]).</span><span class="n">to</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">dropout</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">scale</span> <span class="o">*</span> <span class="n">inputs</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">positional_embedding</span><span class="p">(</span><span class="n">pos_x</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">decoder_layers</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dec_mask</span><span class="p">,</span> <span class="n">enc_dec_mask</span><span class="p">,</span> <span class="n">enc_output</span><span class="p">)</span>
            <span class="n">layer_output</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">decoded_x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">fc_out</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">layer_norm</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>  <span class="c1"># Because of pre-layernorm
</span>        <span class="n">layer_output</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">stack</span><span class="p">(</span><span class="n">layer_output</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">).</span><span class="n">to</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>  <span class="c1"># For Weighted Layer Pool: [N, BS, SEQ_LEN, DIM]
</span>        <span class="k">return</span> <span class="n">decoded_x</span><span class="p">,</span> <span class="n">layer_output</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">Encoder</code> ê°ì²´ì™€ ëª¨ë“  ë¶€ë¶„ì´ ë™ì¼í•˜ë‹¤. ë””í…Œì¼í•œ ì„¤ì •ë§Œ ë””ì½”ë”ì— ë§ê²Œ ë³€ê²½ë˜ì—ˆì„ ë¿ì´ë‹¤. <code class="language-plaintext highlighter-rouge">self.fc_out</code> ì— ì£¼ëª©í•´ë³´ì. ë””ì½”ë”ëŠ” í˜„ì¬ ì‹œì ì— ê°€ì¥ ì í•©í•œ í† í°ì„ ì˜ˆì¸¡í•´ì•¼ í•˜ê¸° ë•Œë¬¸ì— ë””ì½”ë”ì˜ ì¶œë ¥ë¶€ë¶„ì— ë¡œì§“ ê³„ì‚°ì„ ìœ„í•œ ë ˆì´ì–´ê°€ í•„ìš”í•˜ë‹¤. ê·¸ ì—­í• ì„ í•˜ëŠ” ê²ƒì´ ë°”ë¡œ <code class="language-plaintext highlighter-rouge">self.fc_out</code>ì´ë‹¤. í•œí¸, <code class="language-plaintext highlighter-rouge">self.fc_out</code>ì˜ ì¶œë ¥ ì°¨ì›ì´ <code class="language-plaintext highlighter-rouge">vocab_size</code>ìœ¼ë¡œ ë˜ì–´ìˆëŠ”ë°, ë””ì½”ë”ëŠ” ë””ì½”ë”ê°€ ê°€ì§„ ì „ì²´ <code class="language-plaintext highlighter-rouge">vocab</code> ì„ í˜„ì¬ ì‹œì ì— ì í•©í•œ í† í° í›„ë³´êµ°ìœ¼ë¡œ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì´ë‹¤.</p>

<h4 id="-transformer"><strong><code class="language-plaintext highlighter-rouge">ğŸ¦¾ Transformer</code></strong></h4>
<p>ì´ì œ ëŒ€ë§ì˜ ë§ˆì§€ë§‰â€¦ ëª¨ë¸ì˜ ê°€ì¥ ìµœìƒìœ„ ê°ì²´ì¸ <code class="language-plaintext highlighter-rouge">Transformer</code>ì— ëŒ€í•´ì„œ ì‚´í´ë³´ì. ê°ì²´ì˜ ë™ì‘ì€ ì •ë¦¬í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.</p>

<ul>
  <li><strong>1) Make <code class="language-plaintext highlighter-rouge">Input Embedding</code> for Encoder &amp; Decoder respectively, Init <code class="language-plaintext highlighter-rouge">Encoder &amp; Decoder</code> Class</strong></li>
  <li><strong>2) Make 3 types of Masking: <code class="language-plaintext highlighter-rouge">Encoder Padding Mask</code>, <code class="language-plaintext highlighter-rouge">Decoder LM &amp; Padding Mask</code>, <code class="language-plaintext highlighter-rouge">Encoder-Decoder Mask</code></strong></li>
  <li><strong>3) Return <code class="language-plaintext highlighter-rouge">Output</code> from Encoder &amp; Decoder</strong></li>
</ul>

<p>íŠ¹íˆ ê³„ì† ë¯¸ë¤„ì™”ë˜ ë§ˆìŠ¤í‚¹ êµ¬í˜„ì— ëŒ€í•´ì„œ ì‚´í´ë³´ì. ë‚˜ë¨¸ì§€ëŠ” ì´ë¯¸ ì•ì—ì„œ ë§ì´ ì„¤ëª…í–ˆìœ¼ë‹ˆê¹Œ ë„˜ì–´ê°€ë„ë¡ í•˜ê² ë‹¤. ì¼ë‹¨ ë¨¼ì € ì½”ë“œë¥¼ ì½ì–´ë³´ì. ì¶”ê°€ë¡œ <code class="language-plaintext highlighter-rouge">Input Embedding</code> êµ¬í˜„ì€ ì‚¬ìš©ìì˜ <code class="language-plaintext highlighter-rouge">vocab</code> êµ¬ì¶• ë°©ì‹ì— ë”°ë¼ ë‹¬ë¼ì§„ë‹¤. í•„ìì˜ ê²½ìš° ëŒ€ìƒ ì–¸ì–´ì™€ íƒ€ê²Ÿ ì–¸ì–´ì˜ <code class="language-plaintext highlighter-rouge">vocab</code>ì„ ë¶„ë¦¬í•´ ì‚¬ìš©í•˜ëŠ” ê²ƒì„ ê°€ì •í•˜ê³  ì½”ë“œë¥¼ ë§Œë“¤ì–´ ì„ë² ë”© ë ˆì´ì–´ë¥¼ ë”°ë¡œ ë”°ë¡œ êµ¬í˜„í•´ì¤¬ë‹¤. <code class="language-plaintext highlighter-rouge">vocab</code>ì„ í†µí•©ìœ¼ë¡œ êµ¬ì¶•í•˜ì‹œëŠ” ë¶„ì´ë¼ë©´ í•˜ë‚˜ë§Œ ì •ì˜í•´ë„ ìƒê´€ì—†ë‹¤. ëŒ€ì‹  ë‚˜ì¤‘ì— ë””ì½”ë”ì˜ ë¡œì§“ê°’ ê³„ì‚°ì„ ìœ„í•´ ê°œë³„ ì–¸ì–´ì˜ í† í° ì‚¬ì´ì¦ˆëŠ” ì•Œê³  ìˆì–´ì•¼ í•  ê²ƒì´ë‹¤.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Pytorch Implementation of Transformer
</span>
<span class="k">class</span> <span class="nc">Transformer</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="s">"""
    Main class for Pure Transformer, Pytorch implementation
    There are two Masking Method for padding token
        1) Row &amp; Column masking
        2) Column masking only at forward time, Row masking at calculating loss time
    second method is more efficient than first method, first method is complex &amp; difficult to implement
    Args:
        enc_vocab_size: size of vocabulary for Encoder Input Sequence
        dec_vocab_size: size of vocabulary for Decoder Input Sequence
        max_seq: maximum sequence length, default 512 from official paper
        enc_N: number of EncoderLayer, default 6 for base model
        dec_N: number of DecoderLayer, default 6 for base model
    Reference:
        https://arxiv.org/abs/1706.03762
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">enc_vocab_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">dec_vocab_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">max_seq</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">512</span><span class="p">,</span>
        <span class="n">enc_N</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">6</span><span class="p">,</span>
        <span class="n">dec_N</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">6</span><span class="p">,</span>
        <span class="n">dim_model</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">512</span><span class="p">,</span>
        <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span>
        <span class="n">dim_ffn</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2048</span><span class="p">,</span>
        <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Transformer</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">enc_input_embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">enc_vocab_size</span><span class="p">,</span> <span class="n">dim_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dec_input_embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">dec_vocab_size</span><span class="p">,</span> <span class="n">dim_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">Encoder</span><span class="p">(</span><span class="n">max_seq</span><span class="p">,</span> <span class="n">enc_N</span><span class="p">,</span> <span class="n">dim_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">dim_ffn</span><span class="p">,</span> <span class="n">dropout</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="n">Decoder</span><span class="p">(</span><span class="n">dec_vocab_size</span><span class="p">,</span> <span class="n">max_seq</span><span class="p">,</span> <span class="n">dec_N</span><span class="p">,</span> <span class="n">dim_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">dim_ffn</span><span class="p">,</span> <span class="n">dropout</span><span class="p">)</span>

    <span class="o">@</span><span class="nb">staticmethod</span>
    <span class="k">def</span> <span class="nf">enc_masking</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">enc_pad_index</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="s">""" make masking matrix for Encoder Padding Token """</span>
        <span class="n">enc_mask</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span> <span class="o">!=</span> <span class="n">enc_pad_index</span><span class="p">).</span><span class="nb">int</span><span class="p">().</span><span class="n">repeat</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]).</span><span class="n">view</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
        <span class="k">return</span> <span class="n">enc_mask</span>

    <span class="o">@</span><span class="nb">staticmethod</span>
    <span class="k">def</span> <span class="nf">dec_masking</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">dec_pad_index</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="s">""" make masking matrix for Decoder Masked Multi-Head Self-Attention """</span>
        <span class="n">pad_mask</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span> <span class="o">!=</span> <span class="n">dec_pad_index</span><span class="p">).</span><span class="nb">int</span><span class="p">().</span><span class="n">repeat</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]).</span><span class="n">view</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
        <span class="n">lm_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tril</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">ones</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>
        <span class="n">dec_mask</span> <span class="o">=</span> <span class="n">pad_mask</span> <span class="o">*</span> <span class="n">lm_mask</span>
        <span class="k">return</span> <span class="n">dec_mask</span>

    <span class="o">@</span><span class="nb">staticmethod</span>
    <span class="k">def</span> <span class="nf">enc_dec_masking</span><span class="p">(</span><span class="n">enc_x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">dec_x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">enc_pad_index</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="s">""" make masking matrix for Encoder-Decoder Multi-Head Self-Attention in Decoder """</span>
        <span class="n">enc_dec_mask</span> <span class="o">=</span> <span class="p">(</span><span class="n">enc_x</span> <span class="o">!=</span> <span class="n">enc_pad_index</span><span class="p">).</span><span class="nb">int</span><span class="p">().</span><span class="n">repeat</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">dec_x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]).</span><span class="n">view</span><span class="p">(</span>
            <span class="n">enc_x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">dec_x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">enc_x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">enc_dec_mask</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">enc_inputs</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">dec_inputs</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">enc_pad_index</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">dec_pad_index</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
        <span class="n">enc_mask</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">enc_masking</span><span class="p">(</span><span class="n">enc_inputs</span><span class="p">,</span> <span class="n">enc_pad_index</span><span class="p">)</span>  <span class="c1"># enc_x.shape[1] == encoder input sequence length
</span>        <span class="n">dec_mask</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">dec_masking</span><span class="p">(</span><span class="n">dec_inputs</span><span class="p">,</span> <span class="n">dec_pad_index</span><span class="p">)</span>  <span class="c1"># dec_x.shape[1] == decoder input sequence length
</span>        <span class="n">enc_dec_mask</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">enc_dec_masking</span><span class="p">(</span><span class="n">enc_inputs</span><span class="p">,</span> <span class="n">dec_inputs</span><span class="p">,</span> <span class="n">enc_pad_index</span><span class="p">)</span>

        <span class="n">enc_x</span><span class="p">,</span> <span class="n">dec_x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">enc_input_embedding</span><span class="p">(</span><span class="n">enc_inputs</span><span class="p">),</span> <span class="bp">self</span><span class="p">.</span><span class="n">dec_input_embedding</span><span class="p">(</span><span class="n">dec_inputs</span><span class="p">)</span>

        <span class="n">enc_output</span><span class="p">,</span> <span class="n">enc_layer_output</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">enc_x</span><span class="p">,</span> <span class="n">enc_mask</span><span class="p">)</span>
        <span class="n">dec_output</span><span class="p">,</span> <span class="n">dec_layer_output</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">decoder</span><span class="p">(</span><span class="n">dec_x</span><span class="p">,</span> <span class="n">dec_mask</span><span class="p">,</span> <span class="n">enc_dec_mask</span><span class="p">,</span> <span class="n">enc_output</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">enc_output</span><span class="p">,</span> <span class="n">dec_output</span><span class="p">,</span> <span class="n">enc_layer_output</span><span class="p">,</span> <span class="n">dec_layer_output</span>
</code></pre></div></div>

<p>ë§ˆìŠ¤í‚¹ì˜ í•„ìš”ì„±ì´ë‚˜ ë™ì‘ ë°©ì‹ì€ ì´ë¯¸ ìœ„ì—ì„œ ëª¨ë‘ ì„¤ëª…í–ˆê¸° ë•Œë¬¸ì— êµ¬í˜„ìƒ íŠ¹ì§•ë§Œ ì„¤ëª…í•˜ë ¤í•œë‹¤. ì„¸ê°€ì§€ ë§ˆìŠ¤í‚¹ ëª¨ë‘ ê³µí†µì ìœ¼ë¡œ êµ¬í˜„ ì½”ë“œ ë¼ì¸ì— <code class="language-plaintext highlighter-rouge">.int()</code> ê°€ ë“¤ì–´ê°€ ìˆë‹¤. ê·¸ ì´ìœ ëŠ” $\frac{Qâ€¢K^T}{\sqrt{d_h}}$ì— ë§ˆìŠ¤í‚¹ì„ ì ìš©í•  ë•Œ <code class="language-plaintext highlighter-rouge">torch.masked_fill</code> ë©”ì„œë“œë¥¼ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì´ë‹¤. ë¬´ìŠ¨ ì´ìœ  ë•Œë¬¸ì¸ì§€ëŠ” ëª¨ë¥´ê² ìœ¼ë‚˜ <code class="language-plaintext highlighter-rouge">torch.masked_fill</code>ì˜ ê²½ìš° ë§ˆìŠ¤í‚¹ ì¡°ê±´ìœ¼ë¡œ <code class="language-plaintext highlighter-rouge">boolean</code>ì„ ì „ë‹¬í•˜ë©´ ë§ˆìŠ¤í‚¹ì´ ì œëŒ€ë¡œ êµ¬í˜„ë˜ì§€ ì•ŠëŠ” í˜„ìƒì´ ìˆì—ˆë‹¤. í•œí¸, ì •ìˆ˜ê°’ìœ¼ë¡œ ì¡°ê±´ì„ êµ¬í˜„í•˜ë©´ ì˜ë„í•œëŒ€ë¡œ êµ¬í˜„ì´ ë˜ëŠ” ê²ƒì„ í™•ì¸í–ˆë‹¤. ê·¸ë˜ì„œ íŒ¨ë”©ì— í•´ë‹¹ë˜ëŠ” í† í°ì´ ìœ„ì¹˜í•œ ê³³ì˜ ì›ì†Œê°’ì„ ì •ìˆ˜í˜• <code class="language-plaintext highlighter-rouge">Binary</code> ë¡œ ë§Œë“¤ì–´ì£¼ê¸° ìœ„í•´ <code class="language-plaintext highlighter-rouge">int()</code> ë¥¼ ì‚¬ìš©í•œ ê²ƒì´ë‹¤.</p>

<p><strong><code class="language-plaintext highlighter-rouge">ğŸ­Â Decoder Mask</code></strong><br />
ë””ì½”ë”ëŠ” ì´ 2ê°€ì§€ì˜ ë§ˆìŠ¤í‚¹ì´ í•„ìš”í•˜ë‹¤ê³  ì–¸ê¸‰í–ˆì—ˆë‹¤. <code class="language-plaintext highlighter-rouge">pad_mask</code>ì˜ ê²½ìš°ëŠ” ì¸ì½”ë”ì˜ ê²ƒê³¼ ë™ì¼í•œ ì›ë¦¬ë¥¼ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì— ì„¤ëª…ì„ ìƒëµí•˜ê² ë‹¤. <code class="language-plaintext highlighter-rouge">lm_mask</code> ì˜ ê²½ìš°ëŠ” <code class="language-plaintext highlighter-rouge">torch.tril</code>ì„ ì´ìš©í•´ í•˜ì‚¼ê°í–‰ë ¬ í˜•íƒœë¡œ ë§ˆìŠ¤í‚¹ í–‰ë ¬ ì •ì˜ê°€ ì‰½ê²Œ ê°€ëŠ¥í•˜ë‹¤.<br />
í•œí¸, 2ê°œì˜ ë§ˆìŠ¤í‚¹ì„ ë™ì‹œì— ë””ì½”ë” ê°ì²´ì— ë„˜ê¸°ëŠ” ê²ƒì€ ë§¤ìš° ë¹„íš¨ìœ¨ì ì´ë‹¤. ë”°ë¼ì„œ <code class="language-plaintext highlighter-rouge">pad_mask</code> ì™€ <code class="language-plaintext highlighter-rouge">lm_mask</code>ì˜ í•©ì§‘í•©ì— í•´ë‹¹í•˜ëŠ” í–‰ë ¬ì„ ë§Œë“¤ì–´ ìµœì¢… ë””ì½”ë”ì˜ ë§ˆìŠ¤í‚¹ìœ¼ë¡œ ì „ë‹¬í•œë‹¤.</p>

<p><strong><code class="language-plaintext highlighter-rouge">ğŸ™ŒÂ Encoder-Decoder Mask</code></strong><br />
ì´ë²ˆ ê²½ìš°ëŠ” ë§ˆìŠ¤í‚¹ì˜ í–‰ë°©í–¥ ì°¨ì›ì€ ë””ì½”ë” ì…ë ¥ê°’ì˜ ì‹œí€€ìŠ¤ ê¸¸ì´, ì—´ë°©í–¥ ì°¨ì›ì€ ì¸ì½”ë” ì…ë ¥ê°’ì˜ ì‹œí€€ìŠ¤ ê¸¸ì´ë¡œ ì„¤ì •í•´ì•¼ í•œë‹¤. ê·¸ ì´ìœ ëŠ” ë‹¤ë¥¸ <code class="language-plaintext highlighter-rouge">Self-Attention</code> ë ˆì´ì–´ì™€ ë‹¤ë¥´ê²Œ ì„œë¡œ ë‹¤ë¥¸ ì¶œì²˜ë¥¼ í†µí•´ ë§Œë“  í–‰ë ¬ì„ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì— $\frac{Qâ€¢K^T}{\sqrt{d_h}}$ì˜ ëª¨ì–‘ì´ ì •ì‚¬ê°í–‰ë ¬ì´ ì•„ë‹ ìˆ˜ë„ ìˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ í•œêµ­ì–´ ë¬¸ì¥ì„ ì˜ì–´ë¡œ ë°”ê¾¸ëŠ” ê²½ìš°ë¥¼ ìƒê°í•´ë³´ì. ê°™ì€ ëœ»ì´ ë‹´ê¸´ ë¬¸ì¥ì´ë¼ê³  í•´ì„œ ë‘ ë¬¸ì¥ì˜ ê¸¸ì´ê°€ ê°™ì€ê°€?? ì•„ë‹ˆë‹¤. ì„œë¡œ ë‹¤ë¥¸ ì–¸ì–´ë¼ë©´ ê±°ì˜ ëŒ€ë¶€ë¶„ì˜ ê²½ìš° ê¸¸ì´ê°€ ë‹¤ë¥¼ ê²ƒì´ë‹¤. ë”°ë¼ì„œ $\frac{Qâ€¢K^T}{\sqrt{d_h}}$ì˜ í–‰ë°©í–¥ì€ ë””ì½”ë”ì˜ ì‹œí€€ìŠ¤ ê¸¸ì´ì— ë”°ë¥´ê³  ì—´ë°©í–¥ì€ ì¸ì½”ë”ì˜ ì‹œí€€ìŠ¤ ê¸¸ì´ì— ë”°ë¥´ë„ë¡ ë§ˆìŠ¤í‚¹ ì—­ì‹œ êµ¬í˜„í•´ì¤˜ì•¼ í•œë‹¤.<br />
ê·¸ë¦¬ê³  ì´ë²ˆ ë§ˆìŠ¤í‚¹ì„ ë§Œë“œëŠ” ëª©ì ì´ ì¸ì½”ë”ì˜ íŒ¨ë”©ì„ ë§ˆìŠ¤í‚¹ ì²˜ë¦¬í•´ì£¼ê¸° ìœ„í•¨ì´ê¸° ë•Œë¬¸ì— <code class="language-plaintext highlighter-rouge">enc_pad_index</code> ë§¤ê°œë³€ìˆ˜ì—ëŠ” ì¸ì½”ë” <code class="language-plaintext highlighter-rouge">vocab</code>ì—ì„œ ì •ì˜í•œ <code class="language-plaintext highlighter-rouge">pad_token_ID</code>ë¥¼ ì „ë‹¬í•˜ë©´ ëœë‹¤.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># scaled_dot_product_attentionì˜ ì¼ë¶€
</span>
<span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
		<span class="n">attention_matrix</span> <span class="o">=</span> <span class="n">attention_matrix</span><span class="p">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">mask</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="s">'-inf'</span><span class="p">))</span>
</code></pre></div></div>

<p>ì´ë ‡ê²Œ êµ¬í˜„ëœ ë§ˆìŠ¤í‚¹ì€ <code class="language-plaintext highlighter-rouge">scaled_dot_product_attention</code> ë©”ì„œë“œì— êµ¬í˜„ëœ ì¡°ê±´ë¬¸ì„ í†µí•´ ë§ˆìŠ¤í‚¹ ëŒ€ìƒì„ -âˆìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ì—­í• ì„ í•˜ê²Œ ëœë‹¤.</p>]]></content><author><name>qcqced</name><email>qcqced123@gmail.com</email></author><category term="NLP" /><category term="Natural Language Process" /><category term="Transformer" /><category term="Self-Attention" /><category term="Seq2Seq" /><category term="Encoder" /><category term="Decoder" /><summary type="html"><![CDATA[Transformer Official Paper Review with Pytorch Implementation]]></summary></entry><entry><title type="html">ğŸ“ˆÂ Gradient: Directional Derivative</title><link href="http://localhost:4000/calculus/gradient" rel="alternate" type="text/html" title="ğŸ“ˆÂ Gradient: Directional Derivative" /><published>2023-07-31T00:00:00+09:00</published><updated>2023-07-31T23:00:00+09:00</updated><id>http://localhost:4000/calculus/gradient</id><content type="html" xml:base="http://localhost:4000/calculus/gradient"><![CDATA[<h3 id="concept-of-gradient"><code class="language-plaintext highlighter-rouge">ğŸ¤”Â Concept of Gradient</code></h3>

<p>ê·¸ë¼ë””ì–¸íŠ¸ëŠ” ë‹¤ë³€ìˆ˜ í•¨ìˆ˜ì˜ ê¸°ìš¸ê¸°ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ë²¡í„°ë¥¼ ë§í•œë‹¤. ê·¸ë¼ë””ì–¸íŠ¸ì˜ ì›ì†ŒëŠ” í•¨ìˆ˜ì— ì¡´ì¬í•˜ëŠ” ëª¨ë“  ë³€ìˆ˜ë¥¼ ëŒ€ìƒìœ¼ë¡œ í¸ë¯¸ë¶„í•œ ê²°ê³¼ë¡œ êµ¬ì„±ë˜ëŠ”ë°, ì˜ˆë¥¼ ë“¤ì–´ ë³€ìˆ˜ê°€ $x_1, x_2$ 2ê°œì¸ ë‹¤ë³€ìˆ˜ í•¨ìˆ˜ $f(x_1, x_2)$ê°€ ìˆë‹¤ê³  ê°€ì •í•´ë³´ì. ë‹¤ë³€ìˆ˜ í•¨ìˆ˜ $f$ì˜ ê·¸ë¼ë””ì–¸íŠ¸ëŠ” ì•„ë˜ ìˆ˜ì‹ì²˜ëŸ¼ í‘œí˜„í•  ìˆ˜ ìˆë‹¤.</p>

\[f'(x_1, x_2) = \begin{vmatrix}
  \frac{âˆ‚f}{âˆ‚x_1} \\
  \frac{âˆ‚f}{âˆ‚x_2}
\end{vmatrix}\]

<p>ì´ëŸ¬í•œ ê·¸ë¼ë””ì–¸íŠ¸ëŠ” ë¨¸ì‹  ëŸ¬ë‹, ìˆ˜ì¹˜ ìµœì í™” í•™ë¬¸ì—ì„œ ë§¤ìš° ì¤‘ìš”í•œ ê°œë…ìœ¼ë¡œ ê¼½íŒë‹¤. ê·¸ë¼ë””ì–¸íŠ¸ ë²¡í„°ê°€ ê°€ë¦¬í‚¤ëŠ” ë°©í–¥ì´ ë°”ë¡œ ë‹¤ë³€ìˆ˜ í•¨ìˆ˜ê°€ íŠ¹ì • ì§€ì ì—ì„œ ê°€ì¥ ê°€íŒŒë¥´ê²Œ ì¦ê°€í•˜ëŠ” ë°©í–¥ì„ ê°€ë¦¬í‚¤ê¸° ë•Œë¬¸ì´ë‹¤. ì´ì²˜ëŸ¼ ê·¸ë¼ë””ì–¸íŠ¸ëŠ” í•¨ìˆ˜ì˜ ì…ë ¥ ê³µê°„ì„ ë”°ë¼ í•¨ìˆ˜ê°€ ì–´ë–»ê²Œ ë³€í™”í•˜ëŠ”ì§€ë¥¼ ì•Œë ¤ì£¼ëŠ” ê¸¸ì¡ì´ ì—­í• ì„ í•˜ê¸° ë•Œë¬¸ì—, ê·¸ë¼ë””ì–¸íŠ¸ ë°©í–¥ì„ ë”°ë¼ ë³€ìˆ˜ê°’ì„ íŠœë‹í•˜ë‹¤ ë³´ë©´ í•¨ìˆ˜ì˜ ìµœëŒ€ê°’â€¢ìµœì†Œê°’ì— ë„ë‹¬í•˜ì—¬ ìµœì í™” ë¬¸ì œë¥¼ í•´ê²°í•  ìˆ˜ ìˆê²Œ ëœë‹¤. ê·¸ë ‡ë‹¤ë©´ ì™œ ê·¸ë¼ë””ì–¸íŠ¸ ë²¡í„°ì˜ ë°©í–¥ì´ íŠ¹ì • ì§€ì ì—ì„œ í•¨ìˆ˜ê°€ ê°€ì¥ ê°€íŒŒë¥´ê²Œ ì¦ê°€í•˜ëŠ” ë°©í–¥ì„ ë‚˜íƒ€ë‚´ëŠ” ê²ƒì¼ê¹Œ?? í¸ë¯¸ë¶„, ë„í•¨ìˆ˜ ì •ì˜ ê·¸ë¦¬ê³  ë‚´ì ì„ í™œìš©í•´ ì¦ëª…í•  ìˆ˜ ìˆë‹¤.</p>

<h3 id="-proof-of-gradient"><code class="language-plaintext highlighter-rouge">ğŸªª Proof of Gradient</code></h3>

<p align="center">
<img src="/assets/images/gradient/gradient.jpg" alt="Example of multivariate function" class="align-center image-caption" width="60%&quot;, height=&quot;25%" />
<strong><em><a href="https://m.blog.naver.com/PostView.naver?isHttpsRedirect=true&amp;blogId=galaxyenergy&amp;logNo=221431325545">Example of multivariate function</a></em></strong>
</p>

<p>ê·¸ë¼ë””ì–¸íŠ¸ ë²¡í„°ì˜ ë°©í–¥ì´ í•¨ìˆ˜ê°€ ê°€ì¥ ê°€íŒŒë¥´ê²Œ ì¦ê°€í•˜ëŠ” ë°©í–¥ê³¼ ì¼ì¹˜í•œë‹¤ëŠ” ëª…ì œë¥¼ ì¦ëª…í•˜ê¸° ìœ„í•´ ìµœë‹¨ ê²½ë¡œë¡œ ì‚° ì •ìƒì— ì˜¤ë¥´ëŠ” ê³¼ì •ì„ ë– ì˜¬ë ¤ë³´ë ¤ í•œë‹¤. ìš°ë¦¬ëŠ” í˜„ì¬ ì´ë³€ìˆ˜ í•¨ìˆ˜ë¡œ ì •ì˜ë˜ëŠ” ì‚° ì¤‘í„± ì–´ë”˜ê°€, ì  $(x_1^0, x_2^0)$ë¥¼ ì§€ë‚˜ê³  ìˆë‹¤. ì‚° ì •ìƒì„ ìµœë‹¨ ê²½ë¡œë¡œ ì˜¤ë¥´ë ¤ë©´ ì–´ë–»ê²Œ í•´ì•¼í• ê¹Œ?? ê°€ì¥ ê²½ì‚¬ê°€ ê°€íŒŒë¥¸ ê¸‰ê²½ì‚¬ ì§€ëŒ€ë¥¼ í–¥í•´ ë‚˜ì•„ê°€ë©´ ë  ê²ƒì´ë‹¤. í•˜ì§€ë§Œ ì‚° ì¤‘í„±ì— ìˆëŠ” ìš°ë¦¬ê°€ ì–´ëŠ ë°©í–¥ì´ ê°€ì¥ ê°€íŒŒë¥¸ ê¸‰ê²½ì‚¬ ì§€ëŒ€ì¸ì§€ ì§ê´€ì ìœ¼ë¡œ ì•Œ ê¸¸ì´ ì—†ë‹¤. ê·¸ë˜ì„œ ë°©í–¥ ë„í•¨ìˆ˜ë¥¼ ë„ì…í•´ ê¸‰ê²½ì‚¬ ì§€ëŒ€ë¡œ í–¥í•  ìˆ˜ ìˆëŠ” ë°©í–¥ì„ êµ¬í•´ ë³´ê¸°ë¡œ í–ˆë‹¤. ì•„ë˜ ìˆ˜ì‹ì„ ë³´ì.</p>

\[\lim_{\Delta{x}-&gt;0}\frac{f(x+\Delta{x}) - f(x)}{\Delta{x}} =    \frac{df}{dx}= f'(x) \\
df = f'(x)dx\]

<p>ë„ˆë¬´ë‚˜ë„ ìµìˆ™í•œ í˜•íƒœ ì•„ë‹Œê°€?? ìš°ë¦¬ê°€ ì¼ë°˜ì ìœ¼ë¡œ ì•Œê³  ìˆëŠ” ì¼ë³€ìˆ˜ í•¨ìˆ˜ì˜ ë¯¸ë¶„ ì •ì˜ ê·¸ë¦¬ê³  ì¢Œë³€ì˜ $dx$ë¥¼ ìš°ë³€ìœ¼ë¡œ ë„˜ê²¨ ì‚´ì§ ë³€í˜•í•œ ì‹ì´ë‹¤. ì´ê²ƒì„ ì´ì œ ë‹¤ë³€ìˆ˜ í•¨ìˆ˜ì— ì ìš©í•˜ë©´ ë°”ë¡œ ë°©í–¥ ë„í•¨ìˆ˜ê°€ ëœë‹¤. ë‹¤ì‹œ ìš°ë¦¬ê°€ ì˜¤ë¥´ë ¤ëŠ” ì‚°(ì´ë³€ìˆ˜ í•¨ìˆ˜)ìœ¼ë¡œ ëŒì•„ì™€ ë³´ì.</p>

\[f(x_1 + dx_1, x_2) = f(x_1, x_2) + f'(x_1, x_2)dx_1 \\
f(x_1, x_2 + dx_2) = f(x_1, x_2) + f'(x_1, x_2)dx_2 \\\]

<p>ìœ„ì—ì„œ ì„œìˆ í•œ ë„í•¨ìˆ˜ ì •ì˜ë¥¼ í™œìš©í•´ ìš°ë¦¬ê°€ ë‹¤ìŒì— ë°œê±¸ìŒì„ ì˜®ê¸¸ ìœ„ì¹˜ë¥¼ ì   $A$ë¥¼ $(x_1^0 + dx_1, x_2^0+dx_2)$ ì´ë¼ê³  í‘œí˜„í•  ìˆ˜ ìˆë‹¤. ì´ í‘œí˜„ì„ í™œìš©í•´ ë‹¤ë³€ìˆ˜ í•¨ìˆ˜ì˜ ë¯¸ë¶„ì„ ì •ì˜í•´ë³´ì. ìš°ë¦¬ëŠ” ì´ë¯¸ ë‹¤ë³€ìˆ˜ í•¨ìˆ˜ì˜ ê°œë³„ ë³€ìˆ˜ì— í¸ë¯¸ë¶„ì„ ì·¨í•˜ê³  í–‰ë²¡í„°ë¡œ ìŒ“ì€ ê²°ê³¼ê°€ ë°”ë¡œ ì „ë¯¸ë¶„ì´ë¼ëŠ” ê²ƒì„ ì•Œê³  ìˆë‹¤.</p>

\[f(x_1 + dx_1, x_2 + dx_2) - f(x_1, x_2) = f'(x_1)dx_1 + f'(x_2)dx_2\]

<p>ë‹¤ì‹œ í¸ë¯¸ë¶„ì˜ ì •ì˜ë¥¼ í™œìš©í•´ ìˆ˜ì‹ì„ ì •ë¦¬í•˜ë©´ ë°©í–¥ ë²¡í„°ì™€ í¸ë¯¸ë¶„ ê²°ê³¼ì˜ ë‚´ì ìœ¼ë¡œ í‘œí˜„í•  ìˆ˜ ìˆë‹¤.</p>

\[dL = \frac{âˆ‚L}{âˆ‚{x_1}}dx_1 + \frac{âˆ‚L}{âˆ‚{x_2}}dx_2 \\
dL = [dx_1, dx_2]\ â€¢\ \begin{vmatrix}
  \frac{âˆ‚L}{âˆ‚x_1} \\
  \frac{âˆ‚L}{âˆ‚x_2}
\end{vmatrix}\]

<p>ìŸì•„ì§€ëŠ” ìˆ˜ì‹ ì†ì— ìš°ë¦¬ì˜ ë³¸ë˜ ëª©ì ì„ ìŠì–´ì„œëŠ” ì•ˆëœë‹¤. ìš°ë¦¬ëŠ” ì§€ê¸ˆ ê°€ì¥ ë¹ ë¥´ê²Œ ì‚° ì •ìƒì— ë„ë‹¬í•  ìˆ˜ ìˆëŠ” ë°©ë²•ì„ ì°¾ê¸° ìœ„í•´ ì§€ê¸ˆê¹Œì§€ ë‹¬ë ¤ì™”ë‹¤. ì‚° ì •ìƒì— ê°€ì¥ ë¹ ë¥´ê²Œ ë„ë‹¬í•˜ê¸° ìœ„í•´ ê°€ì¥ ê°€íŒŒë¥¸ ê¸‰ê²½ì‚¬ ì§€ëŒ€ë§Œ ì°¾ì•„ì„œ ì˜¬ë¼ê°€ëŠ” ì „ëµì„ ì„¸ì› ì—ˆë‹¤. ë‹¤ì‹œ ë§í•´, ë‹¤ë³€ìˆ˜ í•¨ìˆ˜ $f(x)$ì˜ ê·¹ì†Œ ë³€í™”ëŸ‰ $dL$ì´ ìµœëŒ€ê°€ ë˜ëŠ” ë°©í–¥ìœ¼ë¡œ ë°œê±¸ìŒì„ ì˜®ê¸°ë©´ ëœë‹¤ëŠ” ê²ƒì´ë‹¤. ê·¸ë ‡ë‹¤ë©´ ê·¹ì†Œ ë³€í™”ëŸ‰ $dL$ì€ ì–¸ì œ ìµœëŒ€ê°€ ë ê¹Œ??</p>

<p>ì´ì œ ê¹Œë¨¹ê³  ìˆì—ˆë˜ ë‚´ì ì˜ ê°œë…ì„ ë‹¤ì‹œ í•œ ë²ˆ ìƒê¸°ì‹œì¼œë³´ì. ë‚´ì ì€ ë‹¤ì–‘í•˜ê²Œ í•´ì„ë˜ì§€ë§Œ, ë³¸ë”” ì„œë¡œ ë‹¤ë¥¸ ë‘ ë²¡í„°ì˜ <code class="language-plaintext highlighter-rouge">ë‹®ì€ ì •ë„</code>ë¥¼ ë‚˜íƒ€ë‚¸ë‹¤. ê·¹ì†Œ ë³€í™”ëŸ‰ $dL$ì´ ìµœëŒ€ê°€ ë˜ë ¤ë©´ ìš°ë³€ì˜ ë‚´ì  ê²°ê³¼ê°€ ìµœëŒ€ê°€ ë˜ì–´ì•¼ í•œë‹¤. ë‚´ì ì˜ ìµœëŒ€ê°’ì€ ì„œë¡œ ë‹¤ë¥¸ ë‘ ë²¡í„° ì‚¬ì´ì˜ ë¼ì¸ê°ë„ê°€ 0Ëšì¼ ë•Œ ì¦‰, ë‘ ë²¡í„°ê°€ ë™ì¼í•œ ë°©í–¥ì„ ë‚˜íƒ€ë‚¼ ë•Œ ì •ì˜ëœë‹¤. <strong><u>ë”°ë¼ì„œ ë°©í–¥ ë²¡í„°ê°€ ê·¸ë¼ë””ì–¸íŠ¸(í¸ë¯¸ë¶„ì˜ í–‰ë²¡í„°) ë°©í–¥ì¼ ë•Œ</u></strong> <code class="language-plaintext highlighter-rouge">ë‚´ì  ê²°ê³¼</code>(ê·¹ì†Œ ë³€í™”ëŸ‰ $dL$)<strong><u>ê°€ ìµœëŒ€ê°€ ëœë‹¤.</u></strong></p>

<p><strong><u>í•œí¸, ì‹¤ì œ ê¸°ê³„í•™ìŠµì—ì„œëŠ” ì†ì‹¤í•¨ìˆ˜ì˜ ìµœì í™”ë¥¼ ëª©ì  í•¨ìˆ˜ë¡œ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì— ê·¸ë¼ë””ì–¸íŠ¸(ì†ì‹¤í•¨ìˆ˜ì˜ ì „ë¯¸ë¶„) ë°©í–¥ì— ìŒìˆ˜ë¥¼ ì·¨í•´ì¤€ ê°’ì„ ì‚¬ìš©í•˜ê²Œ ëœë‹¤.</u></strong></p>]]></content><author><name>qcqced</name><email>qcqced123@gmail.com</email></author><category term="Calculus" /><category term="Calculus" /><category term="Partial Derivative" /><category term="Total Derivative" /><category term="loss function" /><category term="Gradient" /><category term="Gradient Descent" /><category term="Machine Learning" /><summary type="html"><![CDATA[Proof of gradient direction with Total Derivative]]></summary></entry><entry><title type="html">ğŸŒ†Â [ViT] An Image Is Worth 16x16 Words: Transformers For Image Recognition At Scale</title><link href="http://localhost:4000/cv/vit" rel="alternate" type="text/html" title="ğŸŒ†Â [ViT] An Image Is Worth 16x16 Words: Transformers For Image Recognition At Scale" /><published>2023-07-26T00:00:00+09:00</published><updated>2023-07-27T02:00:00+09:00</updated><id>http://localhost:4000/cv/ViT</id><content type="html" xml:base="http://localhost:4000/cv/vit"><![CDATA[<h3 id="overview"><code class="language-plaintext highlighter-rouge">ğŸ”­Â Overview</code></h3>

<p>ì‹œì‘í•˜ê¸° ì•ì„œ, ë³¸ ë…¼ë¬¸ ë¦¬ë·°ë¥¼ ìˆ˜ì›”í•˜ê²Œ ì½ìœ¼ë ¤ë©´ <code class="language-plaintext highlighter-rouge">Transformer</code> ì— ëŒ€í•œ ì„ ì´í•´ê°€ í•„ìˆ˜ì ì´ë‹¤. ì•„ì§ <code class="language-plaintext highlighter-rouge">Transformer</code> ì— ëŒ€í•´ì„œ ì˜ ëª¨ë¥¸ë‹¤ë©´ í•„ìê°€ ì‘ì„±í•œ í¬ìŠ¤íŠ¸ë¥¼ ì½ê³  ì˜¤ê¸¸ ê¶Œì¥í•œë‹¤. ë˜í•œ ë³¸ë¬¸ ë‚´ìš©ì„ ì‘ì„±í•˜ë©´ì„œ ì°¸ê³ í•œ ë…¼ë¬¸ê³¼ ì—¬ëŸ¬ í¬ìŠ¤íŠ¸ì˜ ë§í¬ë¥¼ ë§¨ ë°‘ í•˜ë‹¨ì— ì²¨ë¶€í–ˆìœ¼ë‹ˆ ì°¸ê³  ë°”ë€ë‹¤. ì‹œê°„ì´ ì—†ìœ¼ì‹  ë¶„ë“¤ì€ ì¤‘ê°„ì˜ ì½”ë“œ êµ¬í˜„ë¶€ë¥¼ ìƒëµí•˜ê³  <code class="language-plaintext highlighter-rouge">Insight</code> ë¶€í„° ì½ê¸°ë¥¼ ê¶Œì¥í•œë‹¤.</p>

<p><code class="language-plaintext highlighter-rouge">Vision Transformer</code>(ì´í•˜ <code class="language-plaintext highlighter-rouge">ViT</code>)ëŠ” 2020ë…„ 10ì›” Googleì—ì„œ ë°œí‘œí•œ ì»´í“¨í„° ë¹„ì „ìš© ëª¨ë¸ì´ë‹¤. ìì—°ì–´ ì²˜ë¦¬ì—ì„œ ëŒ€ì„±ê³µì„ ê±°ë‘” íŠ¸ë ŒìŠ¤í¬ë¨¸ êµ¬ì¡°ì™€ ê¸°ë²•ì„ ê±°ì˜ ê·¸ëŒ€ë¡œ ë¹„ì „ ë¶„ì•¼ì— ì´ì‹í–ˆë‹¤ëŠ” ì ì—ì„œ í° ì˜ì˜ê°€ ìˆìœ¼ë©°, ì´í›„ ì»´í“¨í„° ë¹„ì „ ë¶„ì•¼ì˜ íŠ¸ë ŒìŠ¤í¬ë¨¸ ì „ì„±ì‹œëŒ€ê°€ ì—´ë¦¬ê²Œ ëœ ê³„ê¸°ë¡œ ì‘ìš©í•œë‹¤.</p>

<p>í•œí¸, <code class="language-plaintext highlighter-rouge">ViT</code> ì˜ ì„¤ê³„ ì² í•™ì€ ë°”ë¡œ <code class="language-plaintext highlighter-rouge">scalability(ë²”ìš©ì„±)</code>ì´ë‹¤. ì‹ ê²½ë§ ì„¤ê³„ì—ì„œ ë²”ìš©ì„±ì´ë€, ëª¨ë¸ì˜ í™•ì¥ ê°€ëŠ¥ì„±ì„ ë§í•œë‹¤. ì˜ˆë¥¼ ë“¤ë©´ í•™ìŠµ ë°ì´í„°ë³´ë‹¤ ë” í¬ê³  ë³µì¡í•œ ë°ì´í„° ì„¸íŠ¸ë¥¼ ì‚¬ìš©í•˜ê±°ë‚˜ ëª¨ë¸ì˜ íŒŒë¼ë¯¸í„°ë¥¼ ëŠ˜ë ¤ ì‚¬ì´ì¦ˆë¥¼ í‚¤ì›Œë„ ì—¬ì „íˆ ìœ íš¨í•œ ì¶”ë¡  ê²°ê³¼ë¥¼ ë„ì¶œí•˜ê±°ë‚˜ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ê³  ë‚˜ì•„ê°€ ê°œì„ ì˜ ì—¬ì§€ê°€ ì—¬ì „íˆ ë‚¨ì•„ìˆì„ ë•Œ <code class="language-plaintext highlighter-rouge">â€œí™•ì¥ì„±ì´ ë†’ë‹¤â€</code> ë¼ê³  í‘œí˜„í•œë‹¤. ì €ìë“¤ì€ ë…¼ë¬¸ ì´ˆë°˜ì— ì½• ì°ì–´ì„œ ì»´í“¨í„° ë¹„ì „ ë¶„ì•¼ì˜ <code class="language-plaintext highlighter-rouge">scalability</code> ë†’ì´ëŠ” ê²ƒì´ ì´ë²ˆ ëª¨ë¸ ì„¤ê³„ì˜ ëª©í‘œì˜€ë‹¤ê³  ë°íˆê³  ìˆë‹¤. <code class="language-plaintext highlighter-rouge">ë²”ìš©ì„±</code>ì€ ì‹ ê²½ë§ ëª¨ë¸ ì„¤ê³„ì—ì„œ ê°€ì¥ í° í™”ë‘ê°€ ë˜ëŠ”ë° ë„ë©”ì¸ë§ˆë‹¤ ì •ì˜í•˜ëŠ” ì˜ë¯¸ì— ì°¨ì´ê°€ ë¯¸ì„¸í•˜ê²Œ ì¡´ì¬í•œë‹¤. ë”°ë¼ì„œ  <code class="language-plaintext highlighter-rouge">ViT</code>ì˜ ì €ìë“¤ì´ ë§í•˜ëŠ” <code class="language-plaintext highlighter-rouge">ë²”ìš©ì„±</code>ì´ ë¬´ì—‡ì„ ì˜ë¯¸í•˜ëŠ”ì§€ ì•Œì•„ë³´ëŠ” ê²ƒì€ êµ¬ì²´ì ì¸ ëª¨ë¸ êµ¬ì¡°ë¥¼ ì´í•´í•˜ëŠ”ë° í° ë„ì›€ì´ ë  ê²ƒì´ë‹¤.</p>

<h3 id="scalability-in-vit"><code class="language-plaintext highlighter-rouge">ğŸ§ Â Scalability in ViT</code></h3>

<p>ë…¼ë¬¸ ì´ˆë°˜ë¶€ì—ì„œ ë‹¤ìŒê³¼ ê°™ì€ ë¬¸ì¥ì´ ì„œìˆ  ë˜ì–´ìˆë‹¤.</p>

<p><code class="language-plaintext highlighter-rouge">â€œOur Vision Transformer (ViT) attains excellent results when pre-trained at sufficient scale and transferred to tasks with fewer datapoints"</code></p>

<p>ì´ êµ¬ë¬¸ì´ <code class="language-plaintext highlighter-rouge">ViT</code> ì˜ <code class="language-plaintext highlighter-rouge">Scalability</code>ë¥¼ ê°€ì¥ ì˜ ì„¤ëª…í•˜ê³  ìˆë‹¤ê³  ìƒê°í•œë‹¤. ì €ìë“¤ì´ ë§í•˜ëŠ” ë²”ìš©ì„±ì€ ê²°êµ­ <code class="language-plaintext highlighter-rouge">backbone</code> êµ¬ì¡°ì˜ í™œìš©ì„ ì˜ë¯¸í•œë‹¤. ìì—°ì–´ ì²˜ë¦¬ì— ìµìˆ™í•œ ë…ìë¼ë©´ ì‰½ê²Œ ì´í•´ê°€ ê°€ëŠ¥í•  ê²ƒì´ë‹¤. <code class="language-plaintext highlighter-rouge">Transformer</code>, <code class="language-plaintext highlighter-rouge">GPT</code>, <code class="language-plaintext highlighter-rouge">BERT</code>ì˜ ë“±ì¥ ì´í›„, ìì—°ì–´ ì²˜ë¦¬ëŠ” ë²”ìš©ì„±ì„ ê°–ëŠ” ë°ì´í„° ì„¸íŠ¸ë¡œ ì‚¬ì „ í›ˆë ¨í•œ ëª¨ë¸ì„ í™œìš©í•´ <code class="language-plaintext highlighter-rouge">Task-Agnostic</code>í•˜ê²Œ í•˜ë‚˜ì˜ <code class="language-plaintext highlighter-rouge">backbone</code>ìœ¼ë¡œ ê±°ì˜ ëª¨ë“  Taskë¥¼ ìˆ˜í–‰í•  ìˆ˜ ìˆìœ¼ë©°, ì‘ì€ ì‚¬ì´ì¦ˆì˜ ë°ì´í„°ë¼ë„ ìƒë‹¹íˆ ë†’ì€ ìˆ˜ì¤€ì˜ ì¶”ë¡  ì„±ëŠ¥ì„ ë‚¼ ìˆ˜ ìˆì—ˆë‹¤. ê·¸ëŸ¬ë‚˜ ë‹¹ì‹œ ì»´í“¨í„° ë¹„ì „ì˜ ë©”ì¸ì´ì—ˆë˜ <code class="language-plaintext highlighter-rouge">Conv</code> ê¸°ë°˜ ëª¨ë¸ë“¤ì€ íŒŒì¸íŠœë‹í•´ë„ ë°ì´í„° í¬ê¸°ê°€ ì‘ìœ¼ë©´ ì¼ë°˜í™” ì„±ëŠ¥ì´ ë§¤ìš° ë–¨ì–´ì§€ê³ , Taskì— ë”°ë¼ì„œ ë‹¤ë¥¸ ì•„í‚¤í…ì²˜ë¥¼ ê°–ëŠ” ëª¨ë¸ì„ ìƒˆë¡­ê²Œ ì •ì˜í•˜ê±°ë‚˜ ë¶ˆëŸ¬ì™€ ì‚¬ìš©í•´ì•¼ í•˜ëŠ” ë²ˆê±°ë¡œì›€ì´ ìˆì—ˆë‹¤. ì˜ˆë¥¼ ë“¤ë©´ <code class="language-plaintext highlighter-rouge">Image Classfication</code> ì—ëŠ” <code class="language-plaintext highlighter-rouge">ResNet</code>, <code class="language-plaintext highlighter-rouge">Segmentation</code> ì—ëŠ” <code class="language-plaintext highlighter-rouge">U-Net</code>, <code class="language-plaintext highlighter-rouge">Object Detection</code> ì€ <code class="language-plaintext highlighter-rouge">YOLO</code> ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì²˜ëŸ¼ ë§ì´ë‹¤. ë°˜ë©´ ìì—°ì–´ ì²˜ë¦¬ëŠ” ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ í•˜ë‚˜ë¡œ ëª¨ë“  NLU, ì‹¬ì§€ì–´ëŠ” NLG Taskë„ ìˆ˜í–‰í•  ìˆ˜ ìˆë‹¤. ì €ìë“¤ì€ ì´ëŸ¬í•œ ë²”ìš©ì„±ì„ ì»´í“¨í„° ë¹„ì „ì—ë„ ì´ì‹ ì‹œí‚¤ê³  ì‹¶ì—ˆë˜ ê²ƒ ê°™ë‹¤. ê·¸ë ‡ë‹¤ë©´ ë¨¼ì € ìì—°ì–´ ì²˜ë¦¬ì—ì„œ íŠ¸ëœìŠ¤í¬ë¨¸ ê³„ì—´ì´ ë²”ìš©ì„±ì„ ê°€ì§ˆ ìˆ˜ ìˆì—ˆë˜ ì´ìœ ëŠ” ë¬´ì—‡ì¸ì§€ ê°„ë‹¨íˆ ì‚´í´ë³´ì.</p>

<p>ì €ìë“¤ì€ <code class="language-plaintext highlighter-rouge">self-attention</code>(ë‚´ì )ì˜ íš¨ìœ¨ì„±, ëª¨ë¸ì˜ êµ¬ì¡°ì  íƒì›”ì„± ê·¸ë¦¬ê³  <code class="language-plaintext highlighter-rouge">self-supervised task</code>ì˜ ì¡´ì¬ë¥¼ ê¼½ëŠ”ë‹¤. ê·¸ëŸ¼ ì´ê²ƒë“¤ì´ ì™œ ë²”ìš©ì„±ì„ ë†’ì´ëŠ”ë° ë„ì›€ì´ ë ê¹Œ??</p>

<p><code class="language-plaintext highlighter-rouge">self-attention(ë‚´ì )</code>ì€ í–‰ë ¬ ê°„ ê³±ì…‰ìœ¼ë¡œ ì •ì˜ ë˜ì–´ ì„¤ê³„ê°€ ë§¤ìš° ê°„í¸í•˜ê³  ë³‘ë ¬ë¡œ í•œë²ˆì— ì²˜ë¦¬í•˜ëŠ” ê²ƒì´ ê°€ëŠ¥í•˜ê¸° ë•Œë¬¸ì— íš¨ìœ¨ì ìœ¼ë¡œ ì „ì²´ ë°ì´í„°ë¥¼ ëª¨ë‘ ê³ ë ¤í•œ ì—°ì‚° ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆë‹¤.</p>

<p><code class="language-plaintext highlighter-rouge">Multi-Head Attention</code> êµ¬ì¡°ëŠ” ì—¬ëŸ¬ ì°¨ì›ì˜ ì˜ë¯¸ ê´€ê³„ë¥¼ ë™ì‹œì— í¬ì°©í•˜ê³  ê·¸ê²ƒì„ ì•™ìƒë¸”í•œ ê²ƒê³¼ ê°™ì€(ì‹¤ì œë¡œëŠ” MLP) ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆë‹¤ëŠ” ì ì—ì„œ êµ¬ì¡°ì ìœ¼ë¡œ íƒì›”í•˜ë‹¤.</p>

<p>ë§ˆì§€ë§‰ìœ¼ë¡œ <code class="language-plaintext highlighter-rouge">MLM</code>, <code class="language-plaintext highlighter-rouge">Auto-Regression(LM) Task</code>ëŠ” ë°ì´í„° ì„¸íŠ¸ì— ë³„ë„ì˜ ì¸ê°„ì˜ ê°œì…(ë¼ë²¨ë§)ì´ í•„ìš”í•˜ì§€ ì•Šê¸° ë•Œë¬¸ì— ê°€ì„±ë¹„ ìˆê²Œ ë°ì´í„°ì™€ ëª¨ë¸ì˜ ì‚¬ì´ì¦ˆë¥¼ ëŠ˜ë¦´ ìˆ˜ ìˆê²Œ ëœë‹¤.<br />
ì´ì œ ë…¼ë¬¸ì—ì„œ íŠ¸ëœìŠ¤í¬ë¨¸ ê³„ì—´ì´ ê°€ì§„ ë²”ìš©ì„±ì„ ì–´ë–»ê²Œ ë¹„ì „ ë¶„ì•¼ì— ì ìš©í–ˆëŠ”ì§€ ì£¼ëª©í•˜ë©´ì„œ ëª¨ë¸ êµ¬ì¡°ë¥¼ í•˜ë‚˜ í•˜ë‚˜ ì‚´í´ë³´ì.</p>

<h3 id="modeling"><code class="language-plaintext highlighter-rouge">ğŸŒŸÂ Modeling</code></h3>

<p align="center">
<img src="/assets/images/vision_transformer/modeling_overview.png" alt="ViT Model Structure" class="align-center image-caption" width="100%&quot;, height=&quot;50%" />
<strong><em><a href="https://arxiv.org/abs/2010.11929">ViT Model Structure</a></em></strong>
</p>

<ul>
  <li><strong>1) Transfer <code class="language-plaintext highlighter-rouge">Scalability</code> from pure <code class="language-plaintext highlighter-rouge">Transformer</code> to Computer Vision</strong>
    <ul>
      <li><strong>Overcome <code class="language-plaintext highlighter-rouge">reliance</code> on Convolution(<code class="language-plaintext highlighter-rouge">Inductive Bias</code>) in Computer Vision</strong></li>
      <li><strong>Apply Self-Attention &amp; Architecture from vanilla NLP Transformers as <code class="language-plaintext highlighter-rouge">closely</code> as possible</strong></li>
      <li><strong>Treat Image as sequence of text token</strong></li>
      <li><strong>Make $P$ sub-patches from whole image, playing same role as token in NLP Transformer</strong></li>
    </ul>
  </li>
</ul>

<p>ì €ìë“¤ì€ ë¨¼ì € <code class="language-plaintext highlighter-rouge">Conv</code> ì— ëŒ€í•œ ì˜ì¡´ì„ ë²„ë¦´ ê²ƒì„ ì£¼ì¥í•œë‹¤. <code class="language-plaintext highlighter-rouge">Conv</code>ê°€ ê°€ì§„ <code class="language-plaintext highlighter-rouge">Inductive Bias</code> ë•Œë¬¸ì— íŒŒì¸íŠœë‹ ë ˆë²¨ì—ì„œ ë°ì´í„° í¬ê¸°ê°€ ì‘ìœ¼ë©´ ì¼ë°˜í™” ì„±ëŠ¥ì´ ë–¨ì–´ì§€ëŠ” ê²ƒì´ë¼ê³  ì„¤ëª…í•˜ê³  ìˆë‹¤. ì´ ë§ì„ ì´í•´í•˜ë ¤ë©´ <code class="language-plaintext highlighter-rouge">Inductive Bias</code>ì— ëŒ€í•´ì„œ ë¨¼ì € ì•Œì•„ì•¼ í•œë‹¤. <code class="language-plaintext highlighter-rouge">Inductive Bias</code>ë€, ì£¼ì–´ì§„ ë°ì´í„°ë¡œë¶€í„° ì¼ë°˜í™” ì„±ëŠ¥ì„ ë†’ì´ê¸° ìœ„í•´ <code class="language-plaintext highlighter-rouge">â€˜ì…ë ¥ë˜ëŠ” ë°ì´í„°ëŠ” ~ í•  ê²ƒì´ë‹¤â€™</code>, <code class="language-plaintext highlighter-rouge">â€˜ì´ëŸ° íŠ¹ì§•ì„ ê°–ê³  ìˆì„ ê²ƒì´ë‹¤â€™</code>ì™€ ê°™ì€ ê°€ì •, ê°€ì¤‘ì¹˜, ê°€ì„¤ ë“±ì„ ê¸°ê³„í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ì— ì ìš©í•˜ëŠ” ê²ƒì„ ë§í•œë‹¤.</p>

<p><code class="language-plaintext highlighter-rouge">Conv</code> ì—°ì‚° ìì²´ (ê°€ì¤‘ì¹˜ ê³µìœ , í’€ë§ ìˆëŠ” <code class="language-plaintext highlighter-rouge">Conv Block</code>ì´ <code class="language-plaintext highlighter-rouge">Invariance</code>)ì˜ ê¸°ë³¸ ê°€ì •ì€ <code class="language-plaintext highlighter-rouge">translation equivariance</code>, <code class="language-plaintext highlighter-rouge">locality</code>ì´ë‹¤. ì‚¬ì‹¤ ì €ìì˜ ì£¼ì¥ì„ ì´í•´í•˜ëŠ”ë° <code class="language-plaintext highlighter-rouge">equivariance</code>ì™€ <code class="language-plaintext highlighter-rouge">locality</code>ì˜ ëœ»ì´ ë¬´ì—‡ì¸ì§€ íŒŒì•…í•˜ëŠ” ê²ƒì€ í¬ê²Œ ì˜ë¯¸ê°€ ì—†ë‹¤ (<code class="language-plaintext highlighter-rouge">equivariance</code>ì™€ <code class="language-plaintext highlighter-rouge">invariance</code>ì— ëŒ€í•´ì„œëŠ” ë‹¤ë¥¸ í¬ìŠ¤íŒ…ì—ì„œ ìì„¸íˆ ì‚´í´ë³´ë„ë¡ í•˜ê² ë‹¤). <strong><u>ì¤‘ìš”í•œ ê²ƒì€ ì…ë ¥ ë°ì´í„°ì— ê°€ì •ì„ ë”í•œë‹¤ëŠ” ì ì´ë‹¤.</u></strong> ë§Œì•½ ì£¼ì–´ì§„ ì…ë ¥ì´ ë¯¸ë¦¬ ê°€ì •í•œ <code class="language-plaintext highlighter-rouge">Inductive Bias</code> ì— ë²—ì–´ë‚œë‹¤ë©´ ì–´ë–»ê²Œ ë ê¹Œ??</p>

<p>ì•„ë§ˆ ì˜¤ë²„í”¼íŒ… ë˜ê±°ë‚˜ ëª¨ë¸ í•™ìŠµì´ ìˆ˜ë ´ì„±ì„ ê°–ì§€ ëª»í•˜ê²Œ ë  ê²ƒì´ë‹¤. ì´ë¯¸ì§€ ë°ì´í„°ë„ Taskì— ë”°ë¼ í•„ìš”í•œ <code class="language-plaintext highlighter-rouge">Inductive Bias</code>ê°€ ë‹¬ë¼ì§„ë‹¤. ì˜ˆë¥¼ ë“¤ì–´ <code class="language-plaintext highlighter-rouge">Segmentation</code>, <code class="language-plaintext highlighter-rouge">Detection</code> ì˜ ê²½ìš°ëŠ” ì´ë¯¸ì§€ ì† ê°ì²´ì˜ ìœ„ì¹˜, í”½ì…€ ì‚¬ì´ì˜ <code class="language-plaintext highlighter-rouge">spatial variance</code> ì •ë³´ê°€ ë§¤ìš° ì¤‘ìš”í•˜ë‹¤. í•œí¸, <code class="language-plaintext highlighter-rouge">Classification</code>ì€ <code class="language-plaintext highlighter-rouge">spatial invariance</code>ê°€ ì¤‘ìš”í•˜ë‹¤. ëª©í‘œ ê°ì²´ì˜ ìœ„ì¹˜ì™€ ì£¼ë³€ íŠ¹ì§•ë³´ë‹¤ íƒ€ê²Ÿ ìì²´ë¥¼ ì‹ ê²½ë§ì´ ì¸ì‹í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•˜ê¸° ë•Œë¬¸ì´ë‹¤. ë”°ë¼ì„œ  <code class="language-plaintext highlighter-rouge">ViT</code> ì €ìë“¤ì€ ì–´ë–¤ Biasë˜ ìƒê´€ì—†ì´ í¸í–¥ì„ ê°–ê³  ë°ì´í„°ë¥¼ ë³¸ë‹¤ëŠ” ê²ƒ ìì²´ì— ì˜ë¬¸ì„ í‘œí•˜ë©°, ì´ë¯¸ì§€ ì—­ì‹œ <code class="language-plaintext highlighter-rouge">Inductive Bias</code>ì—ì„œ ë²—ì–´ë‚˜, ì£¼ì–´ì§„ ë°ì´í„° ì „ì²´ íŠ¹ì§•(íŒ¨ì¹˜) ì‚¬ì´ì˜ ê´€ê³„ë¥¼ íŒŒì•…í•˜ëŠ” ê³¼ì •ì—ì„œ <code class="language-plaintext highlighter-rouge">scalability</code>ë¥¼ íšë“í•  ìˆ˜ ìˆë‹¤ê³  ì£¼ì¥í•œë‹¤.</p>

<p>ê·¸ë˜ì„œ <code class="language-plaintext highlighter-rouge">Conv</code>ì˜ ëŒ€ì•ˆìœ¼ë¡œ ìƒëŒ€ì ìœ¼ë¡œ <code class="language-plaintext highlighter-rouge">Inductive Bias</code> ê°€ ë¶€ì¡±í•œ <code class="language-plaintext highlighter-rouge">Self-Attention</code>, <code class="language-plaintext highlighter-rouge">Transformer Architecture</code>ë¥¼ ì‚¬ìš©í•œë‹¤. ë‘ê°€ì§€ì˜ íš¨ìš©ì„±ì— ëŒ€í•´ì„œëŠ” ì´ë¯¸ ìœ„ì—ì„œ ì–¸ê¸‰í–ˆê¸° ë•Œë¬¸ì— ìƒëµí•˜ê³ , ì—¬ê¸°ì„œ ì§šê³  ë„˜ì–´ê°€ì•¼í•  ì ì€ <code class="language-plaintext highlighter-rouge">Self-Attention</code>ì´ <code class="language-plaintext highlighter-rouge">Conv</code> ëŒ€ë¹„ <code class="language-plaintext highlighter-rouge">Inductive Bias</code>ê°€ ì ë‹¤ëŠ” ì ì´ë‹¤. Self-Attention ê³¼ì •ì—ëŠ” ì—¬ëŸ¬ ì—°ì‚°, ìŠ¤ì¼€ì¼ ì¡°ì •ê°’ë“¤ì´ í¬í•¨ë˜ì§€ë§Œ ë³¸ì§ˆì ìœ¼ë¡œ <code class="language-plaintext highlighter-rouge">â€œë‚´ì â€</code> ì´ ì¤‘ì‹¬ì´ë‹¤. ë‚´ì ì€ ê·¸ ì–´ë–¤ í¸í–¥ (<code class="language-plaintext highlighter-rouge">Conv</code>ì™€ ëŒ€ì¡°í•˜ë ¤ê³  ì´ë ‡ê²Œ ì„œìˆ í–ˆì§€ë§Œ ì‚¬ì‹¤ <code class="language-plaintext highlighter-rouge">Position Embedding</code> ë”í•˜ëŠ” ê²ƒë„ ì¼ì¢…ì˜ ì•½í•œ <code class="language-plaintext highlighter-rouge">Inductive Bias</code>)ì´ ì¡´ì¬í•˜ì§€ ì•ŠëŠ”ë‹¤. ì¼ë‹¨ ì£¼ì–´ì§„ ëª¨ë“  ë°ì´í„°ì— ëŒ€í•´ì„œ ë‚´ì ê°’ì„ ì‚°ì¶œí•˜ê³  ê·¸ ë‹¤ìŒì— ê´€ê³„ê°€ ìˆë‹¤ê³  ìƒê°ë˜ëŠ” ì •ë³´ë¥¼ ì¶”ë¦¬ê¸° ë•Œë¬¸ì´ë‹¤. <code class="language-plaintext highlighter-rouge">Conv</code> ë•Œì™€ ë‹¬ë¦¬ <code class="language-plaintext highlighter-rouge">â€˜ì…ë ¥ë˜ëŠ” ë°ì´í„°ëŠ” ~ í•  ê²ƒì´ë‹¤â€™</code>, <code class="language-plaintext highlighter-rouge">â€˜ì´ëŸ° íŠ¹ì§•ì„ ê°–ê³  ìˆì„ ê²ƒì´ë‹¤â€™</code> ë¼ëŠ” ê°€ì •ì´ ì—†ë‹¤. ì´ë²ˆ í¬ìŠ¤íŒ…ì˜ ë§ˆì§€ë§‰ ì¯¤ì—ì„œ ë‹¤ì‹œ ë‹¤ë£¨ê² ì§€ë§Œ ê·¸ë˜ì„œ <code class="language-plaintext highlighter-rouge">ViT</code>ëŠ” ì¸ìŠ¤í„´ìŠ¤ ì‚¬ì´ì˜ ëª¨ë“  ê´€ê³„ë¥¼ ë½‘ì•„ë³´ëŠ” <code class="language-plaintext highlighter-rouge">Self-Attention(ë‚´ì )</code> ì„ ê¸°ë°˜ìœ¼ë¡œ ë§Œë“¤ì–´ì¡Œê¸° ë•Œë¬¸ì— ì´ë¯¸ì§€ì˜ <code class="language-plaintext highlighter-rouge">Global Information</code>ì„ í¬ì°©í•˜ëŠ”ë° íƒì›”í•œ ì„±ëŠ¥ì„ ë³´ì´ê³ , <code class="language-plaintext highlighter-rouge">Conv</code> ëŠ” <strong><u>â€œì¤‘ìš”í•œ ì •ë³´ëŠ” ê·¼ì²˜ í”½ì…€ì— ëª°ë ¤ìˆë‹¤ë¼ëŠ”â€</u></strong> <code class="language-plaintext highlighter-rouge">Inductive Bias</code>  ë•ë¶„ì— <code class="language-plaintext highlighter-rouge">Local Information</code>ì„ í¬ì°©í•˜ëŠ”ë° íƒì›”í•œ ì„±ëŠ¥ì„ ë‚¸ë‹¤.</p>

<p>ê·¸ë ‡ë‹¤ë©´ í”½ì…€ í•˜ë‚˜ í•˜ë‚˜ë¼ë¦¬ ë‚´ì í•´ì¤€ë‹¤ëŠ” ê²ƒì¼ê¹Œ?? ì•„ë‹ˆë‹¤ ì—¬ê¸°ì„œ ë…¼ë¬¸ì˜ ì œëª©ì´ <code class="language-plaintext highlighter-rouge">An Image Is Worth 16x16 Words</code> ì¸ ì´ìœ ê°€ ë“œëŸ¬ë‚œë‹¤. ì¼ë‹¨ í”½ì…€ í•˜ë‚˜ í•˜ë‚˜ë¼ë¦¬ ìœ ì‚¬ë„ë¥¼ ì¸¡ì •í•˜ëŠ” ê²ƒì´ ìœ ì˜ë¯¸í• ê¹Œ ìƒê°í•´ë³´ì. ìì—°ì–´ì˜ í† í°ê³¼ ë‹¬ë¦¬ ì´ë¯¸ì§€ì˜ ë‹¨ì¼ í”½ì…€ í•œ ê°œëŠ” í° ì¸ì‚¬ì´íŠ¸ë¥¼ ì–»ê¸° í˜ë“¤ë‹¤. í”½ì…€ì€ ë§ ê·¸ëŒ€ë¡œ ì  í•˜ë‚˜ì¼ ë¿ì´ë‹¤. í”½ì…€ì„ ì—¬ëŸ¬ ê°œ ë¬¶ì–´ íŒ¨ì¹˜ ë‹¨ìœ„ë¡œ ë¬¶ëŠ”ë‹¤ë©´ ì´ì•¼ê¸°ëŠ” ë‹¬ë¼ì§„ë‹¤. ì¼ì • í¬ê¸° ì´ìƒì˜ íŒ¨ì¹˜ë¼ë©´ ìì—°ì–´ì˜ í† í°ì²˜ëŸ¼ ê·¸ ìì²´ë¡œ ì–´ë–¤ ì˜ë¯¸ë¥¼ ë‹´ì„ ìˆ˜ ìˆë‹¤. ë”°ë¼ì„œ ì €ìëŠ” ì „ì²´ ì´ë¯¸ì§€ë¥¼ ì—¬ëŸ¬ ê°œì˜ 16x16 í˜¹ì€ 14x14 ì‚¬ì´ì¦ˆ íŒ¨ì¹˜ë¡œ ë‚˜ëˆ„ì–´ í•˜ë‚˜ í•˜ë‚˜ë¥¼ í† í°ìœ¼ë¡œ ê°„ì£¼í•´ ì´ë¯¸ì§€ ì‹œí€€ìŠ¤ë¥¼ ë§Œë“¤ê³  ê·¸ê²ƒì„ ëª¨ë¸ì˜ Inputìœ¼ë¡œ ì‚¬ìš©í•œë‹¤.</p>

<p align="center">
<img src="/assets/images/vision_transformer/class_diagram.png" alt="Class Diagram" class="align-center image-caption" width="50%&quot;, height=&quot;50%" />
<strong><em>Class Diagram</em></strong>
</p>

<p>ëª¨ë¸ êµ¬ì¡°ì˜ ë¼ˆëŒ€ê°€ ë˜ëŠ” ë‚´ìš©ë“¤ì„ ëª¨ë‘ ì‚´í´ë³´ì•˜ê³ , ìœ„ì—ì„œ ì„œìˆ í•œ ë‚´ìš©ì„ êµ¬í˜„í•˜ê¸° ìœ„í•´ ì–´ë–¤ ë¸”ë¡ë“¤ì„ ì‚¬ìš©í–ˆëŠ”ì§€ í•„ìê°€ ì§ì ‘ ë…¼ë¬¸ì„ ë³´ê³  ë”°ë¼ êµ¬í˜„í•œ ì½”ë“œì™€ í•¨ê»˜ ì•Œì•„ë³´ë„ë¡ í•˜ì. ìœ„ì— ì²¨ë¶€í•œ ëª¨ë¸ ëª¨ì‹ë„ì— ë‚˜ì™€ ìˆëŠ” ë¸”ë¡ë“¤ í•˜ë‚˜ í•˜ë‚˜ ì‚´í´ë³¼ ì˜ˆì •ì´ë‹¤. ì—¬ë‹´ìœ¼ë¡œ Google Researchì˜ Official Repo ì—­ì‹œ í•¨ê»˜ ì°¸ê³ í–ˆëŠ”ë°, ì½”ë“œê°€ ëª¨ë‘ êµ¬ê¸€ì´ ìš”ìƒˆ ìƒˆë¡­ê²Œ ë¯¸ëŠ” <code class="language-plaintext highlighter-rouge">Jax</code>, <code class="language-plaintext highlighter-rouge">Flax</code> ë¡œ êµ¬í˜„ ë˜ì–´ ìˆì—ˆë‹¤. íŒŒì´í† ì¹˜ë‚˜ ì¢€ ì¨ë³¸ í•„ì ì…ì¥ì—ì„œëŠ” ì •ë§ â€¦ ì§€ì˜¥ë¶ˆì„ ê²½í—˜í–ˆë‹¤. ì˜¤ëŠ˜ë„ ë‹¤ì‹œ í•œ ë²ˆ í˜ì´ìŠ¤ë¶ íŒŒì´í† ì¹˜ ê°œë°œíŒ€ì— í°ì ˆ ë“œë¦¬ê³  ì‹¶ë‹¤.</p>

<h4 id="linear-projection-of-flattened-patches"><code class="language-plaintext highlighter-rouge">ğŸ”¬Â Linear Projection of Flattened Patches</code></h4>

\[x_p \in R^{N * (P^2â€¢C)}\]

\[z_{0} = [x_{class}; x_p^1E;x_p^2E;x_p^3E....x_p^NE]\]

\[N = \frac{H*W}{P*P}\]

<p><code class="language-plaintext highlighter-rouge">ViT</code>ì˜ ì…ë ¥ ì„ë² ë”©ì„ ìƒì„±í•˜ëŠ” ì—­í• ì„ í•œë‹¤. <code class="language-plaintext highlighter-rouge">ViT</code>ëŠ” $x \in R^{H * W * C}$(H: height, W: width, C: channel)ì˜ í˜•ìƒì„ ê°–ëŠ” ì´ë¯¸ì§€ë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ì•„ ê°€ë¡œ ì„¸ë¡œ ê¸¸ì´ê°€ $P$, ì±„ë„ ê°œìˆ˜ $C$ì¸ $N$ê°œì˜ íŒ¨ì¹˜ë¡œ <code class="language-plaintext highlighter-rouge">reshape</code> í•œë‹¤. í•„ìê°€ ì½”ë“œ êµ¬í˜„ ì¤‘ ê°€ì¥ í˜¼ë™í•œ ë¶€ë¶„ì´ ë°”ë¡œ íŒ¨ì¹˜ ê°œìˆ˜ $N$ì´ì—ˆë‹¤. ì§ê´€ì ìœ¼ë¡œ íŒ¨ì¹˜ ê°œìˆ˜ë¼ê³  í•˜ë©´, ì „ì²´ ì´ë¯¸ì§€ ì‚¬ì´ì¦ˆì—ì„œ íŒ¨ì¹˜ í¬ê¸°ë¥¼ ë‚˜ëˆˆ ê°’ì´ë¼ê³  ìƒê°í•˜ê¸° ì‰½ê¸° ë•Œë¬¸ì´ë‹¤. ì˜ˆë¥¼ ë“¤ë©´ <code class="language-plaintext highlighter-rouge">512x512</code>ì§œë¦¬ ì´ë¯¸ì§€ë¥¼ <code class="language-plaintext highlighter-rouge">16x16</code> ì‚¬ì´ì¦ˆì˜ íŒ¨ì¹˜ë¡œ ë‚˜ëˆˆë‹¤ê³  í•´ë³´ì. í•„ìëŠ” ë‹¨ìˆœíˆ <code class="language-plaintext highlighter-rouge">512/16=32</code> ë¼ëŠ” ê²°ê³¼ë¥¼ ì´ìš©í•´ $N=32$ë¡œ ì„¤ì •í•˜ê³  ì‹¤í—˜ì„ ì§„í–‰í•˜ë‹¤ê°€ í…ì„œ ì°¨ì›ì´ ë§ì§€ ì•Šì•„ ë°œìƒí•˜ëŠ” ì—ëŸ¬ ë¡œê·¸ë¥¼ ë§ˆì£¼í–ˆì—ˆë‹¤. ê·¸ëŸ¬ë‚˜ ë…¼ë¬¸ ì† ìˆ˜ì‹ì„ í™•ì¸í•´ë³´ë©´,  $H * W / P^2$ì´ ë°”ë¡œ íŒ¨ì¹˜ ê°œìˆ˜$N$ìœ¼ë¡œ ì •ì˜ëœë‹¤. ê·¸ë˜ì„œ ë§Œì•½ <code class="language-plaintext highlighter-rouge">512x512</code> ì‚¬ì´ì¦ˆì˜ <code class="language-plaintext highlighter-rouge">RGB</code> ì´ë¯¸ì§€ <code class="language-plaintext highlighter-rouge">10ì¥</code>ì„ ViT ì…ë ¥ ì„ë² ë”©ì— ë§ê²Œ ì°¨ì› ë³€í™˜í•œë‹¤ë©´ ê²°ê³¼ëŠ” <code class="language-plaintext highlighter-rouge">[10, 3, 1024, 768]</code> ì´ ë  ê²ƒì´ë‹¤. (ì´ ì˜ˆì‹œë¥¼ ì•ìœ¼ë¡œ ê³„ì† ì´ìš©í•˜ê² ë‹¤)</p>

<p>ì´ë ‡ê²Œ ì°¨ì›ì„ ë°”ê¿”ì¤€ ì´ë¯¸ì§€ë¥¼ <code class="language-plaintext highlighter-rouge">nn.Linear((channels * patch_size**2), dim_model)</code> ë¥¼ í†µí•´ <code class="language-plaintext highlighter-rouge">ViT</code>ì˜ ì„ë² ë”© ë ˆì´ì–´ì— ì„ í˜• íˆ¬ì˜í•´ì¤€ë‹¤. ì—¬ê¸°ì„œ ìì—°ì–´ ì²˜ë¦¬ì™€ íŒŒì´í† ì¹˜ë¥¼ ìì£¼ ì‚¬ìš©í•˜ì‹œëŠ” ë…ìë¼ë©´ ì™œ <code class="language-plaintext highlighter-rouge">nn.Embedding</code>ì„ ì‚¬ìš©í•˜ì§€ ì•Šì•˜ëŠ”ê°€ ì˜ë¬¸ì„ ê°€ì§ˆ ìˆ˜ ìˆë‹¤.</p>

<p>ìì—°ì–´ ì²˜ë¦¬ì—ì„œ ì…ë ¥ ì„ë² ë”©ì„ ë§Œë“¤ë•ŒëŠ” ëª¨ë¸ì˜ í† í¬ë‚˜ì´ì €ì— ì˜í•´ ì‚¬ì „ ì •ì˜ëœ vocabì˜ ì‚¬ì´ì¦ˆê°€ ì…ë ¥ ë¬¸ì¥ì— ì†í•œ í† í° ê°œìˆ˜ë³´ë‹¤ í›¨ì”¬ í¬ê¸° ë•Œë¬¸ì— ë°ì´í„° ë£©ì—… í…Œì´ë¸” ë°©ì‹ì˜ <code class="language-plaintext highlighter-rouge">nn.Embedding</code> ì„ ì‚¬ìš©í•˜ê²Œ ëœë‹¤. ì´ê²Œ ë¬´ìŠ¨ ë§ì´ëƒë©´, í† í¬ë‚˜ì´ì €ì— ì˜í•´ ì‚¬ì „ì— ì •ì˜ëœ <code class="language-plaintext highlighter-rouge">vocab</code> ì „ì²´ê°€ <code class="language-plaintext highlighter-rouge">nn.Embedding(vocab_size, dim_model)</code>ë¡œ íˆ¬ì˜ ë˜ì–´ ê°€ë¡œëŠ” vocab ì‚¬ì´ì¦ˆ, ì„¸ë¡œëŠ” ëª¨ë¸ì˜ ì°¨ì› í¬ê¸°ì— í•´ë‹¹í•˜ëŠ” ë£©ì—… í…Œì´ë¸”ì´ ìƒì„±ë˜ê³ , ë‚´ê°€ ì…ë ¥í•œ í† í°ë“¤ì€ ì „ì²´ <code class="language-plaintext highlighter-rouge">vocab</code>ì˜ ì¼ë¶€ë¶„ì¼í…Œë‹ˆ ì „ì²´ ì„ë² ë”© ë£©ì—… í…Œì´ë¸”ì—ì„œ ë‚´ê°€ ì„ë² ë”©í•˜ê³  ì‹¶ì€ í† í°ë“¤ì˜ ì¸ë±ìŠ¤ë§Œ ì•Œì•„ë‚¸ë‹¤ëŠ” ê²ƒì´ë‹¤.</p>

<p>ê·¸ë˜ì„œ <code class="language-plaintext highlighter-rouge">nn.Embedding</code> ì— ì •ì˜ëœ ì°¨ì›ê³¼ ì‹¤ì œ ì…ë ¥ ë°ì´í„°ì˜ ì°¨ì›ì´ ë§ì§€ ì•Šì•„ë„ í•¨ìˆ˜ê°€ ë™ì‘í•˜ê²Œ ë˜ëŠ” ê²ƒì´ë‹¤. ê·¸ëŸ¬ë‚˜ ë¹„ì „ì˜ ê²½ìš°, ì‚¬ì „ì— ì •ì˜ëœ <code class="language-plaintext highlighter-rouge">vocab</code>ì´ë¼ëŠ” ê°œë…ì´ ì „í˜€ ì—†ê³  ì…ë ¥ ì´ë¯¸ì§€ ì—­ì‹œ í•­ìƒ ê³ ì •ëœ í¬ê¸°ì˜ ì°¨ì›ìœ¼ë¡œ ë“¤ì–´ì˜¤ê¸° ë•Œë¬¸ì— <code class="language-plaintext highlighter-rouge">nn.Embedding</code>ì´ ì•„ë‹Œ  <code class="language-plaintext highlighter-rouge">nn.Linear</code> ì„ ì‚¬ìš©í•´ ê³§ë°”ë¡œ ì„ í˜• íˆ¬ì˜ì„ êµ¬í˜„í•œ ê²ƒì´ë‹¤. ë‘ ë©”ì„œë“œì— ëŒ€í•œ ìì„¸í•œ ë¹„êµëŠ” íŒŒì´í† ì¹˜ ê´€ë ¨ í¬ìŠ¤íŠ¸ì—ì„œ ë‹¤ì‹œ í•œ ë²ˆ ìì„¸íˆ ë‹¤ë£¨ë„ë¡ í•˜ê² ë‹¤.</p>

<p>í•œí¸, <code class="language-plaintext highlighter-rouge">Position Embedding</code>ì„ ë”í•˜ê¸° ì „, <code class="language-plaintext highlighter-rouge">Input Embedding</code>ì˜ ì°¨ì›ì€ <code class="language-plaintext highlighter-rouge">[10, 1024, 1024]</code> ì´ ëœë‹¤. ì§€ê¸ˆê¹Œì§€ ì„¤ëª…í•œ ë¶€ë¶„(<code class="language-plaintext highlighter-rouge">Linear Projection of Flattened Patches</code> )ì„ íŒŒì´í† ì¹˜ ì½”ë“œë¡œ êµ¬í˜„í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">VisionTransformer</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="p">...</span>
    <span class="n">ì¤‘ëµ</span>
    <span class="p">...</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">num_patches</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">image_size</span> <span class="o">/</span> <span class="n">patch_size</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">input_embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">((</span><span class="n">channels</span> <span class="o">*</span> <span class="n">patch_size</span><span class="o">**</span><span class="mi">2</span><span class="p">),</span> <span class="n">dim_model</span><span class="p">)</span> <span class="c1"># Projection Layer for Input Embedding
</span>    <span class="p">...</span>
    <span class="n">ì¤‘ëµ</span>
    <span class="p">...</span>  
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">any</span><span class="p">:</span>
        <span class="s">""" For cls pooling """</span>
        <span class="k">assert</span> <span class="n">inputs</span><span class="p">.</span><span class="n">ndim</span> <span class="o">!=</span> <span class="mi">4</span><span class="p">,</span> <span class="sa">f</span><span class="s">"Input shape should be [BS, CHANNEL, IMAGE_SIZE, IMAGE_SIZE], but got </span><span class="si">{</span><span class="n">inputs</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="s">"</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">inputs</span> 
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">input_embedding</span><span class="p">(</span>
            <span class="n">x</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="p">.</span><span class="n">num_patches</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">patch_size</span><span class="o">**</span><span class="mi">2</span> <span class="o">*</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span> <span class="c1"># Projection Layer for Input Embedding
</span>        <span class="p">)</span>
        <span class="n">cls_token</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">,</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>  <span class="c1"># can change init method
</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cat</span><span class="p">([</span><span class="n">cls_token</span><span class="p">,</span> <span class="n">x</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="p">...</span>
</code></pre></div></div>

<p>ì„ë² ë”© ë ˆì´ì–´ë¥¼ ê°ì²´ë¡œ ë”°ë¡œ êµ¬í˜„í•´ë„ ë˜ì§€ë§Œ, í•„ìëŠ” êµ³ì´ ì¶”ìƒí™”ê°€ í•„ìš”í•˜ì§€ ì•Šë‹¤ê³  ìƒê°í•´ ViTì˜ ìµœìƒìœ„ í´ë˜ìŠ¤ì¸ <code class="language-plaintext highlighter-rouge">VisionTransformer</code>ì˜ <code class="language-plaintext highlighter-rouge">forward</code> ë©”ì„œë“œ ë§¨ ì´ˆë°˜ë¶€ì— êµ¬í˜„í•˜ê²Œ ë˜ì—ˆë‹¤. ì…ë ¥ ë°›ì€ ì´ë¯¸ì§€ í…ì„œë¥¼ <code class="language-plaintext highlighter-rouge">torch.reshape</code> ì„ í†µí•´ <code class="language-plaintext highlighter-rouge">[íŒ¨ì¹˜ ê°œìˆ˜, í”½ì…€ê°œìˆ˜*ì±„ë„ê°œìˆ˜]</code> ë¡œ ë°”ê¾¼ ë’¤, ë¯¸ë¦¬ ì •ì˜í•´ë‘” <code class="language-plaintext highlighter-rouge">self.input_embedding</code> ì— ë§¤ê°œë³€ìˆ˜ë¡œ ì „ë‹¬í•´ <code class="language-plaintext highlighter-rouge">â€œìœ„ì¹˜ ì„ë² ë”©â€</code> ê°’ì´ ë”í•´ì§€ê¸° ì „ <code class="language-plaintext highlighter-rouge">Input Embedding</code>ì„ ë§Œë“ ë‹¤.</p>

<p>í•œí¸, <code class="language-plaintext highlighter-rouge">CLS Pooling</code>ì„ ìœ„í•´ ë§ˆì§€ë§‰ì— <code class="language-plaintext highlighter-rouge">[batch, 1, image_size]</code> ì˜ ì°¨ì›ì„ ê°–ëŠ” <code class="language-plaintext highlighter-rouge">cls_token</code> ì„ ì •ì˜í•´ íŒ¨ì¹˜ ì‹œí€€ìŠ¤ì™€ <code class="language-plaintext highlighter-rouge">concat</code> (ë§¨ ì•ì—)í•´ì¤€ë‹¤. ì´ ë•Œ ë…¼ë¬¸ì— ì œì‹œëœ ìˆ˜ì‹ ìƒ, <code class="language-plaintext highlighter-rouge">CLS Token</code>ì€ ì„ í˜• íˆ¬ì˜í•˜ì§€ ì•Šìœ¼ë©°, íŒ¨ì¹˜ ì‹œí€€ìŠ¤ì— ì„ í˜• íˆ¬ì˜ì´ ì´ë¤„ì§€ê³  ë‚œ ë’¤ì— ë§¨ ì•ì— <code class="language-plaintext highlighter-rouge">Concat</code> í•˜ê²Œ ëœë‹¤.</p>

<p><code class="language-plaintext highlighter-rouge">CLS Token</code>ê¹Œì§€ ë”í•œ ìµœì¢… <code class="language-plaintext highlighter-rouge">Input Embedding</code> ì˜ í…ì„œ ì°¨ì›ì€ <code class="language-plaintext highlighter-rouge">[10, 1025, 1024]</code> ê°€ ëœë‹¤.</p>

<h4 id="positional-embedding"><code class="language-plaintext highlighter-rouge">ğŸ”¢Â Positional Embedding</code></h4>

\[E_{pos} \in R^{(N+1)*D}\]

<p>ì´ë¯¸ì§€ë¥¼ íŒ¨ì¹˜ ë‹¨ìœ„ì˜ ì„ë² ë”©ìœ¼ë¡œ ë§Œë“¤ì—ˆë‹¤ë©´ ì´ì œ ìœ„ì¹˜ ì„ë² ë”©ì„ ì •ì˜í•´ì„œ ë”í•´ì£¼ë©´ ëª¨ì‹ë„ ì† <code class="language-plaintext highlighter-rouge">Embedded Patches</code> , ì¦‰ ì¸ì½”ë”ì— ë“¤ì–´ê°ˆ ìµœì¢… <code class="language-plaintext highlighter-rouge">Patch Embedding</code> ì´ ì™„ì„± ëœë‹¤. ìœ„ì¹˜ ì„ë² ë”©ì„ ë§Œë“œëŠ” ë°©ì‹ì€ ê¸°ì¡´ <code class="language-plaintext highlighter-rouge">Transformer</code>, <code class="language-plaintext highlighter-rouge">BERT</code> ì™€ ë™ì¼í•˜ë‹¤. ì•„ë˜ <code class="language-plaintext highlighter-rouge">VisionEncoder</code> í´ë˜ìŠ¤ë¥¼ êµ¬í˜„í•œ ì½”ë“œë¥¼ ì‚´í´ë³´ì.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="k">class</span> <span class="nc">VisionEncoder</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="p">...</span>
    <span class="n">ì¤‘ëµ</span>
    <span class="p">...</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">positional_embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Embedding</span><span class="p">((</span><span class="bp">self</span><span class="p">.</span><span class="n">num_patches</span> <span class="o">+</span> <span class="mi">1</span><span class="p">),</span> <span class="n">dim_model</span><span class="p">)</span>  <span class="c1"># add 1 for cls token
</span>    <span class="p">...</span>
    <span class="n">ì¤‘ëµ</span>
    <span class="p">...</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
        <span class="n">layer_output</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">pos_x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">num_patches</span> <span class="o">+</span> <span class="mi">1</span><span class="p">).</span><span class="n">repeat</span><span class="p">(</span><span class="n">inputs</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]).</span><span class="n">to</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>  <span class="c1"># inputs.shape[0] = Batch Size of Input
</span>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">dropout</span><span class="p">(</span>
            <span class="n">inputs</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">positional_embedding</span><span class="p">(</span><span class="n">pos_x</span><span class="p">)</span>
        <span class="p">)</span>
    <span class="p">...</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">Input Embedding</code>ê³¼ ë‹¤ë¥´ê²Œ ìœ„ì¹˜ ì„ë² ë”©ì€ <code class="language-plaintext highlighter-rouge">nn.Embedding</code>ìœ¼ë¡œ êµ¬í˜„í–ˆëŠ”ë°, ì—¬ê¸°ì„œë„ ì‚¬ì‹¤ <code class="language-plaintext highlighter-rouge">nn.Linear</code>ë¥¼ ì‚¬ìš©í•´ë„ ë¬´ë°©í•˜ë‹¤. ê·¸ê²ƒë³´ë‹¤ <code class="language-plaintext highlighter-rouge">nn.Embedding</code>ì˜ ì…ë ¥ ì°¨ì›ì¸ <code class="language-plaintext highlighter-rouge">self.num_patches + 1</code> ì— ì£¼ëª©í•´ë³´ì. ì™œ 1ì„ ë”í•´ì¤€ ê°’ì„ ì‚¬ìš©í–ˆì„ê¹Œ??</p>

<p><code class="language-plaintext highlighter-rouge">ViT</code>ëŠ” BERTì˜ <code class="language-plaintext highlighter-rouge">CLS Token Pooling</code> ì„ ì°¨ìš©í•˜ê¸° ìœ„í•´ íŒ¨ì¹˜ ì‹œí€€ìŠ¤ ë§¨ ì•ì— CLS í† í°ì„ ì¶”ê°€í•˜ê¸° ë•Œë¬¸ì´ë‹¤. ì´ë ‡ê²Œ ì¶”ê°€ëœ <code class="language-plaintext highlighter-rouge">CLS Token</code>ì€ ì¸ì½”ë”ë¥¼ ê±°ì³ ìµœì¢… <code class="language-plaintext highlighter-rouge">MLP Head</code>ì— í˜ëŸ¬ë“¤ì–´ê°€ ë¡œì§“ìœ¼ë¡œ ë³€í™˜ëœë‹¤. ë§Œì•½ ë…ìê»˜ì„œ <code class="language-plaintext highlighter-rouge">CLS Token Pooling</code> ëŒ€ì‹  ë‹¤ë¥¸ í’€ë§ ë°©ì‹ì„ ì‚¬ìš©í• ê±°ë¼ë©´ 1ì„ ì¶”ê°€í•´ì¤„ í•„ìš”ëŠ” ì—†ë‹¤.</p>

<p>ì• ì´ˆì— ê°ì²´ ì¸ìŠ¤í„´ìŠ¤ ì´ˆê¸°í™” ë‹¹ì‹œì— <code class="language-plaintext highlighter-rouge">CLS Token</code> ì„ ì¶”ê°€ë¥¼ ë°˜ì˜í•œ ê°’ì„ ì „ë‹¬í•˜ë©´ ë˜ì§€ ì•ŠëŠ”ê°€í•˜ëŠ” ì˜ë¬¸ì´ ë“¤ ìˆ˜ë„ ìˆë‹¤. í•˜ì§€ë§Œ <code class="language-plaintext highlighter-rouge">VisionEncoder</code> ê°ì²´ ì¸ìŠ¤í„´ìŠ¤ ì´ˆê¸°í™” ë‹¹ì‹œì—ëŠ” <code class="language-plaintext highlighter-rouge">num_patches</code> ê°’ìœ¼ë¡œ <code class="language-plaintext highlighter-rouge">CLS Token</code>ì´ ì¶”ê°€ë˜ê¸° ì´ì „ ê°’(+1 ë°˜ì˜ì´ ì•ˆë˜ì–´ ìˆìŒ)ì„ ì „ë‹¬í•˜ë„ë¡ ì„¤ê³„ ë˜ì–´ ìˆì–´ì„œ  <code class="language-plaintext highlighter-rouge">CLS Pooling</code>ì„ ì‚¬ìš©í• ê±°ë¼ë©´ 1 ì¶”ê°€ë¥¼ ê¼­ í•´ì¤˜ì•¼ í•œë‹¤.</p>

<p align="center">
<img src="/assets/images/vision_transformer/insight5.png" alt="Performance Table by making Position Embedding method" class="align-center image-caption" width="75%&quot;, height=&quot;50%" />
<strong><em><a href="https://arxiv.org/abs/2010.11929">Performance Table by making Position Embedding method</a></em></strong>
</p>

<p>í•œí¸ ì €ìëŠ” <code class="language-plaintext highlighter-rouge">2D Postion Embedding</code>, <code class="language-plaintext highlighter-rouge">Relative Position Embedding</code> ë°©ì‹ë„ ì ìš©í•´ë´¤ì§€ë§Œ, êµ¬í˜„ ë³µì¡ë„ &amp; ì—°ì‚°ëŸ‰ ëŒ€ë¹„ ì„±ëŠ¥ í–¥ìƒ í­ì´ ë§¤ìš° ë¯¸ë¯¸í•´ ì¼ë°˜ì ì¸ <code class="language-plaintext highlighter-rouge">1D Position Embedding</code>ì„ ì‚¬ìš©í•  ê²ƒì„ ì¶”ì²œí•˜ê³  ìˆë‹¤.</p>

<h4 id="-multi-head-attention"><code class="language-plaintext highlighter-rouge">ğŸ‘©â€ğŸ‘©â€ğŸ‘§â€ğŸ‘¦ Multi-Head Attention</code></h4>

\[z_t^{'} = MSA(LN(z_{t-1}) + z_{t-1})\]

\[MSA(z) = [SA_1();SA_2();SA_3()...SA_k()]*U_{msa}, \ \ U_{msa} \in R^{(k*D_h)*D} \\\]

<p>íŠ¸ëœìŠ¤í¬ë¨¸ ê³„ì—´ ëª¨ë¸ì˜ í•µì‹¬ <code class="language-plaintext highlighter-rouge">Multi-Head Self-Attention</code> ëª¨ë“ˆì— ëŒ€í•´ì„œ ì•Œì•„ë³´ì. ì‚¬ì‹¤ ê¸°ì¡´ ìì—°ì–´ ì²˜ë¦¬ <code class="language-plaintext highlighter-rouge">Transformer</code>, <code class="language-plaintext highlighter-rouge">BERT</code> ë“±ì˜ ë™ì‘ ë°©ì‹ê³¼ ì™„ì „íˆ ë™ì¼í•˜ë©°, ì½”ë“œë¡œ êµ¬í˜„í•  ë•Œ ì—­ì‹œ ë™ì¼í•˜ê²Œ ë§Œë“¤ì–´ì£¼ë©´ ëœë‹¤. ìì„¸í•œ ì›ë¦¬ì™€ ë™ì‘ ë°©ì‹ì€ <strong><u>Attention Is All You Need</u></strong> ë¦¬ë·° í¬ìŠ¤íŠ¸ì—ì„œ ì„¤ëª…í–ˆê¸° ë•Œë¬¸ì— ìƒëµí•˜ê³  ë„˜ì–´ê°€ê² ë‹¤. í•œí¸ íŒŒì´í† ì¹˜ë¡œ êµ¬í˜„í•œ <code class="language-plaintext highlighter-rouge">Multi-Head Self-Attention</code> ë¸”ëŸ­ì— ëŒ€í•œ ì½”ë“œëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">scaled_dot_product_attention</span><span class="p">(</span><span class="n">q</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">k</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">v</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">dot_scale</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
    <span class="s">"""
    Scaled Dot-Product Attention
    Args:
        q: query matrix, shape (batch_size, seq_len, dim_head)
        k: key matrix, shape (batch_size, seq_len, dim_head)
        v: value matrix, shape (batch_size, seq_len, dim_head)
        dot_scale: scale factor for Qâ€¢K^T result, same as pure transformer
    Math:
        A = softmax(qâ€¢k^t/sqrt(D_h)), SA(z) = Av
    """</span>
    <span class="n">attention_dist</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="n">softmax</span><span class="p">(</span>
        <span class="n">torch</span><span class="p">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">))</span> <span class="o">/</span> <span class="n">dot_scale</span><span class="p">,</span>
        <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span>
    <span class="p">)</span>
    <span class="n">attention_matrix</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">attention_dist</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">attention_matrix</span>

<span class="k">class</span> <span class="nc">AttentionHead</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="s">"""
    In this class, we implement workflow of single attention head
    Args:
        dim_model: dimension of model's latent vector space, default 1024 from official paper
        dim_head: dimension of each attention head, default 64 from official paper (1024 / 16)
        dropout: dropout rate, default 0.1
    Math:
        [q,k,v]=zâ€¢U_qkv, A = softmax(qâ€¢k^t/sqrt(D_h)), SA(z) = Av
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim_model</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span>  <span class="mi">1024</span><span class="p">,</span> <span class="n">dim_head</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">64</span><span class="p">,</span> <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">AttentionHead</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span> <span class="o">=</span> <span class="n">dim_model</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dim_head</span> <span class="o">=</span> <span class="n">dim_head</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">dropout</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dot_scale</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">dim_head</span><span class="p">))</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">fc_q</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">dim_head</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">fc_k</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">dim_head</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">fc_v</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">dim_head</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="n">attention_matrix</span> <span class="o">=</span> <span class="n">scaled_dot_product_attention</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">fc_q</span><span class="p">(</span><span class="n">x</span><span class="p">),</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">fc_k</span><span class="p">(</span><span class="n">x</span><span class="p">),</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">fc_v</span><span class="p">(</span><span class="n">x</span><span class="p">),</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">dot_scale</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">attention_matrix</span>

<span class="k">class</span> <span class="nc">MultiHeadAttention</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="s">"""
    In this class, we implement workflow of Multi-Head Self-Attention
    Args:
        dim_model: dimension of model's latent vector space, default 1024 from official paper
        num_heads: number of heads in MHSA, default 16 from official paper for ViT-Large
        dim_head: dimension of each attention head, default 64 from official paper (1024 / 16)
        dropout: dropout rate, default 0.1
    Math:
        MSA(z) = [SA1(z); SA2(z); Â· Â· Â· ; SAk(z)]â€¢Umsa
    Reference:
        https://arxiv.org/abs/2010.11929
        https://arxiv.org/abs/1706.03762
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim_model</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1024</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span> <span class="n">dim_head</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">64</span><span class="p">,</span> <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MultiHeadAttention</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span> <span class="o">=</span> <span class="n">dim_model</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dim_head</span> <span class="o">=</span> <span class="n">dim_head</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">dropout</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">attention_heads</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">ModuleList</span><span class="p">(</span>
            <span class="p">[</span><span class="n">AttentionHead</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">dim_head</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">dropout</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">num_heads</span><span class="p">)]</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">fc_concat</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="s">""" x is already passed nn.Layernorm """</span>
        <span class="k">assert</span> <span class="n">x</span><span class="p">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">3</span><span class="p">,</span> <span class="sa">f</span><span class="s">'Expected (batch, seq, hidden) got </span><span class="si">{</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="s">'</span>
        <span class="n">attention_output</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">fc_concat</span><span class="p">(</span>
            <span class="n">torch</span><span class="p">.</span><span class="n">cat</span><span class="p">([</span><span class="n">head</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">head</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">attention_heads</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># concat all dim_head = num_heads * dim_head
</span>        <span class="p">)</span>
        <span class="k">return</span> <span class="n">attention_output</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">MultiHeadAttention</code>ì„ ê°€ì¥ ìµœìƒìœ„ ê°ì²´ë¡œ ë‘ê³ , í•˜ìœ„ì— <code class="language-plaintext highlighter-rouge">AttentionHead</code>ê°ì²´ë¥¼ ë”°ë¡œ êµ¬í˜„í–ˆë‹¤. ì´ë ‡ê²Œ êµ¬í˜„í•˜ë©´, ì–´í…ì…˜ í•´ë“œë³„ë¡œ ì¿¼ë¦¬, í‚¤, ë²¨ë¥˜ ì„ ì˜ íˆ¬ì˜ í–‰ë ¬(<code class="language-plaintext highlighter-rouge">nn.Linear</code>)ì„ ë”°ë¡œ êµ¬í˜„í•´ì¤„ í•„ìš”ê°€ ì—†ì–´ì§€ë©°, <code class="language-plaintext highlighter-rouge">nn.ModuleList</code> ë¥¼ í†µí•´ ê°œë³„ í•´ë“œë¥¼ í•œ ë²ˆì— ê·¸ë£¹í•‘í•˜ê³  <code class="language-plaintext highlighter-rouge">loop</code> ë¥¼ í†µí•´ ì¶œë ¥ ê²°ê³¼ë¥¼ <code class="language-plaintext highlighter-rouge">concat</code> í•´ì¤„ ìˆ˜ ìˆì–´ ë³µì¡í•˜ê³  ë§ì€ ì—ëŸ¬ë¥¼ ìœ ë°œí•˜ëŠ” <strong><u>í…ì„œ ì°¨ì› ì¡°ì‘ì„ í”¼í•  ìˆ˜ ìˆìœ¼ë©°</u></strong>, ì½”ë“œì˜ ê°€ë…ì„±ì´ ì˜¬ë¼ê°€ëŠ” íš¨ê³¼ê°€ ìˆë‹¤.</p>

<h4 id="ï¸-mlp"><code class="language-plaintext highlighter-rouge">ğŸ—³ï¸ MLP</code></h4>

\[z_{t} = MLP(LN(z_{t}^{'}) + z_{t}^{'})\]

<p>ì´ë¦„ë§Œ <code class="language-plaintext highlighter-rouge">MLP</code>ë¡œ ë°”ë€Œì—ˆì„ ë¿, ê¸°ì¡´ íŠ¸ëœìŠ¤í¬ë¨¸ì˜ í”¼ë“œ í¬ì›Œë“œ ë¸”ëŸ­ê³¼ ë™ì¼í•œ ì—­í• ì„ í•œë‹¤. ì—­ì‹œ ìì„¸í•œ ë™ì‘ ë°©ì‹ì€ ì—¬ê¸° í¬ìŠ¤íŠ¸ì—ì„œ í™•ì¸í•˜ì. íŒŒì´í† ì¹˜ë¡œ êµ¬í˜„í•œ ì½”ë“œëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">MLP</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="s">"""
    Class for MLP module in ViT-Large
    Args:
        dim_model: dimension of model's latent vector space, default 512
        dim_mlp: dimension of FFN's hidden layer, default 2048 from official paper
        dropout: dropout rate, default 0.1
    Math:
        MLP(x) = MLP(LN(x))+x
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim_model</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1024</span><span class="p">,</span> <span class="n">dim_mlp</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">4096</span><span class="p">,</span> <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MLP</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">mlp</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">dim_model</span><span class="p">,</span> <span class="n">dim_mlp</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">GELU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">dropout</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">dim_mlp</span><span class="p">,</span> <span class="n">dim_model</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">dropout</span><span class="p">),</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">mlp</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</code></pre></div></div>

<p>íŠ¹ì´í•œ ì ì€ <code class="language-plaintext highlighter-rouge">Activation Function</code>ìœ¼ë¡œ <code class="language-plaintext highlighter-rouge">GELU</code>ë¥¼ ì‚¬ìš©(ê¸°ì¡´ íŠ¸ëœìŠ¤í¬ë¨¸ëŠ” <code class="language-plaintext highlighter-rouge">RELU</code>)í–ˆë‹¤ëŠ” ì ì´ë‹¤.</p>

<h4 id="-vision-encoder-layer"><code class="language-plaintext highlighter-rouge">ğŸ“˜ Vision Encoder Layer</code></h4>

<p><code class="language-plaintext highlighter-rouge">ViT</code> ì¸ì½”ë” ë¸”ëŸ­ 1ê°œì— í•´ë‹¹í•˜ëŠ” í•˜ìœ„ ëª¨ë“ˆê³¼ ë™ì‘ì„ êµ¬í˜„í•œ ê°ì²´ì´ë‹¤. êµ¬í˜„í•œ ì½”ë“œëŠ” ì•„ë˜ì™€ ê°™ë‹¤.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">VisionEncoderLayer</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="s">"""
    Class for encoder_model module in ViT-Large
    In this class, we stack each encoder_model module (Multi-Head Attention, Residual-Connection, Layer Normalization, MLP)
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim_model</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1024</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">16</span><span class="p">,</span> <span class="n">dim_mlp</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">4096</span><span class="p">,</span> <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">VisionEncoderLayer</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">self_attention</span> <span class="o">=</span> <span class="n">MultiHeadAttention</span><span class="p">(</span>
            <span class="n">dim_model</span><span class="p">,</span>
            <span class="n">num_heads</span><span class="p">,</span>
            <span class="nb">int</span><span class="p">(</span><span class="n">dim_model</span> <span class="o">/</span> <span class="n">num_heads</span><span class="p">),</span>
            <span class="n">dropout</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">layer_norm1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">dim_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">layer_norm2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">dim_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">dropout</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">mlp</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">(</span>
            <span class="n">dim_model</span><span class="p">,</span>
            <span class="n">dim_mlp</span><span class="p">,</span>
            <span class="n">dropout</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="n">ln_x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">layer_norm1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">residual_x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">dropout</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">self_attention</span><span class="p">(</span><span class="n">ln_x</span><span class="p">))</span> <span class="o">+</span> <span class="n">x</span>

        <span class="n">ln_x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">layer_norm2</span><span class="p">(</span><span class="n">residual_x</span><span class="p">)</span>
        <span class="n">fx</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">mlp</span><span class="p">(</span><span class="n">ln_x</span><span class="p">)</span> <span class="o">+</span> <span class="n">residual_x</span>  <span class="c1"># from official paper &amp; code by Google Research
</span>        <span class="k">return</span> <span class="n">fx</span>
</code></pre></div></div>

<p><del>íŠ¹ì´ì ì€ ë§ˆì§€ë§‰ <code class="language-plaintext highlighter-rouge">MLP Layer</code>ì™€ <code class="language-plaintext highlighter-rouge">Residual</code> ê²°ê³¼ë¥¼ ë”í•œ ë’¤, ë‹¤ìŒ ì¸ì½”ë” ë¸”ë¡ì— ì „ë‹¬í•˜ê¸° ì „ì— ì¸µ ì •ê·œí™”ë¥¼ í•œ ë²ˆ ë” ì ìš©í•œë‹¤ëŠ” ê²ƒì´ë‹¤. ëª¨ë¸ ëª¨ì‹ë„ì—ëŠ” ë‚˜ì™€ ìˆì§€ ì•Šì§€ë§Œ, ë³¸ë¬¸ì— í•´ë‹¹ ë‚´ìš©ì´ ì‹¤ë ¤ ìˆë‹¤.</del>
ë§ˆì§€ë§‰ ì¸ì½”ë”ì˜ ì¶œë ¥ê°’ì—ë§Œ í•œë²ˆ ë” <code class="language-plaintext highlighter-rouge">layernorm</code>ì„ ì ìš©í•œë‹¤.</p>

<h4 id="-visionencoder"><code class="language-plaintext highlighter-rouge">ğŸ“š VisionEncoder</code></h4>

<p>ì…ë ¥ ì´ë¯¸ì§€ë¥¼ <code class="language-plaintext highlighter-rouge">Patch Embedding</code>ìœ¼ë¡œ ì¸ì½”ë”© í•˜ê³  Nê°œì˜ <code class="language-plaintext highlighter-rouge">VisionEncoderLayer</code>ë¥¼ ìŒ“ê¸° ìœ„í•´ êµ¬í˜„ëœ ê°ì²´ì´ë‹¤. <code class="language-plaintext highlighter-rouge">Patch Embedding</code>ì„ ë§Œë“œëŠ” ë¶€ë¶„ì€ ì´ë¯¸ ìœ„ì—ì„œ ì„¤ëª…í–ˆê¸° ë•Œë¬¸ì— ë„˜ì–´ê°€ê³ , ì¸ì½”ë” ë¸”ëŸ­ì„ Nê°œ ìŒ“ëŠ” ë°©ë²•ì€ ì—­ì‹œë‚˜ <code class="language-plaintext highlighter-rouge">nn.ModuleList</code> ë¥¼ ì‚¬ìš©í•˜ë©´ ê°„í¸í•˜ê²Œ êµ¬í˜„í•  ìˆ˜ ìˆë‹¤. ì•„ë˜ ì½”ë“œë¥¼ ì‚´í´ë³´ì.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">VisionEncoder</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="s">"""
    In this class, encode input sequence(Image) and then we stack N VisionEncoderLayer
    This model is implemented by cls pooling method for classification
    First, we define "positional embedding" and then add to input embedding for making patch embedding
    Second, forward patch embedding to N EncoderLayer and then get output embedding
    Args:
        num_patches: number of patches in input image =&gt; (image_size / patch_size)**2
        N: number of EncoderLayer, default 24 for large model
    """</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_patches</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">N</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">24</span><span class="p">,</span> <span class="n">dim_model</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1024</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">16</span><span class="p">,</span> <span class="n">dim_mlp</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">4096</span><span class="p">,</span> <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">VisionEncoder</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">num_patches</span> <span class="o">=</span> <span class="n">num_patches</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">positional_embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Embedding</span><span class="p">((</span><span class="bp">self</span><span class="p">.</span><span class="n">num_patches</span> <span class="o">+</span> <span class="mi">1</span><span class="p">),</span> <span class="n">dim_model</span><span class="p">)</span>  <span class="c1"># add 1 for cls token
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">num_layers</span> <span class="o">=</span> <span class="n">N</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span> <span class="o">=</span> <span class="n">dim_model</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dim_mlp</span> <span class="o">=</span> <span class="n">dim_mlp</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">dropout</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">encoder_layers</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">ModuleList</span><span class="p">(</span>
            <span class="p">[</span><span class="n">VisionEncoderLayer</span><span class="p">(</span><span class="n">dim_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">dim_mlp</span><span class="p">,</span> <span class="n">dropout</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">num_layers</span><span class="p">)]</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">layer_norm</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">dim_model</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
        <span class="n">layer_output</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">pos_x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">num_patches</span> <span class="o">+</span> <span class="mi">1</span><span class="p">).</span><span class="n">repeat</span><span class="p">(</span><span class="n">inputs</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]).</span><span class="n">to</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">dropout</span><span class="p">(</span>
            <span class="n">inputs</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">positional_embedding</span><span class="p">(</span><span class="n">pos_x</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">encoder_layers</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            <span class="n">layer_output</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">encoded_x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">layer_norm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># from official paper &amp; code by Google Research
</span>        <span class="n">layer_output</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">stack</span><span class="p">(</span><span class="n">layer_output</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">).</span><span class="n">to</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>  <span class="c1"># For Weighted Layer Pool: [N, BS, SEQ_LEN, DIM]
</span>        <span class="k">return</span> <span class="n">encoded_x</span><span class="p">,</span> <span class="n">layer_output</span>
</code></pre></div></div>
<p>ë§ˆì§€ë§‰ ì¸µì˜ ì¸ì½”ë” ì¶œë ¥ê°’ì—ëŠ” <code class="language-plaintext highlighter-rouge">layernorm</code>ì„ ì ìš©í•´ì¤˜ì•¼ í•¨ì„ ìŠì§€ ë§ì. í•œí¸, <code class="language-plaintext highlighter-rouge">layer_output</code>ëŠ” ë ˆì´ì–´ ë³„ ì–´í…ì…˜ ê²°ê³¼ë¥¼ ì‹œê°í™” í•˜ê±°ë‚˜ ë‚˜ì¤‘ì— <code class="language-plaintext highlighter-rouge">WeightedLayerPool</code>ì— ì‚¬ìš©í•˜ë ¤ê³  ë§Œë“¤ì—ˆë‹¤.</p>
<h4 id="-visiontransformer"><code class="language-plaintext highlighter-rouge">ğŸ¤– VisionTransformer</code></h4>

<p align="center">
<img src="/assets/images/vision_transformer/model_variant.png" alt="ViT Model Variant" class="align-center image-caption" width="75%&quot;, height=&quot;50%" />
<strong><em><a href="https://arxiv.org/abs/2010.11929">ViT Model Variant</a></em></strong>
</p>

<p><code class="language-plaintext highlighter-rouge">ViT</code> ëª¨ë¸ì˜ ê°€ì¥ ìµœìƒìœ„ ê°ì²´ë¡œ, ì•ì—ì„œ ì„¤ëª…í•œ ëª¨ë“  ëª¨ë“ˆë“¤ì˜ ë™ì‘ì´ ì´ë¤„ì§€ëŠ” ê³³ì´ë‹¤. ì‚¬ìš©ìë¡œë¶€í„° í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì…ë ¥ ë°›ì•„ ëª¨ë¸ì˜ í¬ê¸°, ê¹Šì´, íŒ¨ì¹˜ í¬ê¸°, ì´ë¯¸ì§€ ì„ë² ë”© ì¶”ì¶œ ë°©ì‹ì„ ì§€ì •í•œë‹¤. ê·¸ë¦¬ê³  ì…ë ¥ ì´ë¯¸ì§€ë¥¼ ì „ë‹¬ë°›ì•„ ì„ë² ë”©ì„ ë§Œë“¤ê³  ì¸ì½”ë”ì— ì „ë‹¬í•œ ë’¤, <code class="language-plaintext highlighter-rouge">MLP Head</code> ë¥¼ í†µí•´ ìµœì¢… ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ë°˜í™˜í•˜ëŠ” ì—­í• ì„ í•œë‹¤.</p>

<p>ì´ë¯¸ì§€ ì„ë² ë”© ì¶”ì¶œ ë°©ì‹ì€ <code class="language-plaintext highlighter-rouge">Linear Projection</code>ê³¼ <code class="language-plaintext highlighter-rouge">Convolution</code>ì´ ìˆë‹¤. ì „ìê°€ ë…¼ë¬¸ì—ì„œ ë§í•˜ëŠ” ì¼ë°˜ì ì¸ <code class="language-plaintext highlighter-rouge">ViT</code>ë¥¼ ë§í•˜ë©° í›„ìëŠ” ì €ìê°€ <code class="language-plaintext highlighter-rouge">Hybrid ViT</code>ë¼ê³  ë”°ë¡œ ëª…ëª…í•˜ëŠ” ëª¨ë¸ì´ë‹¤. ì„ë² ë”© ì¶”ì¶œ ë°©ì‹ ì´ì™¸ì— ë‹¤ë¥¸ ì°¨ì´ëŠ” ì „í˜€ ì—†ë‹¤. <code class="language-plaintext highlighter-rouge">extractor</code> ë§¤ê°œë³€ìˆ˜ë¥¼ í†µí•´ ì„ë² ë”© ì¶”ì¶œ ë°©ì‹ì„ ì§€ì •í•  ìˆ˜ ìˆìœ¼ë‹ˆ ì•„ë˜ ì½”ë“œë¥¼ í™•ì¸í•´ë³´ì.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">VisionTransformer</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="s">"""
    Main class for ViT of cls pooling, Pytorch implementation
    We implement pure ViT, Not hybrid version which is using CNN for extracting patch embedding
    input must be [BS, CHANNEL, IMAGE_SIZE, IMAGE_SIZE]
    In NLP, input_sequence is always smaller than vocab size
    But in vision, input_sequence is always same as image size, not concept of vocab in vision
    So, ViT use nn.Linear instead of nn.Embedding for input_embedding
    Args:
        num_classes: number of classes for classification task
        image_size: size of input image, default 512
        patch_size: size of patch, default 16 from official paper for ViT-Large
        extractor: option for feature extractor, default 'base' which is crop &amp; just flatten
                   if you want to use Convolution for feature extractor, set extractor='cnn' named hybrid ver in paper
        classifier: option for pooling method, default token meaning that do cls pooling
                    if you want to use mean pooling, set classifier='mean'
        mode: option for train type, default fine-tune, if you want pretrain, set mode='pretrain'
              In official paper &amp; code by Google Research, they use different classifier head for pretrain, fine-tune
    Math:
        image2sequence: [batch, channel, image_size, image_size] -&gt; [batch, patch, patch_size^2*channel]
        input_embedding: R^(P^2 Â·C)Ã—D
    Reference:
        https://arxiv.org/abs/2010.11929
        https://arxiv.org/abs/1706.03762
        https://github.com/google-research/vision_transformer/blob/main/vit_jax/models_vit.py#L184
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span>
            <span class="n">num_classes</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
            <span class="n">channels</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span>
            <span class="n">image_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">512</span><span class="p">,</span>
            <span class="n">patch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">16</span><span class="p">,</span>
            <span class="n">num_layers</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">24</span><span class="p">,</span>
            <span class="n">dim_model</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1024</span><span class="p">,</span>
            <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">16</span><span class="p">,</span>
            <span class="n">dim_mlp</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">4096</span><span class="p">,</span>
            <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
            <span class="n">extractor</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s">'base'</span><span class="p">,</span>
            <span class="n">classifier</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s">'token'</span><span class="p">,</span>
            <span class="n">mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s">'fine_tune'</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">VisionTransformer</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">num_patches</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">image_size</span> <span class="o">/</span> <span class="n">patch_size</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">num_layers</span> <span class="o">=</span> <span class="n">num_layers</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">patch_size</span> <span class="o">=</span> <span class="n">patch_size</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span> <span class="o">=</span> <span class="n">dim_model</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dim_mlp</span> <span class="o">=</span> <span class="n">dim_mlp</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">dropout</span><span class="p">)</span>

        <span class="c1"># Input Embedding
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">extractor</span> <span class="o">=</span> <span class="n">extractor</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">input_embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">((</span><span class="n">channels</span> <span class="o">*</span> <span class="n">patch_size</span><span class="o">**</span><span class="mi">2</span><span class="p">),</span> <span class="n">dim_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">conv</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Conv2d</span><span class="p">(</span>
            <span class="n">in_channels</span><span class="o">=</span><span class="n">channels</span><span class="p">,</span>
            <span class="n">out_channels</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span><span class="p">,</span>
            <span class="n">kernel_size</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">patch_size</span><span class="p">,</span>
            <span class="n">stride</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">patch_size</span>
        <span class="p">)</span>

        <span class="c1"># Encoder Multi-Head Self-Attention
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">VisionEncoder</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">num_patches</span><span class="p">,</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">num_layers</span><span class="p">,</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span><span class="p">,</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">num_heads</span><span class="p">,</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">dim_mlp</span><span class="p">,</span>
            <span class="n">dropout</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">classifier</span> <span class="o">=</span> <span class="n">classifier</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">pretrain_classifier</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">Tanh</span><span class="p">(),</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">fine_tune_classifier</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">mode</span> <span class="o">=</span> <span class="n">mode</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">any</span><span class="p">:</span>
        <span class="s">""" For cls pooling """</span>
        <span class="k">assert</span> <span class="n">inputs</span><span class="p">.</span><span class="n">ndim</span> <span class="o">!=</span> <span class="mi">4</span><span class="p">,</span> <span class="sa">f</span><span class="s">"Input shape should be [BS, CHANNEL, IMAGE_SIZE, IMAGE_SIZE], but got </span><span class="si">{</span><span class="n">inputs</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="s">"</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">inputs</span>

        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">extractor</span> <span class="o">==</span> <span class="s">'cnn'</span><span class="p">:</span>
            <span class="c1"># self.conv(x).shape == [batch, dim, image_size/patch_size, image_size/patch_size]
</span>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">conv</span><span class="p">(</span><span class="n">x</span><span class="p">).</span><span class="n">reshape</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">num_patches</span><span class="o">**</span><span class="mi">2</span><span class="p">).</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># self.extractor == 'base':
</span>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">input_embedding</span><span class="p">(</span>
                <span class="n">x</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="p">.</span><span class="n">num_patches</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">patch_size</span><span class="o">**</span><span class="mi">2</span> <span class="o">*</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
            <span class="p">)</span>
        <span class="n">cls_token</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">,</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>  <span class="c1"># can change init method
</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cat</span><span class="p">([</span><span class="n">cls_token</span><span class="p">,</span> <span class="n">x</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="n">x</span><span class="p">,</span> <span class="n">layer_output</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># output
</span>
        <span class="c1"># classification
</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">,</span> <span class="p">:]</span>  <span class="c1"># select cls token, which is position 0 in sequence
</span>        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">mode</span> <span class="o">==</span> <span class="s">'fine_tune'</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">fine_tune_classifier</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">mode</span> <span class="o">==</span> <span class="s">'pretrain'</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">fine_tune_classifier</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">pretrain_classifier</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">x</span>
</code></pre></div></div>

<p>í•œí¸, ì½”ë“œì—ì„œ ëˆˆì—¬ê²¨ë´ì•¼ í•  ì ì€ <code class="language-plaintext highlighter-rouge">MLP Head</code>ë¡œ, ì €ìëŠ” <code class="language-plaintext highlighter-rouge">pre-train</code> ì‹œì ê³¼ <code class="language-plaintext highlighter-rouge">fine-tune</code> ì‹œì ì— ì„œë¡œ ë‹¤ë¥¸ <code class="language-plaintext highlighter-rouge">Classifier Head</code>ë¥¼ ì‚¬ìš©í•œë‹¤. ì „ìì—ëŠ” <code class="language-plaintext highlighter-rouge">Activation Function</code> 1ê°œì™€ ë‘ ê°œì˜ <code class="language-plaintext highlighter-rouge">MLP Layer</code>ë¥¼ ì‚¬ìš©í•˜ê³ , í›„ìì—ëŠ” 1ê°œì˜ <code class="language-plaintext highlighter-rouge">MLP Layer</code>ë¥¼ ì‚¬ìš©í•œë‹¤.</p>

<p>ë‹¤ë§Œ, <code class="language-plaintext highlighter-rouge">pretrain_classifier</code>ì˜ ì…ì¶œë ¥ ì°¨ì›ì— ëŒ€í•œ ì •í™•í•œ ìˆ˜ì¹˜ë¥¼ ë…¼ë¬¸ì´ë‚˜ official repo codeë¥¼ í™•ì¸í•´ë„ ì°¾ì„ ìˆ˜ ì—†ì—ˆë‹¤, ê·¸ë˜ì„œ ì„ì‹œë¡œ ëª¨ë¸ì˜ ì°¨ì›ê³¼ ë˜‘ê°™ì´ ì„¸íŒ…í•˜ê²Œ ë˜ì—ˆë‹¤.</p>

<p>ë˜í•œ ì €ìëŠ” <code class="language-plaintext highlighter-rouge">CLS Pooling</code>ê³¼ ë”ë¶ˆì–´ <code class="language-plaintext highlighter-rouge">GAP</code> ë°©ì‹ë„ ì œì‹œí•˜ëŠ”ë°, <code class="language-plaintext highlighter-rouge">GAP</code> ë°©ì‹ì€ ì¶”í›„ì— ë”°ë¡œ ì¶”ê°€ê°€ í•„ìš”í•˜ë‹¤. ê·¸ë¦¬ê³  ì‚¬ì „ í›ˆë ¨ê³¼ íŒŒì¸ íŠœë‹ ëª¨ë‘ ë¶„ë¥˜ í…ŒìŠ¤í¬ë¥¼ ìˆ˜í–‰í–ˆëŠ”ë° (ì‹¬ì§€ì–´ ê°™ì€ ë°ì´í„° ì„¸íŠ¸ë¥¼ ì‚¬ìš©í•¨) ì™œ êµ³ì´ ì„œë¡œ ë‹¤ë¥¸ <code class="language-plaintext highlighter-rouge">Classifier Head</code>ë¥¼ ì •ì˜í–ˆëŠ”ì§€ ì˜ë„ë¥¼ ì•Œ ìˆ˜ ì—†ì–´ ë…¼ë¬¸ì„ ë‹¤ì‹œ ì½ì–´ë´¤ì§€ë§Œ, ì´ìœ ì— ëŒ€í•´ì„œ ìƒì„¸íˆ ì–¸ê¸‰í•˜ëŠ” ë¶€ë¶„ì´ ì—†ì—ˆë‹¤.</p>

<p><code class="language-plaintext highlighter-rouge">ViT</code>ëŠ” ì…ë ¥ ì„ë² ë”©ì„ ì •ì˜í•˜ëŠ” ë¶€ë¶„ì„ ì œì™¸í•˜ë©´ ì €ìì˜ ì˜ë„ëŒ€ë¡œ ê¸°ì¡´ íŠ¸ëœìŠ¤í¬ë¨¸ì™€ ë™ì¼í•œ ëª¨ë¸ êµ¬ì¡°ë¥¼ ê°€ì¡Œë‹¤. ì™„ì „íˆ ë‹¤ë¥¸ ë°ì´í„°ì¸ ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ì— ê°™ì€ êµ¬ì¡°ì˜ ëª¨ë¸ì„ ì ìš©í•œë‹¤ëŠ” ê²ƒì´ ì •ë§ ì‰½ì§€ ì•Šì•„ ë³´ì˜€ëŠ”ë°, íŒ¨ì¹˜ ê°œë…ì„ ë§Œë“¤ì–´ ìì—°ì–´ì˜ í† í°ì²˜ëŸ¼ ê°„ì£¼í•˜ê³  ì‚¬ìš©í•œ ê²ƒì´ ì˜ë„ëŒ€ë¡œ êµ¬í˜„í•˜ëŠ”ë° ì§ê´€ì ì´ë©´ì„œë„ ì •ë§ íš¨ê³¼ì ì´ì—ˆë‹¤ê³  ìƒê°í•œë‹¤. ì´ì œ ì´ë ‡ê²Œ ë§Œë“¤ì–´ì§„ ëª¨ë¸ì„ í†µí•´ ì§„í–‰í•œ ì—¬ëŸ¬ ì‹¤í—˜ ê²°ê³¼ì— ì–´ë–¤ ì¸ì‚¬ì´íŠ¸ê°€ ë‹´ê²¨ ìˆëŠ”ì§€ ì•Œì•„ë³´ì.</p>

<h3 id="insight-from-experiment"><code class="language-plaintext highlighter-rouge">ğŸ”¬Â Insight from Experiment</code></h3>

<h4 id="insight-1-vitì˜-scalability-ì¦ëª…"><code class="language-plaintext highlighter-rouge">ğŸ’¡Â Insight 1. ViTì˜ Scalability ì¦ëª…</code></h4>
<ul>
  <li><strong><code class="language-plaintext highlighter-rouge">Pre-Train</code>ì— ì‚¬ìš©ë˜ëŠ” ì´ë¯¸ì§€ ë°ì´í„° ì„¸íŠ¸ì˜ í¬ê¸°ê°€ ì»¤ì§ˆìˆ˜ë¡ <code class="language-plaintext highlighter-rouge">Fine-Tune Stage</code>ì—ì„œ <code class="language-plaintext highlighter-rouge">ViT</code>ê°€ <code class="language-plaintext highlighter-rouge">CNN</code>ë³´ë‹¤ ë†’ì€ ì„±ëŠ¥</strong></li>
  <li><strong>ê°™ì€ ì„±ëŠ¥ì´ë¼ë©´ <code class="language-plaintext highlighter-rouge">ViT</code>ê°€ ìƒëŒ€ì ìœ¼ë¡œ ì ì€ ì—°ì‚°ëŸ‰ì„ ê¸°ë¡</strong></li>
</ul>

<p align="center">
<img src="/assets/images/vision_transformer/insight1.png" alt="Performance per Dataset Scale" class="align-center image-caption" width="75%&quot;, height=&quot;50%" />
<strong><em><a href="https://arxiv.org/abs/2010.11929">Performance per Dataset Scale</a></em></strong>
</p>

<p>ìœ„ ë„í‘œëŠ” <code class="language-plaintext highlighter-rouge">Pre-Train Stage</code>ì— ì‚¬ìš©ëœ ì´ë¯¸ì§€ ë°ì´í„° ì„¸íŠ¸ì— ë”°ë¥¸ ëª¨ë¸ì˜ <code class="language-plaintext highlighter-rouge">Fine-Tune</code> ì„±ëŠ¥ ì¶”ì´ë¥¼ ë‚˜íƒ€ë‚¸ ìë£Œë‹¤. ì‚¬ì „ í›ˆë ¨ ë°ì´í„° ìŠ¤ì¼€ì¼ì´ í¬ì§€ ì•Šì„ ë•ŒëŠ” <code class="language-plaintext highlighter-rouge">Conv</code> ê¸°ë°˜ì˜ <code class="language-plaintext highlighter-rouge">ResNet</code> ì‹œë¦¬ì¦ˆê°€ <code class="language-plaintext highlighter-rouge">ViT</code> ì‹œë¦¬ì¦ˆë¥¼ ì••ë„í•˜ëŠ” ëª¨ìŠµì„ ë³´ì—¬ì¤€ë‹¤. í•˜ì§€ë§Œ ë°ì´í„° ì„¸íŠ¸ì˜ í¬ê¸°ê°€ ì»¤ì§ˆìˆ˜ë¡ ì ì  <code class="language-plaintext highlighter-rouge">ViT</code> ì‹œë¦¬ì¦ˆì˜ ì„±ëŠ¥ì´ <code class="language-plaintext highlighter-rouge">ResNet</code>ì„ ëŠ¥ê°€í•˜ëŠ” ê²°ê³¼ë¥¼ ë³¼ ìˆ˜ ìˆë‹¤.</p>

<p>í•œí¸, ViT &amp; ResNet ì„±ëŠ¥ ê²°ê³¼ ëª¨ë‘ ImageNetê³¼ JFT-Imageë¡œ ì‚¬ì „ í›ˆë ¨ ë° íŒŒì¸ íŠœë‹ì„ ê±°ì³ ë‚˜ì™”ë‹¤ê³  í•˜ë‹ˆ ì°¸ê³ í•˜ì. <strong><u>ì¶”ê°€ë¡œ íŒŒì¸ íŠœë‹ ê³¼ì •ì—ì„œ ì‚¬ì „ í›ˆë ¨ ë•Œë³´ë‹¤ ì´ë¯¸ì§€ ì‚¬ì´ì¦ˆë¥¼ í‚¤ì›Œì„œ í›ˆë ¨ì„ ì‹œì¼°ë‹¤ê³  ë…¼ë¬¸ì—ì„œ ë°íˆê³  ìˆëŠ”ë°, ì´ëŠ” ì €ìì˜ ì‹¤í—˜ ê²°ê³¼ì— ê¸°ì¸í•œ ê²ƒì´ë‹¤</u></strong>. ë…¼ë¬¸ì— ë”°ë¥´ë©´ íŒŒì¸ íŠœë‹ ë•Œ ì‚¬ì „ í›ˆë ¨ ë‹¹ì‹œë³´ë‹¤ ë” ë†’ì€ í•´ìƒë„ì˜ ì´ë¯¸ì§€ë¥¼ ì‚¬ìš©í•˜ë©´ ì„±ëŠ¥ì´ í–¥ìƒ ëœë‹¤ê³  í•˜ë‹ˆ ê¸°ì–µí–ˆë‹¤ê°€  ì¨ë¨¹ì–´ë³´ì.</p>

<p align="center">
<img src="/assets/images/vision_transformer/insight1_2.png" alt="Performance per FLOPs Scale" class="align-center image-caption" width="75%&quot;, height=&quot;50%" />
<strong><em><a href="https://arxiv.org/abs/2010.11929">Performance per FLOPs</a></em></strong>
</p>

<p>ìœ„ ë„í‘œëŠ” ì—°ì‚°ëŸ‰ ë³€í™”ì— ë”°ë¥¸ ëª¨ë¸ì˜ ì„±ëŠ¥ ì¶”ì´ë¥¼ ë‚˜íƒ€ë‚¸ ê·¸ë¦¼ì´ë‹¤. ë‘ ì§€í‘œ ëª¨ë‘ ê°™ì€ ì ìˆ˜ë¼ë©´ <code class="language-plaintext highlighter-rouge">ViT</code> ì‹œë¦¬ì¦ˆì˜ ì—°ì‚°ëŸ‰ì´ í˜„ì €íˆ ì ìŒì„ ì•Œ ìˆ˜ ìˆë‹¤. ë˜í•œ ì •í™•ë„ 95% ì´í•˜ êµ¬ê°„ì—ì„œ ê°™ì€ ì„±ëŠ¥ì´ë¼ë©´  <code class="language-plaintext highlighter-rouge">ViT</code>ì˜ <code class="language-plaintext highlighter-rouge">Hybrid</code> ë²„ì „ ëª¨ë¸ì˜ ì—°ì‚°ëŸ‰ì´ ì¼ë°˜ <code class="language-plaintext highlighter-rouge">ViT</code> ë²„ì „ë³´ë‹¤ í˜„ì €íˆ ì ìŒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤. ì´ëŸ¬í•œ ì‚¬ì‹¤ì€ ì¶”í›„ì— <code class="language-plaintext highlighter-rouge">Swin-Transformer</code> ì„¤ê³„ì— ì˜ê°ì„ ì¤€ë‹¤.</p>

<p>ë‘ ê°œì˜ ì‹¤í—˜ ê²°ê³¼ë¥¼ ì¢…í•©í–ˆì„ ë•Œ, <code class="language-plaintext highlighter-rouge">ViT</code>ê°€ <code class="language-plaintext highlighter-rouge">ResNet</code>ë³´ë‹¤ ì¼ë°˜í™” ì„±ëŠ¥ì´ ë” ë†’ìœ¼ë©°(ë„í‘œ 1) ëª¨ë¸ì˜ <code class="language-plaintext highlighter-rouge">Saturation</code> í˜„ìƒì´ ë‘ë“œëŸ¬ì§€ì§€ ì•Šì•„ ì„±ëŠ¥ì˜ í•œê³„ì¹˜(ë„í‘œ 2) ì—­ì‹œ ë” ë†’ë‹¤ê³  ë³¼ ìˆ˜ ìˆë‹¤. ë”°ë¼ì„œ ê¸°ì¡´ íŠ¸ëœìŠ¤í¬ë¨¸ì˜ ì—°ì‚°â€¢êµ¬ì¡°ì  ì¸¡ë©´ì—ì„œ <code class="language-plaintext highlighter-rouge">Scalability</code>ë¥¼ ì„±ê³µì ìœ¼ë¡œ ì´ì‹í–ˆë‹¤ê³  í‰ê°€í•  ìˆ˜ ìˆê² ë‹¤.</p>

<h4 id="insight-2-pure-self-attentionì€-ì¢‹ì€-ì´ë¯¸ì§€-í”¼ì²˜ë¥¼-ì¶”ì¶œí•˜ê¸°ì—-ì¶©ë¶„í•˜ë‹¤"><code class="language-plaintext highlighter-rouge">ğŸ’¡Â Insight 2. Pure Self-Attentionì€ ì¢‹ì€ ì´ë¯¸ì§€ í”¼ì²˜ë¥¼ ì¶”ì¶œí•˜ê¸°ì— ì¶©ë¶„í•˜ë‹¤</code></h4>
<ul>
  <li><strong>Patch Embedding Layerì˜ PCA ê²°ê³¼, íŒ¨ì¹˜ì˜ ê¸°ì €ê°€ ë˜ëŠ” ì°¨ì›ê³¼ ìœ ì‚¬í•œ ëª¨ì–‘ì„ ì¶”ì¶œ</strong>
    <ul>
      <li><strong><code class="language-plaintext highlighter-rouge">Convolution</code> ì—†ì´ <code class="language-plaintext highlighter-rouge">Self-Attention</code>ë§Œìœ¼ë¡œë„ ì¶©ë¶„íˆ ì´ë¯¸ì§€ì˜ ì¢‹ì€ í”¼ì²˜ë¥¼ ì¶”ì¶œí•˜ëŠ” ê²ƒì´ ê°€ëŠ¥</strong></li>
      <li><strong><code class="language-plaintext highlighter-rouge">Vision</code>ì—ì„œ <code class="language-plaintext highlighter-rouge">Convolution</code>ì— ëŒ€í•œ <code class="language-plaintext highlighter-rouge">reliance</code> íƒˆí”¼ ê°€ëŠ¥</strong></li>
    </ul>
  </li>
</ul>

<p align="center">
<img src="/assets/images/vision_transformer/insight2.png" alt="Patch Embedding Layerâ€™s Filter" class="align-center image-caption" width="75%&quot;, height=&quot;50%" />
<strong><em><a href="https://arxiv.org/abs/2010.11929">Patch Embedding Layerâ€™s Filter</a></em></strong>
</p>

<p>ìœ„ ìë£ŒëŠ” ì¶©ë¶„í•œ í•™ìŠµì„ ê±°ì¹˜ê³  ë‚œ <code class="language-plaintext highlighter-rouge">ViT</code>ì˜ <code class="language-plaintext highlighter-rouge">Patch Embedding Layer</code>ì˜ í•„í„°ë¥¼ <code class="language-plaintext highlighter-rouge">PCA</code>í•œ ê²°ê³¼ ì¤‘ì—ì„œ íŠ¹ì‡ê°’ì´ ë†’ì€ ìƒìœ„ 28ê°œì˜ í”¼ì²˜ë¥¼ ë‚˜ì—´í•œ ê·¸ë¦¼ì´ë‹¤. ì´ë¯¸ì§€ì˜ ê¸°ë³¸ ë¼ˆëŒ€ê°€ ë˜ê¸°ì— ì í•©í•´ ë³´ì´ëŠ” í”¼ì²˜ë“¤ì´ ì¶”ì¶œëœ ëª¨ìŠµì„ ë³¼ ìˆ˜ ìˆë‹¤.</p>

<p>ë”°ë¼ì„œ <code class="language-plaintext highlighter-rouge">Inductive Bias</code> ì—†ì´, ë‹¨ì¼  <code class="language-plaintext highlighter-rouge">Self-Attention</code>ë§Œìœ¼ë¡œ ì´ë¯¸ì§€ì˜ í”¼ì²˜ë¥¼ ì¶”ì¶œí•˜ëŠ” ê²ƒì´ ì¶©ë¶„íˆ ê°€ëŠ¥í•˜ë‹¤. ë¹„ì „ ë¶„ì•¼ì— ë§Œì—°í•œ <code class="language-plaintext highlighter-rouge">Convolution</code> ì˜ì¡´ì—ì„œ ë²—ì–´ë‚˜ ìƒˆë¡œìš´ ì•„í‚¤í…ì²˜ì˜ ë„ì…ì´ ê°€ëŠ¥í•¨ì„ ì‹œì‚¬í•œ ë¶€ë¶„ì´ë¼ê³  í•  ìˆ˜ ìˆê² ë‹¤.</p>

<h4 id="insight-3-bottom2general-information-top2specific-information"><code class="language-plaintext highlighter-rouge">ğŸ’¡Â Insight 3. Bottom2General Information, Top2Specific Information</code></h4>

<ul>
  <li><strong><code class="language-plaintext highlighter-rouge">ì…ë ¥</code>ê³¼ ê°€ê¹Œìš´ ì¸ì½”ë”ì¼ìˆ˜ë¡ <code class="language-plaintext highlighter-rouge">Global &amp; General</code>í•œ Informationì„ í¬ì°©</strong></li>
  <li><strong><code class="language-plaintext highlighter-rouge">ì¶œë ¥</code>ê³¼ ê°€ê¹Œìš´ ì¸ì½”ë”ì¼ìˆ˜ë¡ <code class="language-plaintext highlighter-rouge">Local &amp; Specific</code>í•œ Informationì„ í¬ì°©</strong></li>
</ul>
<p align="center">
<img src="/assets/images/vision_transformer/insight3.png" alt="Multi-Head Attention Distance per Network Depth" class="align-center image-caption" width="75%&quot;, height=&quot;50%" />
<strong><em><a href="https://arxiv.org/abs/2010.11929">Multi-Head Attention Distance per Network Depth</a></em></strong>
</p>

<p>ë‹¤ìŒ ìë£ŒëŠ” ì¸ì½”ë”ì˜ ê°œìˆ˜ ë³€í™”ì— ë”°ë¥¸ ê°œë³„ ì–´í…ì…˜ í•´ë“œì˜ ì–´í…ì…˜ ê±°ë¦¬ ë³€í™” ì¶”ì´ë¥¼ ë‚˜íƒ€ë‚¸ ê·¸ë¦¼ì´ë‹¤. ì—¬ê¸°ì„œ ì–´í…ì…˜ ê±°ë¦¬ë€, í•´ë“œê°€ ì–¼ë§ˆë‚˜ ë©€ë¦¬ ë–¨ì–´ì§„ íŒ¨ì¹˜ë¥¼ ì–´í…ì…˜í–ˆëŠ”ì§€ í”½ì…€ ë‹¨ìœ„ë¡œ í‘œí˜„í•œ ì§€í‘œë‹¤. í•´ë‹¹ ê°’ì´ ë†’ì„ìˆ˜ë¡ ê±°ë¦¬ìƒ ë©€ë¦¬ ë–¨ì–´ì§„ íŒ¨ì¹˜ì™€ ì–´í…ì…˜ì„, ì‘ì„ìˆ˜ë¡ ê°€ê¹Œìš´ íŒ¨ì¹˜ì™€ ì–´í…ì…˜ í–ˆë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•œë‹¤. ë‹¤ì‹œ ë„í‘œë¥¼ ì‚´í´ë³´ì. ì…ë ¥ê³¼ ê°€ê¹Œìš´ ì¸ì½”ë”ì¼ìˆ˜ë¡(Depth 0) í•´ë“œë³„ ì–´í…ì…˜ ê±°ë¦¬ì˜ ë¶„ì‚°ì´ ì»¤ì§€ê³ , ì¶œë ¥ê³¼ ê°€ê¹Œìš´ ì¸ì½”ë”ì¼ìˆ˜ë¡(Depth 23) ë¶„ì‚°ì´ ì ì ì¤„ì–´ë“¤ë‹¤ê°€ ê±°ì˜ í•œ ì ì— ìˆ˜ë ´í•˜ëŠ”ë“¯í•œ ì–‘ìƒì„ ë³´ì—¬ì¤€ë‹¤. ë‹¤ì‹œ ë§í•´, ì…ë ¥ê³¼ ê°€ê¹Œìš´ <code class="language-plaintext highlighter-rouge">Bottom Encoder</code>ëŠ” ë©€ë¦¬ ë–¨ì–´ì§„ íŒ¨ì¹˜ë¶€í„° ê°€ê¹Œìš´ íŒ¨ì¹˜ê¹Œì§€ ëª¨ë‘ ì „ì—­ì (<code class="language-plaintext highlighter-rouge">Global</code>)ìœ¼ë¡œ ì–´í…ì…˜ì„ ìˆ˜í–‰í•´ <code class="language-plaintext highlighter-rouge">General</code> í•œ ì •ë³´ë¥¼ í¬ì°©í•˜ê²Œ ë˜ê³  ì¶œë ¥ê³¼ ê°€ê¹Œìš´ <code class="language-plaintext highlighter-rouge">Top Encoder</code>ëŠ” ê°œë³„ í•´ë“œë“¤ì´ ëª¨ë‘ ë¹„ìŠ·í•œ ê±°ë¦¬ì— ìœ„ì¹˜í•œ íŒ¨ì¹˜(<code class="language-plaintext highlighter-rouge">Local</code>)ì— ì–´í…ì…˜ì„ ìˆ˜í–‰í•´ <code class="language-plaintext highlighter-rouge">Specific</code> í•œ ì •ë³´ë¥¼ í¬ì°©í•˜ê²Œ ëœë‹¤.</p>

<p>ì´ ë•Œ <code class="language-plaintext highlighter-rouge">Global</code>ê³¼ <code class="language-plaintext highlighter-rouge">Local</code>ì´ë¼ëŠ” ìš©ì–´ ë•Œë¬¸ì— <code class="language-plaintext highlighter-rouge">Bottom Encoder</code> ëŠ” ë©€ë¦¬ ë–¨ì–´ì§„ íŒ¨ì¹˜ì™€ ì–´í…ì…˜í•˜ê³ , <code class="language-plaintext highlighter-rouge">Top Encoder</code>ëŠ” ê°€ê¹Œìš´ íŒ¨ì¹˜ì™€ ì–´í…ì…˜í•œë‹¤ê³  ì°©ê°í•˜ê¸° ì‰½ë‹¤. <strong><u>ê·¸ëŸ¬ë‚˜ ê°œë³„ í•´ë“œë“¤ì˜ ì–´í…ì…˜ ê±°ë¦¬ê°€ ì–¼ë§ˆë‚˜ ë¶„ì‚°ë˜ì–´ ìˆëŠ”ê°€ê°€ ë°”ë¡œ </u></strong><code class="language-plaintext highlighter-rouge">Global</code>, <code class="language-plaintext highlighter-rouge">Local</code><strong><u>ì„ êµ¬ë¶„í•˜ëŠ” ê¸°ì¤€ì´ ëœë‹¤.</u></strong> ì…ë ¥ë¶€ì— ê°€ê¹Œìš´ ë ˆì´ì–´ë“¤ì€ í—¤ë“œë“¤ì˜ ì–´í…ì…˜ ê±°ë¦¬ ë¶„ì‚°ì´ ë§¤ìš° í° í¸ì¸ë°, ì´ê²ƒì„ ì´íŒ¨ì¹˜ ì €íŒ¨ì¹˜ ëª¨ë‘ ì–´í…ì…˜ í•´ë³´ê³  ë¹„êµí•´ë³¸ë‹¤ê³  í•´ì„í•´ì„œ <code class="language-plaintext highlighter-rouge">Global</code>ì´ë¼ê³  ë¶€ë¥´ê³ , ì¶œë ¥ë¶€ì— ê°€ê¹Œìš´ ë ˆì´ì–´ëŠ” í—¤ë“œë“¤ì˜ ì–´í…ì…˜ ê±°ë¦¬ ë¶„ì‚°ì´ ë§¤ìš° ì‘ì€ í¸ì¸ë°, ì´ê²Œ ë°”ë¡œ ê°ê°ì˜ í—¤ë“œë“¤ì´ ì–´ë–¤ ì •ë³´ì— ì£¼ëª©í•´ì•¼í• ì§€(ë¶„ë¥˜ ì†ì‹¤ì´ ê°€ì¥ ì‘ì•„ì§€ëŠ” íŒ¨ì¹˜) ë²”ìœ„ë¥¼ ì¶©ë¶„íˆ ì¢íŒ ìƒíƒœì—ì„œ íŠ¹ì • ë¶€ë¶„ì—ë§Œ ì§‘ì¤‘í•œë‹¤ëŠ” ì˜ë¯¸ë¡œ í•´ì„í•´ <code class="language-plaintext highlighter-rouge">Local</code> ì´ë¼ê³  ë¶€ë¥´ê²Œ ë˜ì—ˆë‹¤.</p>

<p>&lt;<strong><a href="https://arxiv.org/abs/2006.05987">Revisiting Few-sample BERT Fine-tuning</a></strong>&gt;ë„ ìœ„ì™€ ë¹„ìŠ·í•œ ë§¥ë½ì˜ ì‚¬ì‹¤ì— ëŒ€í•´ ì–¸ê¸‰í•˜ê³  ìˆìœ¼ë‹ˆ ì°¸ê³ í•´ë³´ì. ì´ëŸ¬í•œ ì‚¬ì‹¤ì€ íŠ¸ëœìŠ¤í¬ë¨¸ ì¸ì½”ë” ê³„ì—´ ëª¨ë¸ì„ íŠœë‹í•  ë•Œ <code class="language-plaintext highlighter-rouge">Depth</code> ë³„ë¡œ ë‹¤ë¥¸ <code class="language-plaintext highlighter-rouge">Learning Rate</code>ì„ ì ìš©í•˜ëŠ” <code class="language-plaintext highlighter-rouge">Layerwise Learning Rate Decay</code> ì˜ ì´ˆì„ì´ ë˜ê¸°ë„ í•œë‹¤. <code class="language-plaintext highlighter-rouge">Layerwise Learning Rate Decay</code> ì— ëŒ€í•´ì„œëŠ” <strong><a href="https://towardsdatascience.com/advanced-techniques-for-fine-tuning-transformers-82e4e61e16e">ì—¬ê¸° í¬ìŠ¤íŠ¸</a></strong>ë¥¼ ì°¸ê³ í•˜ë„ë¡ í•˜ì.</p>

<p align="center">
<img src="/assets/images/vision_transformer/insight3_2.png" alt="Output from Last Encoder" class="align-center image-caption" width="40%&quot;, height=&quot;10%" />
<strong><em><a href="https://arxiv.org/abs/2010.11929">Output from Last Encoder</a></em></strong>
</p>

<p>í•œí¸ ë…¼ë¬¸ì—ëŠ” ì–¸ê¸‰ë˜ì§€ ì•Šì€, í•„ìì˜ ë‡Œí”¼ì…œì— ê°€ê¹ì§€ë§Œ, <strong><u>ì¶œë ¥ì— ê°€ê¹Œìš´ ì¸ì½”ë”ë“¤ì˜ í•´ë“œê°€ ê°€ì§„</u></strong> <code class="language-plaintext highlighter-rouge">Attention Distance</code><strong><u>ì´ ëª¨ë‘ ë¹„ìŠ·í•˜ë‹¤ëŠ” ì‚¬ì‹¤ë¡œ ì´ë¯¸ì§€ ë¶„ë¥˜ì— ê²°ì •ì ì¸ ì—­í• ì„ í•˜ëŠ” í”¼ì²˜ê°€ ì´ë¯¸ì§€ì˜ íŠ¹ì • êµ¬ì—­ì— ëª¨ì—¬ ìˆìœ¼ë©°, ê·¸ ìŠ¤íŒŸì€ ì´ë¯¸ì§€ì˜ ì¤‘ì•™ ë¶€ê·¼ì¼ ê°€ëŠ¥ì„±ì´ ë†’ë‹¤ê³  ì¶”ì¸¡ í•´ë³¼ ìˆ˜ ìˆë‹¤.</u></strong> ëª¨ë“  í•´ë“œì˜ í”½ì…€ ê±°ë¦¬ê°€ ì„œë¡œ ë¹„ìŠ·í•˜ë ¤ë©´ ì¼ë‹¨ ë¹„ìŠ·í•œ ìœ„ì¹˜ì˜ íŒ¨ì¹˜ì— ì–´í…ì…˜ì„ í•´ì•¼í•˜ê¸° ë•Œë¬¸ì— ë¶„ë¥˜ ì†ì‹¤ê°’ì„ ìµœì†Œë¡œ ì¤„ì—¬ì£¼ëŠ” í”¼ì²˜ëŠ” ë³´í†µ í•œ êµ¬ì—­(íŒ¨ì¹˜)ì— ëª°ë ¤ ìˆì„ ê²ƒì´ë¼ê³  ìœ ì¶”ê°€ ê°€ëŠ¥í•˜ë‹¤. ë˜í•œ íŠ¹ì • ìŠ¤íŒŸì´ ì¤‘ì•™ì— ìœ„ì¹˜í• ìˆ˜ë¡ ì–´í…ì…˜ ê±°ë¦¬ì˜ ë¶„ì‚°ì´ ì¤„ì–´ë“¤ê²ƒì´ë¼ê³  ìƒê° í•´ë³¼ ìˆ˜ë„ ìˆì—ˆë‹¤. ì €ìëŠ” <code class="language-plaintext highlighter-rouge">Attention Rollout</code>ì´ë¼ëŠ” ê°œë…ì„ í†µí•´ <code class="language-plaintext highlighter-rouge">Attention Distance</code>ì„ ì‚°ì¶œí–ˆë‹¤ê³  ì–¸ê¸‰í•˜ëŠ”ë°, ìì„¸í•œ ë‚´ìš©ì€ ì˜†ì— ë‘ ë§í¬ë¥¼ ì°¸ê³ í•´ë³´ì(<a href="https://hongl.tistory.com/234">í•œêµ­ì–´ ì„¤ëª… ë¸”ë¡œê·¸</a>,  <a href="https://arxiv.org/abs/2005.00928">ì›ë…¼ë¬¸</a>). ì´ëŸ¬í•œ í•„ìì˜ ê°€ì„¤ì´ ë§ë‹¤ë©´, <code class="language-plaintext highlighter-rouge">Convolution</code> ì˜ <code class="language-plaintext highlighter-rouge">Inductive Bias</code>  ì¤‘ <code class="language-plaintext highlighter-rouge">Locality</code> ì˜ íš¨ê³¼ì„±ì„ <code class="language-plaintext highlighter-rouge">Self-Attention</code>ì„ í†µí•´ ì…ì¦ì´ ê°€ëŠ¥í•˜ë©°, ë°˜ëŒ€ë¡œ <code class="language-plaintext highlighter-rouge">Convolution</code>ì— ëŒ€í•œ ì˜ì¡´ì—ì„œ ë²—ì–´ë‚˜ ë‹¨ì¼ <code class="language-plaintext highlighter-rouge">Self-Attention</code> ìœ¼ë¡œë„ ê°™ì€ íš¨ê³¼ë¥¼ ë‚¼ ìˆ˜ ìˆë‹¤ëŠ” ì¦ê±° ì¤‘ í•˜ë‚˜ê°€ ë  ê²ƒì´ë‹¤.</p>

<h4 id="insight-4-vitëŠ”-cls-pooling-ì‚¬ìš©í•˜ëŠ”ê²Œ-íš¨ìœ¨ì "><code class="language-plaintext highlighter-rouge">ğŸ’¡Â Insight 4. ViTëŠ” CLS Pooling ì‚¬ìš©í•˜ëŠ”ê²Œ íš¨ìœ¨ì </code></h4>

<ul>
  <li><strong><code class="language-plaintext highlighter-rouge">CLS Pooling</code>ì€ <code class="language-plaintext highlighter-rouge">GAP</code> ë³´ë‹¤ 2ë°° ì´ìƒ í° í•™ìŠµë¥ ì„ ì‚¬ìš©í•´ë„ ë¹„ìŠ·í•œ ì„±ëŠ¥ì„ ê¸°ë¡</strong>
    <ul>
      <li><strong><u>í•™ìŠµ ì†ë„ëŠ” ë” ë¹ ë¥´ë˜ ì„±ëŠ¥ì´ ë¹„ìŠ·í•˜ê¸° ë•Œë¬¸ì—</u></strong> <code class="language-plaintext highlighter-rouge">CLS Pooling</code> <strong><u>ì´ ë” íš¨ìœ¨ì </u></strong></li>
    </ul>
  </li>
</ul>

<p align="center">
<img src="/assets/images/vision_transformer/insight4.png" alt="Performance Trend by Pooling Method with LR" class="align-center image-caption" width="75%&quot;, height=&quot;50%" />
<strong><em><a href="https://arxiv.org/abs/2010.11929">Performance Trend by Pooling Method with LR</a></em></strong>
</p>

<p>ë‹¤ìŒ ë„í‘œëŠ” í’€ë§ ë°©ì‹ê³¼ í•™ìŠµë¥ ì˜ ë³€ë™ì— ë”°ë¥¸ ì •í™•ë„ ë³€í™” ì¶”ì´ë¥¼ ë‚˜íƒ€ë‚¸ ê·¸ë¦¼ì´ë‹¤. ë¹„ìŠ·í•œ ì„±ëŠ¥ì´ë¼ë©´ <code class="language-plaintext highlighter-rouge">CLS Pooling</code>ì´ <code class="language-plaintext highlighter-rouge">GAP</code>ë³´ë‹¤ 2ë°° ì´ìƒ í° í•™ìŠµë¥ ì„ ì‚¬ìš©í–ˆë‹¤. í•™ìŠµë¥ ì´ í¬ë©´ ëª¨ë¸ì˜ ìˆ˜ë ´ ì†ë„ê°€ ë¹¨ë¼ì ¸ í•™ìŠµ ì†ë„ê°€ ë¹¨ë¼ì§€ëŠ” ì¥ì ì´ ìˆë‹¤. ê·¸ëŸ°ë° ì„±ëŠ¥ê¹Œì§€ ë¹„ìŠ·í•˜ë‹¤ë©´ <code class="language-plaintext highlighter-rouge">ViT</code>ëŠ” <code class="language-plaintext highlighter-rouge">CLS Pooling</code>ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ë” íš¨ìœ¨ì ì´ë¼ê³  í•  ìˆ˜ ìˆê² ë‹¤.</p>

<p>ë‚˜ì¤‘ì— ì‹œê°„ì´ ëœë‹¤ë©´ ë‹¤ë¥¸ í’€ë§ ë°©ì‹, ì˜ˆë¥¼ ë“¤ë©´ <code class="language-plaintext highlighter-rouge">Weighted Layer Pooling</code>, <code class="language-plaintext highlighter-rouge">GeM Pooling</code>, <code class="language-plaintext highlighter-rouge">Attention Pooling</code> ê°™ì€ ê²ƒì„ ì ìš©í•´ ì‹¤í—˜í•´ë³´ê² ë‹¤.</p>

<h4 id="insight-5-vitëŠ”-absolute-1d-position-embedding-ì‚¬ìš©í•˜ëŠ”ê²Œ-ê°€ì¥-íš¨ìœ¨ì "><code class="language-plaintext highlighter-rouge">ğŸ’¡Â Insight 5. ViTëŠ” Absolute 1D-Position Embedding ì‚¬ìš©í•˜ëŠ”ê²Œ ê°€ì¥ íš¨ìœ¨ì </code></h4>

<ul>
  <li><strong>ì–´ë–¤ í˜•íƒœë¡œë“  ìœ„ì¹˜ ì„ë² ë”© ê°’ì„ ì •ì˜í•´ì¤€ë‹¤ë©´, í˜•íƒœì™€ ì¢…ë¥˜ì— ìƒê´€ì—†ì´ ê±°ì˜ ë¹„ìŠ·í•œ ì„±ëŠ¥ì„ ë³´ì„</strong></li>
  <li><strong>ì„±ëŠ¥ì´ ë¹„ìŠ·í•˜ë©´, ì§ê´€ì ì´ê³  êµ¬í˜„ì´ ê°„í¸í•œ <code class="language-plaintext highlighter-rouge">Absolute 1D-Position Embedding</code> ë°©ë²•ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ê°€ì¥ íš¨ìœ¨ì </strong></li>
  <li><strong><code class="language-plaintext highlighter-rouge">ViT</code>ëŠ” <code class="language-plaintext highlighter-rouge">Patch-Level</code> ì‚¬ìš©í•´, <code class="language-plaintext highlighter-rouge">Pixel-Level</code>ë³´ë‹¤ ìƒëŒ€ì ìœ¼ë¡œ ì‹œí€€ìŠ¤ ê¸¸ì´ê°€ ì§§ì•„ ìœ„ì¹˜â€¢ê³µê°„ ì •ë³´ë¥¼ ì¸ì½”ë”©í•˜ëŠ” ë°©ì‹ì— ì˜í–¥ì„ ëœ ë°›ìŒ</strong></li>
</ul>

<p align="center">
<img src="/assets/images/vision_transformer/insight5.png" alt="Performance Table by making Position Embedding method" class="align-center image-caption" width="75%&quot;, height=&quot;50%" />
<strong><em><a href="https://arxiv.org/abs/2010.11929">Performance Table by making Position Embedding method</a></em></strong>
</p>

<p>ìœ„ ì‹¤í—˜ ê²°ê³¼ëŠ” <code class="language-plaintext highlighter-rouge">Position Embedding</code> ì¸ì½”ë”© ë°©ì‹ì— ë”°ë¥¸ <code class="language-plaintext highlighter-rouge">ViT</code> ëª¨ë¸ì˜ ì„±ëŠ¥ ë³€í™” ì¶”ì´ë¥¼ ë‚˜íƒ€ë‚¸ ìë£Œë‹¤. ì¸ì½”ë”© í˜•íƒœì™€ ìƒê´€ì—†ì´ ìœ„ì¹˜ ì„ë² ë”©ì˜ ìœ ë¬´ê°€ ì„±ëŠ¥ì— í° ì˜í–¥ì„ ë¯¸ì¹œë‹¤ëŠ” ì‚¬ì‹¤ì„ ì•Œë ¤ì£¼ê³  ìˆë‹¤. í•œí¸, ì¸ì½”ë”© í˜•íƒœ ë³€í™”ì— ë”°ë¥¸ ìœ ì˜ë¯¸í•œ ì„±ëŠ¥ ë³€í™”ëŠ” ì—†ì—ˆë‹¤. í•˜ì§€ë§Œ <code class="language-plaintext highlighter-rouge">Absolute 1D-Position Embedding</code>ì˜ ì»¨ì…‰ì´ ê°€ì¥ ì§ê´€ì ì´ë©° êµ¬í˜„í•˜ê¸° í¸í•˜ê³  ì—°ì‚°ëŸ‰ì´ ë‹¤ë¥¸ ì¸ì½”ë”©ë³´ë‹¤ ì ë‹¤ëŠ” ê²ƒì„ ê°ì•ˆí•˜ë©´ ViTì— ê°€ì¥ íš¨ìœ¨ì ì¸ ìœ„ì¹˜ ì„ë² ë”© ë°©ì‹ì´ë¼ê³  íŒë‹¨í•  ìˆ˜ ìˆë‹¤.</p>

<p>ë…¼ë¬¸ì€ ê²°ê³¼ì— ëŒ€í•´ <code class="language-plaintext highlighter-rouge">ViT</code>ê°€ ì‚¬ìš©í•˜ëŠ” <code class="language-plaintext highlighter-rouge">Patch-Level Embedding</code>ì´ <code class="language-plaintext highlighter-rouge">Pixel-Level</code>ë³´ë‹¤ ìƒëŒ€ì ìœ¼ë¡œ ì§§ì€ ì‹œí€€ìŠ¤ ê¸¸ì´ë¥¼ ê°–ê¸° ë•Œë¬¸ì´ë¼ê³  ì„¤ëª…í•œë‹¤. ì˜ˆë¥¼ ë“¤ì–´ <code class="language-plaintext highlighter-rouge">224x224</code> ì‚¬ì´ì¦ˆì˜ ì´ë¯¸ì§€ë¥¼ <code class="language-plaintext highlighter-rouge">16x16</code> ì‚¬ì´ì¦ˆì˜ íŒ¨ì¹˜ ì—¬ëŸ¬ì¥ìœ¼ë¡œ ë§Œë“ ë‹¤ê³  ìƒê°í•´ë³´ì. ì„ë² ë”© ì°¨ì›ì— ë“¤ì–´ê°€ëŠ” $N$ ì€ $(224/16)^2$ , ì¦‰ <code class="language-plaintext highlighter-rouge">196</code>ì´ ëœë‹¤. í•œí¸ ì´ê²ƒì„ <code class="language-plaintext highlighter-rouge">Pixel-Level</code>ë¡œ ì„ë² ë”© í•˜ê²Œ ë˜ë©´ $224^2$, ì¦‰ <code class="language-plaintext highlighter-rouge">50176</code> ê°œì˜ ì‹œí€€ìŠ¤ê°€ ìƒê¸´ë‹¤. ë”°ë¼ì„œ <code class="language-plaintext highlighter-rouge">Pixel-Level</code> ì— ë¹„í•˜ë©´ í›¨ì”¬ ì§§ì€ ì‹œí€€ìŠ¤ ê¸¸ì´ë¥¼ ê°–ê¸° ë•Œë¬¸ì— <code class="language-plaintext highlighter-rouge">Absolute 1D-Position Embedding</code> ë§Œìœ¼ë¡œë„ ì¶©ë¶„íˆ <code class="language-plaintext highlighter-rouge">Spatial Relation</code>ì„ í•™ìŠµí•  ìˆ˜ ìˆëŠ” ê²ƒì´ë‹¤.</p>

<p align="center">
<img src="/assets/images/vision_transformer/insight5_2.png" alt="Absolute 1D-Position Embedding" class="align-center image-caption" width="50%&quot;, height=&quot;50%" />
<strong><em><a href="https://arxiv.org/abs/2010.11929">Absolute 1D-Position Embedding</a></em></strong>
</p>

<p>í•˜ì§€ë§Œ, í•„ìëŠ” ìì—°ì–´ ì²˜ë¦¬ì˜ <code class="language-plaintext highlighter-rouge">Transformer-XL</code>, <code class="language-plaintext highlighter-rouge">XLNet</code>, <code class="language-plaintext highlighter-rouge">DeBERTa</code> ê°™ì€ ëª¨ë¸ë“¤ì´ <code class="language-plaintext highlighter-rouge">Relative Position Embedding</code> ë°©ì‹ì„ ì ìš©í•´ í° ì„±ê³µì„ ê±°ë‘” ë°”ê°€ ìˆë‹¤ëŠ” ì ì„ ìƒê°í•˜ë©´ ì´ëŸ° ê²°ê³¼ê°€ ë‚©ë“ì´ ê°€ë©´ì„œë„ ì˜ì•„í–ˆë‹¤.</p>

<p>ì €ìëŠ” ì‹¤í—˜ì— ì‚¬ìš©í•œ ëª¨ë“  ë°ì´í„° ì„¸íŠ¸ë¥¼ <code class="language-plaintext highlighter-rouge">224x224</code>ë¡œ <code class="language-plaintext highlighter-rouge">resize</code> í–ˆë‹¤ê³  ë°íˆê³  ìˆëŠ”ë°, ë§Œì•½ ì´ë¯¸ì§€ ì‚¬ì´ì¦ˆê°€ <code class="language-plaintext highlighter-rouge">512x512</code>ì •ë„ë§Œ ë˜ë”ë¼ë„ $N$ ê°’ì´ <code class="language-plaintext highlighter-rouge">1024</code> ì´ë¼ì„œ ìœ„ ê²°ê³¼ì™€ ìƒë‹¹íˆ ë‹¤ë¥¸ ì–‘ìƒì´ ë‚˜íƒ€ë‚˜ì§€ ì•Šì„ê¹Œ í•˜ëŠ” ìƒê°ì´ ë“ ë‹¤. ì¶”í›„ì— ì‹œê°„ì´ ëœë‹¤ë©´ ì´ ë¶€ë¶„ë„ ê¼­ ì‹¤í—˜í•´ë´ì•¼ê² ë‹¤. ì˜ˆì¸¡ì»¨ë° ì´ë¯¸ì ì‚¬ì´ì¦ˆê°€ ì»¤ì§ˆìˆ˜ë¡ <code class="language-plaintext highlighter-rouge">2D Position Embedding</code> í˜¹ì€ <code class="language-plaintext highlighter-rouge">Relative Position Embedding</code>ì´ ë” íš¨ìœ¨ì ì¼ ê²ƒì´ë¼ ì˜ˆìƒí•œë‹¤.</p>

<h3 id="ï¸conclusion"><code class="language-plaintext highlighter-rouge">ğŸ§‘â€âš–ï¸Â Conclusion</code></h3>

<p>ì´ë ‡ê²Œ <code class="language-plaintext highlighter-rouge">ViT</code> ëª¨ë¸ì„ ì œì•ˆí•œ <a href="https://arxiv.org/abs/2010.11929">&lt;An Image Is Worth 16x16 Words: Transformers For Image Recognition At Scale&gt;</a>ì— ì‹¤ë¦° ë‚´ìš©ì„ ëª¨ë‘ ì‚´í´ë³´ì•˜ë‹¤. <code class="language-plaintext highlighter-rouge">Conv</code> ì— ëŒ€í•œ ì˜ì¡´ì„ íƒˆí”¼ í–ˆë‹¤ëŠ” ì ì—ì„œ ë§¤ìš° ì˜ë¯¸ê°€ ìˆëŠ” ì‹œë„ì˜€ìœ¼ë©°, Self-Attention &amp; Transformer êµ¬ì¡° ì±„íƒë§Œìœ¼ë¡œë„ ì»´í“¨í„° ë¹„ì „ ì˜ì—­ì— ì–´ëŠ ì •ë„  <code class="language-plaintext highlighter-rouge">scalability</code> ë¥¼  ì´ì‹í•˜ëŠ”ë° ì„±ê³µí–ˆë‹¤ëŠ” ì ì—ì„œ í›„ëŒ€ ì—°êµ¬ì— ì¤‘ìš”í•œ ì‹œì‚¬ì ì„ ë‚¨ê²¼ë‹¤. ìƒëŒ€ì ìœ¼ë¡œ ì •ì²´(??)ë˜ì–´ ìˆë˜ ë¹„ì „ ì˜ì—­ì´ ì„±ëŠ¥ì˜ í•œê³„ë¥¼ í•œë‹¨ê³„ ë›°ì–´ë„˜ì„ ìˆ˜ ìˆëŠ” ì´ˆì„ì„ ë§ˆë ¨í•´ì¤€ ì…ˆì´ë‹¤.</p>

<p>í•˜ì§€ë§Œ, <code class="language-plaintext highlighter-rouge">ViT</code>ì˜ <code class="language-plaintext highlighter-rouge">Pretrain Stage</code>ì— ì í•©í•œ <code class="language-plaintext highlighter-rouge">Self-Supervised Learning</code> ë°©ë²•ì„ ì°¾ì§€ ëª»í•´ ì—¬ì „íˆ <code class="language-plaintext highlighter-rouge">Supervised Learning</code> ë°©ì‹ì„ ì±„íƒí•œ ì ì€ ë§¤ìš° ì•„ì‰¬ì› ë‹¤. <strong><u>ì´ëŠ” ê²°êµ­ ë°ì´í„°</u></strong> <code class="language-plaintext highlighter-rouge">Scale</code> <strong><u>í™•ì¥ì— í•œê³„ë¥¼ ì˜ë¯¸í•˜ê¸° ë•Œë¬¸ì´ë‹¤.</u></strong> ì˜¤ëŠ˜ë‚  BERTì™€ GPTì˜ ì„±ê³µ ì‹ í™”ëŠ” ë¹„ë‹¨ <code class="language-plaintext highlighter-rouge">Self-Attention</code>ì™€ <code class="language-plaintext highlighter-rouge">Transformer</code>ì˜ êµ¬ì¡°ì  íƒì›”ì„±ì— ì˜í•´ì„œë§Œ íƒ„ìƒí•œê²Œ ì•„ë‹ˆë‹¤. ì´ì— ëª»ì§€ ì•Šê²Œ(ê°œì¸ì ìœ¼ë¡œ ì œì¼ ì¤‘ìš”í•˜ë‹¤ ìƒê°) ì£¼ìš”í–ˆë˜ ê²ƒì´ ë°”ë¡œ ë°ì´í„° <code class="language-plaintext highlighter-rouge">Scale</code> í™•ì¥ì´ë‹¤.  <code class="language-plaintext highlighter-rouge">MLM</code>, <code class="language-plaintext highlighter-rouge">AR</code> ë“±ì˜ <code class="language-plaintext highlighter-rouge">Self-Supervised Learning</code> ë•ë¶„ì— ë°ì´í„° <code class="language-plaintext highlighter-rouge">Scale</code>ì„ íš¨ìœ¨ì ìœ¼ë¡œ ìŠ¤ì¼€ì¼ ì—… ì‹œí‚¬ ìˆ˜ ìˆì—ˆê³ , ì‚¬ì „ í›ˆë ¨ ë°ì´í„°ì˜ ì¦ê°€ëŠ” ëª¨ë¸ ê¹Šì´, ë„ˆë¹„, ì°¨ì›ê¹Œì§€ ë”ìš± í¬ì¼€ í‚¤ìš°ëŠ”ë° ê¸°ì—¬í–ˆë‹¤.</p>

<p>ë˜í•œ <code class="language-plaintext highlighter-rouge">ViT</code>ëŠ” ì„ ì²œì ìœ¼ë¡œ <code class="language-plaintext highlighter-rouge">Patch-Level Embedding</code>ì„ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì— ë‹¤ì–‘í•œ ì´ë¯¸ì§€ í…ŒìŠ¤í¬ì— ì ìš©í•˜ëŠ” ê²ƒì´ í˜ë“¤ë‹¤. <code class="language-plaintext highlighter-rouge">Segmentation</code>, <code class="language-plaintext highlighter-rouge">Object Detection</code> ê°™ì€ TaskëŠ” í”½ì…€ ë‹¨ìœ„ë¡œ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•´ ê°ì²´ë¥¼ íƒì§€í•˜ê±°ë‚˜ ë¶„í• í•´ì•¼ í•œë‹¤. í•˜ì§€ë§Œ <code class="language-plaintext highlighter-rouge">Patch</code> ë‹¨ìœ„ë¡œ í›ˆë ¨ì„ ìˆ˜í–‰í–ˆë˜ <code class="language-plaintext highlighter-rouge">ViT</code>ëŠ” <code class="language-plaintext highlighter-rouge">Pixel</code> ë‹¨ìœ„ì˜ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•˜ëŠ”ë° ì–´ë ¤ì›€ì„ ê²ªëŠ”ë‹¤.</p>

<p>ë§ˆì§€ë§‰ìœ¼ë¡œ <code class="language-plaintext highlighter-rouge">Self-Attention</code> ìì²´ì˜ <code class="language-plaintext highlighter-rouge">Computational Overhead</code>ê°€ ë„ˆë¬´ ì‹¬í•´ ê³ í•´ìƒë„ì˜ ì´ë¯¸ì§€ë¥¼ ì ì ˆíˆ ë‹¤ë£¨ê¸° í˜ë“¤ë‹¤. ìœ„ì—ì„œë„ ì–¸ê¸‰í–ˆì§€ë§Œ ì´ë¯¸ì§€ì˜ ì‚¬ì´ì¦ˆê°€ <code class="language-plaintext highlighter-rouge">512x512</code>ë§Œ ë˜ì–´ë„ ì´ë¯¸ íŒ¨ì¹˜ì˜ ê°œìˆ˜ê°€ <code class="language-plaintext highlighter-rouge">1024</code>ê°€ ëœë‹¤. ì‚¬ì´ì¦ˆê°€ ì»¤ì§ˆìˆ˜ë¡ ì‹œí€€ìŠ¤ ê¸¸ì´ ì—­ì‹œ ê¸°í•˜ê¸‰ìˆ˜ì ìœ¼ë¡œ ì»¤ì§€ëŠ”ë°ë‹¤ê°€ <code class="language-plaintext highlighter-rouge">Self-Attention</code> ëŠ” ì¿¼ë¦¬ì™€ í‚¤ í–‰ë ¬ì„ ë‚´ì  (ìê¸° ìì‹ ê³¼ ê³±ì´ë¼ ë³¼ ìˆ˜ ìˆìŒ) í•˜ê¸° ë•Œë¬¸ì— <code class="language-plaintext highlighter-rouge">Computational Overhead</code>ê°€ $N^2$ì´ ëœë‹¤.</p>

<p>í•„ìëŠ” <code class="language-plaintext highlighter-rouge">ViT</code>ë¥¼ ì ˆë°˜ì˜ ì„±ê³µì´ë¼ê³  í‰í•˜ê³  ì‹¶ë‹¤. ë³¸ë˜ <code class="language-plaintext highlighter-rouge">ViT</code>ì˜ ì„¤ê³„ ëª©ì ì€ ë¹„ì „ ë¶„ì•¼ì˜ <code class="language-plaintext highlighter-rouge">Conv</code>ì— ëŒ€í•œ ì˜ì¡´ì„ íƒˆí”¼í•˜ë©´ì„œ, í“¨ì–´í•œ <code class="language-plaintext highlighter-rouge">Self-Attention</code>ì„ ë„ì…í•´ <code class="language-plaintext highlighter-rouge">Scalabilty</code> ë¥¼ ì´ì‹í•˜ëŠ” ê²ƒì´ì—ˆë‹¤. <code class="language-plaintext highlighter-rouge">Self-Attention</code>ì„ ë„ì…í•˜ëŠ”ë°ëŠ” ì„±ê³µí–ˆì§€ë§Œ, ì—¬ì „íˆ ë‹¤ë£° ìˆ˜ ìˆëŠ” ì´ë¯¸ì§€ ì‚¬ì´ì¦ˆë‚˜ Taskì—ëŠ” í•œê³„ê°€ ë¶„ëª…í•˜ë©° ê²°ì •ì ìœ¼ë¡œ <code class="language-plaintext highlighter-rouge">Self-Supervised Learning</code> ë°©ì‹ì„ ë„ì…í•˜ì§€ ëª»í–ˆë‹¤. <code class="language-plaintext highlighter-rouge">Scalabilty</code> ë¼ëŠ” ë‹¨ì–´ì˜ ì˜ë¯¸ë¥¼ ìƒê°í•˜ë©´, ë°©ê¸ˆ ë§í•œ ë¶€ë¶„ì—ì„œê¹Œì§€ í™•ì¥ì„±ì´ ìˆì–´ì•¼ ì„¤ê³„ ì˜ë„ì— ë¶€í•©í•˜ëŠ” ê²°ê³¼ë¼ê³  ìƒê°í•œë‹¤.</p>]]></content><author><name>qcqced</name><email>qcqced123@gmail.com</email></author><category term="Computer Vision" /><category term="Computer Vision" /><category term="Vision Transformer" /><category term="ViT" /><category term="Transformer" /><category term="Self-Attention" /><category term="Image Classification" /><summary type="html"><![CDATA[ViT Official Paper Review with Pytorch Implementation]]></summary></entry><entry><title type="html">ğŸ”¢Â Vector Space: Column Space, Basis, Rank, Null Space</title><link href="http://localhost:4000/linear-algebra/vector-subspace" rel="alternate" type="text/html" title="ğŸ”¢Â Vector Space: Column Space, Basis, Rank, Null Space" /><published>2023-07-19T00:00:00+09:00</published><updated>2023-07-10T13:00:00+09:00</updated><id>http://localhost:4000/linear-algebra/vector_space</id><content type="html" xml:base="http://localhost:4000/linear-algebra/vector-subspace"><![CDATA[<h3 id="-column-space"><code class="language-plaintext highlighter-rouge">ğŸ”¢ Column Space</code></h3>

\[C(A) = Range(A)\]

<p>ì—´ë²¡í„°ê°€ <code class="language-plaintext highlighter-rouge">span</code>í•˜ëŠ” ê³µê°„ì„ ì˜ë¯¸í•œë‹¤. <code class="language-plaintext highlighter-rouge">span</code> ì´ë€, ë²¡í„°ì˜ ì§‘í•©ì— ì˜í•´ ìƒì„±ëœ ëª¨ë“  <code class="language-plaintext highlighter-rouge">linear combination</code>ì˜ ê²°ê³¼ë¡œ ìƒì„±í•  ìˆ˜ ìˆëŠ” ë¶€ë¶„ ê³µê°„ì„ ë§í•œë‹¤. ë”°ë¼ì„œ <code class="language-plaintext highlighter-rouge">column space</code> ëŠ” ì—´ë²¡í„°ì˜ <code class="language-plaintext highlighter-rouge">linear combination</code> ê²°ê³¼ë¡œ ìƒì„±í•  ìˆ˜ ìˆëŠ” <code class="language-plaintext highlighter-rouge">vector space</code>ì˜ ë¶€ë¶„ ê³µê°„ì„ ë§í•œë‹¤.</p>
<h3 id="-basis"><code class="language-plaintext highlighter-rouge">ğŸ– Basis</code></h3>
<figure class="half">
  <a href="https://twlab.tistory.com/24"><img src="/assets/images/linear_independent.png" title="Linear Independent" /></a>
  <a href="https://twlab.tistory.com/24"><img src="/assets/images/linear_dependent.png" title="Linear Independent" /></a>
</figure>
<p>ê¸°ì €ì— ëŒ€í•´ ì•Œê¸° ìœ„í•´ì„œëŠ” ë¨¼ì € <code class="language-plaintext highlighter-rouge">linear independent(ì„ í˜• ë…ë¦½)</code>ì˜ ì˜ë¯¸ë¥¼ ì•Œì•„ì•¼ í•œë‹¤. ì„ í˜•ë…ë¦½ì´ë€, ì™¼ìª½ ê·¸ë¦¼ì²˜ëŸ¼ ì„œë¡œ ë‹¤ë¥¸ ë²¡í„°ë“¤ì´ ê´€ë ¨ì„± ì—†ì´ ë…ë¦½ì ìœ¼ë¡œ ì¡´ì¬í•˜ëŠ” ìƒíƒœë¥¼ ë§í•œë‹¤. ë”°ë¼ì„œ ì„œë¡œ ë‹¤ë¥¸ ë‘ ë²¡í„°ê°€ ì„ í˜• ë…ë¦½ì´ë¼ë©´ í•œ ë²¡í„°ì˜ ì„ í˜•ì¡°í•©ìœ¼ë¡œ ë‹¤ë¥¸ ë²¡í„°ë¥¼ í‘œí˜„í•  ìˆ˜ ì—†ë‹¤. ë°˜ëŒ€ë¡œ ì„ í˜• ì¢…ì† ìƒíƒœë©´ ì˜¤ë¥¸ìª½ ê·¸ë¦¼ì²˜ëŸ¼ ë²¡í„°ë¥¼ ë‹¤ë¥¸ ë²¡í„°ì˜ ì„ í˜•ì¡°í•©ìœ¼ë¡œ í‘œí˜„ ê°€ëŠ¥í•˜ë‹¤.</p>

<p>ì´ì œ ê¸°ì €ì— ëŒ€í•´ ì•Œì•„ë³´ì. ê¸°ì €ë€ ì„ í˜• ë…ë¦½ì´ë©´ì„œ ë²¡í„° ê³µê°„ì„ <code class="language-plaintext highlighter-rouge">span</code> í•˜ëŠ” ë²¡í„° ì§‘í•©ì„ ë§í•œë‹¤. ë‹¤ì‹œ ë§í•´, ê³µê°„ ë˜ëŠ” ì°¨ì›ì„ í‘œí˜„í•˜ëŠ”ë° í•„ìš”í•œ ìš”ì†Œë“¤ì˜ ì§‘í•©ì´ë¼ê³  ë³¼ ìˆ˜ ìˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ 2ì°¨ì› ê³µê°„ì„ í‘œí˜„í•˜ê³  ì‹¶ë‹¤ë©´ ì„œë¡œ ì„ í˜• ë…ë¦½ì¸ ë²¡í„° 2ê°œê°€ í•„ìš”í•˜ë‹¤. ì˜¤ë¥¸ìª½ ê·¸ë¦¼ì²˜ëŸ¼ ë²¡í„° 2ê°œê°€ ì¡´ì¬í•´ë„ ì„œë¡œ ì¢…ì† ê´€ê³„ë¼ë©´ í‘œí˜„(span)í•  ìˆ˜ ìˆëŠ” ê³µê°„ì€ 1ì°¨ì›ì˜ ì§ì„ ì´ ë˜ê¸° ë•Œë¬¸ì´ë‹¤. ì •ë¦¬í•˜ë©´, $N$ì°¨ì› ê³µê°„ì˜ ê¸°ì €ë€ ì„ í˜• ë…ë¦½ì´ë©´ì„œ ë²¡í„° ê³µê°„ì„ <code class="language-plaintext highlighter-rouge">span</code>í•˜ëŠ” ë²¡í„°ê°€ $N$ê°œ ìˆëŠ” ìƒíƒœë‹¤. ì¶”ê°€ë¡œ, $N$ì°¨ì› ê³µê°„ì˜ ê¸°ì €ëŠ”<code class="language-plaintext highlighter-rouge">NxN</code> í¬ê¸°ì˜ <code class="language-plaintext highlighter-rouge">Invertable</code>í•œ í–‰ë ¬ê³¼ ë™ì¹˜ë¥¼ ì´ë£¬ë‹¤. ë’¤ì—ì„œ ë” ìì„¸íˆ ë‹¤ë£¨ê² ì§€ë§Œ ì—­í–‰ë ¬ì€ ì¢Œí‘œí‰ë©´ ìƒì—ì„œ <code class="language-plaintext highlighter-rouge">reverse linear combination</code> ì˜ ì—­í• ì„ í•˜ê¸° ë•Œë¬¸ì´ë‹¤.<br />
í•œí¸ ê¸°ì €ëŠ” ìœ ì¼í•˜ì§€ ì•Šë‹¤. ìœ„ì—ì„œ ì–¸ê¸‰í•œ $N$ì°¨ì› ê¸°ì €ì˜ í•„ìš”ì¶©ë¶„ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” ëª¨ë“  ë²¡í„° ì§‘í•©ì€ ëª¨ë‘ ê¸°ì €ê°€ ë  ìˆ˜ ìˆë‹¤.</p>

<h3 id="-standard-basis"><code class="language-plaintext highlighter-rouge">ğŸ¦´ Standard Basis</code></h3>

\[I= 
   \begin{pmatrix} 
   1 &amp; 0 &amp; 0  \\
   0 &amp; 1 &amp; 0  \\
   0 &amp; 0 &amp; 1  \\
   \end{pmatrix}\]

<p>í‘œì¤€ ê¸°ì €ë€, ê¸°ì €ê°€ í‘œí˜„í•˜ëŠ” ì°¨ì›ì˜ ì¶•ì´ ìš°ë¦¬ê°€ í”íˆ ì•„ëŠ” <code class="language-plaintext highlighter-rouge">xì¶•, yì¶•, zì¶•</code> ì´ ë˜ëŠ” ê¸°ì € ë²¡í„°ë¥¼ ë§í•œë‹¤. ìˆ˜í•™ì ìœ¼ë¡œëŠ” ì£¼ëŒ€ê°ì„±ë¶„ì˜ ê°’ì´ ëª¨ë‘ 1ì¸ ëŒ€ê°í–‰ë ¬ $D$, ì¦‰ ë‹¨ìœ„ í–‰ë ¬ $I$ê°€ ê¸°ì €ì¼ ë•Œ ìš°ë¦¬ëŠ” í‘œì¤€ ê¸°ì €ë¼ê³  ì •ì˜í•œë‹¤.</p>

<h3 id="-rank"><code class="language-plaintext highlighter-rouge">ğŸ§® Rank</code></h3>

<p align="center">
<img src="/assets/images/column_space.png" alt="Column Space Image" class="align-center image-caption" width="100%&quot;, height=&quot;50%" />
<strong><em><a href="https://www.researchgate.net/figure/Example-of-a-projection-of-a-matrix-3-2-on-the-column-space_fig2_220103928">Column Space Image</a></em></strong>
</p>

<p>í–‰ë ¬ì—ì„œ <code class="language-plaintext highlighter-rouge">independent</code>í•œ <code class="language-plaintext highlighter-rouge">column</code>ì˜ ê°œìˆ˜ë¥¼ ì˜ë¯¸í•˜ë©°, ê¸°í•˜í•™ì ìœ¼ë¡œëŠ” <code class="language-plaintext highlighter-rouge">column space</code>ê°€ ì‹¤ì œ <code class="language-plaintext highlighter-rouge">span</code>í•˜ëŠ” ê³µê°„ì˜ ì°¨ì›ì„ ë§í•œë‹¤. <code class="language-plaintext highlighter-rouge">Rank Theorem</code> ì— ì˜í•´, í–‰ë ¬ $A$ column vectorëŠ” í–‰ë ¬ $A^T$ì˜ row vectorì™€ ê°™ë‹¤. ë”°ë¼ì„œ column rankì™€ row rank ê°’ ì—­ì‹œ í•­ìƒ ë™ì¼í•˜ë‹¤. í–‰ë ¬ $A$ì˜ ë­í¬ëŠ” $rank(A)$ë¡œ í‘œê¸°í•œë‹¤.</p>

<p>í–‰ë ¬ì˜ ë­í¬ëŠ” í–‰ë ¬ì˜ ìƒê¹€ìƒˆì— ë”°ë¼ ë¶€ë¥´ëŠ” ëª…ì¹­ì´ ì¡°ê¸ˆì”© ë°”ë€ë‹¤. ì˜ˆë¥¼ ë“¤ì–´ ì—´ë²¡í„°ê°€ ëª¨ë‘ ì„ í˜• ë…ë¦½ì´ë©´ì„œ í¬ê¸°ê°€ <code class="language-plaintext highlighter-rouge">10x3</code> ì¸ í–‰ë ¬ $C$ê°€ ìˆë‹¤ê³  ê°€ì •í•´ë³´ì. ëª¨ë“  ì—´ë²¡í„°ê°€ ì„ í˜• ë…ë¦½ì´ê¸° ë•Œë¬¸ì— ìš°ë¦¬ëŠ” í–‰ë ¬ $C$ì˜ ë­í¬ê°€ 3ì´ë¼ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤. ì´ ë•Œ í–‰ë ¬ $C$ë¥¼  <code class="language-plaintext highlighter-rouge">full-column rank</code> ë¼ê³  ë¶€ë¥¸ë‹¤. ê·¸ë¦¬ê³  í–‰ë²¡í„°ì˜ ë­í¬ ì—­ì‹œ ë­í¬ ì •ë¦¬ ì´ë¡ ì— ì˜í•´ 3ì´ ë  ê²ƒì´ë‹¤. ì´ë²ˆì—ëŠ” í–‰ë ¬ $C$ì˜ ì—´ë²¡í„° ë­í¬ê°€ 2ë¼ê³  ê°€ì •í•´ë³´ì. ìš°ë¦¬ëŠ” ì´ ë•Œ í–‰ë ¬ $C$ë¥¼ <code class="language-plaintext highlighter-rouge">rank-deficient</code>ë¡œ ì •ì˜í•œë‹¤. ë§Œì•½ í–‰ë ¬ $C$ì˜ ì—´ë²¡í„°ê°€ ëª¨ë‘ ì„ í˜•ë…ë¦½ì´ê³  ê·¸ í¬ê¸°ê°€ <code class="language-plaintext highlighter-rouge">10x10</code>ì´ë¼ë©´ ë­ë¼ê³  ë¶€ë¥¼ê¹Œ?? ì´ ë•ŒëŠ” ì—´ë²¡í„°, í–‰ë²¡í„° ëª¨ë‘ ë­í¬ê°€ 10ì´ ë˜ê¸° ë•Œë¬¸ì— <code class="language-plaintext highlighter-rouge">full-rank</code> ë¼ê³  ë¶€ë¥¸ë‹¤.</p>

<p>ì •ë¦¬í•˜ë©´ í–‰ë ¬ì˜ ë­í¬ë€, í–‰ë ¬ì˜ í–‰ì˜ í¬ê¸° M ê·¸ë¦¬ê³  ì—´ì˜ í¬ê¸° N ì¤‘ì—ì„œ ë” ì‘ì€ê°’ë³´ë‹¤ ê°™ê±°ë‚˜ ì‘ìœ¼ë©´ì„œ <code class="language-plaintext highlighter-rouge">independent</code>í•œ <code class="language-plaintext highlighter-rouge">column</code>ì˜ ê°œìˆ˜ë¼ëŠ” ì˜ë¯¸ë¥¼ ë‚´í¬í•œ ê°œë…ì´ë¼ê³  ë³¼ ìˆ˜ ìˆê² ë‹¤.</p>

<p>ì¶”ê°€ë¡œ, column vectorì™€ row vectorë¥¼ ìˆœì„œëŒ€ë¡œ ê³±í•˜ë©´ í•­ìƒ $Rank = 1$ì¸ í–‰ë ¬ $A$ê°€ ë§Œë“¤ì–´ì§„ë‹¤ëŠ” ê²ƒì´ë‹¤. ê·¸ë ‡ê²Œ ë§Œë“¤ì–´ì§„ í–‰ë ¬ì˜ ì›ì†Œê°€ ë‘ ë²¡í„°ì˜ <code class="language-plaintext highlighter-rouge">linear combination</code>  ìœ¼ë¡œ êµ¬ì„±ëœ ê²ƒì´ë¼ì„œ ë‹¹ì—°í•œ ì†Œë¦¬ë¼ê³  ìƒê°í•  ìˆ˜ ìˆì§€ë§Œ, ì´ê²ƒì€ ì„ í˜•ëŒ€ìˆ˜í•™ì—ì„œ ë§¤ìš° ì¤‘ìš”í•œ ì„±ì§ˆì´ ëœë‹¤. ë’¤ì§‘ì–´ì„œ ë³´ë©´ ì–´ë–¤ í–‰ë ¬ì˜ $Rank=1$ì´ë¼ëŠ” ê²ƒì€ ê·¸ í–‰ë ¬ì´ ì–´ë–¤ ë‹¤ë¥¸ í–‰ë ¬ì˜ ê¸°ë³¸ ë‹¨ìœ„ ìš”ì†Œê°€ ëœë‹¤ëŠ” ì˜ë¯¸ì´ê¸° ë•Œë¬¸ì´ë‹¤. ì–´ë–¤ í–‰ë ¬ì˜ ë­í¬ê°€ 4ë¼ëŠ” ê²ƒì€ ë­í¬ 1ì§œë¦¬ í–‰ë ¬ 4ê°œì˜ ì¡°í•©ì´ë¼ê³  ìƒê°í•´ë³¼ ìˆ˜ ìˆë‹¤.</p>

<h3 id="-null-space"><code class="language-plaintext highlighter-rouge">ğŸ‘Œ Null Space</code></h3>

\[Ax=0\]

<p>ìœ„ ìˆ˜ì‹ì„ ë§Œì¡±í•˜ëŠ” ë²¡í„° $x$ì˜ ì§‘í•©ì„ ë§í•œë‹¤. ë‹¤ì‹œ ë§í•´, ì„ í˜• ë³€í™˜ $A$(í¬ê¸°ê°€ MxNì¸ í–‰ë ¬)ë¥¼ í†µí•´ 0ì´ ë˜ëŠ” ë²¡í„° ì§‘í•© $x$ê°€ ë°”ë¡œ <code class="language-plaintext highlighter-rouge">null space(ì˜ê³µê°„)</code>ì´ë‹¤. ì˜ê³µê°„ì€ ì„ í˜•ë³€í™˜ $A$ì˜ ë­í¬ì™€ ë¬´ê´€í•˜ë©° ì„ í˜•ë³€í™˜ Aì˜ ì—´ì°¨ì›ì¸ $R^N$ìƒì— ì¡´ì¬í•˜ëŠ” ê³µê°„ì´ë‹¤. ê·¸ë˜ì„œ $Ax=0$ì„ í–‰ë ¬ê³¼ ë²¡í„°ì˜ ë‚´ì ìœ¼ë¡œ í•´ì„í•˜ë©´ ì˜ê³µê°„ì€ ì„ í˜•ë³€í™˜ $A$ì˜ row spaceì™€ ìˆ˜ì§ì´ë‹¤ë¼ëŠ” ì‚¬ì‹¤ì„ ì•Œ ìˆ˜ ìˆë‹¤.</p>

\[N_A = dim(Null(A)) - rank(A)\]

<p>í•œí¸, ì˜ê³µê°„ì´ <code class="language-plaintext highlighter-rouge">span</code> í•˜ëŠ” ê³µê°„ì˜ ì°¨ì›ê³¼ ì…˜í˜•ë³€í™˜ $A$ì˜ ë­í¬ë¥¼ ë”í•˜ë©´ ì…˜í˜•ë³€í™˜ $A$ì˜ ì—´ì°¨ì›ì„ ì•Œ ìˆ˜ ìˆë‹¤. ìˆ˜ì‹ìœ¼ë¡œ í‘œí˜„í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.</p>

<h3 id="-left-null-space"><code class="language-plaintext highlighter-rouge">ğŸ«² Left Null Space</code></h3>

\[A^Tx=0\]

<p>ì„ í˜•ë³€í™˜ $A$ì˜ í¬ê¸°ê°€ MxNì¼ ë•Œ, $A$ì˜ ì¢Œ ì˜ê³µê°„ì€ $A$ì˜ ëª¨ë“  ì—´ë“¤ì— ëŒ€í•´ ì„ í˜• ì¡°í•©ìœ¼ë¡œ 0 ë²¡í„°(ì˜ë²¡í„°)ê°€ ë˜ëŠ” ëª¨ë“  ë²¡í„° ì§‘í•© $x$ì˜ ê³µê°„ì„ <code class="language-plaintext highlighter-rouge">ì¢Œì˜ê³µê°„</code>ì´ë¼ê³  í•œë‹¤. $A$ì˜ ì—´ë²¡í„°ì— ëŒ€í•œ ì˜ê³µê°„ì´ë¼ëŠ” ê²ƒì´ í¬ì¸íŠ¸ê°€ ëœë‹¤. ë”°ë¼ì„œ ì¢Œì˜ê³µê°„ì€ ì„ í˜•ë³€í™˜ $A$ì˜ ì „ì¹˜í–‰ë ¬ì¸ $A^T$ì˜ ì˜ê³µê°„ì„ êµ¬í•˜ëŠ” ê²ƒê³¼ ê°™ìœ¼ë©°, ì„ í˜•ë³€í™˜ $A$ì˜ column spaceì™€ ìˆ˜ì§í•˜ê²Œ ëœë‹¤.</p>]]></content><author><name>qcqced</name><email>qcqced123@gmail.com</email></author><category term="Linear Algebra" /><category term="Linear Algebra" /><category term="linear independent" /><category term="vector space" /><category term="rank" /><category term="column space" /><category term="null space" /><category term="basis" /><summary type="html"><![CDATA[ğŸ’¡ Concept of main sub-space]]></summary></entry><entry><title type="html">ğŸ² RuntimeError: CUDA error: device-side assert triggered</title><link href="http://localhost:4000/framework-library/mismatch-dimension" rel="alternate" type="text/html" title="ğŸ² RuntimeError: CUDA error: device-side assert triggered" /><published>2023-07-17T00:00:00+09:00</published><updated>2023-07-18T07:00:00+09:00</updated><id>http://localhost:4000/framework-library/dim_mismatch</id><content type="html" xml:base="http://localhost:4000/framework-library/mismatch-dimension"><![CDATA[<h3 id="-ì‚¬ì „ì—-ì •ì˜-ì…ì¶œë ¥-ì°¨ì›--ì‹¤ì œ-ì…ì¶œë ¥-ì°¨ì›"><code class="language-plaintext highlighter-rouge">ğŸ˜µ ì‚¬ì „ì— ì •ì˜ ì…ì¶œë ¥ ì°¨ì› â‰  ì‹¤ì œ ì…ì¶œë ¥ ì°¨ì›</code></h3>

<p>ë‹¤ì–‘í•œ ì›ì¸ì´ ìˆë‹¤ê³  ì•Œë ¤ì ¸ ìˆëŠ” ì—ëŸ¬ì§€ë§Œ, í•„ìì˜ ê²½ìš° ìœ„ ì—ëŸ¬ëŠ” ì‚¬ì „ì— ì •ì˜í•œ ë°ì´í„°ì˜ ì…ì¶œë ¥ ì°¨ì›ê³¼ ì‹¤ì œ ì…ì¶œë ¥ ë°ì´í„° ì°¨ì›ì´ ì„œë¡œ ìƒì´í•  ë•Œ ë°œìƒí–ˆë‹¤. í•˜ì§€ë§Œ ì›ì¸ì„ í™•ì‹¤íˆ íŠ¹ì •í•˜ê³  ì‹¶ë‹¤ë©´ ì•„ë˜ ì˜ˆì‹œ ì½”ë“œë¥¼ ë¨¼ì € ì¶”ê°€í•œ ë’¤, ë‹¤ì‹œ í•œ ë²ˆ ì—ëŸ¬ ë¡œê·¸ë¥¼ í™•ì¸í•´ë³´ê¸¸ ê¶Œì¥í•œë‹¤.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">os</span>
<span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="s">'CUDA_LAUNCH_BLOCKING'</span><span class="p">]</span> <span class="o">=</span> <span class="s">"1"</span>
<span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="s">"CUDA_VISIBLE_DEVICES"</span><span class="p">]</span> <span class="o">=</span> <span class="s">"0"</span>
</code></pre></div></div>
<p>ì˜ˆì‹œ ì½”ë“œì²˜ëŸ¼ í™˜ê²½ë³€ìˆ˜ë¥¼ ì¶”ê°€í•˜ë©´ ì—ëŸ¬ê°€ ì–´ëŠ ë¶€ë¶„ì—ì„œ ë°œìƒí–ˆëŠ”ì§€ ë¡œê·¸ê°€ ì¢€ ë” êµ¬ì²´ì ìœ¼ë¡œ ë‚˜ì˜¨ë‹¤. ê±°ì˜ ëŒ€ë¶€ë¶„ì´ ì…ì¶œë ¥ ì°¨ì› ë¬¸ì œì¼í…Œë‹ˆ ê·€ì°®ìœ¼ë©´ ë°”ë¡œ ì°¨ì›ì„ ìˆ˜ì •í•˜ë„ë¡ í•˜ì.</p>]]></content><author><name>qcqced</name><email>qcqced123@gmail.com</email></author><category term="Framework &amp; Library" /><category term="Pytorch" /><category term="Dimension Mismatch" /><category term="CUDA" /><category term="Error Handling" /><summary type="html"><![CDATA[Pytorch Error: Mis-match between pre-defined dimension and input dimension]]></summary></entry><entry><title type="html">ğŸ² RuntimeError: CUDA error: CUBLAS_STATUS_ALLOC_FAILED when callingÂ cublasCreate(handâ‰¤)</title><link href="http://localhost:4000/framework-library/mismatch-embedding" rel="alternate" type="text/html" title="ğŸ² RuntimeError: CUDA error: CUBLAS_STATUS_ALLOC_FAILED when callingÂ cublasCreate(handâ‰¤)" /><published>2023-07-17T00:00:00+09:00</published><updated>2023-07-18T02:00:00+09:00</updated><id>http://localhost:4000/framework-library/embedding_mismatch</id><content type="html" xml:base="http://localhost:4000/framework-library/mismatch-embedding"><![CDATA[<h3 id="-nnembedding-ì°¨ì›--ì‹¤ì œ-ë°ì´í„°-ì…ë ¥-ì°¨ì›"><code class="language-plaintext highlighter-rouge">ğŸ˜µ nn.Embedding ì°¨ì› â‰  ì‹¤ì œ ë°ì´í„° ì…ë ¥ ì°¨ì›</code></h3>
<p><code class="language-plaintext highlighter-rouge">torch.nn.Embedding</code>ì—ì„œ ì •ì˜í•œ ì…ì¶œë ¥ ì°¨ì›ê³¼ ì‹¤ì œ ë°ì´í„°ì˜ ì°¨ì›ì´ ë‹¤ë¥¸ ê²½ìš°ì— ë°œìƒí•˜ëŠ” ì—ëŸ¬ë‹¤. ë‹¤ì–‘í•œ ìƒí™©ì—ì„œ ë§ˆì£¼í•  ìˆ˜ ìˆëŠ” ì—ëŸ¬ì§€ë§Œ, í•„ìì˜ ê²½ìš° <code class="language-plaintext highlighter-rouge">Huggingface</code>ì—ì„œ ë¶ˆëŸ¬ì˜¨<code class="language-plaintext highlighter-rouge">pretrained tokenizer</code>ì— <code class="language-plaintext highlighter-rouge">special token</code> ì„ ì¶”ê°€í•´ ì‚¬ìš©í•  ë•Œ, í† í°ì„ ì¶”ê°€í–ˆë‹¤ëŠ” ì‚¬ì‹¤ì„ ìŠê³  <code class="language-plaintext highlighter-rouge">nn.Embedding</code> ì— ì •ì˜í•œ ì…ì¶œë ¥ ì°¨ì›ì„ ë³€ê²½í•˜ì§€ ì•Šì•„ì„œ ë°œìƒí•˜ëŠ” ê²½ìš°ê°€ ë§ì•˜ë‹¤.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoConfig</span><span class="p">,</span> <span class="n">AutoModel</span>

<span class="k">class</span> <span class="nc">CFG</span><span class="p">:</span>
    <span class="n">model_name</span> <span class="o">=</span> <span class="s">'microsoft/deberta-v3-large'</span>
    <span class="n">config</span> <span class="o">=</span> <span class="n">AutoConfig</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>
    <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">AutoModel</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">add_markdown_token</span><span class="p">(</span><span class="n">cfg</span><span class="p">:</span> <span class="n">sCFG</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
    <span class="s">"""
    Add MarkDown token to pretrained tokenizer ('[MD]')
    Args:
        cfg: CFG, needed to load tokenizer from Huggingface AutoTokenizer
    """</span>
    <span class="n">markdown_token</span> <span class="o">=</span> <span class="s">'[MD]'</span>
    <span class="n">special_tokens_dict</span> <span class="o">=</span> <span class="p">{</span><span class="s">'additional_special_tokens'</span><span class="p">:</span> <span class="p">[</span><span class="sa">f</span><span class="s">'</span><span class="si">{</span><span class="n">markdown_token</span><span class="si">}</span><span class="s">'</span><span class="p">]}</span>
    <span class="n">cfg</span><span class="p">.</span><span class="n">tokenizer</span><span class="p">.</span><span class="n">add_special_tokens</span><span class="p">(</span><span class="n">special_tokens_dict</span><span class="p">)</span>
    <span class="n">markdown_token_id</span> <span class="o">=</span> <span class="n">cfg</span><span class="p">.</span><span class="n">tokenizer</span><span class="p">(</span><span class="sa">f</span><span class="s">'</span><span class="si">{</span><span class="n">markdown_token</span><span class="si">}</span><span class="s">'</span><span class="p">,</span> <span class="n">add_special_tokens</span><span class="o">=</span><span class="bp">False</span><span class="p">)[</span><span class="s">'input_ids'</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>

    <span class="nb">setattr</span><span class="p">(</span><span class="n">cfg</span><span class="p">.</span><span class="n">tokenizer</span><span class="p">,</span> <span class="s">'markdown_token'</span><span class="p">,</span> <span class="sa">f</span><span class="s">'</span><span class="si">{</span><span class="n">markdown_token</span><span class="si">}</span><span class="s">'</span><span class="p">)</span>
    <span class="nb">setattr</span><span class="p">(</span><span class="n">cfg</span><span class="p">.</span><span class="n">tokenizer</span><span class="p">,</span> <span class="s">'markdown_token_id'</span><span class="p">,</span> <span class="n">markdown_token_id</span><span class="p">)</span>
    <span class="n">cfg</span><span class="p">.</span><span class="n">tokenizer</span><span class="p">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="sa">f</span><span class="s">'</span><span class="si">{</span><span class="n">cfg</span><span class="p">.</span><span class="n">checkpoint_dir</span><span class="si">}</span><span class="s">/tokenizer/'</span><span class="p">)</span>


<span class="n">add_markdown_token</span><span class="p">(</span><span class="n">CFG</span><span class="p">)</span>
<span class="n">CFG</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">resize_token_embeddings</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">))</span>
</code></pre></div></div>
<p>êµ¬ê¸€ë§í•´ë³´ë‹ˆ í•´ê²°í•˜ëŠ” ë°©ë²•ì€ ë‹¤ì–‘í•œ ê²ƒ ê°™ì€ë°, <code class="language-plaintext highlighter-rouge">torch.nn.Embedding</code>ì— ì •ì˜ëœ ì…ì¶œë ¥ ì°¨ì›ì„ ì‹¤ì œ ë°ì´í„° ì°¨ì›ê³¼ ë§ì¶°ì£¼ë©´ ê°„ë‹¨í•˜ê²Œ í•´ê²°ëœë‹¤. í•„ìì²˜ëŸ¼ <code class="language-plaintext highlighter-rouge">special token</code> ì„ ì¶”ê°€í•´ ì‚¬ìš©í•˜ë‹¤ í•´ë‹¹ ì—ëŸ¬ê°€ ë°œìƒí•˜ëŠ” ìƒí™©ì´ë¼ë©´ ìƒˆë¡œìš´ í† í°ì´ ì¶”ê°€ëœ í† í¬ë‚˜ì´ì €ì˜ ê¸¸ì´ë¥¼ ë‹¤ì‹œ ì¸¡ì •í•œ ë’¤ ê°’ì„ <code class="language-plaintext highlighter-rouge">resize_token_embeddings</code> ë©”ì„œë“œì— ì „ë‹¬í•´ <code class="language-plaintext highlighter-rouge">nn.Embedding</code>ì„ ì—…ë°ì´íŠ¸ í•´ì£¼ë©´ ëœë‹¤.</p>]]></content><author><name>qcqced</name><email>qcqced123@gmail.com</email></author><category term="Framework &amp; Library" /><category term="Pytorch" /><category term="Dimension Mismatch" /><category term="nn.Embedding" /><category term="CUDA" /><category term="Error Handling" /><summary type="html"><![CDATA[Pytorch Error: Mis-match between pre-defined dimension and input dimension]]></summary></entry><entry><title type="html">ğŸ¤” RuntimeError: Function â€˜LogSoftmaxBackward0â€™ returned nan values in its 0th output</title><link href="http://localhost:4000/framework-library/backward-nan/" rel="alternate" type="text/html" title="ğŸ¤” RuntimeError: Function â€˜LogSoftmaxBackward0â€™ returned nan values in its 0th output" /><published>2023-07-11T00:00:00+09:00</published><updated>2023-07-11T13:00:00+09:00</updated><id>http://localhost:4000/framework-library/backward-nan</id><content type="html" xml:base="http://localhost:4000/framework-library/backward-nan/"><![CDATA[<h3 id="-pytorch-backward-ê³¼ì •ì—ì„œ-nan-ë°œìƒí•˜ëŠ”-ë¬¸ì œ"><code class="language-plaintext highlighter-rouge">ğŸ”¥ Pytorch Backward ê³¼ì •ì—ì„œ NaN ë°œìƒí•˜ëŠ” ë¬¸ì œ</code></h3>

<p>ì»¤ìŠ¤í…€ìœ¼ë¡œ ëª¨ë¸, ì—¬ëŸ¬ í’€ë§, ë§¤íŠ¸ë¦­, ì†ì‹¤ í•¨ìˆ˜ë“¤ì„ ì •ì˜í•˜ë©´ì„œë¶€í„° ì œì¼ ë§ì´ ë§ˆì£¼í•˜ê²Œ ë˜ëŠ” ì—ëŸ¬ë‹¤. ì§„ì‹¬ìœ¼ë¡œ ìš”ì¦˜ <code class="language-plaintext highlighter-rouge">CUDA OOM</code> ë³´ë‹¤ í›¨ì”¬ ìì£¼ ë³´ëŠ” ê²ƒ ê°™ë‹¤. í•´ë‹¹ ì—ëŸ¬ëŠ” <code class="language-plaintext highlighter-rouge">LogSoftmax</code> ë ˆì´ì–´ì— ì „ë‹¬ëœ ì…ë ¥ê°’ ì¤‘ì—ì„œ <code class="language-plaintext highlighter-rouge">nan</code>, <code class="language-plaintext highlighter-rouge">inf</code> ê°€ í¬í•¨ë˜ì–´ ì—°ì‚°ì„ ì§„í–‰í•  ìˆ˜ ì—†ë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•œë‹¤. ë”¥ëŸ¬ë‹ ì‹¤í—˜ì„ ì§„í–‰í•˜ë©´ì„œ ê°€ì¥ í•´ê²°í•˜ê¸° ê¹Œë‹¤ë¡œìš´ ë…€ì„ìœ¼ë¡œ ì›ì¸ì„ íŠ¹ì •í•˜ê¸° í˜ë“¤ê¸° ë•Œë¬¸ì´ë‹¤. ì›ì¸ì„ ì¡ê¸° ì–´ë ¤ìš´ ì´ìœ ëŠ” ë°”ë¡œ ìš°ë¦¬ê°€ ì§€ê¸ˆ í•˜ê³  ìˆëŠ”ê²Œ <code class="language-plaintext highlighter-rouge">â€˜ë”¥ëŸ¬ë‹â€™</code> ì´ë¼ì„œ ê·¸ë ‡ë‹¤. ìœ„ ì—ëŸ¬ëŠ” ëŒ€ë¶€ë¶„ ì—°ì‚°ìê°€ ìš°ë¦¬ê°€ ì˜ë„í•˜ì§€ ì•Šì€ ë™ì‘ì„ í•˜ëŠ” ì¼€ì´ìŠ¤ ë•Œë¬¸ì¸ë°, í•˜ë‚˜ í•˜ë‚˜ ë””ë²„ê¹…í•˜ê¸°ì—ëŠ” ë„ˆë¬´ë‚˜ë„ ì—°ì‚°ìê°€ ë§ë‹¤. ë˜í•œ ë”¥ëŸ¬ë‹ì€ ì…ì¶œë ¥ìœ¼ë¡œ ì—„ì²­ë‚˜ê²Œ í° ì‚¬ì´ì¦ˆì˜ í–‰ë ¬ì„ ì‚¬ìš©í•œë‹¤. ìš°ë¦¬ê°€ <code class="language-plaintext highlighter-rouge">nan</code>, <code class="language-plaintext highlighter-rouge">inf</code> ê°’ ì¡´ì¬ì— ëŒ€í•´ì„œ ì¸ì§€í•˜ê¸° ì‰½ì§€ ì•Šë‹¤.</p>

<p><strong><u>ìœ„ ì—ëŸ¬ëŠ” í•„ìì˜ ê²½í—˜ìƒ ëŒ€ë¶€ë¶„ ì»¤ìŠ¤í…€ìœ¼ë¡œ ì •ì˜í•œ ë ˆì´ì–´ì—ì„œ ë°œìƒí•˜ëŠ” ê²½ìš°ê°€ ë§ì•˜ìœ¼ë©° íŠ¹íˆ</u></strong> <code class="language-plaintext highlighter-rouge">ë¶„ìˆ˜</code>, <code class="language-plaintext highlighter-rouge">ê°ë„</code>, <code class="language-plaintext highlighter-rouge">ì œê³±ê·¼</code>, <code class="language-plaintext highlighter-rouge">ì§€ìˆ˜</code> <strong><u>ê°œë…ì„ ì‚¬ìš©í•˜ëŠ” ì—°ì‚°ìê°€ ëŒ€ë¶€ë¶„ ì›ì¸ì´ì—ˆë‹¤.</u></strong> ì˜ˆë¥¼ ë“¤ì–´ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ êµ¬í•˜ëŠ” ê³¼ì •ì—ì„œ ì—°ì‚° ëŒ€ìƒ ë²¡í„°ê°’ì—  <code class="language-plaintext highlighter-rouge">zero-value</code> ê°€ í¬í•¨ëœ ê²½ìš° ë¶„ëª¨ê°€ 0ì´ ë˜ê¸° ë•Œë¬¸ì— ì—°ì‚° ì •ì˜ê°€ ë˜ì§€ ì•Šì•„ <code class="language-plaintext highlighter-rouge">nan</code> ì„ ë°˜í™˜í•´ ìœ„ì™€ ê°™ì€ ì—ëŸ¬ê°€ ë°œìƒí•˜ëŠ” ê²½ìš°ê°€ ìˆë‹¤.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">check_nan</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
    <span class="s">""" Check if there is NaN in tensor """</span>
    <span class="n">checker</span> <span class="o">=</span> <span class="bp">False</span>
    <span class="k">if</span> <span class="bp">True</span> <span class="ow">in</span> <span class="n">torch</span><span class="p">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
        <span class="n">checker</span> <span class="o">=</span> <span class="bp">True</span>
    <span class="k">return</span> <span class="n">checker</span>

<span class="k">def</span> <span class="nf">zero_filtering</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="s">"""
    Add eps value for zero embedding, because competition metric is cosine similarity
    Cosine Similarity will be returned NaN, when input value has zero, like as torch.clamp()
    """</span>
    <span class="n">eps</span> <span class="o">=</span> <span class="mf">1e-4</span>
    <span class="n">x</span><span class="p">[</span><span class="n">x</span> <span class="o">&lt;=</span> <span class="n">eps</span><span class="p">]</span> <span class="o">=</span> <span class="n">eps</span>
    <span class="k">return</span> <span class="n">x</span>

<span class="k">def</span> <span class="nf">nan_filtering</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">eps</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-4</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="s">"""
    Change eps value for NaN Embedding, because competition metric is cosine similarity
    Cosine Similarity will be returned NaN
    """</span>
    <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">nan</span><span class="o">=</span><span class="n">eps</span><span class="p">)</span>

<span class="k">class</span> <span class="nc">CLIPGEMPooling</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="s">"""
    Generalized Mean Pooling for Natural Language Processing
    This class version of GEMPooling for CLIP, Transfer from NLP Task Code
    ViT don't use attention mask, because input image shape will be same

    Mean Pooling &lt;= GEMPooling &lt;= Max Pooling
    Because of doing exponent to each token embeddings, GEMPooling is like as weight to more activation token

    In original paper, they use p=3, but in this class, we use p=4 because torch doesn't support pow calculation
    for negative value tensor, only for non-negative value in odd number exponent
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">auto_cfg</span><span class="p">:</span> <span class="n">AutoConfig</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">CLIPGEMPooling</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>

    <span class="o">@</span><span class="nb">staticmethod</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">last_hidden_state</span><span class="p">,</span> <span class="n">p</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="s">"""
        last_hidden_state.size: [batch_size, patches_sequence, hidden_size]
        1) Pow last_hidden_state with p and then take a averaging
        2) pow sum_embeddings with 1/p
        """</span>
        <span class="n">p_embeddings</span> <span class="o">=</span> <span class="n">zero_filtering</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nb">pow</span><span class="p">(</span><span class="n">last_hidden_state</span><span class="p">,</span> <span class="n">p</span><span class="p">))</span>
        <span class="c1"># Check NaN value in Embedding after applying torch.pow
</span>        <span class="k">if</span> <span class="n">check_nan</span><span class="p">(</span><span class="n">p_embeddings</span><span class="p">):</span>
            <span class="n">p_embeddings</span> <span class="o">=</span> <span class="n">nan_filtering</span><span class="p">(</span><span class="n">p_embeddings</span><span class="p">)</span>
        <span class="n">sum_embeddings</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">p_embeddings</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

        <span class="n">gem_embeddings</span> <span class="o">=</span> <span class="n">zero_filtering</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nb">pow</span><span class="p">(</span><span class="n">sum_embeddings</span><span class="p">,</span> <span class="mf">1.</span> <span class="o">/</span> <span class="n">p</span><span class="p">))</span>
        <span class="c1"># Check NaN value in Embedding after applying torch.pow
</span>        <span class="k">if</span> <span class="n">check_nan</span><span class="p">(</span><span class="n">gem_embeddings</span><span class="p">):</span>
            <span class="n">gem_embeddings</span> <span class="o">=</span> <span class="n">nan_filtering</span><span class="p">(</span><span class="n">gem_embeddings</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">gem_embeddings</span>

<span class="k">class</span> <span class="nc">CLIPMultipleNegativeRankingLoss</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="s">"""
    Multiple Negative Ranking Loss for CLIP Model
    main concept is same as original one, but append suitable for other type of model (Not Sentence-Transformers)
    if you set more batch size, you can get more negative pairs for each anchor &amp; positive pair
    Args:
        scale: output of similarity function is multiplied by this value =&gt; I don't know why this is needed
        similarity_fct: standard of distance metrics, default cosine similarity
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">reduction</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">scale</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">20.0</span><span class="p">,</span> <span class="n">similarity_fct</span><span class="o">=</span><span class="n">cos_sim</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">reduction</span> <span class="o">=</span> <span class="n">reduction</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">scale</span> <span class="o">=</span> <span class="n">scale</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">similarity_fct</span> <span class="o">=</span> <span class="n">similarity_fct</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">reduction</span> <span class="o">=</span> <span class="n">reduction</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">cross_entropy_loss</span> <span class="o">=</span> <span class="n">CrossEntropyLoss</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">reduction</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">embeddings_a</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">embeddings_b</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="n">similarity_scores</span> <span class="o">=</span> <span class="n">zero_filtering</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">similarity_fct</span><span class="p">(</span><span class="n">embeddings_a</span><span class="p">,</span> <span class="n">embeddings_b</span><span class="p">))</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">scale</span>
        <span class="k">if</span> <span class="n">check_nan</span><span class="p">(</span><span class="n">similarity_scores</span><span class="p">):</span>
            <span class="s">""" Check NaN Value in similarity_scores """</span>
            <span class="n">similarity_scores</span> <span class="o">=</span> <span class="n">nan_filtering</span><span class="p">(</span><span class="n">similarity_scores</span><span class="p">)</span>

        <span class="n">labels</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">(</span>
            <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">similarity_scores</span><span class="p">)),</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">long</span><span class="p">,</span>
            <span class="n">device</span><span class="o">=</span><span class="n">similarity_scores</span><span class="p">.</span><span class="n">device</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">cross_entropy_loss</span><span class="p">(</span><span class="n">similarity_scores</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
</code></pre></div></div>

<p>í•„ìì˜ ê²½ìš°, ë‘ ê°œì˜ ì…ë ¥ í–‰ë ¬ì— ê°ê°  <code class="language-plaintext highlighter-rouge">sqrt()</code> ë¥¼ ì ìš©í•˜ê³  ë‘ í–‰ë ¬ì˜ ê°œë³„ ì›ì†Œ ì‚¬ì´ì˜ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ êµ¬í•´ì•¼ í–ˆë˜ ì ì´ ìˆë‹¤. <code class="language-plaintext highlighter-rouge">sqrt</code> <strong><u>ê³¼ì •ì—ì„œ ë„ˆë¬´ ì‘ì€ ê°’ë“¤ì´ ì…ë ¥ìœ¼ë¡œ ë“¤ì–´ê°€</u></strong> <code class="language-plaintext highlighter-rouge">underflow</code> <strong><u>ê°€ ë°œìƒí•´ í–‰ë ¬ì—</u></strong> <code class="language-plaintext highlighter-rouge">zero-value</code> <strong><u>ê°€ ìƒê²¼ê³ , ì´ë¥¼ ëª¨ë¥¸ì±„ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ êµ¬í•˜ë‹¤ê°€ í•œì°¸ì„ ìœ„ ì—ëŸ¬ì™€ ì‹¸ì› ë˜ ì ì´ ìˆë‹¤.</u></strong> ì‹¬ì§€ì–´ ì—°ì‚°ì†ë„ í–¥ìƒì„ ìœ„í•´ì„œ <strong><code class="language-plaintext highlighter-rouge">torch.autocast</code></strong> í´ë˜ìŠ¤ì˜ <code class="language-plaintext highlighter-rouge">grad_scaler(float32 to float16)</code> ê¹Œì§€ ì ìš©í•˜ê³  ìˆì—ˆë‹¤.</p>

<h3 id="ï¸-ë‚´ê°€-í•´ê²°í•œ-ë°©ë²•"><code class="language-plaintext highlighter-rouge">ğŸ–ï¸ ë‚´ê°€ í•´ê²°í•œ ë°©ë²•</code></h3>
<p>ì´ ê¸€ì„ ì½ëŠ” ë‹¹ì‹ ì´ ë§Œì•½ <code class="language-plaintext highlighter-rouge">sqrt</code> í˜¹ì€ <code class="language-plaintext highlighter-rouge">pow</code>ë¥¼ í™œìš©í•˜ëŠ” ê²½ìš°, <code class="language-plaintext highlighter-rouge">underflow</code> ë°©ì§€ë¥¼ ìœ„í•´ì„œ <del>ìœ„ ì˜ˆì‹œ ì½”ë“œì²˜ëŸ¼ ê¼­ ì ë‹¹í•œ ì…ì‹¤ë¡  ê°’ì„ ì—°ì‚° ì „í›„ì— í•„ìš”ì— ë”°ë¼ ë”í•´ì¤„ ê²ƒì„ ê¶Œì¥í•œë‹¤.</del> ì…ì‹¤ë¡  ê°’ì˜ ì„¤ì •ì€ í˜„ì¬ ìì‹ ì´ ì‚¬ìš©í•˜ê³  ìˆëŠ” ë¶€ë™ ì†Œìˆ˜ì  ì •í™•ë„ì— ë§ê²Œ ì„¤ì •í•´ì£¼ë©´ ë  ê²ƒ ê°™ë‹¤. <code class="language-plaintext highlighter-rouge">float32</code> ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²½ìš°ì—ëŠ” ëŒ€ë¶€ë¶„ <code class="language-plaintext highlighter-rouge">1e-6</code> ì„ ë§ì´ ì‚¬ìš©í•˜ëŠ” ê²ƒ ê°™ë‹¤. í•„ìë„ ì •í™•íˆ ì–´ë–¤ ê°’ì´ ì ë‹¹í•œì§€ ì•„ì§ ì˜ ëª¨ë¥´ê² ë‹¤â€¦ ê·¸ë¦¬ê³  ë”¥ëŸ¬ë‹ ì‹¤í—˜í•˜ë©´ì„œ <code class="language-plaintext highlighter-rouge">overflow</code> ë•Œë¬¸ì— <code class="language-plaintext highlighter-rouge">inf</code> ì´ ë°œìƒí–ˆë˜ ì ì€ ì—†ì—ˆë‹¤.</p>

<p>ì…ì‹¤ë¡  ê°’ì„ ë¬¸ì œê°€ ë˜ëŠ” ì—°ì‚° ì „ì— ì¼ê´„ì ìœ¼ë¡œ ë”í•  ê²½ìš°, ì•„ë¬´ë¦¬ ì‘ì€ ê°’ì´ë¼ë„ ì—°ì‚° ì¢…ë¥˜ì— ë”°ë¼ì„œ ê²°ê³¼ê°€ í¬ê²Œ ì™œê³¡ë˜ëŠ” ê²½ìš°ê°€ ë°œìƒí•œë‹¤. ë”°ë¼ì„œ ì—°ì‚°ì„ ë¨¼ì € ì ìš©í•œ ë’¤ ê²°ê³¼ì— <code class="language-plaintext highlighter-rouge">NaN</code>, <code class="language-plaintext highlighter-rouge">Inf</code>, <code class="language-plaintext highlighter-rouge">Zero</code>ê°€ ë°œìƒí•˜ëŠ”ì§€ ì²´í¬í•˜ê³ , ë°œìƒí•œ ë¶€ë¶„ì— í•œí•´ì„œ ì…ì‹¤ë¡  ê°’ì„ ë”í•´ì£¼ëŠ” ì»¤ìŠ¤í…€ <code class="language-plaintext highlighter-rouge">function</code>ìš¸ ì •ì˜í•´ ë¬¸ì œë¥¼ í•´ê²°í–ˆë‹¤.<br />
(ìœ„ì˜ ì½”ë“œ ì˜ˆì œ <code class="language-plaintext highlighter-rouge">check_nan</code>, <code class="language-plaintext highlighter-rouge">zero_filtering</code>, <code class="language-plaintext highlighter-rouge">nan_filtering</code>)</p>

<p>í•œí¸ <code class="language-plaintext highlighter-rouge">torch.autograd.set_detect_anomaly(True)</code> ë¥¼ í›ˆë ¨ ë£¨í”„ ì´ˆë°˜ì— ì •ì˜í•´ì£¼ë©´, <code class="language-plaintext highlighter-rouge">NaN</code>ì´ ë°œìƒí•˜ëŠ” ì¦‰ì‹œ ì‹¤í–‰ì´ ë©ˆì¶”ê³  <code class="language-plaintext highlighter-rouge">NaN</code>ì„ ìœ ë°œí•œ ë¼ì¸ì„ ì¶œë ¥í•´ì¤€ë‹¤. ê¼­ í™œìš©í•´ë³´ì.</p>]]></content><author><name>qcqced</name><email>qcqced123@gmail.com</email></author><category term="Framework &amp; Library" /><category term="Pytorch" /><category term="Logsoftmax" /><category term="NaN" /><category term="Error Handling" /><summary type="html"><![CDATA[Pytorch Error: Backward NaN values]]></summary></entry></feed>