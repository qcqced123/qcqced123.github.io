<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2024-03-16T14:34:55+09:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">AI/Business Study Log</title><subtitle>NLP, Marketing</subtitle><author><name>qcqced</name><email>qcqced123@gmail.com</email></author><entry><title type="html">🌆 [Linear Attention] Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention</title><link href="http://localhost:4000/nlp/linear_attention" rel="alternate" type="text/html" title="🌆 [Linear Attention] Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention" /><published>2024-03-14T00:00:00+09:00</published><updated>2024-03-15T02:00:00+09:00</updated><id>http://localhost:4000/nlp/linear_attention</id><content type="html" xml:base="http://localhost:4000/nlp/linear_attention"><![CDATA[<h3 id="overview"><code class="language-plaintext highlighter-rouge">🔭 Overview</code></h3>

<p><code class="language-plaintext highlighter-rouge">DistilBERT</code> 는 허깅 페이스 연구진이 2019년 발표한 BERT의 변형으로서, On-Device Ai 개발을 목표로 경량화에 초점을 맞춘 모델이다. GPT, BERT의 등장 이후, NLP 분야에서 비약적인 성능 향상이 이뤄졌음에도 불구하고, 터무니 없는 모델 사이즈와 컴퓨팅 리소스 요구로 인해 실생활 적용 같은 활용성은 여전히 해결해야할 문제로 남아 있었다. Google에서 발표한 초기 <code class="language-plaintext highlighter-rouge">BERT-base-uncased</code> 만 해도 파라미터가 1억 1천만개 수준에 달한다.</p>

<p>이를 다양한 비즈니스 요구 상황에 적용할 수 있으려면 최소한 8GB 이상의 가속기 전용 RAM 공간을 요구로 한다. 오늘날 개인용 PC 혹은 서버 컴퓨터의 경우, 8GB 이상의 VRAM이 달린 GPU가 일반적으로 탑재되기 때문에 크게 문제 될 것 없는 요구사항이지만, On-Device 환경에서는 이야기가 달라진다. 최신 하이엔드 스마트폰인 Galaxy S24 Ultra, iPhone 15 Pro의 경우 12GB, 8GB의 램 용량을 보유하고 있다. 그마저도 대부분의 온디바이스 환경은 SoC 구조를 채택하고 있기 때문에 전용 가속기가 온전히 저 모든 램 공간을 활용할 수 없다.</p>

<p>따라서 온디바이스에 Ai를 적용하기 위해서는 획기적인 모델 경량화가 필요한 상황이고 그 출발점이 된 연구가 바로 <code class="language-plaintext highlighter-rouge">DistilBERT</code>다. 로컬 디바이스 환경에서도 언어 모델을 활용하기 위해 허깅 페이스 연구진은 지식 증류 기법을 활용해 인코더 기반 언어 모델의 파라미터를 획기적으로 줄이는데 성공한다.</p>

<p>정리하자면, <code class="language-plaintext highlighter-rouge">DistilBERT</code> 모델은 기존 BERT의 구조적 측면 개선이 아닌, 사전학습 방법 특히 경량화에 초점을 맞춘 시도라고 볼 수 있다. 따라서 어떤 모델이더라도, 인코더 언어 모델이라면 모두 <code class="language-plaintext highlighter-rouge">DistilBERT</code> 구조를 사용할 수 있으며, 기존 논문에서는 원본 BERT 구조를 사용했다. 이번 포스팅에서도 BERT 구조에 대한 설명 대신, <code class="language-plaintext highlighter-rouge">DistilBERT</code>의 사전 학습 방법론인 <code class="language-plaintext highlighter-rouge">Knowledge Distillation</code>에 대해서만 다루려고 한다.</p>

<h3 id="knowledge-distillations"><code class="language-plaintext highlighter-rouge">🌆 Knowledge Distillations</code></h3>

\[\min_{\theta}\sum_{x \in X} \alpha \mathcal{L}_{\text{KL}}(x, \theta) + \beta \mathcal{L}_{\text{MLM}}(x, \theta) + \gamma \mathcal{L}_{\text{Cos}}(x, \theta)\]

<p><code class="language-plaintext highlighter-rouge">DistilBERT</code>는 Teacher-Student Architecture를 차용해 상대적으로 작은 파라미터 사이즈를 갖는 <code class="language-plaintext highlighter-rouge">Student</code> 모델에게 <code class="language-plaintext highlighter-rouge">Teacher</code>의 지식을 전수하는 것을 목표로 한다. 따라서 <code class="language-plaintext highlighter-rouge">Teacher</code> 모델은 이미 사전 학습을 마치고 수렴된 상태의 가중치를 갖고 있는 모델을 사용해야 한다. 더불어 Teacher 모델은 구조만 기존 BERT를 따르되, 사전 학습 방식은 RoBERTa의 방식과 동일(NSP 제거, Dynamic Masking 적용)하게 훈련되어야 한다.</p>

<p>한편, <code class="language-plaintext highlighter-rouge">Student</code> 모델은 <code class="language-plaintext highlighter-rouge">Teacher</code>의 60%정도 파라미터 사이즈를 갖도록 축소하여 사용한다. 이 때 축소는 모델의 <code class="language-plaintext highlighter-rouge">depth</code>(레이어 개수)에만 적용하는데, 연구진에 따르면 <code class="language-plaintext highlighter-rouge">width</code>(은닉층 크기)는 축소를 적용해도 연산 효율이 증가하지 않는다고 한다. 정리하면 <code class="language-plaintext highlighter-rouge">Teacher</code> 모델의 <code class="language-plaintext highlighter-rouge">레이어 개수*0.6</code>의 개수만큼 인코더를 쌓으면 된다는 것이다.</p>

<p>그리고 최대한 <code class="language-plaintext highlighter-rouge">Teacher</code>의 지식을 전수해야 하기 때문에, 데이터는 <code class="language-plaintext highlighter-rouge">Teacher</code> 를 수렴시킨 것과 동일한 세트를 이용해야 한다. 이 때, Teacher 모델은 이미 MLE 방식으로 훈련이 된 상태라서 로짓이 단일 토큰 하나 쪽으로 쏠려 있을 가능성이 매우 높다. 이는 <code class="language-plaintext highlighter-rouge">Student</code> 모델의 일반화 능력에 악영향을 미칠 가능성이 높다. 따라서 Temperature 변수 $T$ 도입해 소프트 맥스(로짓)의 분포를 평탄화 한다. 이렇게 하면, <code class="language-plaintext highlighter-rouge">argmax()</code> 가 아닌 다른 토큰 표현에 대해서도 <code class="language-plaintext highlighter-rouge">Student</code> 모델이 지식을 습득할 수 있어서 풍부한 문맥을 학습하고 일반화 능력을 높이는데 도움이 된다. 이를 <code class="language-plaintext highlighter-rouge">암흑 지식(Dark Knowledge)</code> 을 활용한다고 표현한다. Temperature 변수 $T$ 도입한 소프트맥스 함수 수식은 아래와 같다.</p>

\[\text{softmax}(x_i) = \frac{e^{\frac{x_i}{\tau}}}{\sum_{j} e^{\frac{x_j}{\tau}}}\]

<p>수식상 변수 $T$의 값을 1이상으로 세팅해야 평탄화를 할 수 있다. 따라서 연구진은 $T =2$ 로 두고 사전 학습을 진행했다(논문에 공개안됨, GitHub에 있음). 이번 파트 맨 처음에 등장한 수식을 다시 보자. 결국 <code class="language-plaintext highlighter-rouge">DisilBERT</code>의 목적함수는 3가지 손실의 가중합으로 구성된다. 이제부터는 개별 손실에 대해서 자세히 살펴보자.</p>

<h4 id="distillation-loss-kl-divergence-loss"><code class="language-plaintext highlighter-rouge">🌆 Distillation Loss: KL-Divergence Loss</code></h4>

\[\text{KL-Divergence}(P || Q) = \sum_{i} P(i) \log \frac{P(i)}{Q(i)}\]

<p>증류 손실로 사용되는 <code class="language-plaintext highlighter-rouge">KL-Divergence Loss</code>는 두 확률 분포 간의 차이를 측정하는 지표 중 하나다. 주로 확률 분포 P와 Q 사이의 차이를 나타내는데, 개별 요소의 확률값 차이가 클수록 합산값은 커져 손실이 커지게 된다. 반대로 두 분포의 개별 요소 확률값 차이가 작다면 당연히, 두 분포가 유사하다는 의미이므로 손실 역시 작아지게 된다. 일반적으로 <code class="language-plaintext highlighter-rouge">KL-Divergence Loss</code> 에서 확률분포 $P$ 가 이상적인 확률 분포를, $Q$ 가 모델이 예측한 확률분포를 의미한다. 따라서 <code class="language-plaintext highlighter-rouge">DistilBERT</code>의 경우 확률분포 $P$ 자리에는 <code class="language-plaintext highlighter-rouge">Teacher</code> 모델의 소프트맥스 분포가, $Q$ 에는 <code class="language-plaintext highlighter-rouge">Student</code> 모델의 소프트맥스 분포가 대입되면 된다. 이 때 두 확률분포 모두, 암흑 지식 획득을 위해 소프트맥스 평탄화를 적용한 결과를 사용한다. 논문에서, 선생 모델 예측에 평탄화를 적용한 것을 <code class="language-plaintext highlighter-rouge">소프트 라벨</code>, 학생 모델의 것에 적용한 결과는 <code class="language-plaintext highlighter-rouge">소프트 예측</code>이라고 부른다.</p>

<h4 id="student-loss-mlm-loss"><code class="language-plaintext highlighter-rouge">🌆 Student Loss: MLM Loss</code></h4>

\[\mathcal{L}_{\text{MLM}} = - \sum_{i=1}^{N} \sum_{j=1}^{L} \mathbb{1}_{m_{ij}} \log \text{softmax}(x_{ij})\]

<p>학생 손실은 말그대로 기본적인 MLM 손실을 말한다. 정확한 손실값 계산을 위해서 학생의 소프트맥스 분포에 평탄화를 적용하지 않는다. 이를 논문에서는 <code class="language-plaintext highlighter-rouge">하드 예측</code>이라고 부른다. 라벨 역시 <code class="language-plaintext highlighter-rouge">Teacher</code>로부터 나온 것이 아닌 원래 MLM 수행에 사용되는 마스킹 라벨을 사용한다.</p>

<h4 id="cosine-embedding-loss-contrastive-loss-by-cosine-similarity"><code class="language-plaintext highlighter-rouge">🌆 Cosine Embedding Loss: Contrastive Loss by cosine similarity</code></h4>

\[\mathcal{L}_{\text{COS}}(x,y) = \begin{cases} 1 - \cos(x_1, x_2), &amp; \text{if } y = 1 \\ \max(0, \cos(x_1, x_2) - \text{margin}), &amp; \text{if } y = -1 \end{cases}\]

<p><code class="language-plaintext highlighter-rouge">Teacher</code> 모델과 <code class="language-plaintext highlighter-rouge">Student</code> 모델의 마지막 인코더 모델이 출력하는 은닉값에 대한 <code class="language-plaintext highlighter-rouge">Contrastive Loss</code>를 의미한다. 이 때 <code class="language-plaintext highlighter-rouge">Distance Metric</code>은 코사인 유사도를 사용한다. 그래서 코사인 임베딩 손실이라고 논문에서 정의하는 것으로 추정된다. 위 수식을 최적화하는 것을 목적으로 한다. 이 때 라벨은 <code class="language-plaintext highlighter-rouge">[BS, Seq_len]</code>의 크기를 갖되, 모든 원소는 1이 되도록 만든다. 이유는 간단하다. <code class="language-plaintext highlighter-rouge">Student</code> 모델의 은닉값이 <code class="language-plaintext highlighter-rouge">Teacher</code> 모델의 것과 최대한 비슷해지도록 만드는게 우리 목적이기 때문이다.</p>

<h3 id="implementation-by-pytorch"><code class="language-plaintext highlighter-rouge">👩‍💻 Implementation by Pytorch</code></h3>
<p>논문의 내용과 오피셜로 공개된 코드를 종합하여 파이토치로 <code class="language-plaintext highlighter-rouge">DistilBERT</code>를 구현해봤다. 논문에 포함된 아이디어를 이해하는데는 역시 어렵지 않았지만, 페이퍼에 hyper-param 테이블이 따로 제시되어 있지 않아 공개된 코드를 안 볼수가 없었다.</p>

<p>전체 모델 구조 대한 코드는 <strong><a href="https://github.com/qcqced123/model_study">여기 링크</a></strong>를 통해 참고바란다.</p>

<h4 id="knowledge-distillation-pipeline"><code class="language-plaintext highlighter-rouge">👩‍💻 Knowledge Distillation Pipeline</code></h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">train_val_fn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">loader_train</span><span class="p">,</span> <span class="n">model</span><span class="p">:</span> <span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">criterion</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">],</span> <span class="n">optimizer</span><span class="p">,</span><span class="n">scheduler</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]:</span>
    <span class="s">""" Function for train loop with validation for each batch*N Steps
    DistillBERT has three loss:

        1) distillation loss, calculated by soft targets &amp; soft predictions
            (nn.KLDIVLoss(reduction='batchmean'))

        2) student loss, calculated by hard targets &amp; hard predictions
            (nn.CrossEntropyLoss(reduction='mean')), same as pure MLM Loss

        3) cosine similarity loss, calculated by student &amp; teacher logit similarity
            (nn.CosineEmbeddingLoss(reduction='mean')), similar as contrastive loss

    Those 3 losses are summed jointly and then backward to student model
    """</span>
    <span class="n">scaler</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">amp</span><span class="p">.</span><span class="n">GradScaler</span><span class="p">(</span><span class="n">enabled</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">amp_scaler</span><span class="p">)</span>
    <span class="n">model</span><span class="p">.</span><span class="n">train</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">step</span><span class="p">,</span> <span class="n">batch</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">tqdm</span><span class="p">(</span><span class="n">loader_train</span><span class="p">)):</span>
        <span class="n">optimizer</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">(</span><span class="n">set_to_none</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="n">inputs</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="s">'input_ids'</span><span class="p">].</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">device</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="n">labels</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="s">'labels'</span><span class="p">].</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">device</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="n">padding_mask</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="s">'padding_mask'</span><span class="p">].</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">device</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

        <span class="n">mask</span> <span class="o">=</span> <span class="n">padding_mask</span><span class="p">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">).</span><span class="n">expand</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">dim_model</span><span class="p">)</span>  <span class="c1"># for hidden states dim
</span>        <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">t_hidden_state</span><span class="p">,</span> <span class="n">soft_target</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">teacher_fw</span><span class="p">(</span>
                <span class="n">inputs</span><span class="o">=</span><span class="n">inputs</span><span class="p">,</span>
                <span class="n">padding_mask</span><span class="o">=</span><span class="n">padding_mask</span><span class="p">,</span>
                <span class="n">mask</span><span class="o">=</span><span class="n">mask</span>
            <span class="p">)</span>  <span class="c1"># teacher model's pred =&gt; hard logit
</span>
        <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">amp</span><span class="p">.</span><span class="n">autocast</span><span class="p">(</span><span class="n">enabled</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">amp_scaler</span><span class="p">):</span>
            <span class="n">s_hidden_state</span><span class="p">,</span> <span class="n">s_logit</span><span class="p">,</span> <span class="n">soft_pred</span><span class="p">,</span> <span class="n">c_labels</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">student_fw</span><span class="p">(</span>
                <span class="n">inputs</span><span class="o">=</span><span class="n">inputs</span><span class="p">,</span>
                <span class="n">padding_mask</span><span class="o">=</span><span class="n">padding_mask</span><span class="p">,</span>
                <span class="n">mask</span><span class="o">=</span><span class="n">mask</span>
            <span class="p">)</span>
            <span class="n">d_loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">[</span><span class="s">"KLDivLoss"</span><span class="p">](</span><span class="n">soft_pred</span><span class="p">.</span><span class="n">log</span><span class="p">(),</span> <span class="n">soft_target</span><span class="p">)</span>  <span class="c1"># nn.KLDIVLoss
</span>            <span class="n">s_loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">[</span><span class="s">"CrossEntropyLoss"</span><span class="p">](</span><span class="n">s_logit</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">vocab_size</span><span class="p">),</span> <span class="n">labels</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>  <span class="c1"># nn.CrossEntropyLoss
</span>            <span class="n">c_loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">[</span><span class="s">"CosineEmbeddingLoss"</span><span class="p">](</span><span class="n">s_hidden_state</span><span class="p">,</span> <span class="n">t_hidden_state</span><span class="p">,</span> <span class="n">c_labels</span><span class="p">)</span>  <span class="c1"># nn.CosineEmbeddingLoss
</span>            <span class="n">loss</span> <span class="o">=</span> <span class="n">d_loss</span><span class="o">*</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">alpha_distillation</span> <span class="o">+</span> <span class="n">s_loss</span><span class="o">*</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">alpha_student</span> <span class="o">+</span> <span class="n">c_loss</span><span class="o">*</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">alpha_cosine</span>  <span class="c1"># linear combination loss
</span>
        <span class="n">scaler</span><span class="p">.</span><span class="n">scale</span><span class="p">(</span><span class="n">loss</span><span class="p">).</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">scaler</span><span class="p">.</span><span class="n">step</span><span class="p">(</span><span class="n">optimizer</span><span class="p">)</span>
        <span class="n">scaler</span><span class="p">.</span><span class="n">update</span><span class="p">()</span>
        <span class="n">scheduler</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>
</code></pre></div></div>

<h4 id="knowledge-distillation-model"><code class="language-plaintext highlighter-rouge">👩‍💻 Knowledge Distillation Model</code></h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">DistillationKnowledge</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">AbstractTask</span><span class="p">):</span>
    <span class="s">""" Custom Task Module for Knowledge Distillation by DistilBERT Style Architecture
    DistilBERT Style Architecture is Teacher-Student Framework for Knowledge Distillation,

    And then they have 3 objective functions:
        1) distillation loss, calculated by soft targets &amp; soft predictions
            (nn.KLDIVLoss(reduction='batchmean'))
        2) student loss, calculated by hard targets &amp; hard predictions
            (nn.CrossEntropyLoss(reduction='mean')), same as pure MLM Loss
        3) cosine similarity loss, calculated by student &amp; teacher logit similarity
            (nn.CosineEmbeddingLoss(reduction='mean')), similar as contrastive loss

    References:
        https://arxiv.org/pdf/1910.01108.pdf
        https://github.com/huggingface/transformers/blob/main/examples/research_projects/distillation/distiller.py
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">cfg</span><span class="p">:</span> <span class="n">CFG</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">DistillationKnowledge</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span> <span class="o">=</span> <span class="n">CFG</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">DistilBERT</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">,</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">select_model</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">_init_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">teacher_load_pretrained</span><span class="p">:</span>  <span class="c1"># for teacher model
</span>            <span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">teacher</span><span class="p">.</span><span class="n">load_state_dict</span><span class="p">(</span>
                <span class="n">torch</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="n">cfg</span><span class="p">.</span><span class="n">checkpoint_dir</span> <span class="o">+</span> <span class="n">cfg</span><span class="p">.</span><span class="n">teacher_state_dict</span><span class="p">),</span>
                <span class="n">strict</span><span class="o">=</span><span class="bp">False</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">student_load_pretrained</span><span class="p">:</span>  <span class="c1"># for student model
</span>            <span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">student</span><span class="p">.</span><span class="n">load_state_dict</span><span class="p">(</span>
                <span class="n">torch</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="n">cfg</span><span class="p">.</span><span class="n">checkpoint_dir</span> <span class="o">+</span> <span class="n">cfg</span><span class="p">.</span><span class="n">student_state_dict</span><span class="p">),</span>
                <span class="n">strict</span><span class="o">=</span><span class="bp">True</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">freeze</span><span class="p">:</span>
            <span class="n">freeze</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">teacher</span><span class="p">)</span>
            <span class="n">freeze</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">mlm_head</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">gradient_checkpoint</span><span class="p">:</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">gradient_checkpointing_enable</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">teacher_fw</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">inputs</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">padding_mask</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">mask</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
        <span class="n">is_valid</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="bp">False</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
        <span class="s">""" teacher forward pass to make soft target, last_hidden_state for distillation loss """</span>
        <span class="c1"># 1) make soft target
</span>        <span class="n">temperature</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="k">if</span> <span class="n">is_valid</span> <span class="k">else</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">temperature</span>
        <span class="n">last_hidden_state</span><span class="p">,</span> <span class="n">t_logit</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">teacher_fw</span><span class="p">(</span>
            <span class="n">inputs</span><span class="p">,</span>
            <span class="n">padding_mask</span><span class="p">,</span>
            <span class="n">attention_mask</span>
        <span class="p">)</span>
        <span class="n">last_hidden_state</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">masked_select</span><span class="p">(</span><span class="n">last_hidden_state</span><span class="p">,</span> <span class="o">~</span><span class="n">mask</span><span class="p">)</span>  <span class="c1"># for inverse select
</span>        <span class="n">last_hidden_state</span> <span class="o">=</span> <span class="n">last_hidden_state</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">dim_model</span><span class="p">)</span>  <span class="c1"># flatten last_hidden_state
</span>        <span class="n">soft_target</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="n">softmax</span><span class="p">(</span>
            <span class="n">t_logit</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">vocab_size</span><span class="p">)</span> <span class="o">/</span> <span class="n">temperature</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span>  <span class="c1"># flatten softmax distribution
</span>            <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span>
        <span class="p">)</span>  <span class="c1"># [bs* seq, vocab_size]
</span>        <span class="k">return</span> <span class="n">last_hidden_state</span><span class="p">,</span> <span class="n">soft_target</span>

    <span class="k">def</span> <span class="nf">student_fw</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">inputs</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">padding_mask</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">mask</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
        <span class="n">is_valid</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="bp">False</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
        <span class="s">""" student forward pass to make soft prediction, hard prediction for student loss """</span>
        <span class="n">temperature</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="k">if</span> <span class="n">is_valid</span> <span class="k">else</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">temperature</span>
        <span class="n">last_hidden_state</span><span class="p">,</span> <span class="n">s_logit</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">teacher_fw</span><span class="p">(</span>
            <span class="n">inputs</span><span class="p">,</span>
            <span class="n">padding_mask</span><span class="p">,</span>
            <span class="n">attention_mask</span>
        <span class="p">)</span>
        <span class="n">last_hidden_state</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">masked_select</span><span class="p">(</span><span class="n">last_hidden_state</span><span class="p">,</span> <span class="o">~</span><span class="n">mask</span><span class="p">)</span>  <span class="c1"># for inverse select
</span>        <span class="n">last_hidden_state</span> <span class="o">=</span> <span class="n">last_hidden_state</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">dim_model</span><span class="p">)</span>  <span class="c1"># flatten last_hidden_state
</span>        <span class="n">c_labels</span> <span class="o">=</span> <span class="n">last_hidden_state</span><span class="p">.</span><span class="n">new</span><span class="p">(</span><span class="n">last_hidden_state</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)).</span><span class="n">fill_</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">soft_pred</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="n">softmax</span><span class="p">(</span>
            <span class="n">s_logit</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">vocab_size</span><span class="p">)</span> <span class="o">/</span> <span class="n">temperature</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span>  <span class="c1"># flatten softmax distribution
</span>            <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">last_hidden_state</span><span class="p">,</span> <span class="n">s_logit</span><span class="p">,</span> <span class="n">soft_pred</span><span class="p">,</span> <span class="n">c_labels</span>
</code></pre></div></div>

<h4 id="distilbert-model"><code class="language-plaintext highlighter-rouge">👩‍💻 DistilBERT Model</code></h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">DistilBERT</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">AbstractModel</span><span class="p">):</span>
    <span class="s">""" Main class for DistilBERT Style Model, Teacher-Student Framework
    for Knowledge Distillation aim to lighter Large Scale LLM model. This model have 3 objective functions:

        1) distillation loss, calculated by soft targets &amp; soft predictions
            (nn.KLDIVLoss(reduction='batchmean'))

        2) student loss, calculated by hard targets &amp; hard predictions
            (nn.CrossEntropyLoss(reduction='mean')), same as pure MLM Loss

        3) cosine similarity loss, calculated by student &amp; teacher logit similarity
            (nn.CosineEmbeddingLoss(reduction='mean')), similar as contrastive loss

    soft targets &amp; soft predictions are meaning that logit are passed through softmax function applied with temperature T
    temperature T aim to flatten softmax layer distribution for making "Dark Knowledge" from teacher model

    hard targets &amp; hard predictions are meaning that logit are passed through softmax function without temperature T
    hard targets are same as just simple labels from MLM Collator returns for calculating cross entropy loss

    cosine similarity loss is calculated by cosine similarity between student &amp; teacher
    in official repo, they mask padding tokens for calculating cosine similarity, target for this task is 1
    cosine similarity is calculated by nn.CosineSimilarity() function, values are range to [-1, 1]

    you can select any other backbone model architecture for Teacher &amp; Student Model for knowledge distillation
    but, in original paper, BERT is used for Teacher Model &amp; Student
    and you must select pretrained model for Teacher Model, because Teacher Model is used for knowledge distillation,
    which is containing pretrained mlm head

    Do not pass gradient backward to teacher model!!
    (teacher model must be frozen or register_buffer to model or use no_grad() context manager)

    Args:
        cfg: configuration.CFG
        model_func: make model instance in runtime from config.json

    References:
        https://arxiv.org/pdf/1910.01108.pdf
        https://github.com/huggingface/transformers/blob/main/examples/research_projects/distillation/distiller.py
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">cfg</span><span class="p">:</span> <span class="n">CFG</span><span class="p">,</span> <span class="n">model_func</span><span class="p">:</span> <span class="n">Callable</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">DistilBERT</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span> <span class="o">=</span> <span class="n">cfg</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">teacher</span> <span class="o">=</span> <span class="n">model_func</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">teacher_num_layers</span><span class="p">)</span>  <span class="c1"># must be loading pretrained model containing mlm head
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">mlm_head</span> <span class="o">=</span> <span class="n">MLMHead</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">)</span>  <span class="c1"># must be loading pretrained model's mlm head
</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">student</span> <span class="o">=</span> <span class="n">model_func</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">student_num_layers</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">s_mlm_head</span> <span class="o">=</span> <span class="n">MLMHead</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">teacher_fw</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">inputs</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">padding_mask</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
        <span class="s">""" forward pass for teacher model
        """</span>
        <span class="n">last_hidden_state</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">teacher</span><span class="p">(</span>
            <span class="n">inputs</span><span class="p">,</span>
            <span class="n">padding_mask</span><span class="p">,</span>
            <span class="n">attention_mask</span>
        <span class="p">)</span>
        <span class="n">t_logit</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">mlm_head</span><span class="p">(</span><span class="n">last_hidden_state</span><span class="p">)</span>  <span class="c1"># hard logit =&gt; to make soft logit
</span>        <span class="k">return</span> <span class="n">last_hidden_state</span><span class="p">,</span> <span class="n">t_logit</span>

    <span class="k">def</span> <span class="nf">student_fw</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">inputs</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">padding_mask</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="bp">None</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
        <span class="s">""" forward pass for student model
        """</span>
        <span class="n">last_hidden_state</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">student</span><span class="p">(</span>
            <span class="n">inputs</span><span class="p">,</span>
            <span class="n">padding_mask</span><span class="p">,</span>
            <span class="n">attention_mask</span>
        <span class="p">)</span>
        <span class="n">s_logit</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">s_mlm_head</span><span class="p">(</span><span class="n">last_hidden_state</span><span class="p">)</span>  <span class="c1"># hard logit =&gt; to make soft logit
</span>        <span class="k">return</span> <span class="n">last_hidden_state</span><span class="p">,</span> <span class="n">s_logit</span>
</code></pre></div></div>]]></content><author><name>qcqced</name><email>qcqced123@gmail.com</email></author><category term="NLP" /><category term="Natural Language Process" /><category term="Linear-Attention" /><category term="Transformer" /><category term="BERT" /><category term="Kernel Trick" /><category term="Self-Attention" /><category term="Pytorch" /><summary type="html"><![CDATA[Linear Attention Official Paper Review with Pytorch Implementation]]></summary></entry><entry><title type="html">👮 [ELECTRA] Pre-training Text Encoders as Discriminators Rather Than Generators</title><link href="http://localhost:4000/nlp/electra" rel="alternate" type="text/html" title="👮 [ELECTRA] Pre-training Text Encoders as Discriminators Rather Than Generators" /><published>2024-03-11T00:00:00+09:00</published><updated>2024-03-12T02:00:00+09:00</updated><id>http://localhost:4000/nlp/electra</id><content type="html" xml:base="http://localhost:4000/nlp/electra"><![CDATA[<h3 id="overview"><code class="language-plaintext highlighter-rouge">🔭 Overview</code></h3>

<p><code class="language-plaintext highlighter-rouge">ELECTRA</code>는 2020년 Google에서 처음 발표한 모델로, GAN(Generative Adversarial Networks) Style 아키텍처를 NLP에 적용한 것이 특징이다. 새로운 구조 차용에 맞춰서 <code class="language-plaintext highlighter-rouge">RTD(Replace Token Dection)</code> Task를 고안에 사전 학습으로 사용했다. 모든 아이디어는 기존 MLM(Masked Language Model)을 사전학습 방법론으로 사용하는 인코더 언어 모델(BERT 계열)의 단점으로부터 출발한다.</p>

<p><strong>[MLM 단점]</strong></p>
<ul>
  <li>1) 사전학습과 파인튜닝 사이 불일치
    <ul>
      <li>파인튜닝 때 Masking Task가 없음</li>
    </ul>
  </li>
  <li>2) 연산량 대비 학습량은 적은편
    <ul>
      <li>전체 시퀀스의 15%만 마스킹 활용(15%만 학습)</li>
      <li>전역 어텐션의 시공간 복잡도 고려하면 상당히 비효율적인 수치
        <ul>
          <li>시퀀스길이 ** 2의 복잡도</li>
          <li>Vocab Size만큼의 차원을 갖는 소프트맥스 계산 반복</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p>그래서 MLM은 활용하되, 파인튜닝과 괴리는 크지 않은 목적함수를 설계함으로서 입력된 전체 시퀀스에 대해서 모델이 학습하여 연산량 대비 학습량을 늘리고자 했던게 바로 ELECTRA 모델이다.</p>

<p>정리하자면, ELECTRA 모델은 기존 BERT의 구조적 측면 개선이 아닌, 사전학습 방법에 대한 개선 시도라고 볼 수 있다. 따라서 어떤 모델이더라도, 인코더 언어 모델이라면 모두 ELECTRA 구조를 사용할 수 있으며, 기존 논문에서는 원본 BERT 구조를 사용했다. 그래서 본 포스팅에서도 BERT에 대한 설명 없이 RTD에 대해서만 다루려고 한다.</p>

<h3 id="rtd-new-pre-train-task"><code class="language-plaintext highlighter-rouge">👮 RTD: New Pre-train Task</code></h3>

<p align="center">
<img src="/assets/images/electra/electra.png" alt="RTD Task" class="align-center image-caption" width="100%&quot;, height=&quot;100%" />
<strong><em><a href="https://arxiv.org/abs/2003.10555">RTD Task</a></em></strong>
</p>

<p>RTD의 아이디어는 간단하다. 생성자(Generator)가 출력으로 내놓은 토큰 시퀀스에 대해서 판별자(Discriminator)가 개별 토큰들이 원본인지 아닌지를 판정(이진 분류)하도록 만든다. 생성자는 기존의 MLM을 그대로 수행하고, 판별자는 생성자의 예측에 대해 진짜인지 가짜인지 분류하는 식이다.</p>

<p>위 그림을 예시로 살펴보자. 모델에 입력으로 <code class="language-plaintext highlighter-rouge">the chef cooked the meal</code>라는 시퀀스 준다. 그러면 MLM 규칙에 따라서 15%의 토큰이 무작위로 선택된다. 그래서 <code class="language-plaintext highlighter-rouge">the</code>, <code class="language-plaintext highlighter-rouge">cooked</code>가 마스킹 되었다. 이제 생성자는 마스킹 토큰에 대해 <code class="language-plaintext highlighter-rouge">the</code>, <code class="language-plaintext highlighter-rouge">ate</code>라는 결과를 내놓는다. 그래서 최종적으로 생성자가 반환하는 시퀀스는 <code class="language-plaintext highlighter-rouge">the chef ate the meal</code>이 된다. 이제 생성자가 반환한 시퀀스를 판별자에 입력으로 대입한다. 판별자는 개별 토큰들이 원본인지 아닌지를 판정해 결과를 출력한다.</p>

<p>이러한 구조 및 사전학습 방식의 장점은 판별자가 MLM 학습에 따른 지식을 생성자로부터 전수 받는 동시에 전체 시퀀스에 대해서 학습할 기회가 생긴다는 것이다. 시퀀스 내부 모든 토큰에 대해서 예측을 수행하고 손실을 계산할 수 있기 때문에 같은 크기의 시퀀스를 사용해도 기존 MLM 대비 더 풍부한 문맥 정보를 모델이 포착할 수 있게 된다. 또한 판별자를 파인튜닝의 BackBone으로 사용하면, 판별자의 사전학습은 결국 마스킹 없이 모든 시퀀스를 활용한 이진 분류라고 볼 수 있기 때문에, 사전학습과 파인튜닝 사이의 괴리도 상당히 많이 줄어들게 된다.</p>

<h3 id="architecture"><code class="language-plaintext highlighter-rouge">🌟 Architecture</code></h3>

<p align="center">
<img src="/assets/images/electra/electra_experiment.png" alt="Model Architecture" class="align-center image-caption" width="100%&quot;, height=&quot;100%" />
<strong><em><a href="https://arxiv.org/abs/2003.10555">Model Architecture</a></em></strong>
</p>

<p>저자는 위와 같은 실험 결과를 바탕으로, 생성자의 width (은닉층) 크기가 판별자보다 작도록 모델 크기를 세팅하는게 가장 효율적이라고 주장한다. 제시된 그래프는 생성자와 판별자의 크기 변화 대비 파인튜닝 성능의 추이를 나타낸다. 생성자의 width 크기가 256, 판별자의 width 크기가 768일 때 가장 점수가 높다. depth(레이어 개수)에 대한 언급은 따로 없지만, 저자에 의해 공개된 Hyper-Param 테이블을 보면 은닉층의 크기만 줄이고, 레이어 개수는 생성자와 판별자가 같은 것으로 추정된다.</p>

<p>추가로, 생성자와 판별자가 임베딩 층을 서로 공유하는게 가장 높은 성능을 낸다고 주장한다. 오른쪽 그래프 추이를 보면 같은 연산량이라면, 임베딩 공유(파란색 실선) 방식이 가장 높은 파인튜닝 성능을 보여준다는 것을 알 수 있다. 따라서 단어 임베딩, 절대 위치 임베딩을 서로 공유하도록 설계한다. 대신 생성자 은닉층의 크기가 더 작은게 유리하다고 언급했기 때문에, 이것을 실제로 구현하려면 임베딩 층으로부터 나온 결과값을 생성자의 은닉층 차원으로 선형 투영해줘야 한다. 그래서 생성자의 임베딩 층과 인코더 사이에 linear layer가 추가되어야 한다.</p>

\[\min_{\theta_G, \theta_D}\sum_{x \in X} \mathcal{L}_{\text{MLM}}(x, \theta_G) + \lambda \mathcal{L}_{\text{Disc}}(x, \theta_D)\]

<p>따라서, 지금까지 살펴본 모든 내용을 종합해보면 ELECTRA의 목적함수는 다음 수식과 같다. 생성자의 MLM 손실과 판별자의 이진 분류 손실을 더해서 모델에 오차 역전해주면 되는데, 특이한 점은 판별자의 손실에 상수값인 람다가 곱해진다는 것이다. 실제 모델을 구현하고 사전학습을 해보면, 데이터의 양이나 모델 크기 혹은 종류에 따라 달라지겠지만 두 손실 사이의 스케일의 차이가 10배정도 차이 나게 된다. 두 손실의 스케일을 맞춰주는 동시에, 임베딩층의 학습이 판별자의 손실을 줄이는데 더 집중하도록 만들기 위해 도입한 것으로 추정된다. 논문과 코드를 보면 저자는 $\lambda=50$ 으로 두고 학습하고 있다.</p>

<h3 id="implementation-by-pytorch"><code class="language-plaintext highlighter-rouge">👩‍💻 Implementation by Pytorch</code></h3>

<p>논문의 내용과 저자가 직접 공개한 코드를 종합하여 파이토치로 ELECTRA를 구현해봤다. 두 개의 서로 다른 모델을 같은 스탭에서 학습시켜야 하기 때문에, 제시된 내용에 비해 실제 구현은 매우 까다로운 편이었다. 본 포스팅에서는 ELECTRA 모델 구조를 비롯해 RTD 학습 파이프라인 구성에 필수적인 요소 몇 가지에 대해서만 설명하려 한다. 전체 구조에 대한 코드는 <strong><a href="https://github.com/qcqced123/model_study">여기 링크</a></strong>를 통해 참고 부탁드린다.</p>

<p>ELECTRA의 사전 학습인 RTD의 학습 파이프라인을 구현한 코드를 본 뒤, 세부 구성 요소들에 대해서 살펴보자.</p>

<h4 id="-rtd-trainer-method"><strong><code class="language-plaintext highlighter-rouge">🌆 RTD trainer method</code></strong></h4>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">train_val_fn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">loader_train</span><span class="p">,</span> <span class="n">model</span><span class="p">:</span> <span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">criterion</span><span class="p">:</span> <span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">scheduler</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]:</span>
  <span class="n">scaler</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">amp</span><span class="p">.</span><span class="n">GradScaler</span><span class="p">(</span><span class="n">enabled</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">amp_scaler</span><span class="p">)</span>
  <span class="n">model</span><span class="p">.</span><span class="n">train</span><span class="p">()</span>
  <span class="k">for</span> <span class="n">step</span><span class="p">,</span> <span class="n">batch</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">tqdm</span><span class="p">(</span><span class="n">loader_train</span><span class="p">)):</span>
      <span class="n">optimizer</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">(</span><span class="n">set_to_none</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
      <span class="n">inputs</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="s">'input_ids'</span><span class="p">].</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">device</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
      <span class="n">labels</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="s">'labels'</span><span class="p">].</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">device</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>  
      <span class="n">padding_mask</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="s">'padding_mask'</span><span class="p">].</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">device</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>  

      <span class="n">mask_labels</span> <span class="o">=</span> <span class="bp">None</span>
      <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">rtd_masking</span> <span class="o">==</span> <span class="s">'SpanBoundaryObjective'</span><span class="p">:</span>
          <span class="n">mask_labels</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="s">'mask_labels'</span><span class="p">].</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">device</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

      <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">amp</span><span class="p">.</span><span class="n">autocast</span><span class="p">(</span><span class="n">enabled</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">amp_scaler</span><span class="p">):</span>
          <span class="n">g_logit</span><span class="p">,</span> <span class="n">d_inputs</span><span class="p">,</span> <span class="n">d_labels</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">generator_fw</span><span class="p">(</span>
              <span class="n">inputs</span><span class="p">,</span>
              <span class="n">labels</span><span class="p">,</span>
              <span class="n">padding_mask</span><span class="p">,</span>
              <span class="n">mask_labels</span>
          <span class="p">)</span>
          <span class="n">d_logit</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">discriminator_fw</span><span class="p">(</span>
              <span class="n">d_inputs</span><span class="p">,</span>
              <span class="n">padding_mask</span>
          <span class="p">)</span>
          <span class="n">g_loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">g_logit</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">vocab_size</span><span class="p">),</span> <span class="n">labels</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>
          <span class="n">d_loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">d_logit</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">d_labels</span><span class="p">)</span>
          <span class="n">loss</span> <span class="o">=</span> <span class="n">g_loss</span> <span class="o">+</span> <span class="n">d_loss</span><span class="o">*</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">discriminator_lambda</span>

      <span class="n">scaler</span><span class="p">.</span><span class="n">scale</span><span class="p">(</span><span class="n">loss</span><span class="p">).</span><span class="n">backward</span><span class="p">()</span>
      <span class="n">scaler</span><span class="p">.</span><span class="n">step</span><span class="p">(</span><span class="n">optimizer</span><span class="p">)</span>
      <span class="n">scaler</span><span class="p">.</span><span class="n">update</span><span class="p">()</span>
      <span class="n">scheduler</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>
</code></pre></div></div>
<p>데이터로더로부터 받은 입력들을 생성자에 넣고 MLM 예측 결과, RTD 수행을 위해 필요한 새로운 라벨값을 반환 받는다. 그리고 이것을 다시 판별자의 입력으로 사용하고, 판별자의 예측 결과를 반환받아 서로 다른 두 모델에 대한 가중 손실합산을 구한 뒤, 옵티마이저에 보내고 최적화를 수행한다. 이 때, 처음에 데이터로더가 반환하는 입력 시퀀스와 라벨은 MLM의 그것과 동일하다,</p>

<p>구현하면서 가장 어려웠던게, 옵티마이저 및 스케줄러의 구성이었다. 두 개의 모델을 같은 스탭에서 학습시키는 경험이 처음이라서 처음에 모델 개수만큼 옵티마이저와 스케줄러 객체를 만들어줘야 한다고 생각했다. 특히 두 모델의 스케일이 다르기 때문에 서로 다른 옵티마이저, 스케줄러 도입으로 각기 다른 학습률을 적용하는게 정확할 것이라 생각했다.</p>

<p>하지만, 옵티마이저를 두 개 사용하는 것은 매우 많은 메모리를 차지할 뿐더러 논문에서 공개한 하이퍼파라미터 테이블을 보면 두 모델에 같은 학습률을 적용하고 있는 것을 알 수 있었다. 따라서 그에 맞게 같은 옵티마이저, 스케줄러를 사용해 동시에 두 모델이 학습되도록 파이프라인을 만들게 되었다.</p>

<p>추가로, 공개된 오피셜 코드 역시 단일 옵티마이저 및 스케줄러를 사용하는 것을 확인했다.</p>

<h4 id="-electra-module"><strong><code class="language-plaintext highlighter-rouge">🌆 ELECTRA Module</code></strong></h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="nn">experiment.models.abstract_model</span> <span class="kn">import</span> <span class="n">AbstractModel</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">Tensor</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">Callable</span>
<span class="kn">from</span> <span class="nn">einops.layers.torch</span> <span class="kn">import</span> <span class="n">Rearrange</span>
<span class="kn">from</span> <span class="nn">experiment.tuner.mlm</span> <span class="kn">import</span> <span class="n">MLMHead</span>
<span class="kn">from</span> <span class="nn">experiment.tuner.sbo</span> <span class="kn">import</span> <span class="n">SBOHead</span>
<span class="kn">from</span> <span class="nn">experiment.tuner.rtd</span> <span class="kn">import</span> <span class="n">get_discriminator_input</span><span class="p">,</span> <span class="n">RTDHead</span>
<span class="kn">from</span> <span class="nn">configuration</span> <span class="kn">import</span> <span class="n">CFG</span>

<span class="k">class</span> <span class="nc">ELECTRA</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">AbstractModel</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">cfg</span><span class="p">:</span> <span class="n">CFG</span><span class="p">,</span> <span class="n">model_func</span><span class="p">:</span> <span class="n">Callable</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">ELECTRA</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span> <span class="o">=</span> <span class="n">cfg</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">generator</span> <span class="o">=</span> <span class="n">model_func</span><span class="p">(</span><span class="n">cfg</span><span class="p">.</span><span class="n">generator_num_layers</span><span class="p">)</span>  <span class="c1"># init generator
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">mlm_head</span> <span class="o">=</span> <span class="n">MLMHead</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">rtd_masking</span> <span class="o">==</span> <span class="s">'SpanBoundaryObjective'</span><span class="p">:</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">mlm_head</span> <span class="o">=</span> <span class="n">SBOHead</span><span class="p">(</span>
                <span class="n">cfg</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">,</span>
                <span class="n">is_concatenate</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">is_concatenate</span><span class="p">,</span>
                <span class="n">max_span_length</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">max_span_length</span>
            <span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">discriminator</span> <span class="o">=</span> <span class="n">model_func</span><span class="p">(</span><span class="n">cfg</span><span class="p">.</span><span class="n">discriminator_num_layers</span><span class="p">)</span>  <span class="c1"># init generator
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">rtd_head</span> <span class="o">=</span> <span class="n">RTDHead</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">share_embed_method</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">share_embed_method</span>  <span class="c1"># instance, es, gdes
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">share_embedding</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">share_embedding</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="k">def</span> <span class="nf">discriminator_hook</span><span class="p">(</span><span class="n">module</span><span class="p">:</span> <span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">,</span> <span class="o">*</span><span class="n">inputs</span><span class="p">):</span>
            <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">share_embed_method</span> <span class="o">==</span> <span class="s">'instance'</span><span class="p">:</span>  <span class="c1"># Instance Sharing
</span>                <span class="bp">self</span><span class="p">.</span><span class="n">discriminator</span><span class="p">.</span><span class="n">embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">generator</span><span class="p">.</span><span class="n">embeddings</span>

            <span class="k">elif</span> <span class="bp">self</span><span class="p">.</span><span class="n">share_embed_method</span> <span class="o">==</span> <span class="s">'ES'</span><span class="p">:</span>  <span class="c1"># ES (Embedding Sharing)
</span>                <span class="bp">self</span><span class="p">.</span><span class="n">discriminator</span><span class="p">.</span><span class="n">embeddings</span><span class="p">.</span><span class="n">word_embedding</span><span class="p">.</span><span class="n">weight</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">generator</span><span class="p">.</span><span class="n">embeddings</span><span class="p">.</span><span class="n">word_embedding</span><span class="p">.</span><span class="n">weight</span>
                <span class="bp">self</span><span class="p">.</span><span class="n">discriminator</span><span class="p">.</span><span class="n">embeddings</span><span class="p">.</span><span class="n">abs_pos_emb</span><span class="p">.</span><span class="n">weight</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">generator</span><span class="p">.</span><span class="n">embeddings</span><span class="p">.</span><span class="n">abs_pos_emb</span><span class="p">.</span><span class="n">weight</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">discriminator</span><span class="p">.</span><span class="n">register_forward_pre_hook</span><span class="p">(</span><span class="n">discriminator_hook</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">generator_fw</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span>
            <span class="n">inputs</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
            <span class="n">labels</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
            <span class="n">padding_mask</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
            <span class="n">mask_labels</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
            <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="bp">None</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
        <span class="n">g_last_hidden_states</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">generator</span><span class="p">(</span>
            <span class="n">inputs</span><span class="p">,</span>
            <span class="n">padding_mask</span><span class="p">,</span>
            <span class="n">attention_mask</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">rtd_masking</span> <span class="o">==</span> <span class="s">'MaskedLanguageModel'</span><span class="p">:</span>
            <span class="n">g_logit</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">mlm_head</span><span class="p">(</span>
                <span class="n">g_last_hidden_states</span>
            <span class="p">)</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">rtd_masking</span> <span class="o">==</span> <span class="s">'SpanBoundaryObjective'</span><span class="p">:</span>
            <span class="n">g_logit</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">mlm_head</span><span class="p">(</span>
                <span class="n">g_last_hidden_states</span><span class="p">,</span>
                <span class="n">mask_labels</span>
            <span class="p">)</span>
        <span class="n">pred</span> <span class="o">=</span> <span class="n">g_logit</span><span class="p">.</span><span class="n">clone</span><span class="p">().</span><span class="n">detach</span><span class="p">()</span>
        <span class="n">d_inputs</span><span class="p">,</span> <span class="n">d_labels</span> <span class="o">=</span> <span class="n">get_discriminator_input</span><span class="p">(</span>
            <span class="n">inputs</span><span class="p">,</span>
            <span class="n">labels</span><span class="p">,</span>
            <span class="n">pred</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">g_logit</span><span class="p">,</span> <span class="n">d_inputs</span><span class="p">,</span> <span class="n">d_labels</span>

    <span class="k">def</span> <span class="nf">discriminator_fw</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span>
            <span class="n">inputs</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
            <span class="n">padding_mask</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
            <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="bp">None</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="n">d_last_hidden_states</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">discriminator</span><span class="p">(</span>
            <span class="n">inputs</span><span class="p">,</span>
            <span class="n">padding_mask</span><span class="p">,</span>
            <span class="n">attention_mask</span>
        <span class="p">)</span>
        <span class="n">d_logit</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">rtd_head</span><span class="p">(</span>
            <span class="n">d_last_hidden_states</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">d_logit</span>

</code></pre></div></div>
<p>ELECTRA 모델 객체는 크게 임배딩 레이어 공유, 생성자 포워드, 판별자 포워드 파트로 나뉜다. 먼저 임베딩 레이어 공유는 크게 두 가지 방식으로 구현 가능하다. 하나는 임베딩 레이어 인스턴스 자체를 공유하는 방식으로, 생성자와 판별자의 스케일이 동일할 때 사용할 수 있다. 나머지는 단어 임베딩, 포지션 임베딩의 가중치 행렬만 공유하는 방식으로, 서로 스케일이 달라도 사용할 수 있다. 논문에서 제시하는 가장 효율적인 방법은 후자이며, 판별자의 임베딩 행렬이 생성자의 임베딩 행렬의 주소를 가리키도록 함으로서 구현 가능하다.</p>

<h4 id="-rtd-input-making"><strong><code class="language-plaintext highlighter-rouge">🌆 RTD Input Making</code></strong></h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">Tensor</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Tuple</span>
<span class="kn">from</span> <span class="nn">configuration</span> <span class="kn">import</span> <span class="n">CFG</span>

<span class="k">def</span> <span class="nf">get_discriminator_input</span><span class="p">(</span><span class="n">inputs</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">labels</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">pred</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
    <span class="s">""" Post Processing for Replaced Token Detection Task
    1) get index of the highest probability of [MASK] token in pred tensor
    2) convert [MASK] token to prediction token
    3) make label for Discriminator

    Args:
        inputs: pure inputs from tokenizing by tokenizer
        labels: labels for masked language modeling
        pred: prediction tensor from Generator

    returns:
        d_inputs: torch.Tensor, shape of [Batch, Sequence], for Discriminator inputs
        d_labels: torch.Tensor, shape of [Sequence], for Discriminator labels
    """</span>
    <span class="c1"># 1) flatten pred to 2D Tensor
</span>    <span class="n">d_inputs</span><span class="p">,</span> <span class="n">d_labels</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">.</span><span class="n">clone</span><span class="p">().</span><span class="n">detach</span><span class="p">().</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="bp">None</span>  <span class="c1"># detach to prevent back-propagation
</span>    <span class="n">flat_pred</span><span class="p">,</span> <span class="n">flat_label</span> <span class="o">=</span> <span class="n">pred</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">pred</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)),</span> <span class="n">labels</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># (batch * sequence, vocab_size)
</span>
    <span class="c1"># 2) get index of the highest probability of [MASK] token
</span>    <span class="n">pred_token_idx</span><span class="p">,</span> <span class="n">mlm_mask_idx</span> <span class="o">=</span> <span class="n">flat_pred</span><span class="p">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">),</span> <span class="n">torch</span><span class="p">.</span><span class="n">where</span><span class="p">(</span><span class="n">flat_label</span> <span class="o">!=</span> <span class="o">-</span><span class="mi">100</span><span class="p">)</span>
    <span class="n">pred_tokens</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">index_select</span><span class="p">(</span><span class="n">pred_token_idx</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">mlm_mask_idx</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

    <span class="c1"># 3) convert [MASK] token to prediction token
</span>    <span class="n">d_inputs</span><span class="p">[</span><span class="n">mlm_mask_idx</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span> <span class="o">=</span> <span class="n">pred_tokens</span>

    <span class="c1"># 4) make label for Discriminator
</span>    <span class="n">original_tokens</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">.</span><span class="n">clone</span><span class="p">().</span><span class="n">detach</span><span class="p">().</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">original_tokens</span><span class="p">[</span><span class="n">mlm_mask_idx</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span> <span class="o">=</span> <span class="n">flat_label</span><span class="p">[</span><span class="n">mlm_mask_idx</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span>
    <span class="n">d_labels</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">eq</span><span class="p">(</span><span class="n">original_tokens</span><span class="p">,</span> <span class="n">d_inputs</span><span class="p">).</span><span class="nb">long</span><span class="p">()</span>
    <span class="n">d_inputs</span> <span class="o">=</span> <span class="n">d_inputs</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="n">pred</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># covert to [batch, sequence]
</span>    <span class="k">return</span> <span class="n">d_inputs</span><span class="p">,</span> <span class="n">d_labels</span>
</code></pre></div></div>
<p>이제 마지막으로 판별자의 입력을 만드는 알고리즘에 대한 코드를 보자. 알고리즘은 다음과 같다.</p>
<ul>
  <li>1) 개별 마스킹 토큰에 대한 예측 토큰 구하기
    <ul>
      <li>로짓을 실제 토큰 인덱스로 변환</li>
    </ul>
  </li>
  <li>2) 모든 마스킹 부분에 예측 토큰들로 대체</li>
  <li>3) 기존 입력과 2번으로 만들어진 시퀀스 비교해 라벨 생성
    <ul>
      <li>서로 같으면 0</li>
      <li>서로 다르면 1
이렇게 만들어진 새로운 입력 시퀀스와 라벨을 ELECTRA 모델 인스턴스의 판별자 포워드 메서드에 인자로 전달하면 된다.</li>
    </ul>
  </li>
</ul>

<h3 id="-future-work-읽고-구현하면서-느낀점--개선방향"><code class="language-plaintext highlighter-rouge">🌟 Future Work (읽고 구현하면서 느낀점 &amp; 개선방향)</code></h3>

<p>이렇게 ELECTRA 모델에 대한 구현까지 살펴봤다. 논문을 읽고 구현하면서 가장 의문스러웠던 부분은 임베딩 공유 방법이었다. 수학적으로 엄밀하게 계산하고 따져보지 못했지만, 직관적으로도 생성자의 MLM과 판별자의 RTD는 서로 성격이 상당히 다른 사전 학습 방법론이라는 것을 알 수 있다. 그렇다면 단순히 단어, 포지션 임베딩을 공유하는 경우 학습 방향성이 달라서 간섭이 발생하고 모델이 수렴하지 못할 여지가 생긴다. 이러한 <code class="language-plaintext highlighter-rouge">줄다리기 현상(tag-of-war)</code>을 어떻게 해결할 수 있을까에 대한 고민이 더 필요하다고 생각한다.</p>

<p>그래서 다음 포스팅에서는 이러한 줄다리기 현상을 해결하고자한 논문인 <strong><a href="https://arxiv.org/abs/2111.09543">&lt;DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing&gt;</a></strong>을 리뷰해보고자 한다.</p>]]></content><author><name>qcqced</name><email>qcqced123@gmail.com</email></author><category term="NLP" /><category term="Natural Language Process" /><category term="ELECTRA" /><category term="BERT" /><category term="GAN" /><category term="Transformer" /><category term="Self-Attention" /><category term="Pytorch" /><summary type="html"><![CDATA[ELECTRA Official Paper Review with Pytorch Implementation]]></summary></entry><entry><title type="html">🧑‍🏫 [DistilBERT] DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter</title><link href="http://localhost:4000/nlp/distilbert" rel="alternate" type="text/html" title="🧑‍🏫 [DistilBERT] DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter" /><published>2024-03-11T00:00:00+09:00</published><updated>2024-03-12T02:00:00+09:00</updated><id>http://localhost:4000/nlp/distilbert</id><content type="html" xml:base="http://localhost:4000/nlp/distilbert"><![CDATA[<h3 id="overview"><code class="language-plaintext highlighter-rouge">🔭 Overview</code></h3>

<p><code class="language-plaintext highlighter-rouge">DistilBERT</code> 는 허깅 페이스 연구진이 2019년 발표한 BERT의 변형으로서, On-Device Ai 개발을 목표로 경량화에 초점을 맞춘 모델이다. GPT, BERT의 등장 이후, NLP 분야에서 비약적인 성능 향상이 이뤄졌음에도 불구하고, 터무니 없는 모델 사이즈와 컴퓨팅 리소스 요구로 인해 실생활 적용 같은 활용성은 여전히 해결해야할 문제로 남아 있었다. Google에서 발표한 초기 <code class="language-plaintext highlighter-rouge">BERT-base-uncased</code> 만 해도 파라미터가 1억 1천만개 수준에 달한다.</p>

<p>이를 다양한 비즈니스 요구 상황에 적용할 수 있으려면 최소한 8GB 이상의 가속기 전용 RAM 공간을 요구로 한다. 오늘날 개인용 PC 혹은 서버 컴퓨터의 경우, 8GB 이상의 VRAM이 달린 GPU가 일반적으로 탑재되기 때문에 크게 문제 될 것 없는 요구사항이지만, On-Device 환경에서는 이야기가 달라진다. 최신 하이엔드 스마트폰인 Galaxy S24 Ultra, iPhone 15 Pro의 경우 12GB, 8GB의 램 용량을 보유하고 있다. 그마저도 대부분의 온디바이스 환경은 SoC 구조를 채택하고 있기 때문에 전용 가속기가 온전히 저 모든 램 공간을 활용할 수 없다.</p>

<p>따라서 온디바이스에 Ai를 적용하기 위해서는 획기적인 모델 경량화가 필요한 상황이고 그 출발점이 된 연구가 바로 <code class="language-plaintext highlighter-rouge">DistilBERT</code>다. 로컬 디바이스 환경에서도 언어 모델을 활용하기 위해 허깅 페이스 연구진은 지식 증류 기법을 활용해 인코더 기반 언어 모델의 파라미터를 획기적으로 줄이는데 성공한다.</p>

<p>정리하자면, <code class="language-plaintext highlighter-rouge">DistilBERT</code> 모델은 기존 BERT의 구조적 측면 개선이 아닌, 사전학습 방법 특히 경량화에 초점을 맞춘 시도라고 볼 수 있다. 따라서 어떤 모델이더라도, 인코더 언어 모델이라면 모두 <code class="language-plaintext highlighter-rouge">DistilBERT</code> 구조를 사용할 수 있으며, 기존 논문에서는 원본 BERT 구조를 사용했다. 이번 포스팅에서도 BERT 구조에 대한 설명 대신, <code class="language-plaintext highlighter-rouge">DistilBERT</code>의 사전 학습 방법론인 <code class="language-plaintext highlighter-rouge">Knowledge Distillation</code>에 대해서만 다루려고 한다.</p>

<h3 id="knowledge-distillations"><code class="language-plaintext highlighter-rouge">📲 Knowledge Distillations</code></h3>

\[\min_{\theta}\sum_{x \in X} \alpha \mathcal{L}_{\text{KL}}(x, \theta) + \beta \mathcal{L}_{\text{MLM}}(x, \theta) + \gamma \mathcal{L}_{\text{Cos}}(x, \theta)\]

<p><code class="language-plaintext highlighter-rouge">DistilBERT</code>는 Teacher-Student Architecture를 차용해 상대적으로 작은 파라미터 사이즈를 갖는 <code class="language-plaintext highlighter-rouge">Student</code> 모델에게 <code class="language-plaintext highlighter-rouge">Teacher</code>의 지식을 전수하는 것을 목표로 한다. 따라서 <code class="language-plaintext highlighter-rouge">Teacher</code> 모델은 이미 사전 학습을 마치고 수렴된 상태의 가중치를 갖고 있는 모델을 사용해야 한다. 더불어 Teacher 모델은 구조만 기존 BERT를 따르되, 사전 학습 방식은 RoBERTa의 방식과 동일(NSP 제거, Dynamic Masking 적용)하게 훈련되어야 한다.</p>

<p>한편, <code class="language-plaintext highlighter-rouge">Student</code> 모델은 <code class="language-plaintext highlighter-rouge">Teacher</code>의 60%정도 파라미터 사이즈를 갖도록 축소하여 사용한다. 이 때 축소는 모델의 <code class="language-plaintext highlighter-rouge">depth</code>(레이어 개수)에만 적용하는데, 연구진에 따르면 <code class="language-plaintext highlighter-rouge">width</code>(은닉층 크기)는 축소를 적용해도 연산 효율이 증가하지 않는다고 한다. 정리하면 <code class="language-plaintext highlighter-rouge">Teacher</code> 모델의 <code class="language-plaintext highlighter-rouge">레이어 개수*0.6</code>의 개수만큼 인코더를 쌓으면 된다는 것이다.</p>

<p>그리고 최대한 <code class="language-plaintext highlighter-rouge">Teacher</code>의 지식을 전수해야 하기 때문에, 데이터는 <code class="language-plaintext highlighter-rouge">Teacher</code> 를 수렴시킨 것과 동일한 세트를 이용해야 한다. 이 때, Teacher 모델은 이미 MLE 방식으로 훈련이 된 상태라서 로짓이 단일 토큰 하나 쪽으로 쏠려 있을 가능성이 매우 높다. 이는 <code class="language-plaintext highlighter-rouge">Student</code> 모델의 일반화 능력에 악영향을 미칠 가능성이 높다. 따라서 Temperature 변수 $T$ 도입해 소프트 맥스(로짓)의 분포를 평탄화 한다. 이렇게 하면, <code class="language-plaintext highlighter-rouge">argmax()</code> 가 아닌 다른 토큰 표현에 대해서도 <code class="language-plaintext highlighter-rouge">Student</code> 모델이 지식을 습득할 수 있어서 풍부한 문맥을 학습하고 일반화 능력을 높이는데 도움이 된다. 이를 <code class="language-plaintext highlighter-rouge">암흑 지식(Dark Knowledge)</code> 을 활용한다고 표현한다. Temperature 변수 $T$ 도입한 소프트맥스 함수 수식은 아래와 같다.</p>

\[\text{softmax}(x_i) = \frac{e^{\frac{x_i}{\tau}}}{\sum_{j} e^{\frac{x_j}{\tau}}}\]

<p>수식상 변수 $T$의 값을 1이상으로 세팅해야 평탄화를 할 수 있다. 따라서 연구진은 $T =2$ 로 두고 사전 학습을 진행했다(논문에 공개안됨, GitHub에 있음). 이번 파트 맨 처음에 등장한 수식을 다시 보자. 결국 <code class="language-plaintext highlighter-rouge">DisilBERT</code>의 목적함수는 3가지 손실의 가중합으로 구성된다. 이제부터는 개별 손실에 대해서 자세히 살펴보자.</p>

<h4 id="distillation-loss-kl-divergence-loss"><code class="language-plaintext highlighter-rouge">🪢 Distillation Loss: KL-Divergence Loss</code></h4>

\[\text{KL-Divergence}(P || Q) = \sum_{i} P(i) \log \frac{P(i)}{Q(i)}\]

<p>증류 손실로 사용되는 <code class="language-plaintext highlighter-rouge">KL-Divergence Loss</code>는 두 확률 분포 간의 차이를 측정하는 지표 중 하나다. 주로 확률 분포 P와 Q 사이의 차이를 나타내는데, 개별 요소의 확률값 차이가 클수록 합산값은 커져 손실이 커지게 된다. 반대로 두 분포의 개별 요소 확률값 차이가 작다면 당연히, 두 분포가 유사하다는 의미이므로 손실 역시 작아지게 된다. 일반적으로 <code class="language-plaintext highlighter-rouge">KL-Divergence Loss</code> 에서 확률분포 $P$ 가 이상적인 확률 분포를, $Q$ 가 모델이 예측한 확률분포를 의미한다. 따라서 <code class="language-plaintext highlighter-rouge">DistilBERT</code>의 경우 확률분포 $P$ 자리에는 <code class="language-plaintext highlighter-rouge">Teacher</code> 모델의 소프트맥스 분포가, $Q$ 에는 <code class="language-plaintext highlighter-rouge">Student</code> 모델의 소프트맥스 분포가 대입되면 된다. 이 때 두 확률분포 모두, 암흑 지식 획득을 위해 소프트맥스 평탄화를 적용한 결과를 사용한다. 논문에서, 선생 모델 예측에 평탄화를 적용한 것을 <code class="language-plaintext highlighter-rouge">소프트 라벨</code>, 학생 모델의 것에 적용한 결과는 <code class="language-plaintext highlighter-rouge">소프트 예측</code>이라고 부른다.</p>

<h4 id="student-loss-mlm-loss"><code class="language-plaintext highlighter-rouge">🧑‍🎓 Student Loss: MLM Loss</code></h4>

\[\mathcal{L}_{\text{MLM}} = - \sum_{i=1}^{N} \sum_{j=1}^{L} \mathbb{1}_{m_{ij}} \log \text{softmax}(x_{ij})\]

<p>학생 손실은 말그대로 기본적인 MLM 손실을 말한다. 정확한 손실값 계산을 위해서 학생의 소프트맥스 분포에 평탄화를 적용하지 않는다. 이를 논문에서는 <code class="language-plaintext highlighter-rouge">하드 예측</code>이라고 부른다. 라벨 역시 <code class="language-plaintext highlighter-rouge">Teacher</code>로부터 나온 것이 아닌 원래 MLM 수행에 사용되는 마스킹 라벨을 사용한다.</p>

<h4 id="cosine-embedding-loss-contrastive-loss-by-cosine-similarity"><code class="language-plaintext highlighter-rouge">🌆 Cosine Embedding Loss: Contrastive Loss by cosine similarity</code></h4>

\[\mathcal{L}_{\text{COS}}(x,y) = \begin{cases} 1 - \cos(x_1, x_2), &amp; \text{if } y = 1 \\ \max(0, \cos(x_1, x_2) - \text{margin}), &amp; \text{if } y = -1 \end{cases}\]

<p><code class="language-plaintext highlighter-rouge">Teacher</code> 모델과 <code class="language-plaintext highlighter-rouge">Student</code> 모델의 마지막 인코더 모델이 출력하는 은닉값에 대한 <code class="language-plaintext highlighter-rouge">Contrastive Loss</code>를 의미한다. 이 때 <code class="language-plaintext highlighter-rouge">Distance Metric</code>은 코사인 유사도를 사용한다. 그래서 코사인 임베딩 손실이라고 논문에서 정의하는 것으로 추정된다. 위 수식을 최적화하는 것을 목적으로 한다. 이 때 라벨은 <code class="language-plaintext highlighter-rouge">[BS, Seq_len]</code>의 크기를 갖되, 모든 원소는 1이 되도록 만든다. 이유는 간단하다. <code class="language-plaintext highlighter-rouge">Student</code> 모델의 은닉값이 <code class="language-plaintext highlighter-rouge">Teacher</code> 모델의 것과 최대한 비슷해지도록 만드는게 우리 목적이기 때문이다.</p>

<h3 id="implementation-by-pytorch"><code class="language-plaintext highlighter-rouge">👩‍💻 Implementation by Pytorch</code></h3>
<p>논문의 내용과 오피셜로 공개된 코드를 종합하여 파이토치로 <code class="language-plaintext highlighter-rouge">DistilBERT</code>를 구현해봤다. 논문에 포함된 아이디어를 이해하는데는 역시 어렵지 않았지만, 페이퍼에 hyper-param 테이블이 따로 제시되어 있지 않아 공개된 코드를 안 볼수가 없었다.</p>

<p>전체 모델 구조 대한 코드는 <strong><a href="https://github.com/qcqced123/model_study">여기 링크</a></strong>를 통해 참고바란다.</p>

<h4 id="knowledge-distillation-pipeline"><code class="language-plaintext highlighter-rouge">👩‍💻 Knowledge Distillation Pipeline</code></h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">train_val_fn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">loader_train</span><span class="p">,</span> <span class="n">model</span><span class="p">:</span> <span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">criterion</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">],</span> <span class="n">optimizer</span><span class="p">,</span><span class="n">scheduler</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]:</span>
    <span class="s">""" Function for train loop with validation for each batch*N Steps
    DistillBERT has three loss:

        1) distillation loss, calculated by soft targets &amp; soft predictions
            (nn.KLDIVLoss(reduction='batchmean'))

        2) student loss, calculated by hard targets &amp; hard predictions
            (nn.CrossEntropyLoss(reduction='mean')), same as pure MLM Loss

        3) cosine similarity loss, calculated by student &amp; teacher logit similarity
            (nn.CosineEmbeddingLoss(reduction='mean')), similar as contrastive loss

    Those 3 losses are summed jointly and then backward to student model
    """</span>
    <span class="n">scaler</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">amp</span><span class="p">.</span><span class="n">GradScaler</span><span class="p">(</span><span class="n">enabled</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">amp_scaler</span><span class="p">)</span>
    <span class="n">model</span><span class="p">.</span><span class="n">train</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">step</span><span class="p">,</span> <span class="n">batch</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">tqdm</span><span class="p">(</span><span class="n">loader_train</span><span class="p">)):</span>
        <span class="n">optimizer</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">(</span><span class="n">set_to_none</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="n">inputs</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="s">'input_ids'</span><span class="p">].</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">device</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="n">labels</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="s">'labels'</span><span class="p">].</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">device</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="n">padding_mask</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="s">'padding_mask'</span><span class="p">].</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">device</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

        <span class="n">mask</span> <span class="o">=</span> <span class="n">padding_mask</span><span class="p">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">).</span><span class="n">expand</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">dim_model</span><span class="p">)</span>  <span class="c1"># for hidden states dim
</span>        <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">t_hidden_state</span><span class="p">,</span> <span class="n">soft_target</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">teacher_fw</span><span class="p">(</span>
                <span class="n">inputs</span><span class="o">=</span><span class="n">inputs</span><span class="p">,</span>
                <span class="n">padding_mask</span><span class="o">=</span><span class="n">padding_mask</span><span class="p">,</span>
                <span class="n">mask</span><span class="o">=</span><span class="n">mask</span>
            <span class="p">)</span>  <span class="c1"># teacher model's pred =&gt; hard logit
</span>
        <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">amp</span><span class="p">.</span><span class="n">autocast</span><span class="p">(</span><span class="n">enabled</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">amp_scaler</span><span class="p">):</span>
            <span class="n">s_hidden_state</span><span class="p">,</span> <span class="n">s_logit</span><span class="p">,</span> <span class="n">soft_pred</span><span class="p">,</span> <span class="n">c_labels</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">student_fw</span><span class="p">(</span>
                <span class="n">inputs</span><span class="o">=</span><span class="n">inputs</span><span class="p">,</span>
                <span class="n">padding_mask</span><span class="o">=</span><span class="n">padding_mask</span><span class="p">,</span>
                <span class="n">mask</span><span class="o">=</span><span class="n">mask</span>
            <span class="p">)</span>
            <span class="n">d_loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">[</span><span class="s">"KLDivLoss"</span><span class="p">](</span><span class="n">soft_pred</span><span class="p">.</span><span class="n">log</span><span class="p">(),</span> <span class="n">soft_target</span><span class="p">)</span>  <span class="c1"># nn.KLDIVLoss
</span>            <span class="n">s_loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">[</span><span class="s">"CrossEntropyLoss"</span><span class="p">](</span><span class="n">s_logit</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">vocab_size</span><span class="p">),</span> <span class="n">labels</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>  <span class="c1"># nn.CrossEntropyLoss
</span>            <span class="n">c_loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">[</span><span class="s">"CosineEmbeddingLoss"</span><span class="p">](</span><span class="n">s_hidden_state</span><span class="p">,</span> <span class="n">t_hidden_state</span><span class="p">,</span> <span class="n">c_labels</span><span class="p">)</span>  <span class="c1"># nn.CosineEmbeddingLoss
</span>            <span class="n">loss</span> <span class="o">=</span> <span class="n">d_loss</span><span class="o">*</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">alpha_distillation</span> <span class="o">+</span> <span class="n">s_loss</span><span class="o">*</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">alpha_student</span> <span class="o">+</span> <span class="n">c_loss</span><span class="o">*</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">alpha_cosine</span>  <span class="c1"># linear combination loss
</span>
        <span class="n">scaler</span><span class="p">.</span><span class="n">scale</span><span class="p">(</span><span class="n">loss</span><span class="p">).</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">scaler</span><span class="p">.</span><span class="n">step</span><span class="p">(</span><span class="n">optimizer</span><span class="p">)</span>
        <span class="n">scaler</span><span class="p">.</span><span class="n">update</span><span class="p">()</span>
        <span class="n">scheduler</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>
</code></pre></div></div>

<h4 id="knowledge-distillation-model"><code class="language-plaintext highlighter-rouge">👩‍💻 Knowledge Distillation Model</code></h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">DistillationKnowledge</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">AbstractTask</span><span class="p">):</span>
    <span class="s">""" Custom Task Module for Knowledge Distillation by DistilBERT Style Architecture
    DistilBERT Style Architecture is Teacher-Student Framework for Knowledge Distillation,

    And then they have 3 objective functions:
        1) distillation loss, calculated by soft targets &amp; soft predictions
            (nn.KLDIVLoss(reduction='batchmean'))
        2) student loss, calculated by hard targets &amp; hard predictions
            (nn.CrossEntropyLoss(reduction='mean')), same as pure MLM Loss
        3) cosine similarity loss, calculated by student &amp; teacher logit similarity
            (nn.CosineEmbeddingLoss(reduction='mean')), similar as contrastive loss

    References:
        https://arxiv.org/pdf/1910.01108.pdf
        https://github.com/huggingface/transformers/blob/main/examples/research_projects/distillation/distiller.py
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">cfg</span><span class="p">:</span> <span class="n">CFG</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">DistillationKnowledge</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span> <span class="o">=</span> <span class="n">CFG</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">DistilBERT</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">,</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">select_model</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">_init_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">teacher_load_pretrained</span><span class="p">:</span>  <span class="c1"># for teacher model
</span>            <span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">teacher</span><span class="p">.</span><span class="n">load_state_dict</span><span class="p">(</span>
                <span class="n">torch</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="n">cfg</span><span class="p">.</span><span class="n">checkpoint_dir</span> <span class="o">+</span> <span class="n">cfg</span><span class="p">.</span><span class="n">teacher_state_dict</span><span class="p">),</span>
                <span class="n">strict</span><span class="o">=</span><span class="bp">False</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">student_load_pretrained</span><span class="p">:</span>  <span class="c1"># for student model
</span>            <span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">student</span><span class="p">.</span><span class="n">load_state_dict</span><span class="p">(</span>
                <span class="n">torch</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="n">cfg</span><span class="p">.</span><span class="n">checkpoint_dir</span> <span class="o">+</span> <span class="n">cfg</span><span class="p">.</span><span class="n">student_state_dict</span><span class="p">),</span>
                <span class="n">strict</span><span class="o">=</span><span class="bp">True</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">freeze</span><span class="p">:</span>
            <span class="n">freeze</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">teacher</span><span class="p">)</span>
            <span class="n">freeze</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">mlm_head</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">gradient_checkpoint</span><span class="p">:</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">gradient_checkpointing_enable</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">teacher_fw</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">inputs</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">padding_mask</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">mask</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
        <span class="n">is_valid</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="bp">False</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
        <span class="s">""" teacher forward pass to make soft target, last_hidden_state for distillation loss """</span>
        <span class="c1"># 1) make soft target
</span>        <span class="n">temperature</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="k">if</span> <span class="n">is_valid</span> <span class="k">else</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">temperature</span>
        <span class="n">last_hidden_state</span><span class="p">,</span> <span class="n">t_logit</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">teacher_fw</span><span class="p">(</span>
            <span class="n">inputs</span><span class="p">,</span>
            <span class="n">padding_mask</span><span class="p">,</span>
            <span class="n">attention_mask</span>
        <span class="p">)</span>
        <span class="n">last_hidden_state</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">masked_select</span><span class="p">(</span><span class="n">last_hidden_state</span><span class="p">,</span> <span class="o">~</span><span class="n">mask</span><span class="p">)</span>  <span class="c1"># for inverse select
</span>        <span class="n">last_hidden_state</span> <span class="o">=</span> <span class="n">last_hidden_state</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">dim_model</span><span class="p">)</span>  <span class="c1"># flatten last_hidden_state
</span>        <span class="n">soft_target</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="n">softmax</span><span class="p">(</span>
            <span class="n">t_logit</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">vocab_size</span><span class="p">)</span> <span class="o">/</span> <span class="n">temperature</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span>  <span class="c1"># flatten softmax distribution
</span>            <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span>
        <span class="p">)</span>  <span class="c1"># [bs* seq, vocab_size]
</span>        <span class="k">return</span> <span class="n">last_hidden_state</span><span class="p">,</span> <span class="n">soft_target</span>

    <span class="k">def</span> <span class="nf">student_fw</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">inputs</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">padding_mask</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">mask</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
        <span class="n">is_valid</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="bp">False</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
        <span class="s">""" student forward pass to make soft prediction, hard prediction for student loss """</span>
        <span class="n">temperature</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="k">if</span> <span class="n">is_valid</span> <span class="k">else</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">temperature</span>
        <span class="n">last_hidden_state</span><span class="p">,</span> <span class="n">s_logit</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">teacher_fw</span><span class="p">(</span>
            <span class="n">inputs</span><span class="p">,</span>
            <span class="n">padding_mask</span><span class="p">,</span>
            <span class="n">attention_mask</span>
        <span class="p">)</span>
        <span class="n">last_hidden_state</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">masked_select</span><span class="p">(</span><span class="n">last_hidden_state</span><span class="p">,</span> <span class="o">~</span><span class="n">mask</span><span class="p">)</span>  <span class="c1"># for inverse select
</span>        <span class="n">last_hidden_state</span> <span class="o">=</span> <span class="n">last_hidden_state</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">dim_model</span><span class="p">)</span>  <span class="c1"># flatten last_hidden_state
</span>        <span class="n">c_labels</span> <span class="o">=</span> <span class="n">last_hidden_state</span><span class="p">.</span><span class="n">new</span><span class="p">(</span><span class="n">last_hidden_state</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)).</span><span class="n">fill_</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">soft_pred</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="n">softmax</span><span class="p">(</span>
            <span class="n">s_logit</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">vocab_size</span><span class="p">)</span> <span class="o">/</span> <span class="n">temperature</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span>  <span class="c1"># flatten softmax distribution
</span>            <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">last_hidden_state</span><span class="p">,</span> <span class="n">s_logit</span><span class="p">,</span> <span class="n">soft_pred</span><span class="p">,</span> <span class="n">c_labels</span>
</code></pre></div></div>

<h4 id="distilbert-model"><code class="language-plaintext highlighter-rouge">👩‍💻 DistilBERT Model</code></h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">DistilBERT</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">AbstractModel</span><span class="p">):</span>
    <span class="s">""" Main class for DistilBERT Style Model, Teacher-Student Framework
    for Knowledge Distillation aim to lighter Large Scale LLM model. This model have 3 objective functions:

        1) distillation loss, calculated by soft targets &amp; soft predictions
            (nn.KLDIVLoss(reduction='batchmean'))

        2) student loss, calculated by hard targets &amp; hard predictions
            (nn.CrossEntropyLoss(reduction='mean')), same as pure MLM Loss

        3) cosine similarity loss, calculated by student &amp; teacher logit similarity
            (nn.CosineEmbeddingLoss(reduction='mean')), similar as contrastive loss

    soft targets &amp; soft predictions are meaning that logit are passed through softmax function applied with temperature T
    temperature T aim to flatten softmax layer distribution for making "Dark Knowledge" from teacher model

    hard targets &amp; hard predictions are meaning that logit are passed through softmax function without temperature T
    hard targets are same as just simple labels from MLM Collator returns for calculating cross entropy loss

    cosine similarity loss is calculated by cosine similarity between student &amp; teacher
    in official repo, they mask padding tokens for calculating cosine similarity, target for this task is 1
    cosine similarity is calculated by nn.CosineSimilarity() function, values are range to [-1, 1]

    you can select any other backbone model architecture for Teacher &amp; Student Model for knowledge distillation
    but, in original paper, BERT is used for Teacher Model &amp; Student
    and you must select pretrained model for Teacher Model, because Teacher Model is used for knowledge distillation,
    which is containing pretrained mlm head

    Do not pass gradient backward to teacher model!!
    (teacher model must be frozen or register_buffer to model or use no_grad() context manager)

    Args:
        cfg: configuration.CFG
        model_func: make model instance in runtime from config.json

    References:
        https://arxiv.org/pdf/1910.01108.pdf
        https://github.com/huggingface/transformers/blob/main/examples/research_projects/distillation/distiller.py
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">cfg</span><span class="p">:</span> <span class="n">CFG</span><span class="p">,</span> <span class="n">model_func</span><span class="p">:</span> <span class="n">Callable</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">DistilBERT</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span> <span class="o">=</span> <span class="n">cfg</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">teacher</span> <span class="o">=</span> <span class="n">model_func</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">teacher_num_layers</span><span class="p">)</span>  <span class="c1"># must be loading pretrained model containing mlm head
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">mlm_head</span> <span class="o">=</span> <span class="n">MLMHead</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">)</span>  <span class="c1"># must be loading pretrained model's mlm head
</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">student</span> <span class="o">=</span> <span class="n">model_func</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">student_num_layers</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">s_mlm_head</span> <span class="o">=</span> <span class="n">MLMHead</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">teacher_fw</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">inputs</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">padding_mask</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
        <span class="s">""" forward pass for teacher model
        """</span>
        <span class="n">last_hidden_state</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">teacher</span><span class="p">(</span>
            <span class="n">inputs</span><span class="p">,</span>
            <span class="n">padding_mask</span><span class="p">,</span>
            <span class="n">attention_mask</span>
        <span class="p">)</span>
        <span class="n">t_logit</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">mlm_head</span><span class="p">(</span><span class="n">last_hidden_state</span><span class="p">)</span>  <span class="c1"># hard logit =&gt; to make soft logit
</span>        <span class="k">return</span> <span class="n">last_hidden_state</span><span class="p">,</span> <span class="n">t_logit</span>

    <span class="k">def</span> <span class="nf">student_fw</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">inputs</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">padding_mask</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="bp">None</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
        <span class="s">""" forward pass for student model
        """</span>
        <span class="n">last_hidden_state</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">student</span><span class="p">(</span>
            <span class="n">inputs</span><span class="p">,</span>
            <span class="n">padding_mask</span><span class="p">,</span>
            <span class="n">attention_mask</span>
        <span class="p">)</span>
        <span class="n">s_logit</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">s_mlm_head</span><span class="p">(</span><span class="n">last_hidden_state</span><span class="p">)</span>  <span class="c1"># hard logit =&gt; to make soft logit
</span>        <span class="k">return</span> <span class="n">last_hidden_state</span><span class="p">,</span> <span class="n">s_logit</span>
</code></pre></div></div>]]></content><author><name>qcqced</name><email>qcqced123@gmail.com</email></author><category term="NLP" /><category term="Natural Language Process" /><category term="DistilBERT" /><category term="BERT" /><category term="Self-Attention" /><category term="Pytorch" /><summary type="html"><![CDATA[DistilBERT Official Paper Review with Pytorch Implementation]]></summary></entry><entry><title type="html">🪢 [DeBERTa-V3] DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing</title><link href="http://localhost:4000/nlp/deberta_v3" rel="alternate" type="text/html" title="🪢 [DeBERTa-V3] DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing" /><published>2024-03-11T00:00:00+09:00</published><updated>2024-03-12T02:00:00+09:00</updated><id>http://localhost:4000/nlp/deberta-v3</id><content type="html" xml:base="http://localhost:4000/nlp/deberta_v3"><![CDATA[<h3 id="overview"><code class="language-plaintext highlighter-rouge">🔭 Overview</code></h3>

<p>2021년 Microsoft에서 공개한 <code class="language-plaintext highlighter-rouge">DeBERTa-V3</code>은 기존 DeBERTa의 모델 구조는 그대로 유지하되, ELECTRA의 Generator-Discriminator 구조를 차용하여 전작 대비 성능을 향상 시킨 모델이다. ELECTRA에서 BackBone 모델로 BERT 대신 <code class="language-plaintext highlighter-rouge">DeBERTa을</code> 사용했다고 생각하면 된다. 거기에 더해 ELECTRA의 <code class="language-plaintext highlighter-rouge">Tug-of-War</code> 현상을 방지하기 위해 새로운 임베딩 공유 기법인 <code class="language-plaintext highlighter-rouge">GDES(Gradient Disentagnled Embedding Sharing)</code>방법을 제시했다.</p>

<p>이번 포스팅에서는 구현 코드와 함께 GDES에 대해서만 살펴보려 한다. ELECTRA, DeBERTa에 대해 궁금하다면 이전 포스팅을, 전체 구조에 대한 코드는 <strong><a href="https://github.com/qcqced123/model_study">여기 링크</a></strong>를 통해 확인 가능하다.</p>

<h3 id="gdes-gradient-disentangled-embedding-sharing"><code class="language-plaintext highlighter-rouge">🪢GDES: Gradient Disentangled Embedding Sharing</code></h3>

<p align="center">
<img src="/assets/images/deberta_v3/deberta_v3.png" alt="GDES" class="align-center image-caption" width="100%&quot;, height=&quot;100%" />
<strong><em><a href="https://arxiv.org/abs/2111.09543">GDES</a></em></strong>
</p>

<p>그림의 (a)가 기존 ELECTRA의 가중치 공유 방식, (c)가 GDES에 해당된다. 그림 속 모식도와 설명이 좀 복잡해 보이지만 아이디어는 매우 간단하다.</p>

<p>생성자와 판별자가 서로 포워드 패스 시점에는 단어, 위치 임베딩을 공유하되, 백워드 패스 시점에서는 공유되지 못하도록 하여, 판별자의 학습 결과에 의해 생성자의 단어 임베딩, 위치 임베딩이 업데이트 되지 못하도록 하지는 것이다. 오직 생성자의 MLM 학습에 의해서만 단어 및 위치 임베딩이 업데이트 되어야 한다.</p>

\[E_{D} = \text{sg}(E_{G}) + E_{\Delta}\]

<p>필자가 추정하기로는 <code class="language-plaintext highlighter-rouge">Skip-Connection</code>에서 영감을 받지 않았나 싶은 이 수식은, 생성자의 임베딩에 잔차값들을 더해 판별자의 임베딩 행렬이 RTD에 최적화 되도록 설계 되었다. 여기서 <code class="language-plaintext highlighter-rouge">sg()</code> 는 <code class="language-plaintext highlighter-rouge">stop gradient</code>를 의미한다. 다시 말해, 생성자의 임베딩 가중치를 판별자 학습에 사용하되, 해당 시점에서는 계산 그래프 작성을 중단시켜 판별자의 학습 결과(이진 분류 손실)가 생성자의 임베딩 가중치에 영향을 미치지 못하도록 한 것이다.</p>

<p>이러한 아이디어는 실제로 어떻게 코드로 구현해야할까, 아래 코드와 함께 살펴보자.</p>
<h3 id="implementation-by-pytorch"><code class="language-plaintext highlighter-rouge">👩‍💻 Implementation by Pytorch</code></h3>
<p>ELECTRA 모듈 <strong><code class="language-plaintext highlighter-rouge">__init__</code></strong>의 <strong><code class="language-plaintext highlighter-rouge">share_embed_method</code></strong>에 따라 브랜치가 발생하는 구간과, 아래 <strong><code class="language-plaintext highlighter-rouge">share_embedding()</code></strong> 메서드에 주목해보자.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="nn">experiment.models.abstract_model</span> <span class="kn">import</span> <span class="n">AbstractModel</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">Tensor</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">Callable</span>
<span class="kn">from</span> <span class="nn">einops.layers.torch</span> <span class="kn">import</span> <span class="n">Rearrange</span>
<span class="kn">from</span> <span class="nn">experiment.tuner.mlm</span> <span class="kn">import</span> <span class="n">MLMHead</span>
<span class="kn">from</span> <span class="nn">experiment.tuner.sbo</span> <span class="kn">import</span> <span class="n">SBOHead</span>
<span class="kn">from</span> <span class="nn">experiment.tuner.rtd</span> <span class="kn">import</span> <span class="n">get_discriminator_input</span><span class="p">,</span> <span class="n">RTDHead</span>
<span class="kn">from</span> <span class="nn">configuration</span> <span class="kn">import</span> <span class="n">CFG</span>


<span class="k">class</span> <span class="nc">ELECTRA</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">AbstractModel</span><span class="p">):</span>
    <span class="s">""" If you want to use pure ELECTRA, you should set share_embedding = ES
    elif you want to use ELECTRA with GDES, you should set share_embedding = GDES
    GDES is new approach of embedding sharing method from DeBERTa-V3 paper

    Args:
        cfg: configuration.CFG
        model_func: make model instance in runtime from config.json

    Var:
        cfg: configuration.CFG
        generator: Generator, which is used for generating replaced tokens for RTD
                   should select backbone model ex) BERT, RoBERTa, DeBERTa, ...
        discriminator: Discriminator, which is used for detecting replaced tokens for RTD
                       should select backbone model ex) BERT, RoBERTa, DeBERTa, ...
        share_embedding: whether or not to share embedding layer (word &amp; pos) between Generator &amp; Discriminator
        self.word_bias: Delta_E in paper
        self.abs_pos_bias: Delta_E in paper
        self.rel_pos_bias: Delta_E in paper

    References:
        https://arxiv.org/pdf/2003.10555.pdf
        https://arxiv.org/pdf/2111.09543.pdf
        https://github.com/google-research/electra
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">cfg</span><span class="p">:</span> <span class="n">CFG</span><span class="p">,</span> <span class="n">model_func</span><span class="p">:</span> <span class="n">Callable</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">ELECTRA</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span> <span class="o">=</span> <span class="n">cfg</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">generator</span> <span class="o">=</span> <span class="n">model_func</span><span class="p">(</span><span class="n">cfg</span><span class="p">.</span><span class="n">generator_num_layers</span><span class="p">)</span>  <span class="c1"># init generator
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">mlm_head</span> <span class="o">=</span> <span class="n">MLMHead</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">rtd_masking</span> <span class="o">==</span> <span class="s">'SpanBoundaryObjective'</span><span class="p">:</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">mlm_head</span> <span class="o">=</span> <span class="n">SBOHead</span><span class="p">(</span>
                <span class="n">cfg</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">,</span>
                <span class="n">is_concatenate</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">is_concatenate</span><span class="p">,</span>
                <span class="n">max_span_length</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">max_span_length</span>
            <span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">discriminator</span> <span class="o">=</span> <span class="n">model_func</span><span class="p">(</span><span class="n">cfg</span><span class="p">.</span><span class="n">discriminator_num_layers</span><span class="p">)</span>  <span class="c1"># init generator
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">rtd_head</span> <span class="o">=</span> <span class="n">RTDHead</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">share_embed_method</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">share_embed_method</span>  <span class="c1"># instance, es, gdes
</span>        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">share_embed_method</span> <span class="o">==</span> <span class="s">'GDES'</span><span class="p">:</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">word_bias</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Parameter</span><span class="p">(</span>
                <span class="n">torch</span><span class="p">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">discriminator</span><span class="p">.</span><span class="n">embeddings</span><span class="p">.</span><span class="n">word_embedding</span><span class="p">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">abs_pos_bias</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Parameter</span><span class="p">(</span>
                <span class="n">torch</span><span class="p">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">discriminator</span><span class="p">.</span><span class="n">embeddings</span><span class="p">.</span><span class="n">abs_pos_emb</span><span class="p">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>
            <span class="p">)</span>
            <span class="nb">delattr</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">discriminator</span><span class="p">.</span><span class="n">embeddings</span><span class="p">.</span><span class="n">word_embedding</span><span class="p">,</span> <span class="s">'weight'</span><span class="p">)</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">discriminator</span><span class="p">.</span><span class="n">embeddings</span><span class="p">.</span><span class="n">word_embedding</span><span class="p">.</span><span class="n">register_parameter</span><span class="p">(</span><span class="s">'_weight'</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">word_bias</span><span class="p">)</span>

            <span class="nb">delattr</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">discriminator</span><span class="p">.</span><span class="n">embeddings</span><span class="p">.</span><span class="n">abs_pos_emb</span><span class="p">,</span> <span class="s">'weight'</span><span class="p">)</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">discriminator</span><span class="p">.</span><span class="n">embeddings</span><span class="p">.</span><span class="n">abs_pos_emb</span><span class="p">.</span><span class="n">register_parameter</span><span class="p">(</span><span class="s">'_weight'</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">abs_pos_bias</span><span class="p">)</span>

            <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">model_name</span> <span class="o">==</span> <span class="s">'DeBERTa'</span><span class="p">:</span>
                <span class="bp">self</span><span class="p">.</span><span class="n">rel_pos_bias</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Parameter</span><span class="p">(</span>
                    <span class="n">torch</span><span class="p">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">discriminator</span><span class="p">.</span><span class="n">embeddings</span><span class="p">.</span><span class="n">rel_pos_emb</span><span class="p">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>
                <span class="p">)</span>
                <span class="nb">delattr</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">discriminator</span><span class="p">.</span><span class="n">embeddings</span><span class="p">.</span><span class="n">rel_pos_emb</span><span class="p">,</span> <span class="s">'weight'</span><span class="p">)</span>
                <span class="bp">self</span><span class="p">.</span><span class="n">discriminator</span><span class="p">.</span><span class="n">embeddings</span><span class="p">.</span><span class="n">rel_pos_emb</span><span class="p">.</span><span class="n">register_parameter</span><span class="p">(</span><span class="s">'_weight'</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">rel_pos_emb</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">share_embedding</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">share_embedding</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="k">def</span> <span class="nf">discriminator_hook</span><span class="p">(</span><span class="n">module</span><span class="p">:</span> <span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">,</span> <span class="o">*</span><span class="n">inputs</span><span class="p">):</span>
            <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">share_embed_method</span> <span class="o">==</span> <span class="s">'instance'</span><span class="p">:</span>  <span class="c1"># Instance Sharing
</span>                <span class="bp">self</span><span class="p">.</span><span class="n">discriminator</span><span class="p">.</span><span class="n">embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">generator</span><span class="p">.</span><span class="n">embeddings</span>

            <span class="k">elif</span> <span class="bp">self</span><span class="p">.</span><span class="n">share_embed_method</span> <span class="o">==</span> <span class="s">'ES'</span><span class="p">:</span>  <span class="c1"># ES (Embedding Sharing)
</span>                <span class="bp">self</span><span class="p">.</span><span class="n">discriminator</span><span class="p">.</span><span class="n">embeddings</span><span class="p">.</span><span class="n">word_embedding</span><span class="p">.</span><span class="n">weight</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">generator</span><span class="p">.</span><span class="n">embeddings</span><span class="p">.</span><span class="n">word_embedding</span><span class="p">.</span><span class="n">weight</span>
                <span class="bp">self</span><span class="p">.</span><span class="n">discriminator</span><span class="p">.</span><span class="n">embeddings</span><span class="p">.</span><span class="n">abs_pos_emb</span><span class="p">.</span><span class="n">weight</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">generator</span><span class="p">.</span><span class="n">embeddings</span><span class="p">.</span><span class="n">abs_pos_emb</span><span class="p">.</span><span class="n">weight</span>
                <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">model_name</span> <span class="o">==</span> <span class="s">'DeBERTa'</span><span class="p">:</span>
                    <span class="bp">self</span><span class="p">.</span><span class="n">discriminator</span><span class="p">.</span><span class="n">embeddings</span><span class="p">.</span><span class="n">rel_pos_emb</span><span class="p">.</span><span class="n">weight</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">generator</span><span class="p">.</span><span class="n">embeddings</span><span class="p">.</span><span class="n">rel_pos_emb</span><span class="p">.</span><span class="n">weight</span>

            <span class="k">elif</span> <span class="bp">self</span><span class="p">.</span><span class="n">share_embed_method</span> <span class="o">==</span> <span class="s">'GDES'</span><span class="p">:</span>  <span class="c1"># GDES (Generator Discriminator Embedding Sharing)
</span>                <span class="n">g_w_emb</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">generator</span><span class="p">.</span><span class="n">embeddings</span><span class="p">.</span><span class="n">word_embedding</span>
                <span class="n">d_w_emb</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">discriminator</span><span class="p">.</span><span class="n">embeddings</span><span class="p">.</span><span class="n">word_embedding</span>
                <span class="bp">self</span><span class="p">.</span><span class="n">_set_param</span><span class="p">(</span><span class="n">d_w_emb</span><span class="p">,</span> <span class="s">'weight'</span><span class="p">,</span> <span class="n">g_w_emb</span><span class="p">.</span><span class="n">weight</span><span class="p">.</span><span class="n">detach</span><span class="p">()</span> <span class="o">+</span> <span class="n">d_w_emb</span><span class="p">.</span><span class="n">_weight</span><span class="p">)</span>
                <span class="n">g_p_emb</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">generator</span><span class="p">.</span><span class="n">embeddings</span><span class="p">.</span><span class="n">abs_pos_emb</span>
                <span class="n">d_p_emb</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">discriminator</span><span class="p">.</span><span class="n">embeddings</span><span class="p">.</span><span class="n">abs_pos_emb</span>
                <span class="bp">self</span><span class="p">.</span><span class="n">_set_param</span><span class="p">(</span><span class="n">d_p_emb</span><span class="p">,</span> <span class="s">'weight'</span><span class="p">,</span> <span class="n">g_p_emb</span><span class="p">.</span><span class="n">weight</span><span class="p">.</span><span class="n">detach</span><span class="p">()</span> <span class="o">+</span> <span class="n">d_p_emb</span><span class="p">.</span><span class="n">_weight</span><span class="p">)</span>

                <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">model_name</span> <span class="o">==</span> <span class="s">'DeBERTa'</span><span class="p">:</span>
                    <span class="n">g_rp_emb</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">generator</span><span class="p">.</span><span class="n">embeddings</span><span class="p">.</span><span class="n">rel_pos_emb</span>
                    <span class="n">d_rp_emb</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">discriminator</span><span class="p">.</span><span class="n">embeddings</span><span class="p">.</span><span class="n">rel_pos_emb</span>
                    <span class="bp">self</span><span class="p">.</span><span class="n">_set_param</span><span class="p">(</span><span class="n">d_rp_emb</span><span class="p">,</span> <span class="s">'weight'</span><span class="p">,</span> <span class="n">g_rp_emb</span><span class="p">.</span><span class="n">weight</span><span class="p">.</span><span class="n">detach</span><span class="p">()</span> <span class="o">+</span> <span class="n">d_rp_emb</span><span class="p">.</span><span class="n">_weight</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">discriminator</span><span class="p">.</span><span class="n">register_forward_pre_hook</span><span class="p">(</span><span class="n">discriminator_hook</span><span class="p">)</span>

    <span class="o">@</span><span class="nb">staticmethod</span>
    <span class="k">def</span> <span class="nf">_set_param</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">param_name</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="n">module</span><span class="p">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="n">param_name</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">generator_fw</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">labels</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">padding_mask</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">mask_labels</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="bp">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
        <span class="n">g_last_hidden_states</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">generator</span><span class="p">(</span>
            <span class="n">inputs</span><span class="p">,</span>
            <span class="n">padding_mask</span><span class="p">,</span>
            <span class="n">attention_mask</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">rtd_masking</span> <span class="o">==</span> <span class="s">'MaskedLanguageModel'</span><span class="p">:</span>
            <span class="n">g_logit</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">mlm_head</span><span class="p">(</span>
                <span class="n">g_last_hidden_states</span>
            <span class="p">)</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">rtd_masking</span> <span class="o">==</span> <span class="s">'SpanBoundaryObjective'</span><span class="p">:</span>
            <span class="n">g_logit</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">mlm_head</span><span class="p">(</span>
                <span class="n">g_last_hidden_states</span><span class="p">,</span>
                <span class="n">mask_labels</span>
            <span class="p">)</span>
        <span class="n">pred</span> <span class="o">=</span> <span class="n">g_logit</span><span class="p">.</span><span class="n">clone</span><span class="p">().</span><span class="n">detach</span><span class="p">()</span>
        <span class="n">d_inputs</span><span class="p">,</span> <span class="n">d_labels</span> <span class="o">=</span> <span class="n">get_discriminator_input</span><span class="p">(</span>
            <span class="n">inputs</span><span class="p">,</span>
            <span class="n">labels</span><span class="p">,</span>
            <span class="n">pred</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">g_logit</span><span class="p">,</span> <span class="n">d_inputs</span><span class="p">,</span> <span class="n">d_labels</span>

    <span class="k">def</span> <span class="nf">discriminator_fw</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">padding_mask</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span><span class="n">attention_mask</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="bp">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="n">d_last_hidden_states</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">discriminator</span><span class="p">(</span>
            <span class="n">inputs</span><span class="p">,</span>
            <span class="n">padding_mask</span><span class="p">,</span>
            <span class="n">attention_mask</span>
        <span class="p">)</span>
        <span class="n">d_logit</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">rtd_head</span><span class="p">(</span>
            <span class="n">d_last_hidden_states</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">d_logit</span>
</code></pre></div></div>
<p>먼저 <strong><code class="language-plaintext highlighter-rouge">__init__</code></strong>의 브랜치 구간을 살펴보자. <strong><code class="language-plaintext highlighter-rouge">word_bias</code></strong>, <strong><code class="language-plaintext highlighter-rouge">pos_bias</code></strong>를 만들어 <code class="language-plaintext highlighter-rouge">register_parameter</code>화를 하고 있다. 새롭게 생성되어 <strong><code class="language-plaintext highlighter-rouge">_weight</code></strong>이란 이름으로 생성자의 파라미터가 된 두 가중치가 바로 $E_{\Delta}$ 가 된다.</p>

<p>다음 <strong><code class="language-plaintext highlighter-rouge">share_embedding()</code></strong> 메서드를 보자. $E_{G}$ 에 <code class="language-plaintext highlighter-rouge">torch.detach()</code>를 사용해 수식의 <code class="language-plaintext highlighter-rouge">stop gradient</code> 효과를 적용한다. 그리고 두 가중치를 더하고, <code class="language-plaintext highlighter-rouge">torch.register_buffer</code>를 호출해 포워드 패스에 활용은 되지만 백워드 패스에 그라디언트가 해당 가중치를 업데이트 하지 못하도록 설정한다. 그리고 마지막에 <code class="language-plaintext highlighter-rouge">torch.register_forward_pre_hook</code>을 호출하는데, 그 이유는 $E_{G}$ 에 <code class="language-plaintext highlighter-rouge">torch.detach()</code> 를 사용했기 때문에 현재 판별자의 버퍼에 있는 $E_{G}$ 는 이전 시점의 생성자 MLM 손실에 의해 새롭게 업데이트 $E_{G}$ 가 아니다. 따라서 매번 판별자의 포워드 패스가 호출(시작)되는 시점에 업데이트 된 $E_{G}$ 를 반영해 RTD를 수행할 수 있도록 하기 위해 <code class="language-plaintext highlighter-rouge">register_forward_pre_hook</code> 를 사용했다.</p>

<h3 id="-gdes-experiment"><strong><code class="language-plaintext highlighter-rouge">🤔 GDES Experiment</code></strong></h3>

<p>GDES가 제대로 구현되었는지, 논문 주장대로 판별자 학습 결과가 간섭을 발생시키지 않는지 확인하기 위해 한가지 실험을 진행했다. 실험 내용은 이렇다. 만약 GDES가 의도대로 구현된게 맞다면, 인코더 모델의 MLM 학습 결과 추이와 ELECTRA의 생성자 학습 결과 추이 양상이 유사해야 한다. 만약 최적화 추세가 다르다면, 필자가 잘못 구현했거나, 저자의 주장과 다르게 간섭이 발생하는 것이라 볼 수 있을 것이다. Backbone을 DeBERTa로 두고 각각 학습을 진행했다. 모든 하이퍼 파라미터를 고정한 뒤, 학습 초반 120스탭에 대한 결과 추이를 비교해봤다.</p>

<p align="center">
<img src="/assets/images/deberta_v3/deberta_test.png" alt="DeBERTa MLM Result" class="align-center image-caption" width="100%&quot;, height=&quot;100%" />
<strong><em><a href="https://arxiv.org/abs/2111.09543">DeBERTa MLM Result</a></em></strong>
</p>

<p align="center">
<img src="/assets/images/deberta_v3/gdes_test.png" alt="GDES Result" class="align-center image-caption" width="100%&quot;, height=&quot;100%" />
<strong><em><a href="https://arxiv.org/abs/2111.09543">GDES Result</a></em></strong>
</p>

<p>미처 까먹고 <code class="language-plaintext highlighter-rouge">torch.backends.cudnn.deterministic = False</code>로 두고 실험을 진행하여, 생성자의 수렴이 좀 더 빨리 진행되는 양상을 보이고 있다. 아마도 생성자 학습을 할 때 <code class="language-plaintext highlighter-rouge">cudnn</code> 이 열심히 일을 한 것 같댜. 수렴 속도에는 차이가 조금 나지만, 최적화 되는 추세 자체는 동일한 것을 알 수 있다.</p>

<p>따라서 GDES를 사용하면 간섭이 발생하지 않아 <code class="language-plaintext highlighter-rouge">Tug-of-War</code> 현상을 방지할 수 있다. 다만, 실험이 다소 엄밀하지 못한 측면이 있다. 추후에 좀 더 엄밀한 증명을 할 수 있는 실험 방법을 생각해봐야겠다.</p>]]></content><author><name>qcqced</name><email>qcqced123@gmail.com</email></author><category term="NLP" /><category term="Natural Language Process" /><category term="DeBERTa-V3" /><category term="DeBERTa" /><category term="ELECTRA" /><category term="Weight Sharing" /><category term="GDES" /><category term="Pytorch" /><summary type="html"><![CDATA[DeBERTa-V3 Official Paper Review with Pytorch Implementation]]></summary></entry><entry><title type="html">🗂️[SpanBERT] SpanBERT: Improving Pre-training by Representing and Predicting Spans</title><link href="http://localhost:4000/nlp/spanbert" rel="alternate" type="text/html" title="🗂️[SpanBERT] SpanBERT: Improving Pre-training by Representing and Predicting Spans" /><published>2024-03-11T00:00:00+09:00</published><updated>2024-03-12T02:00:00+09:00</updated><id>http://localhost:4000/nlp/spanbert</id><content type="html" xml:base="http://localhost:4000/nlp/spanbert"><![CDATA[<h3 id="overview"><code class="language-plaintext highlighter-rouge">🔭 Overview</code></h3>

<p><code class="language-plaintext highlighter-rouge">SpanBERT</code>는 2020년 페이스북에서 발표한 BERT 계열 모델로, 새로운 방법론인 <code class="language-plaintext highlighter-rouge">SBO(Span Boundary Objective)</code>를 고안해 사전학습을 하여 기존 대비 높은 성능을 기록했다. 기존 <code class="language-plaintext highlighter-rouge">MLM</code>, <code class="language-plaintext highlighter-rouge">CLM</code>은 단일 토큰을 예측하는 방식을 사용하기 때문에 Word-Level Task에 아주 적합하지만 상대적으로 QA, Sentence-Similarity 같은 문장 단위 테스크에 그대로 활용하기에는 부족한 점이 있었다. 이러한 문제를 해결하기 위해 고안된 방법론이 바로 <code class="language-plaintext highlighter-rouge">SBO</code>다. <code class="language-plaintext highlighter-rouge">SBO</code>란, MLM과 비슷하지만, Span(절•구문) 단위로 마스킹하고 다시 Denoising을 하기 때문에, Sentence-Level Task에 속하는 Down-Stream Task를 위한 모델의 사전 훈련으로 적합하다.</p>

<p>정리하자면, <code class="language-plaintext highlighter-rouge">SpanBERT</code> 모델은 기존 BERT의 구조적 측면 개선이 아닌, 사전학습 방법에 대한 개선 시도라고 볼 수 있다. 따라서 어떤 모델이더라도, 인코더 언어 모델이라면 모두 <code class="language-plaintext highlighter-rouge">SpanBERT</code> 구조를 사용할 수 있으며, 기존 논문에서는 원본 BERT 구조를 사용했다. 그래서 본 포스팅에서도 BERT에 대한 설명 없이 SBO에 대해서만 다루려고 한다.</p>

<h3 id="sbo-span-boundary-objective"><code class="language-plaintext highlighter-rouge">📚 SBO: Span Boundary Objective</code></h3>

<p align="center">
<img src="/assets/images/spanbert/sbo.png" alt="SBO Task" class="align-center image-caption" width="100%&quot;, height=&quot;100%" />
<strong><em><a href="https://arxiv.org/abs/1907.10529">SBO Task</a></em></strong>
</p>

<p><strong>[SBO Algorithm Summary]</strong></p>
<ul>
  <li><strong>1) 연속된 범위의 Span 생성</strong>
    <ul>
      <li><strong>무작위로 Span의 양쪽 끝 토큰 지정 ($x_{4}, x_{9}$)</strong>
        <ul>
          <li><strong>$x_{5}$ to $x_{8}$ 은 스팬 내부 토큰</strong></li>
        </ul>
      </li>
      <li><strong>마스킹 예산 계산</strong>
        <ul>
          <li><strong>문장 당 마스킹 예산(합산 Span 길이)은 문장 길이의 15%</strong></li>
          <li><strong>예시 시퀀스 길이: 512</strong></li>
          <li><strong>마스킹 예산: 대략 75 = 512*0.15</strong></li>
        </ul>
      </li>
      <li><strong>기하 분포 사용해서 개별 스팬 길이 지정</strong>
        <ul>
          <li><strong>개별 스팬당 최대 길이 지정, 최대 10이 넘지 않도록 설정</strong></li>
          <li><strong>최대 스팬 합산 길이 도달까지 마스킹 반복</strong>
            <ul>
              <li><strong>남은 마스킹 예산 &lt; 현재 스팬 길이</strong>
                <ul>
                  <li><strong>남은 마스킹 예산을 현재 스팬 길이로 설정</strong></li>
                </ul>
              </li>
            </ul>
          </li>
        </ul>
      </li>
      <li><strong>따라서 Subword Tokenizing이 아니라 Whole Word Masking 단위 작업이 필요</strong></li>
    </ul>
  </li>
  <li><strong>2) 시작 토큰 기준, 상대 위치 계산</strong>
    <ul>
      <li><strong>스팬 내부 토큰의 상대 위치 임베딩 생성 및 계산</strong></li>
      <li><strong>시작토큰, 마지막토큰, 스팬 내부 토큰의 상대 위치 임베딩을 concat, 은닉 벡터 생성</strong></li>
      <li><strong>SpanHead에 은닉 벡터 통과시키기</strong></li>
    </ul>
  </li>
  <li><strong>3) SpanHead 출력값을 마스킹에 대한 예측 표현으로 사용</strong></li>
</ul>

<p>SBO의 아이디어 자체는 상당히 간단하다. 기존 MLM처럼 무작위로 시퀀스에서 아무 토큰이나 선택하는게 아니라, 주어진 문장에서 일정 길이의 연속된 토큰들을 한번에 선택해 마스킹 처리하여 학습하겠다는 것이다. 논문에서 제시한 SBO 알고리즘을 정리하면 아래와 같다.</p>

\[\begin{align*}
h_0 &amp;= [x_{s-1}; x_{e+1}; p_{i-s+1}] \\
h_1 &amp;= \text{LayerNorm}(\text{GeLU}(W_1 h_0)) \\
y_i &amp;= \text{LayerNorm}(\text{GeLU}(W_2 h_1))
\end{align*}\]

<p>위 그림을 예시로 알고리즘을 살펴보자. 먼저 주어진 스팬 길이에 맞게, 스팬의 시작과 끝 지점이 되는 토큰을 무작위로 선택한다. 그다음 시작 위치를 기준으로, 스팬 내부에 속하는 토큰들의 상대 위치 인덱스를 계산해준다. 그림 속 $x_{7}$ 토큰의 상대 위치 번호는 3이 된다. 미리 정의한 상대 위치 임베딩에서 행 인덱스가 3인 행벡터를 가져온다. 그 다음 양쪽 끝 벡터와 concat을 수행하여 $h_{0}$ 을 만든다. 그리고 미리 정의된 <code class="language-plaintext highlighter-rouge">SBOHead</code>에 통과시킨다. <code class="language-plaintext highlighter-rouge">SBOHead</code>에게 반환 받은 은닉 벡터값은 해당 위치의 마스킹에 대한 예측값($y_{i}$)으로 사용하고 이를 이용해 SBO 손실을 구한다. 지금까지 내용을 정리해 수식으로 표현하면 위와 같다.</p>

\[L(x_i) = L_{MLM}(x_i) + L_{SBO}(x_i)\]

<p><code class="language-plaintext highlighter-rouge">SpanBERT</code>의 목적함수는 SBO 손실 뿐만 아니라 기존 MLM 손실도 함꼐 포함되어 있다. 다만 MLM 손실을 구하기 위해 주어진 시퀀스에 대해 따로 마스킹을 하는 것은 아니고, SBO를 위해 적용했던 Span Masking을 그대로 활용한다. 대신 위의 SBO 수식의 $h_{0}$ 이 아니라, $x_{i-s+1}$ ($i-s+1$ 번째 토큰의 인코더 출력값)을 그대로 MLM 손실을 구하는데 사용한다. 정리하면, <code class="language-plaintext highlighter-rouge">SpanBERT</code>의 최종 손실은 위 수식과 같다. 한편, <code class="language-plaintext highlighter-rouge">ELECTRA</code> 때와는 다르게 두 손실의 스케일 차이가 거의 없어 따로 스케일 상수를 곱해주지는 않는 것 같다.</p>

<h3 id="implementation-by-pytorch"><code class="language-plaintext highlighter-rouge">👩‍💻 Implementation by Pytorch</code></h3>

<p>논문의 내용 종합하여 파이토치로 <code class="language-plaintext highlighter-rouge">SpanBERT</code>를 구현해봤다. 논문에 포함된 아이디어를 이해하는데는 어렵지 않았지만, 제한된 조건에 맞는 스팬을 찾고, 마스킹하는 과정을 실제 구현하는 것은 매우 까다로운 편이었다.
본 포스팅에서는 <code class="language-plaintext highlighter-rouge">SpanBERT</code>의 SBO 학습을 위한 입력 만들기, SBOHead에 대해서만 설명하려고 한다. <code class="language-plaintext highlighter-rouge">BERT</code>, <code class="language-plaintext highlighter-rouge">Whole World Masking</code>에 대해 궁금하다면 이전 포스팅을, 전체 모델 구조 대한 코드는 <strong><a href="https://github.com/qcqced123/model_study">여기 링크</a></strong>를 통해 참고바란다.</p>

<p>공개할 코드는 아직 완벽하게 벡터화를 적용하지 못해, GPU 병렬 연산에 최적화 되지 못한 점 양해 부탁한다. 빠른 시일 이내에 벡터화를 적용해서 다시 수정된 코드를 올리겠다.</p>

<h4 id="span-masking-algoritm"><code class="language-plaintext highlighter-rouge">👩‍💻 Span Masking Algoritm</code></h4>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">random</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="nn">torch.nn.utils.rnn</span> <span class="kn">import</span> <span class="n">pad_sequence</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">Tensor</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">List</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">Any</span>
<span class="kn">from</span> <span class="nn">..tuner.mlm</span> <span class="kn">import</span> <span class="n">WholeWordMaskingCollator</span>
<span class="kn">from</span> <span class="nn">configuration</span> <span class="kn">import</span> <span class="n">CFG</span>

<span class="n">BPE</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s">'RobertaTokenizerFast'</span><span class="p">,</span>
    <span class="s">'GPT2TokenizerFast'</span><span class="p">,</span>
<span class="p">]</span>

<span class="n">SPM</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s">'DebertaV2TokenizerFast'</span><span class="p">,</span>
    <span class="s">'DebertaTokenizerFast'</span><span class="p">,</span>
    <span class="s">'XLMRobertaTokenizerFast'</span><span class="p">,</span>
<span class="p">]</span>

<span class="n">WORDPIECE</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s">'BertTokenizerFast'</span><span class="p">,</span>
    <span class="s">'ElectraTokenizerFast'</span><span class="p">,</span>
<span class="p">]</span>

<span class="k">def</span> <span class="nf">random_non_negative_integer</span><span class="p">(</span><span class="n">max_value</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">random</span><span class="p">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">max_value</span><span class="p">)</span>

<span class="k">class</span> <span class="nc">SpanCollator</span><span class="p">(</span><span class="n">WholeWordMaskingCollator</span><span class="p">):</span>
    <span class="s">""" Custom Collator for Span Boundary Objective Task, which is used for span masking algorithm
    Span Masking is similar to Whole Word Masking, but it has some differences:
        1) Span Masking does not use 10% of selected token left &amp; 10% of selected token replaced other vocab token
            - just replace all selected token to [MASK] token
    Algorithm:
    1) Select 2 random tokens from input tokens for spanning
    2) Calculate relative position embedding for each token with 2 random tokens froms step 1.
    3) Calculate span boundary objective with 2 random tokens from step 1 &amp; pos embedding from step 2.
    Args:
        cfg: configuration.CFG
        masking_budget: masking budget for Span Masking
                        (default: 0.15 =&gt; Recommended by original paper)
        span_probability: probability of span length for Geometric Distribution
                         (default: 0.2 =&gt; Recommended by original paper)
        max_span_length: maximum span length of each span in one batch sequence
                         (default: 10 =&gt; Recommended by original paper)
    References:
        https://arxiv.org/pdf/1907.10529.pdf
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">cfg</span><span class="p">:</span> <span class="n">CFG</span><span class="p">,</span>
        <span class="n">masking_budget</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.15</span><span class="p">,</span>
        <span class="n">span_probability</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.2</span><span class="p">,</span>
        <span class="n">max_span_length</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">SpanCollator</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">(</span><span class="n">cfg</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span> <span class="o">=</span> <span class="n">cfg</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">tokenizer</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">tokenizer</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">masking_budget</span> <span class="o">=</span> <span class="n">masking_budget</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">span_probability</span> <span class="o">=</span> <span class="n">span_probability</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">max_span_length</span> <span class="o">=</span> <span class="n">max_span_length</span>

    <span class="k">def</span> <span class="nf">_whole_word_mask</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">input_tokens</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span>
        <span class="n">max_predictions</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">CFG</span><span class="p">.</span><span class="n">max_seq</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]:</span>
        <span class="s">"""
        0) apply Whole Word Masking Algorithm for make gathering original token index in natural language
        1) calculate number of convert into masking tokens with masking budget*len(input_tokens)
        2) define span length of this iteration
            - span length follow geometric distribution
            - span length is limited by max_span_length
        """</span>
        <span class="n">cand_indexes</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">token</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">input_tokens</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">token</span> <span class="o">==</span> <span class="s">"[CLS]"</span> <span class="ow">or</span> <span class="n">token</span> <span class="o">==</span> <span class="s">"[SEP]"</span><span class="p">:</span>
                <span class="k">continue</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">cand_indexes</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="mi">1</span> <span class="ow">and</span> <span class="bp">self</span><span class="p">.</span><span class="n">select_post_string</span><span class="p">(</span><span class="n">token</span><span class="p">):</span>  <span class="c1"># method from WholeWordMaskingCollator
</span>                <span class="n">cand_indexes</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">].</span><span class="n">append</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
            <span class="k">elif</span> <span class="bp">self</span><span class="p">.</span><span class="n">select_src_string</span><span class="p">(</span><span class="n">token</span><span class="p">):</span>  <span class="c1"># method from WholeWordMaskingCollator
</span>                <span class="n">cand_indexes</span><span class="p">.</span><span class="n">append</span><span class="p">([</span><span class="n">i</span><span class="p">])</span>

        <span class="n">l</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">input_tokens</span><span class="p">)</span>
        <span class="n">src_l</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">cand_indexes</span><span class="p">)</span>
        <span class="n">num_convert_tokens</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">masking_budget</span> <span class="o">*</span> <span class="n">l</span><span class="p">)</span>
        <span class="n">budget</span> <span class="o">=</span> <span class="n">num_convert_tokens</span>  <span class="c1"># int is immutable object, so do not copy manually
</span>        <span class="n">masked_lms</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">covered_indexes</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
        <span class="k">while</span> <span class="n">budget</span><span class="p">:</span>
            <span class="n">span_length</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">min</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">distributions</span><span class="p">.</span><span class="n">Geometric</span><span class="p">(</span><span class="n">probs</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">span_probability</span><span class="p">).</span><span class="n">sample</span><span class="p">())))</span>
            <span class="n">src_index</span> <span class="o">=</span> <span class="n">random_non_negative_integer</span><span class="p">(</span><span class="n">src_l</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">span_length</span> <span class="o">&gt;</span> <span class="n">budget</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">budget</span> <span class="o">&lt;</span> <span class="mi">5</span><span class="p">:</span>  <span class="c1"># Set the span length to budget to avoid a large number of iter if the remaining budget is too small
</span>                    <span class="n">span_length</span> <span class="o">=</span> <span class="n">budget</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">continue</span>
            <span class="k">if</span> <span class="n">cand_indexes</span><span class="p">[</span><span class="n">src_index</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">span_length</span> <span class="o">&gt;</span> <span class="n">l</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>  <span class="c1"># If the index of the last token in the span is outside the full sequence range
</span>                <span class="k">continue</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">cand_indexes</span><span class="p">[</span><span class="n">src_index</span><span class="p">])</span> <span class="o">&gt;</span> <span class="n">span_length</span><span class="p">:</span>  <span class="c1"># handling bad case: violating WWM algorithm at start
</span>                <span class="k">continue</span>
            <span class="n">span_token_index</span> <span class="o">=</span> <span class="n">cand_indexes</span><span class="p">[</span><span class="n">src_index</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>  <span class="c1"># init span token index: src token
</span>            <span class="k">while</span> <span class="n">span_length</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">span_length</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="k">break</span>
                <span class="k">if</span> <span class="n">span_token_index</span> <span class="ow">in</span> <span class="n">covered_indexes</span><span class="p">:</span> <span class="c1"># If it encounters an index that is already masked, it ends, and starts the next iteration
</span>                    <span class="k">break</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">covered_indexes</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">span_token_index</span><span class="p">)</span>
                    <span class="n">masked_lms</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">span_token_index</span><span class="p">)</span>
                    <span class="n">span_length</span> <span class="o">-=</span> <span class="mi">1</span>
                    <span class="n">budget</span> <span class="o">-=</span> <span class="mi">1</span>
                    <span class="n">span_token_index</span> <span class="o">+=</span> <span class="mi">1</span>
                    <span class="k">continue</span>

        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">covered_indexes</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">masked_lms</span><span class="p">):</span>
            <span class="k">raise</span> <span class="nb">ValueError</span><span class="p">(</span><span class="s">"Length of covered_indexes is not equal to length of masked_lms."</span><span class="p">)</span>
        <span class="n">mask_labels</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span> <span class="k">if</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">covered_indexes</span> <span class="k">else</span> <span class="mi">0</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">input_tokens</span><span class="p">))]</span>
        <span class="k">return</span> <span class="n">mask_labels</span>

    <span class="k">def</span> <span class="nf">get_mask_tokens</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">inputs</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">mask_labels</span><span class="p">:</span> <span class="n">Tensor</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
        <span class="s">""" Prepare masked tokens inputs/labels for Span Boundary Objective with MLM (15%),
        All of masked tokens (15%) are replaced by [MASK] token,
        Unlike BERT MLM which is replaced by random token or stay original token left
        """</span>
        <span class="n">labels</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">.</span><span class="n">clone</span><span class="p">()</span>
        <span class="n">probability_matrix</span> <span class="o">=</span> <span class="n">mask_labels</span>

        <span class="n">special_tokens_mask</span> <span class="o">=</span> <span class="p">[</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">tokenizer</span><span class="p">.</span><span class="n">get_special_tokens_mask</span><span class="p">(</span><span class="n">val</span><span class="p">,</span> <span class="n">already_has_special_tokens</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span> <span class="k">for</span> <span class="n">val</span> <span class="ow">in</span> <span class="n">labels</span><span class="p">.</span><span class="n">tolist</span><span class="p">()</span>
        <span class="p">]</span>
        <span class="n">probability_matrix</span><span class="p">.</span><span class="n">masked_fill_</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">special_tokens_mask</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">bool</span><span class="p">),</span> <span class="n">value</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">tokenizer</span><span class="p">.</span><span class="n">pad_token</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">padding_mask</span> <span class="o">=</span> <span class="n">labels</span><span class="p">.</span><span class="n">eq</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">tokenizer</span><span class="p">.</span><span class="n">pad_token_id</span><span class="p">)</span>
            <span class="n">probability_matrix</span><span class="p">.</span><span class="n">masked_fill_</span><span class="p">(</span><span class="n">padding_mask</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)</span>

        <span class="n">masked_indices</span> <span class="o">=</span> <span class="n">probability_matrix</span><span class="p">.</span><span class="nb">bool</span><span class="p">()</span>
        <span class="n">labels</span><span class="p">[</span><span class="o">~</span><span class="n">masked_indices</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mi">100</span>  <span class="c1"># We only compute loss on masked tokens
</span>        <span class="n">inputs</span><span class="p">[</span><span class="n">masked_indices</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">tokenizer</span><span class="p">.</span><span class="n">convert_tokens_to_ids</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">tokenizer</span><span class="p">.</span><span class="n">mask_token</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">labels</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batched</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]])</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">:</span>
        <span class="s">""" Abstract Method for Collator, you must implement this method in child class """</span>
        <span class="n">input_ids</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="s">"input_ids"</span><span class="p">])</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">batched</span><span class="p">]</span>
        <span class="n">padding_mask</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="p">.</span><span class="n">get_padding_mask</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">input_ids</span><span class="p">]</span>

        <span class="n">padding_mask</span> <span class="o">=</span> <span class="n">pad_sequence</span><span class="p">(</span><span class="n">padding_mask</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">padding_value</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="n">input_ids</span> <span class="o">=</span> <span class="n">pad_sequence</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">padding_value</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

        <span class="n">mask_labels</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">batched</span><span class="p">:</span>
            <span class="n">ref_tokens</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">input_id</span> <span class="ow">in</span> <span class="n">x</span><span class="p">[</span><span class="s">"input_ids"</span><span class="p">]:</span>
                <span class="n">token</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">tokenizer</span><span class="p">.</span><span class="n">_convert_id_to_token</span><span class="p">(</span><span class="n">input_id</span><span class="p">)</span>
                <span class="n">ref_tokens</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">token</span><span class="p">)</span>
            <span class="n">mask_labels</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">_whole_word_mask</span><span class="p">(</span><span class="n">ref_tokens</span><span class="p">))</span>

        <span class="n">mask_labels</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">mask_labels</span><span class="p">]</span>
        <span class="n">mask_labels</span> <span class="o">=</span> <span class="n">pad_sequence</span><span class="p">(</span><span class="n">mask_labels</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">padding_value</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">input_ids</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">get_mask_tokens</span><span class="p">(</span>
            <span class="n">input_ids</span><span class="p">,</span>
            <span class="n">mask_labels</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="p">{</span>
            <span class="s">"input_ids"</span><span class="p">:</span> <span class="n">input_ids</span><span class="p">,</span>
            <span class="s">"labels"</span><span class="p">:</span> <span class="n">labels</span><span class="p">,</span>
            <span class="s">"padding_mask"</span><span class="p">:</span> <span class="n">padding_mask</span><span class="p">,</span>
            <span class="s">"mask_labels"</span><span class="p">:</span> <span class="n">mask_labels</span>
        <span class="p">}</span>
</code></pre></div></div>

<h4 id="sbo-head"><code class="language-plaintext highlighter-rouge">👩‍💻 SBO Head</code></h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">SBOHead</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="s">""" Custom Head for Span Boundary Objective Task, this module return logit value for each token
    we use z for class logit, each Fully Connected Layer doesn't have bias term in original paper
    so we don't use bias term in this module =&gt; nn.Linear(bias=False)

    You must select option for matrix sum or concatenate with x_s-1, x_e+1, p_i-s+1
    If you select concatenate option, you must pass is_concatenate=True to cfg.is_concatenate, default is True
    
    Math:
        h_0 = [x_s-1;x_e+1;p_i-s+1]
        h_t = LayerNorm(GELU(W_0•h_0))
        z = LayerNorm(GELU(W_1•h_t))

    Args:
        cfg: configuration.CFG
        is_concatenate: option for matrix sum or concatenate with x_s-1, x_e+1, p_i-s+1, default True
        max_span_length: maximum span length of each span in one batch sequence
                         (default: 10 =&gt; Recommended by original paper)
    References:
        https://arxiv.org/pdf/1907.10529.pdf
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">cfg</span><span class="p">:</span> <span class="n">CFG</span><span class="p">,</span>
        <span class="n">is_concatenate</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="bp">True</span><span class="p">,</span>
        <span class="n">max_span_length</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">SBOHead</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span> <span class="o">=</span> <span class="n">cfg</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">is_concatenate</span> <span class="o">=</span> <span class="n">is_concatenate</span>  <span class="c1"># for matrix sum or concatenate with x_s-1, x_e+1, p_i-s+1
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">projector</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">dim_model</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">dim_model</span><span class="o">*</span><span class="mi">3</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>  <span class="c1"># for concatenate x_s-1, x_e+1, p_i-s+1
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">span_pos_emb</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">max_span_length</span><span class="o">*</span><span class="mi">3</span><span class="p">,</span> <span class="n">cfg</span><span class="p">.</span><span class="n">dim_model</span><span class="p">)</span>  <span class="c1"># size of dim_model is research topic
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">head</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">dim_model</span><span class="o">*</span><span class="mi">3</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">dim_ffn</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">GELU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">dim_ffn</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">dim_ffn</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">dim_model</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">GELU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">dim_model</span><span class="p">),</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">classifier</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">dim_model</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">cfg</span><span class="p">.</span><span class="n">vocab_size</span><span class="p">))</span>  <span class="c1"># for matching vocab size
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">classifier</span><span class="p">.</span><span class="n">bias</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">bias</span>

    <span class="o">@</span><span class="nb">staticmethod</span>
    <span class="k">def</span> <span class="nf">find_consecutive_groups</span><span class="p">(</span><span class="n">mask_labels</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">target_value</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">Dict</span><span class="p">]]:</span>
        <span class="s">""" Get the start and end positions of consecutive groups in tensor for the target value
        This method is used for SBO Objective Function, this version is not best performance to make span groups

        Args:
            mask_labels: masking tensor for span
            target_value: target value for finding consecutive groups
        """</span>
        <span class="n">all_consecutive_groups</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">mask_label</span> <span class="ow">in</span> <span class="n">mask_labels</span><span class="p">:</span>
            <span class="n">consecutive_groups</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="n">current_group</span> <span class="o">=</span> <span class="bp">None</span>
            <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">mask_label</span><span class="p">):</span>
                <span class="k">if</span> <span class="n">value</span> <span class="o">==</span> <span class="n">target_value</span><span class="p">:</span>
                    <span class="k">if</span> <span class="n">current_group</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
                        <span class="n">current_group</span> <span class="o">=</span> <span class="p">{</span><span class="s">"start"</span><span class="p">:</span> <span class="n">i</span><span class="p">,</span> <span class="s">"end"</span><span class="p">:</span> <span class="n">i</span><span class="p">}</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">current_group</span><span class="p">[</span><span class="s">"end"</span><span class="p">]</span> <span class="o">=</span> <span class="n">i</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">if</span> <span class="n">current_group</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
                        <span class="n">consecutive_groups</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">current_group</span><span class="p">)</span>
                        <span class="n">current_group</span> <span class="o">=</span> <span class="bp">None</span>
            <span class="k">if</span> <span class="n">current_group</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
                <span class="n">consecutive_groups</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">current_group</span><span class="p">)</span>
            <span class="n">all_consecutive_groups</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">consecutive_groups</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">all_consecutive_groups</span>

    <span class="k">def</span> <span class="nf">cal_span_emb</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">h</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">consecutive_groups</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">Dict</span><span class="p">]])</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="s">""" Calculate span embedding for each span in one batch sequence

        Args:
            h: hidden states, already passed through projection layer (dim*3)
            hidden_states: hidden states from encoder
            consecutive_groups: consecutive groups for each batch sequence
        """</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">batch</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">consecutive_groups</span><span class="p">):</span>  <span class="c1"># batch level
</span>            <span class="k">for</span> <span class="n">j</span><span class="p">,</span> <span class="n">span</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">batch</span><span class="p">):</span>  <span class="c1"># span level
</span>                <span class="n">start</span><span class="p">,</span> <span class="n">end</span> <span class="o">=</span> <span class="n">span</span><span class="p">[</span><span class="s">"start"</span><span class="p">],</span> <span class="n">span</span><span class="p">[</span><span class="s">"end"</span><span class="p">]</span>
                <span class="n">length</span> <span class="o">=</span> <span class="n">end</span> <span class="o">-</span> <span class="n">start</span> <span class="o">+</span> <span class="mi">1</span>
                <span class="n">idx</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="n">length</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>   <span class="c1"># .to(self.cfg.device)
</span>                <span class="n">context_s</span><span class="p">,</span> <span class="n">context_e</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">start</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="p">:],</span> <span class="n">hidden_states</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">end</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="p">:]</span>
                <span class="n">span_pos_emb</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">span_pos_emb</span><span class="p">(</span><span class="n">idx</span><span class="p">).</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">length</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                    <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">p_h</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">span_pos_emb</span><span class="p">):</span>  <span class="c1"># length of span_pos_emb == length of span of this iterations
</span>                        <span class="n">h</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">start</span><span class="o">+</span><span class="n">k</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cat</span><span class="p">([</span><span class="n">context_s</span><span class="p">,</span> <span class="n">p_h</span><span class="p">,</span> <span class="n">context_e</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">h</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">start</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cat</span><span class="p">([</span><span class="n">context_s</span><span class="p">,</span> <span class="n">span_pos_emb</span><span class="p">,</span> <span class="n">context_e</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">h</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">mask_labels</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="n">consecutive_groups</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">find_consecutive_groups</span><span class="p">(</span><span class="n">mask_labels</span><span class="p">)</span>  <span class="c1"># [batch, num_consecutive_groups]
</span>        <span class="n">h</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">projector</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>  <span class="c1"># [batch, seq, dim_model*3]
</span>        <span class="n">h_t</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">cal_span_emb</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">,</span> <span class="n">consecutive_groups</span><span class="p">)</span>
        <span class="n">z</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">head</span><span class="p">(</span><span class="n">h_t</span><span class="p">)</span>
        <span class="n">logit</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">classifier</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">logit</span>

</code></pre></div></div>]]></content><author><name>qcqced</name><email>qcqced123@gmail.com</email></author><category term="NLP" /><category term="Natural Language Process" /><category term="SpanBERT" /><category term="BERT" /><category term="Self-Attention" /><category term="Pytorch" /><summary type="html"><![CDATA[SpanBERT Official Paper Review with Pytorch Implementation]]></summary></entry><entry><title type="html">🎡 [Roformer] RoFormer: Enhanced Transformer with Rotary Position Embedding</title><link href="http://localhost:4000/nlp/roformer" rel="alternate" type="text/html" title="🎡 [Roformer] RoFormer: Enhanced Transformer with Rotary Position Embedding" /><published>2024-03-11T00:00:00+09:00</published><updated>2024-03-12T02:00:00+09:00</updated><id>http://localhost:4000/nlp/roformer</id><content type="html" xml:base="http://localhost:4000/nlp/roformer"><![CDATA[<h3 id="overview"><code class="language-plaintext highlighter-rouge">🔭 Overview</code></h3>

<p><code class="language-plaintext highlighter-rouge">DistilBERT</code> 는 허깅 페이스 연구진이 2019년 발표한 BERT의 변형으로서, On-Device Ai 개발을 목표로 경량화에 초점을 맞춘 모델이다. GPT, BERT의 등장 이후, NLP 분야에서 비약적인 성능 향상이 이뤄졌음에도 불구하고, 터무니 없는 모델 사이즈와 컴퓨팅 리소스 요구로 인해 실생활 적용 같은 활용성은 여전히 해결해야할 문제로 남아 있었다. Google에서 발표한 초기 <code class="language-plaintext highlighter-rouge">BERT-base-uncased</code> 만 해도 파라미터가 1억 1천만개 수준에 달한다.</p>

<p>이를 다양한 비즈니스 요구 상황에 적용할 수 있으려면 최소한 8GB 이상의 가속기 전용 RAM 공간을 요구로 한다. 오늘날 개인용 PC 혹은 서버 컴퓨터의 경우, 8GB 이상의 VRAM이 달린 GPU가 일반적으로 탑재되기 때문에 크게 문제 될 것 없는 요구사항이지만, On-Device 환경에서는 이야기가 달라진다. 최신 하이엔드 스마트폰인 Galaxy S24 Ultra, iPhone 15 Pro의 경우 12GB, 8GB의 램 용량을 보유하고 있다. 그마저도 대부분의 온디바이스 환경은 SoC 구조를 채택하고 있기 때문에 전용 가속기가 온전히 저 모든 램 공간을 활용할 수 없다.</p>

<p>따라서 온디바이스에 Ai를 적용하기 위해서는 획기적인 모델 경량화가 필요한 상황이고 그 출발점이 된 연구가 바로 <code class="language-plaintext highlighter-rouge">DistilBERT</code>다. 로컬 디바이스 환경에서도 언어 모델을 활용하기 위해 허깅 페이스 연구진은 지식 증류 기법을 활용해 인코더 기반 언어 모델의 파라미터를 획기적으로 줄이는데 성공한다.</p>

<p>정리하자면, <code class="language-plaintext highlighter-rouge">DistilBERT</code> 모델은 기존 BERT의 구조적 측면 개선이 아닌, 사전학습 방법 특히 경량화에 초점을 맞춘 시도라고 볼 수 있다. 따라서 어떤 모델이더라도, 인코더 언어 모델이라면 모두 <code class="language-plaintext highlighter-rouge">DistilBERT</code> 구조를 사용할 수 있으며, 기존 논문에서는 원본 BERT 구조를 사용했다. 이번 포스팅에서도 BERT 구조에 대한 설명 대신, <code class="language-plaintext highlighter-rouge">DistilBERT</code>의 사전 학습 방법론인 <code class="language-plaintext highlighter-rouge">Knowledge Distillation</code>에 대해서만 다루려고 한다.</p>

<h3 id="knowledge-distillations"><code class="language-plaintext highlighter-rouge">🌆 Knowledge Distillations</code></h3>

\[\min_{\theta}\sum_{x \in X} \alpha \mathcal{L}_{\text{KL}}(x, \theta) + \beta \mathcal{L}_{\text{MLM}}(x, \theta) + \gamma \mathcal{L}_{\text{Cos}}(x, \theta)\]

<p><code class="language-plaintext highlighter-rouge">DistilBERT</code>는 Teacher-Student Architecture를 차용해 상대적으로 작은 파라미터 사이즈를 갖는 <code class="language-plaintext highlighter-rouge">Student</code> 모델에게 <code class="language-plaintext highlighter-rouge">Teacher</code>의 지식을 전수하는 것을 목표로 한다. 따라서 <code class="language-plaintext highlighter-rouge">Teacher</code> 모델은 이미 사전 학습을 마치고 수렴된 상태의 가중치를 갖고 있는 모델을 사용해야 한다. 더불어 Teacher 모델은 구조만 기존 BERT를 따르되, 사전 학습 방식은 RoBERTa의 방식과 동일(NSP 제거, Dynamic Masking 적용)하게 훈련되어야 한다.</p>

<p>한편, <code class="language-plaintext highlighter-rouge">Student</code> 모델은 <code class="language-plaintext highlighter-rouge">Teacher</code>의 60%정도 파라미터 사이즈를 갖도록 축소하여 사용한다. 이 때 축소는 모델의 <code class="language-plaintext highlighter-rouge">depth</code>(레이어 개수)에만 적용하는데, 연구진에 따르면 <code class="language-plaintext highlighter-rouge">width</code>(은닉층 크기)는 축소를 적용해도 연산 효율이 증가하지 않는다고 한다. 정리하면 <code class="language-plaintext highlighter-rouge">Teacher</code> 모델의 <code class="language-plaintext highlighter-rouge">레이어 개수*0.6</code>의 개수만큼 인코더를 쌓으면 된다는 것이다.</p>

<p>그리고 최대한 <code class="language-plaintext highlighter-rouge">Teacher</code>의 지식을 전수해야 하기 때문에, 데이터는 <code class="language-plaintext highlighter-rouge">Teacher</code> 를 수렴시킨 것과 동일한 세트를 이용해야 한다. 이 때, Teacher 모델은 이미 MLE 방식으로 훈련이 된 상태라서 로짓이 단일 토큰 하나 쪽으로 쏠려 있을 가능성이 매우 높다. 이는 <code class="language-plaintext highlighter-rouge">Student</code> 모델의 일반화 능력에 악영향을 미칠 가능성이 높다. 따라서 Temperature 변수 $T$ 도입해 소프트 맥스(로짓)의 분포를 평탄화 한다. 이렇게 하면, <code class="language-plaintext highlighter-rouge">argmax()</code> 가 아닌 다른 토큰 표현에 대해서도 <code class="language-plaintext highlighter-rouge">Student</code> 모델이 지식을 습득할 수 있어서 풍부한 문맥을 학습하고 일반화 능력을 높이는데 도움이 된다. 이를 <code class="language-plaintext highlighter-rouge">암흑 지식(Dark Knowledge)</code> 을 활용한다고 표현한다. Temperature 변수 $T$ 도입한 소프트맥스 함수 수식은 아래와 같다.</p>

\[\text{softmax}(x_i) = \frac{e^{\frac{x_i}{\tau}}}{\sum_{j} e^{\frac{x_j}{\tau}}}\]

<p>수식상 변수 $T$의 값을 1이상으로 세팅해야 평탄화를 할 수 있다. 따라서 연구진은 $T =2$ 로 두고 사전 학습을 진행했다(논문에 공개안됨, GitHub에 있음). 이번 파트 맨 처음에 등장한 수식을 다시 보자. 결국 <code class="language-plaintext highlighter-rouge">DisilBERT</code>의 목적함수는 3가지 손실의 가중합으로 구성된다. 이제부터는 개별 손실에 대해서 자세히 살펴보자.</p>

<h4 id="distillation-loss-kl-divergence-loss"><code class="language-plaintext highlighter-rouge">🌆 Distillation Loss: KL-Divergence Loss</code></h4>

\[\text{KL-Divergence}(P || Q) = \sum_{i} P(i) \log \frac{P(i)}{Q(i)}\]

<p>증류 손실로 사용되는 <code class="language-plaintext highlighter-rouge">KL-Divergence Loss</code>는 두 확률 분포 간의 차이를 측정하는 지표 중 하나다. 주로 확률 분포 P와 Q 사이의 차이를 나타내는데, 개별 요소의 확률값 차이가 클수록 합산값은 커져 손실이 커지게 된다. 반대로 두 분포의 개별 요소 확률값 차이가 작다면 당연히, 두 분포가 유사하다는 의미이므로 손실 역시 작아지게 된다. 일반적으로 <code class="language-plaintext highlighter-rouge">KL-Divergence Loss</code> 에서 확률분포 $P$ 가 이상적인 확률 분포를, $Q$ 가 모델이 예측한 확률분포를 의미한다. 따라서 <code class="language-plaintext highlighter-rouge">DistilBERT</code>의 경우 확률분포 $P$ 자리에는 <code class="language-plaintext highlighter-rouge">Teacher</code> 모델의 소프트맥스 분포가, $Q$ 에는 <code class="language-plaintext highlighter-rouge">Student</code> 모델의 소프트맥스 분포가 대입되면 된다. 이 때 두 확률분포 모두, 암흑 지식 획득을 위해 소프트맥스 평탄화를 적용한 결과를 사용한다. 논문에서, 선생 모델 예측에 평탄화를 적용한 것을 <code class="language-plaintext highlighter-rouge">소프트 라벨</code>, 학생 모델의 것에 적용한 결과는 <code class="language-plaintext highlighter-rouge">소프트 예측</code>이라고 부른다.</p>

<h4 id="student-loss-mlm-loss"><code class="language-plaintext highlighter-rouge">🌆 Student Loss: MLM Loss</code></h4>

\[\mathcal{L}_{\text{MLM}} = - \sum_{i=1}^{N} \sum_{j=1}^{L} \mathbb{1}_{m_{ij}} \log \text{softmax}(x_{ij})\]

<p>학생 손실은 말그대로 기본적인 MLM 손실을 말한다. 정확한 손실값 계산을 위해서 학생의 소프트맥스 분포에 평탄화를 적용하지 않는다. 이를 논문에서는 <code class="language-plaintext highlighter-rouge">하드 예측</code>이라고 부른다. 라벨 역시 <code class="language-plaintext highlighter-rouge">Teacher</code>로부터 나온 것이 아닌 원래 MLM 수행에 사용되는 마스킹 라벨을 사용한다.</p>

<h4 id="cosine-embedding-loss-contrastive-loss-by-cosine-similarity"><code class="language-plaintext highlighter-rouge">🌆 Cosine Embedding Loss: Contrastive Loss by cosine similarity</code></h4>

\[\mathcal{L}_{\text{COS}}(x,y) = \begin{cases} 1 - \cos(x_1, x_2), &amp; \text{if } y = 1 \\ \max(0, \cos(x_1, x_2) - \text{margin}), &amp; \text{if } y = -1 \end{cases}\]

<p><code class="language-plaintext highlighter-rouge">Teacher</code> 모델과 <code class="language-plaintext highlighter-rouge">Student</code> 모델의 마지막 인코더 모델이 출력하는 은닉값에 대한 <code class="language-plaintext highlighter-rouge">Contrastive Loss</code>를 의미한다. 이 때 <code class="language-plaintext highlighter-rouge">Distance Metric</code>은 코사인 유사도를 사용한다. 그래서 코사인 임베딩 손실이라고 논문에서 정의하는 것으로 추정된다. 위 수식을 최적화하는 것을 목적으로 한다. 이 때 라벨은 <code class="language-plaintext highlighter-rouge">[BS, Seq_len]</code>의 크기를 갖되, 모든 원소는 1이 되도록 만든다. 이유는 간단하다. <code class="language-plaintext highlighter-rouge">Student</code> 모델의 은닉값이 <code class="language-plaintext highlighter-rouge">Teacher</code> 모델의 것과 최대한 비슷해지도록 만드는게 우리 목적이기 때문이다.</p>

<h3 id="implementation-by-pytorch"><code class="language-plaintext highlighter-rouge">👩‍💻 Implementation by Pytorch</code></h3>
<p>논문의 내용과 오피셜로 공개된 코드를 종합하여 파이토치로 <code class="language-plaintext highlighter-rouge">DistilBERT</code>를 구현해봤다. 논문에 포함된 아이디어를 이해하는데는 역시 어렵지 않았지만, 페이퍼에 hyper-param 테이블이 따로 제시되어 있지 않아 공개된 코드를 안 볼수가 없었다.</p>

<p>전체 모델 구조 대한 코드는 <strong><a href="https://github.com/qcqced123/model_study">여기 링크</a></strong>를 통해 참고바란다.</p>

<h4 id="knowledge-distillation-pipeline"><code class="language-plaintext highlighter-rouge">👩‍💻 Knowledge Distillation Pipeline</code></h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">train_val_fn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">loader_train</span><span class="p">,</span> <span class="n">model</span><span class="p">:</span> <span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">criterion</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">],</span> <span class="n">optimizer</span><span class="p">,</span><span class="n">scheduler</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]:</span>
    <span class="s">""" Function for train loop with validation for each batch*N Steps
    DistillBERT has three loss:

        1) distillation loss, calculated by soft targets &amp; soft predictions
            (nn.KLDIVLoss(reduction='batchmean'))

        2) student loss, calculated by hard targets &amp; hard predictions
            (nn.CrossEntropyLoss(reduction='mean')), same as pure MLM Loss

        3) cosine similarity loss, calculated by student &amp; teacher logit similarity
            (nn.CosineEmbeddingLoss(reduction='mean')), similar as contrastive loss

    Those 3 losses are summed jointly and then backward to student model
    """</span>
    <span class="n">scaler</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">amp</span><span class="p">.</span><span class="n">GradScaler</span><span class="p">(</span><span class="n">enabled</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">amp_scaler</span><span class="p">)</span>
    <span class="n">model</span><span class="p">.</span><span class="n">train</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">step</span><span class="p">,</span> <span class="n">batch</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">tqdm</span><span class="p">(</span><span class="n">loader_train</span><span class="p">)):</span>
        <span class="n">optimizer</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">(</span><span class="n">set_to_none</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="n">inputs</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="s">'input_ids'</span><span class="p">].</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">device</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="n">labels</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="s">'labels'</span><span class="p">].</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">device</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="n">padding_mask</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="s">'padding_mask'</span><span class="p">].</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">device</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

        <span class="n">mask</span> <span class="o">=</span> <span class="n">padding_mask</span><span class="p">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">).</span><span class="n">expand</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">dim_model</span><span class="p">)</span>  <span class="c1"># for hidden states dim
</span>        <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">t_hidden_state</span><span class="p">,</span> <span class="n">soft_target</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">teacher_fw</span><span class="p">(</span>
                <span class="n">inputs</span><span class="o">=</span><span class="n">inputs</span><span class="p">,</span>
                <span class="n">padding_mask</span><span class="o">=</span><span class="n">padding_mask</span><span class="p">,</span>
                <span class="n">mask</span><span class="o">=</span><span class="n">mask</span>
            <span class="p">)</span>  <span class="c1"># teacher model's pred =&gt; hard logit
</span>
        <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">amp</span><span class="p">.</span><span class="n">autocast</span><span class="p">(</span><span class="n">enabled</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">amp_scaler</span><span class="p">):</span>
            <span class="n">s_hidden_state</span><span class="p">,</span> <span class="n">s_logit</span><span class="p">,</span> <span class="n">soft_pred</span><span class="p">,</span> <span class="n">c_labels</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">student_fw</span><span class="p">(</span>
                <span class="n">inputs</span><span class="o">=</span><span class="n">inputs</span><span class="p">,</span>
                <span class="n">padding_mask</span><span class="o">=</span><span class="n">padding_mask</span><span class="p">,</span>
                <span class="n">mask</span><span class="o">=</span><span class="n">mask</span>
            <span class="p">)</span>
            <span class="n">d_loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">[</span><span class="s">"KLDivLoss"</span><span class="p">](</span><span class="n">soft_pred</span><span class="p">.</span><span class="n">log</span><span class="p">(),</span> <span class="n">soft_target</span><span class="p">)</span>  <span class="c1"># nn.KLDIVLoss
</span>            <span class="n">s_loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">[</span><span class="s">"CrossEntropyLoss"</span><span class="p">](</span><span class="n">s_logit</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">vocab_size</span><span class="p">),</span> <span class="n">labels</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>  <span class="c1"># nn.CrossEntropyLoss
</span>            <span class="n">c_loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">[</span><span class="s">"CosineEmbeddingLoss"</span><span class="p">](</span><span class="n">s_hidden_state</span><span class="p">,</span> <span class="n">t_hidden_state</span><span class="p">,</span> <span class="n">c_labels</span><span class="p">)</span>  <span class="c1"># nn.CosineEmbeddingLoss
</span>            <span class="n">loss</span> <span class="o">=</span> <span class="n">d_loss</span><span class="o">*</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">alpha_distillation</span> <span class="o">+</span> <span class="n">s_loss</span><span class="o">*</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">alpha_student</span> <span class="o">+</span> <span class="n">c_loss</span><span class="o">*</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">alpha_cosine</span>  <span class="c1"># linear combination loss
</span>
        <span class="n">scaler</span><span class="p">.</span><span class="n">scale</span><span class="p">(</span><span class="n">loss</span><span class="p">).</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">scaler</span><span class="p">.</span><span class="n">step</span><span class="p">(</span><span class="n">optimizer</span><span class="p">)</span>
        <span class="n">scaler</span><span class="p">.</span><span class="n">update</span><span class="p">()</span>
        <span class="n">scheduler</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>
</code></pre></div></div>

<h4 id="knowledge-distillation-model"><code class="language-plaintext highlighter-rouge">👩‍💻 Knowledge Distillation Model</code></h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">DistillationKnowledge</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">AbstractTask</span><span class="p">):</span>
    <span class="s">""" Custom Task Module for Knowledge Distillation by DistilBERT Style Architecture
    DistilBERT Style Architecture is Teacher-Student Framework for Knowledge Distillation,

    And then they have 3 objective functions:
        1) distillation loss, calculated by soft targets &amp; soft predictions
            (nn.KLDIVLoss(reduction='batchmean'))
        2) student loss, calculated by hard targets &amp; hard predictions
            (nn.CrossEntropyLoss(reduction='mean')), same as pure MLM Loss
        3) cosine similarity loss, calculated by student &amp; teacher logit similarity
            (nn.CosineEmbeddingLoss(reduction='mean')), similar as contrastive loss

    References:
        https://arxiv.org/pdf/1910.01108.pdf
        https://github.com/huggingface/transformers/blob/main/examples/research_projects/distillation/distiller.py
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">cfg</span><span class="p">:</span> <span class="n">CFG</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">DistillationKnowledge</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span> <span class="o">=</span> <span class="n">CFG</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">DistilBERT</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">,</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">select_model</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">_init_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">teacher_load_pretrained</span><span class="p">:</span>  <span class="c1"># for teacher model
</span>            <span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">teacher</span><span class="p">.</span><span class="n">load_state_dict</span><span class="p">(</span>
                <span class="n">torch</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="n">cfg</span><span class="p">.</span><span class="n">checkpoint_dir</span> <span class="o">+</span> <span class="n">cfg</span><span class="p">.</span><span class="n">teacher_state_dict</span><span class="p">),</span>
                <span class="n">strict</span><span class="o">=</span><span class="bp">False</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">student_load_pretrained</span><span class="p">:</span>  <span class="c1"># for student model
</span>            <span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">student</span><span class="p">.</span><span class="n">load_state_dict</span><span class="p">(</span>
                <span class="n">torch</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="n">cfg</span><span class="p">.</span><span class="n">checkpoint_dir</span> <span class="o">+</span> <span class="n">cfg</span><span class="p">.</span><span class="n">student_state_dict</span><span class="p">),</span>
                <span class="n">strict</span><span class="o">=</span><span class="bp">True</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">freeze</span><span class="p">:</span>
            <span class="n">freeze</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">teacher</span><span class="p">)</span>
            <span class="n">freeze</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">mlm_head</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">gradient_checkpoint</span><span class="p">:</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">gradient_checkpointing_enable</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">teacher_fw</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">inputs</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">padding_mask</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">mask</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
        <span class="n">is_valid</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="bp">False</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
        <span class="s">""" teacher forward pass to make soft target, last_hidden_state for distillation loss """</span>
        <span class="c1"># 1) make soft target
</span>        <span class="n">temperature</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="k">if</span> <span class="n">is_valid</span> <span class="k">else</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">temperature</span>
        <span class="n">last_hidden_state</span><span class="p">,</span> <span class="n">t_logit</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">teacher_fw</span><span class="p">(</span>
            <span class="n">inputs</span><span class="p">,</span>
            <span class="n">padding_mask</span><span class="p">,</span>
            <span class="n">attention_mask</span>
        <span class="p">)</span>
        <span class="n">last_hidden_state</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">masked_select</span><span class="p">(</span><span class="n">last_hidden_state</span><span class="p">,</span> <span class="o">~</span><span class="n">mask</span><span class="p">)</span>  <span class="c1"># for inverse select
</span>        <span class="n">last_hidden_state</span> <span class="o">=</span> <span class="n">last_hidden_state</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">dim_model</span><span class="p">)</span>  <span class="c1"># flatten last_hidden_state
</span>        <span class="n">soft_target</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="n">softmax</span><span class="p">(</span>
            <span class="n">t_logit</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">vocab_size</span><span class="p">)</span> <span class="o">/</span> <span class="n">temperature</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span>  <span class="c1"># flatten softmax distribution
</span>            <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span>
        <span class="p">)</span>  <span class="c1"># [bs* seq, vocab_size]
</span>        <span class="k">return</span> <span class="n">last_hidden_state</span><span class="p">,</span> <span class="n">soft_target</span>

    <span class="k">def</span> <span class="nf">student_fw</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">inputs</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">padding_mask</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">mask</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
        <span class="n">is_valid</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="bp">False</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
        <span class="s">""" student forward pass to make soft prediction, hard prediction for student loss """</span>
        <span class="n">temperature</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="k">if</span> <span class="n">is_valid</span> <span class="k">else</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">temperature</span>
        <span class="n">last_hidden_state</span><span class="p">,</span> <span class="n">s_logit</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">teacher_fw</span><span class="p">(</span>
            <span class="n">inputs</span><span class="p">,</span>
            <span class="n">padding_mask</span><span class="p">,</span>
            <span class="n">attention_mask</span>
        <span class="p">)</span>
        <span class="n">last_hidden_state</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">masked_select</span><span class="p">(</span><span class="n">last_hidden_state</span><span class="p">,</span> <span class="o">~</span><span class="n">mask</span><span class="p">)</span>  <span class="c1"># for inverse select
</span>        <span class="n">last_hidden_state</span> <span class="o">=</span> <span class="n">last_hidden_state</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">dim_model</span><span class="p">)</span>  <span class="c1"># flatten last_hidden_state
</span>        <span class="n">c_labels</span> <span class="o">=</span> <span class="n">last_hidden_state</span><span class="p">.</span><span class="n">new</span><span class="p">(</span><span class="n">last_hidden_state</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)).</span><span class="n">fill_</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">soft_pred</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="n">softmax</span><span class="p">(</span>
            <span class="n">s_logit</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">vocab_size</span><span class="p">)</span> <span class="o">/</span> <span class="n">temperature</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span>  <span class="c1"># flatten softmax distribution
</span>            <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">last_hidden_state</span><span class="p">,</span> <span class="n">s_logit</span><span class="p">,</span> <span class="n">soft_pred</span><span class="p">,</span> <span class="n">c_labels</span>
</code></pre></div></div>

<h4 id="distilbert-model"><code class="language-plaintext highlighter-rouge">👩‍💻 DistilBERT Model</code></h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">DistilBERT</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">AbstractModel</span><span class="p">):</span>
    <span class="s">""" Main class for DistilBERT Style Model, Teacher-Student Framework
    for Knowledge Distillation aim to lighter Large Scale LLM model. This model have 3 objective functions:

        1) distillation loss, calculated by soft targets &amp; soft predictions
            (nn.KLDIVLoss(reduction='batchmean'))

        2) student loss, calculated by hard targets &amp; hard predictions
            (nn.CrossEntropyLoss(reduction='mean')), same as pure MLM Loss

        3) cosine similarity loss, calculated by student &amp; teacher logit similarity
            (nn.CosineEmbeddingLoss(reduction='mean')), similar as contrastive loss

    soft targets &amp; soft predictions are meaning that logit are passed through softmax function applied with temperature T
    temperature T aim to flatten softmax layer distribution for making "Dark Knowledge" from teacher model

    hard targets &amp; hard predictions are meaning that logit are passed through softmax function without temperature T
    hard targets are same as just simple labels from MLM Collator returns for calculating cross entropy loss

    cosine similarity loss is calculated by cosine similarity between student &amp; teacher
    in official repo, they mask padding tokens for calculating cosine similarity, target for this task is 1
    cosine similarity is calculated by nn.CosineSimilarity() function, values are range to [-1, 1]

    you can select any other backbone model architecture for Teacher &amp; Student Model for knowledge distillation
    but, in original paper, BERT is used for Teacher Model &amp; Student
    and you must select pretrained model for Teacher Model, because Teacher Model is used for knowledge distillation,
    which is containing pretrained mlm head

    Do not pass gradient backward to teacher model!!
    (teacher model must be frozen or register_buffer to model or use no_grad() context manager)

    Args:
        cfg: configuration.CFG
        model_func: make model instance in runtime from config.json

    References:
        https://arxiv.org/pdf/1910.01108.pdf
        https://github.com/huggingface/transformers/blob/main/examples/research_projects/distillation/distiller.py
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">cfg</span><span class="p">:</span> <span class="n">CFG</span><span class="p">,</span> <span class="n">model_func</span><span class="p">:</span> <span class="n">Callable</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">DistilBERT</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span> <span class="o">=</span> <span class="n">cfg</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">teacher</span> <span class="o">=</span> <span class="n">model_func</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">teacher_num_layers</span><span class="p">)</span>  <span class="c1"># must be loading pretrained model containing mlm head
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">mlm_head</span> <span class="o">=</span> <span class="n">MLMHead</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">)</span>  <span class="c1"># must be loading pretrained model's mlm head
</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">student</span> <span class="o">=</span> <span class="n">model_func</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">student_num_layers</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">s_mlm_head</span> <span class="o">=</span> <span class="n">MLMHead</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">teacher_fw</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">inputs</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">padding_mask</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
        <span class="s">""" forward pass for teacher model
        """</span>
        <span class="n">last_hidden_state</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">teacher</span><span class="p">(</span>
            <span class="n">inputs</span><span class="p">,</span>
            <span class="n">padding_mask</span><span class="p">,</span>
            <span class="n">attention_mask</span>
        <span class="p">)</span>
        <span class="n">t_logit</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">mlm_head</span><span class="p">(</span><span class="n">last_hidden_state</span><span class="p">)</span>  <span class="c1"># hard logit =&gt; to make soft logit
</span>        <span class="k">return</span> <span class="n">last_hidden_state</span><span class="p">,</span> <span class="n">t_logit</span>

    <span class="k">def</span> <span class="nf">student_fw</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">inputs</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">padding_mask</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="bp">None</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
        <span class="s">""" forward pass for student model
        """</span>
        <span class="n">last_hidden_state</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">student</span><span class="p">(</span>
            <span class="n">inputs</span><span class="p">,</span>
            <span class="n">padding_mask</span><span class="p">,</span>
            <span class="n">attention_mask</span>
        <span class="p">)</span>
        <span class="n">s_logit</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">s_mlm_head</span><span class="p">(</span><span class="n">last_hidden_state</span><span class="p">)</span>  <span class="c1"># hard logit =&gt; to make soft logit
</span>        <span class="k">return</span> <span class="n">last_hidden_state</span><span class="p">,</span> <span class="n">s_logit</span>
</code></pre></div></div>]]></content><author><name>qcqced</name><email>qcqced123@gmail.com</email></author><category term="NLP" /><category term="Natural Language Process" /><category term="Roformer" /><category term="Linear-Attention" /><category term="Self-Attention" /><category term="Pytorch" /><category term="Transformation Matrix" /><category term="Complex Space" /><summary type="html"><![CDATA[Roformer Official Paper Review with Pytorch Implementation]]></summary></entry><entry><title type="html">👩‍💻🎄 [baekjoon] 1987번: 알파벳</title><link href="http://localhost:4000/ps/baekjoon-1987" rel="alternate" type="text/html" title="👩‍💻🎄 [baekjoon] 1987번: 알파벳" /><published>2024-01-30T00:00:00+09:00</published><updated>2024-01-31T02:00:00+09:00</updated><id>http://localhost:4000/ps/baekjoon_1987</id><content type="html" xml:base="http://localhost:4000/ps/baekjoon-1987"><![CDATA[<h3 id="️solution"><strong><code class="language-plaintext highlighter-rouge">🖍️ solution</code></strong></h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">sys</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">List</span>

<span class="k">def</span> <span class="nf">backtracking</span><span class="p">(</span><span class="n">y</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">count</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">visit</span><span class="p">:</span> <span class="n">List</span><span class="p">,</span> <span class="n">graph</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">]):</span>
    <span class="k">global</span> <span class="n">result</span>
    <span class="n">visit</span><span class="p">[</span><span class="nb">ord</span><span class="p">(</span><span class="n">graph</span><span class="p">[</span><span class="n">y</span><span class="p">][</span><span class="n">x</span><span class="p">])</span> <span class="o">-</span> <span class="mi">65</span><span class="p">]</span> <span class="o">=</span> <span class="bp">True</span>
    <span class="n">result</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">count</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">):</span>
        <span class="n">ny</span><span class="p">,</span> <span class="n">nx</span> <span class="o">=</span> <span class="n">dy</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">y</span><span class="p">,</span> <span class="n">dx</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">x</span>
        <span class="k">if</span> <span class="o">-</span><span class="mi">1</span> <span class="o">&lt;</span> <span class="n">ny</span> <span class="o">&lt;</span> <span class="n">r</span> <span class="ow">and</span> <span class="o">-</span><span class="mi">1</span> <span class="o">&lt;</span> <span class="n">nx</span> <span class="o">&lt;</span> <span class="n">c</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">visit</span><span class="p">[</span><span class="nb">ord</span><span class="p">(</span><span class="n">graph</span><span class="p">[</span><span class="n">ny</span><span class="p">][</span><span class="n">nx</span><span class="p">])</span> <span class="o">-</span> <span class="mi">65</span><span class="p">]:</span>
            <span class="n">backtracking</span><span class="p">(</span><span class="n">ny</span><span class="p">,</span> <span class="n">nx</span><span class="p">,</span> <span class="n">count</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">visit</span><span class="p">,</span> <span class="n">graph</span><span class="p">)</span>
            <span class="n">visit</span><span class="p">[</span><span class="nb">ord</span><span class="p">(</span><span class="n">graph</span><span class="p">[</span><span class="n">ny</span><span class="p">][</span><span class="n">nx</span><span class="p">])</span> <span class="o">-</span> <span class="mi">65</span><span class="p">]</span> <span class="o">=</span> <span class="bp">False</span>

<span class="n">r</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="nb">map</span><span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="n">sys</span><span class="p">.</span><span class="n">stdin</span><span class="p">.</span><span class="n">readline</span><span class="p">().</span><span class="n">split</span><span class="p">())</span>

<span class="n">result</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
<span class="n">dy</span><span class="p">,</span> <span class="n">dx</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">grid</span><span class="p">,</span> <span class="n">visited</span> <span class="o">=</span> <span class="p">[</span><span class="nb">list</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="nb">str</span><span class="p">,</span> <span class="n">sys</span><span class="p">.</span><span class="n">stdin</span><span class="p">.</span><span class="n">readline</span><span class="p">().</span><span class="n">rstrip</span><span class="p">()))</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">r</span><span class="p">)],</span> <span class="p">[</span><span class="bp">False</span><span class="p">]</span> <span class="o">*</span> <span class="mi">26</span>
<span class="n">backtracking</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">visited</span><span class="p">,</span> <span class="n">grid</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="nb">max</span><span class="p">(</span><span class="n">result</span><span class="p">))</span>
</code></pre></div></div>

<h3 id="idea"><strong><code class="language-plaintext highlighter-rouge">💡 idea</code></strong></h3>

<ul>
  <li><strong>Back Tracking</strong></li>
  <li><strong>1) 방문 기록 배열 변경</strong>
    <ul>
      <li><strong>조건 중에서 경로에 알파벳 중복이 불가능하다는 점 이용</strong></li>
      <li><strong>전체 격자 사이즈와 동일한 배열 대신 알파벳 사이즈(26)만 선언</strong></li>
    </ul>
  </li>
</ul>

<p>일반적인 백트래킹 문제라고 볼 수 있다. 하지만 파이썬으로 해결하려는 경우 시간, 메모리 제한 때문에 빡센 코드 최적화가 필요하다. 격자 문제라서 <code class="language-plaintext highlighter-rouge">bfs</code> 선택도 가능한데 그렇다면 <code class="language-plaintext highlighter-rouge">python3</code>로도 해결가능하다. 한편, 일반적인 <code class="language-plaintext highlighter-rouge">dfs</code>라면 빡센 최적화를 통해 <code class="language-plaintext highlighter-rouge">pypy3</code>으로만 통과 가능하다.</p>

<p>문제를 리뷰하던 도중 일반적인 <code class="language-plaintext highlighter-rouge">dfs</code> 백트래킹 방식의 비효율성에 대해 고찰해봤다. 아래와 같은 입력이 있다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">IEFCJ</span>
<span class="n">FHFKC</span>
<span class="n">FFALF</span>
<span class="n">HFGCF</span>
<span class="n">HMCHH</span>
</code></pre></div></div>

<p>일반적인 백트래킹 알고리즘이 탐색하는 과정을 생각해보자. 빨간색으로 칠해진 글자를 <code class="language-plaintext highlighter-rouge">IFHE</code> 순서로 탐색했다면, 다음은 <code class="language-plaintext highlighter-rouge">F</code>를 탐색해 방문해도 되는지 여부를 판정할 것이다. 이미 <code class="language-plaintext highlighter-rouge">F</code>는 방문했기 때문에 아마도 스택 프레임 할당을 취소하면서, 결국에는 <code class="language-plaintext highlighter-rouge">I</code>까지 되돌아 갈 것이다.</p>

<p>그리고 다시 오른쪽에 있는 <code class="language-plaintext highlighter-rouge">E</code>를  방문한 뒤, <code class="language-plaintext highlighter-rouge">FCK</code> 순서로 방문하게 될 것이다. 이 때 들게 되는 의문은 바로 이렇다. 굳이 <code class="language-plaintext highlighter-rouge">I</code>까지 되돌아갔다가 탐색해야 할까?? 이미 <code class="language-plaintext highlighter-rouge">IE</code> 는 탐색이 가능한 경로라는 것을 우리는 충분히 알 수 있다. 따라서 <code class="language-plaintext highlighter-rouge">DP Tabulation</code> 개념을 차용한다면 훨씬 빠르게 풀이가 가능할 것이다.</p>

<p>경로의 유일성을 보장하면서 수정 가능한 자료구조가 필요하기 때문에 배열 대신 세트를 사용해보자. 세트에는 현재까지의 경로 그리고 해당 경로의 마지막 인덱스를 저장해줘야 한다. 같은 경로라고 할 지라도 서로 다른 인덱스에 의해 만들어졌을 가능성이 있기 때문이다. 이렇게 세트를 구성한 뒤, 하나씩 pop해서 경로를 얻어낸다. 그 다음 해당 경로로부터 파생되는 여러 잠재적 경로들을 모두 검사해 경로가 만들어질 수 있는지 여부를 판정하면 된다. 코드는 다음과 같다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">sys</span>

<span class="k">def</span> <span class="nf">dfs</span><span class="p">(</span><span class="n">y</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
    <span class="n">dp</span><span class="p">,</span> <span class="n">result</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(),</span> <span class="mi">0</span>
    <span class="n">dp</span><span class="p">.</span><span class="n">add</span><span class="p">((</span><span class="n">y</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">grid</span><span class="p">[</span><span class="n">y</span><span class="p">][</span><span class="n">x</span><span class="p">]))</span>
    <span class="k">while</span> <span class="n">dp</span><span class="p">:</span>
        <span class="n">vy</span><span class="p">,</span> <span class="n">vx</span><span class="p">,</span> <span class="n">path</span> <span class="o">=</span> <span class="n">dp</span><span class="p">.</span><span class="n">pop</span><span class="p">()</span>
        <span class="n">result</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">result</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">path</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">result</span> <span class="o">==</span> <span class="mi">26</span><span class="p">:</span>
            <span class="k">return</span> <span class="mi">26</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">):</span>
            <span class="n">ny</span><span class="p">,</span> <span class="n">nx</span> <span class="o">=</span> <span class="n">dy</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">vy</span><span class="p">,</span> <span class="n">dx</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">vx</span>
            <span class="k">if</span> <span class="o">-</span><span class="mi">1</span> <span class="o">&lt;</span> <span class="n">ny</span> <span class="o">&lt;</span> <span class="n">r</span> <span class="ow">and</span> <span class="o">-</span><span class="mi">1</span> <span class="o">&lt;</span> <span class="n">nx</span> <span class="o">&lt;</span> <span class="n">c</span> <span class="ow">and</span> <span class="n">grid</span><span class="p">[</span><span class="n">ny</span><span class="p">][</span><span class="n">nx</span><span class="p">]</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">path</span><span class="p">:</span>
                <span class="n">dp</span><span class="p">.</span><span class="n">add</span><span class="p">((</span><span class="n">ny</span><span class="p">,</span> <span class="n">nx</span><span class="p">,</span> <span class="n">grid</span><span class="p">[</span><span class="n">ny</span><span class="p">][</span><span class="n">nx</span><span class="p">]</span> <span class="o">+</span> <span class="n">path</span><span class="p">))</span>
                
    <span class="k">return</span> <span class="n">result</span>

<span class="n">r</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="nb">map</span><span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="n">sys</span><span class="p">.</span><span class="n">stdin</span><span class="p">.</span><span class="n">readline</span><span class="p">().</span><span class="n">split</span><span class="p">())</span>
<span class="n">dy</span><span class="p">,</span> <span class="n">dx</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">grid</span> <span class="o">=</span> <span class="p">[</span><span class="nb">list</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="nb">str</span><span class="p">,</span> <span class="n">sys</span><span class="p">.</span><span class="n">stdin</span><span class="p">.</span><span class="n">readline</span><span class="p">().</span><span class="n">rstrip</span><span class="p">()))</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">r</span><span class="p">)]</span>
<span class="k">print</span><span class="p">(</span><span class="n">dfs</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
</code></pre></div></div>

<p align="center">
<img src="/assets/images/ps/after.png" alt="Common BackTracking" class="align-center image-caption" width="100%&quot;, height=&quot;100%" />
<strong><em>Common BackTracking</em></strong>
</p>

<p align="center">
<img src="/assets/images/ps/before.png" alt="DP Tabulation BackTracking" class="align-center image-caption" width="100%&quot;, height=&quot;100%" />
<strong><em>DP Tabulation BackTracking</em></strong>
</p>

<p>위에는 개선이전 결과고 아래는 개선 이후 결과다. 비약적인 속도 상승하는 동시에 메모리 역시 3배나 덜 사용하는 모습이다. 세트에 있는 유니크한 경로들을 하나씩 꺼내는 방식을 선택했기 때문에 알고리즘 성능이 시드에 영향(<code class="language-plaintext highlighter-rouge">set.pop()</code>은 랜덤으로 원소 선택)을 받는다는 점만 감안한다면 매우 좋은 풀이라고 생각한다.</p>]]></content><author><name>qcqced</name><email>qcqced123@gmail.com</email></author><category term="Problem Solving" /><category term="Python" /><category term="Codeing Test" /><category term="Algorithm" /><category term="Baekjoon" /><category term="Graph" /><category term="DFS" /><category term="BackTracking" /><summary type="html"><![CDATA[백준 1987번: 알파벳]]></summary></entry><entry><title type="html">🔥 Pytorch Tensor Indexing 자주 사용하는 메서드 모음집</title><link href="http://localhost:4000/framework-library/torch-indexing-function" rel="alternate" type="text/html" title="🔥 Pytorch Tensor Indexing 자주 사용하는 메서드 모음집" /><published>2024-01-09T00:00:00+09:00</published><updated>2024-01-10T02:00:00+09:00</updated><id>http://localhost:4000/framework-library/Pytorch-Tensor-Indexing-Function</id><content type="html" xml:base="http://localhost:4000/framework-library/torch-indexing-function"><![CDATA[<p>파이토치에서 필자가 자주 사용하는 텐서 인덱싱 관련 메서드의 사용법 및 사용 예시를 한방에 정리한 포스트다. 메서드 하나당 하나의 포스트로 만들기에는 너무 길이가 짧다 생각해 한 페이지에 모두 넣게 되었다. 지속적으로 업데이트 될 예정이다. 또한 텐서 인덱싱 말고도 다른 주제로도 관련 메서드를 정리해 올릴 예정이니 많은 관심 부탁드린다.</p>

<h3 id="torchargmax"><code class="language-plaintext highlighter-rouge">🔎 torch.argmax</code></h3>

<p>입력 텐서에서 가장 큰 값을 갖고 있는 원소의 인덱스를 반환한다. 최대값을 찾을 차원을 지정해줄 수 있다. 아래 예시 코드를 확인해보자.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># torch.argmax params
</span><span class="n">torch</span><span class="p">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

<span class="c1"># torch.argmax example 1
</span><span class="n">test</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">29</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">45</span><span class="p">,</span><span class="mi">22</span><span class="p">,</span><span class="mi">3</span><span class="p">])</span>
<span class="n">torch</span><span class="p">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">test</span><span class="p">)</span>
<span class="n">torch</span><span class="p">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">test2</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="o">&lt;</span><span class="n">Result</span><span class="o">&gt;</span>
<span class="n">tensor</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>

<span class="c1"># torch.argmax example 2
</span><span class="n">test</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
                     <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">]])</span>

<span class="n">torch</span><span class="p">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">test</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="o">&lt;</span><span class="n">Result</span><span class="o">&gt;</span>
<span class="n">tensor</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>

<span class="c1"># torch.argmax example 3
</span><span class="n">test</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
                     <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">]])</span>

<span class="n">torch</span><span class="p">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">test</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">tensor</span><span class="p">([[</span><span class="mi">0</span><span class="p">],</span>
        <span class="p">[</span><span class="mi">2</span><span class="p">]])</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">dim</code> 매개변수에 원하는 차원을 입력하면 해당 차원 뷰에서 가장 큰 원소를 찾아 인덱스 값을 반환해줄 것이다. 이 때 <code class="language-plaintext highlighter-rouge">keepdim=True</code> 로 설정한다면 입력 차원에서 가장 큰 원소의 인덱스를 반환하되 원본 텐서의 차원과 동일한 형태로 출력해준다. <code class="language-plaintext highlighter-rouge">example 2</code> 의 경우 <code class="language-plaintext highlighter-rouge">dim=0</code> 라서 행이 누적된 방향으로 텐서를 바라봐야 한다. 행이 누적된 방향으로 텐서를 보게 되면 <code class="language-plaintext highlighter-rouge">tensor([[0, 1, 1]])</code>이 된다.</p>

<h3 id="torchstack"><code class="language-plaintext highlighter-rouge">📚 torch.stack</code></h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="s">"""
torch.stack
Args:
	tensors(sequence of Tensors): 텐서가 담긴 파이썬 시퀀스 객체
	dim(int): 추가할 차원 방향을 세팅, 기본값은 0
"""</span>
<span class="n">torch</span><span class="p">.</span><span class="n">stack</span><span class="p">(</span><span class="n">tensors</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</code></pre></div></div>

<p>매개변수로 주어진 파이썬 시퀀스 객체(리스트, 튜플)를 사용자가 지정한 새로운 차원에 쌓는 기능을 한다. 매개변수 <code class="language-plaintext highlighter-rouge">tensors</code> 는 텐서가 담긴 파이썬의 시퀀스 객체를 입력으로 받는다. <code class="language-plaintext highlighter-rouge">dim</code> 은 사용자가 텐서 적재를 하고 싶은 새로운 차원을 지정해주면 된다. 기본값은 0차원으로 지정 되어있으며, 텐서의 맨 앞차원이 새롭게 생기게 된다. <code class="language-plaintext highlighter-rouge">torch.stack</code> 은 기계학습, 특히 딥러닝에서 정말 자주 사용되기 때문에 사용법 및 사용상황을 익혀두면 도움이 된다. 예시를 통해 해당 메서드를 어떤 상황에서 어떻게 사용하는지 알아보자.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="s">""" torch.stack example """</span>

<span class="k">class</span> <span class="nc">Projector</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="s">"""
    Making projection matrix(Q, K, V) for each attention head
    When you call this class, it returns projection matrix of each attention head
    For example, if you call this class with 8 heads, it returns 8 set of projection matrices (Q, K, V)
    Args:
        num_heads: number of heads in MHA, default 8
        dim_head: dimension of each attention head, default 64
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span> <span class="n">dim_head</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">64</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Projector</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span> <span class="o">=</span> <span class="n">num_heads</span> <span class="o">*</span> <span class="n">dim_head</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dim_head</span> <span class="o">=</span> <span class="n">dim_head</span>

    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">fc_q</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">dim_head</span><span class="p">)</span>
        <span class="n">fc_k</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">dim_head</span><span class="p">)</span>
        <span class="n">fc_v</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">dim_head</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">fc_q</span><span class="p">,</span> <span class="n">fc_k</span><span class="p">,</span> <span class="n">fc_v</span>

<span class="n">num_heads</span> <span class="o">=</span> <span class="mi">8</span>
<span class="n">dim_head</span> <span class="o">=</span> <span class="mi">64</span>
<span class="n">projector</span> <span class="o">=</span> <span class="n">Projector</span><span class="p">(</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">dim_head</span><span class="p">)</span>  <span class="c1"># init instance
</span><span class="n">projector_list</span> <span class="o">=</span> <span class="p">[</span><span class="nb">list</span><span class="p">(</span><span class="n">projector</span><span class="p">())</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_heads</span><span class="p">)]</span>  <span class="c1"># call instance
</span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">512</span><span class="p">)</span> <span class="c1"># x.shape: [Batch_Size, Sequence_Length, Dim_model]
</span><span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">V</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[],</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_heads</span><span class="p">):</span>
    <span class="n">Q</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">projector_list</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">0</span><span class="p">](</span><span class="n">x</span><span class="p">))</span> <span class="c1"># [10, 512, 64]
</span>    <span class="n">K</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">projector_list</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">1</span><span class="p">](</span><span class="n">x</span><span class="p">))</span> <span class="c1"># [10, 512, 64]
</span>	  <span class="n">V</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">projector_list</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">2</span><span class="p">](</span><span class="n">x</span><span class="p">))</span> <span class="c1"># [10, 512, 64]
</span> 
<span class="n">Q</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">stack</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># Q.shape: [10, 8, 512, 64]
</span><span class="n">K</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">stack</span><span class="p">(</span><span class="n">K</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># K.shape: [10, 8, 512, 64]
</span><span class="n">V</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">stack</span><span class="p">(</span><span class="n">V</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># V.shape: [10, 8, 512, 64]
</span></code></pre></div></div>

<p>위 코드는 <code class="language-plaintext highlighter-rouge">Transformer</code> 의 <code class="language-plaintext highlighter-rouge">Multi-Head Attention</code> 구현체 일부를 발췌해온 것이다. <code class="language-plaintext highlighter-rouge">Multi-Head Attention</code> 은 개별 어텐션 해드별로 행렬 $Q, K, V$를 가져야 한다. 따라서 입력 임베딩을 개별 어텐션 헤드에 <code class="language-plaintext highlighter-rouge">Linear Combination</code> 해줘야 하는데 헤드 개수가 8개나 되기 때문에 개별적으로 <code class="language-plaintext highlighter-rouge">Projection Matrix</code> 를 선언해주는 것은 매우 비효율적이다. 따라서 객체  <code class="language-plaintext highlighter-rouge">Projector</code> 에 행렬 $Q, K, V$에 대한 <code class="language-plaintext highlighter-rouge">Projection Matrix</code> 를 정의해줬다. 이후 헤드 개수만큼 객체  <code class="language-plaintext highlighter-rouge">Projector</code> 를 호출해 리스트에 해드별 <code class="language-plaintext highlighter-rouge">Projection Matrix</code> 를 담아준다. 그 다음 <code class="language-plaintext highlighter-rouge">torch.stack</code>을 사용해 <code class="language-plaintext highlighter-rouge">Attention Head</code> 방향의 차원으로 리스트 내부 텐서들을 쌓아주면 된다.</p>

<h3 id="torcharange"><code class="language-plaintext highlighter-rouge">🔢 torch.arange</code></h3>

<p>사용자가 지정한 시작점부터 끝점까지 일정한 간격으로 텐서를 나열한다. Python의 내장 메서드 <code class="language-plaintext highlighter-rouge">range</code>와 동일한 역할을 하는데, 대신 텐서 그 결과를 텐서 구조체로 반환한다고 생각하면 되겠다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># torch.arange usage
</span><span class="n">torch</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="n">start</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">end</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
<span class="n">tensor</span><span class="p">([</span> <span class="mi">0</span><span class="p">,</span>  <span class="mi">1</span><span class="p">,</span>  <span class="mi">2</span><span class="p">,</span>  <span class="mi">3</span><span class="p">,</span>  <span class="mi">4</span><span class="p">])</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">tensor</span><span class="p">([</span> <span class="mi">1</span><span class="p">,</span>  <span class="mi">2</span><span class="p">,</span>  <span class="mi">3</span><span class="p">])</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">)</span>
<span class="n">tensor</span><span class="p">([</span> <span class="mf">1.0000</span><span class="p">,</span>  <span class="mf">1.5000</span><span class="p">,</span>  <span class="mf">2.0000</span><span class="p">])</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">step</code> 매개변수로 원소간 간격 조정을 할 수 있는데, 기본은 1로 지정 되어 있으니 참고하자. 필자의 경우에는 <code class="language-plaintext highlighter-rouge">nn.Embedding</code>의 입력 텐서를 만들 때 가장 많이 사용했다. <code class="language-plaintext highlighter-rouge">nn.Embedding</code> 의 경우 Input으로 <code class="language-plaintext highlighter-rouge">IntTensor</code>, <code class="language-plaintext highlighter-rouge">LongTensor</code>를 받게 되어 있으니 알아두자.</p>

<h3 id="torchrepeat"><code class="language-plaintext highlighter-rouge">🔁 torch.repeat</code></h3>

<p>입력값으로 주어진 텐서를 사용자가 지정한 반복 횟수만큼 특정 차원 방향으로 늘린다. 예를 들면 <code class="language-plaintext highlighter-rouge">[1,2,3] * 3</code>의 결과는 <code class="language-plaintext highlighter-rouge">[1, 2, 3, 1, 2, 3, 1, 2, 3]</code> 인데, 이것을 사용자가 지정한 반복 횟수만큼 특정 차원으로 수행하겠다는 것이다. 아래 사용 예제를 확인해보자.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># torch.repeat example
</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">x</span><span class="p">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">tensor</span><span class="p">([[</span> <span class="mi">1</span><span class="p">,</span>  <span class="mi">2</span><span class="p">,</span>  <span class="mi">3</span><span class="p">,</span>  <span class="mi">1</span><span class="p">,</span>  <span class="mi">2</span><span class="p">,</span>  <span class="mi">3</span><span class="p">],</span>
        <span class="p">[</span> <span class="mi">1</span><span class="p">,</span>  <span class="mi">2</span><span class="p">,</span>  <span class="mi">3</span><span class="p">,</span>  <span class="mi">1</span><span class="p">,</span>  <span class="mi">2</span><span class="p">,</span>  <span class="mi">3</span><span class="p">],</span>
        <span class="p">[</span> <span class="mi">1</span><span class="p">,</span>  <span class="mi">2</span><span class="p">,</span>  <span class="mi">3</span><span class="p">,</span>  <span class="mi">1</span><span class="p">,</span>  <span class="mi">2</span><span class="p">,</span>  <span class="mi">3</span><span class="p">],</span>
        <span class="p">[</span> <span class="mi">1</span><span class="p">,</span>  <span class="mi">2</span><span class="p">,</span>  <span class="mi">3</span><span class="p">,</span>  <span class="mi">1</span><span class="p">,</span>  <span class="mi">2</span><span class="p">,</span>  <span class="mi">3</span><span class="p">]])</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">x</span><span class="p">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">tensor</span><span class="p">([[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
         <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]],</span>

        <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
         <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]],</span>

        <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
         <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]],</span>

        <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
         <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]]])</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">x</span><span class="p">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">).</span><span class="n">size</span>
<span class="n">torch</span><span class="p">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">x</span><span class="p">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">tensor</span><span class="p">([[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
         <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]],</span>

        <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
         <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]],</span>

        <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
         <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]],</span>

        <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
         <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]]])</span>
</code></pre></div></div>

<p>$t$를 어떤 텐서 구조체 $x$의 최대 차원이라고 했을 , $x_t$를 가장 왼쪽에 넣고 가장 낮은 차원인 0차원에 대한 반복 횟수를 오른쪽 끝에 대입해서 사용하면 된다. (<code class="language-plaintext highlighter-rouge">torch.repeat(</code>$x_t, x_{t-1}, … x_2, x_1, x_0$<code class="language-plaintext highlighter-rouge">))</code>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># torch.arange &amp; torch.repeate usage example
</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">pos_x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">num_patches</span> <span class="o">+</span> <span class="mi">1</span><span class="p">).</span><span class="n">repeat</span><span class="p">(</span><span class="n">inputs</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]).</span><span class="n">to</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">pos_x</span><span class="p">.</span><span class="n">shape</span>
<span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">16</span><span class="p">,</span> <span class="mi">1025</span><span class="p">])</span>
</code></pre></div></div>

<p>필자의 경우, <code class="language-plaintext highlighter-rouge">position embedding</code>의 입력을 만들고 싶을 때 <code class="language-plaintext highlighter-rouge">torch.arange</code> 와 연계해 자주 사용 했던 것 같다. 위 코드를 참고하자.</p>

<h3 id="torchclamp"><code class="language-plaintext highlighter-rouge">🔬 torch.clamp</code></h3>

<p>입력 텐서의 원소값을 사용자가 지정한 최대•최소값 범위 이내로 제한하는 메서드다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># torch.clamp params
</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">clamp</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="nb">min</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span> <span class="err">→</span> <span class="n">Tensor</span>

<span class="c1"># torch.clamp usage example
</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">a</span>
<span class="n">tensor</span><span class="p">([</span><span class="o">-</span><span class="mf">1.7120</span><span class="p">,</span>  <span class="mf">0.1734</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0478</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0922</span><span class="p">])</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="nb">min</span><span class="o">=-</span><span class="mf">0.5</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">tensor</span><span class="p">([</span><span class="o">-</span><span class="mf">0.5000</span><span class="p">,</span>  <span class="mf">0.1734</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0478</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0922</span><span class="p">])</span>
</code></pre></div></div>

<p>입력된 텐서의 원소를 지정 최대•최소 설정값과 하나 하나 대조해서 텐서 내부의 모든 원소가 지정 범위 안에 들도록 만들어준다. <code class="language-plaintext highlighter-rouge">torch.clamp</code> 역시 다양한 상황에서 사용되는데, 필자의 경우 모델 레이어 중간에 제곱근이나 지수, 분수 혹은 각도 관련 연산이 들어가 <code class="language-plaintext highlighter-rouge">Backward Pass</code>에서 <code class="language-plaintext highlighter-rouge">NaN</code>이 발생할 수 있는 경우에 안전장치로 많이 사용하고 있다. (<a href="https://qcqced123.github.io/framework-library/backward-nan/">자세히 알고 싶다면 클릭</a>)</p>

<h3 id="torchgather"><code class="language-plaintext highlighter-rouge">👩‍👩‍👧‍👦 torch.gather</code></h3>

<p>텐서 객체 내부에서 원하는 인덱스에 위치한 원소만 추출하고 싶을 때 사용하면 매우 유용한 메서드다. 텐서 역시 <code class="language-plaintext highlighter-rouge">iterable</code> 객체라서 <code class="language-plaintext highlighter-rouge">loop</code> 를 사용해 접근하는 것이 직관적으로 보일 수 있으나, 통상적으로 텐서를 사용하는 상황이라면 객체의 차원이 어마무시 하기 때문에 루프로 접근해 관리하는 것은 매우 비효율적이다. 루프를 통해 접근하면 파이썬의 내장 리스트를 사용하는 것과 별반 다를게 없어지기 때문에, 텐서를 사용하는 메리트가 사라진다. 비교적 크지 않은 2~3차원의 텐서 정도라면 사용해도 크게 문제는 없을거라 생각하지만 그래도 코드의 일관성을 위해 <code class="language-plaintext highlighter-rouge">torch.gather</code> 사용을 권장한다. 이제 <code class="language-plaintext highlighter-rouge">torch.gather</code>의 사용법에 대해 알아보자.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># torch.gather params
</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">gather</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">sparse_grad</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">dim</code>과 <code class="language-plaintext highlighter-rouge">index</code>에 주목해보자. 먼저 <code class="language-plaintext highlighter-rouge">dim</code>은 사용자가 인덱싱을 적용하고 싶은 차원을 지정해주는 역할을 한다. <code class="language-plaintext highlighter-rouge">index</code> 매개변수로 전달하는 텐서 안에는 원소의 <code class="language-plaintext highlighter-rouge">‘인덱스’</code>를 의미하는 숫자들이 마구잡이로 담겨있는데, 해당 인덱스가 대상 텐서의 어느 차원을 가리킬 것인지를 컴퓨터에게 알려준다고 생각하면 된다. <code class="language-plaintext highlighter-rouge">index</code> 는 앞에서 설명했듯이 원소의 <code class="language-plaintext highlighter-rouge">‘인덱스’</code>를 의미하는 숫자들이 담긴 텐서를 입력으로 하는 매개변수다. 이 때 주의할 점은 대상 텐서(<code class="language-plaintext highlighter-rouge">input</code>)와 인덱스 텐서의 차원 형태가 반드시 동일해야 한다는 것이다. 역시 말로만 들으면 이해하기 힘드니 사용 예시를 함꼐 살펴보자.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># torch.gather usage example
</span><span class="o">&gt;&gt;&gt;</span> <span class="n">q</span><span class="p">,</span> <span class="n">kr</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1024</span><span class="p">,</span> <span class="mi">64</span><span class="p">),</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1024</span><span class="p">,</span> <span class="mi">64</span><span class="p">)</span> <span class="c1"># [batch, sequence, dim_head], [batch, 2*sequence, dim_head]
</span><span class="o">&gt;&gt;&gt;</span> <span class="n">tmp_c2p</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">kr</span><span class="p">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">))</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">tmp_c2p</span><span class="p">,</span> <span class="n">tmp_c2p</span><span class="p">.</span><span class="n">shape</span>
<span class="p">(</span><span class="n">tensor</span><span class="p">([[</span><span class="o">-</span><span class="mf">2.6477</span><span class="p">,</span> <span class="o">-</span><span class="mf">4.7478</span><span class="p">,</span> <span class="o">-</span><span class="mf">5.3250</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mf">1.6062</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.9717</span><span class="p">,</span>  <span class="mf">3.8004</span><span class="p">],</span>
         <span class="p">[</span> <span class="mf">0.0662</span><span class="p">,</span>  <span class="mf">1.5240</span><span class="p">,</span>  <span class="mf">0.1182</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mf">0.1653</span><span class="p">,</span>  <span class="mf">2.8476</span><span class="p">,</span>  <span class="mf">1.6337</span><span class="p">],</span>
         <span class="p">[</span><span class="o">-</span><span class="mf">0.5010</span><span class="p">,</span> <span class="o">-</span><span class="mf">4.2267</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.1179</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mf">1.1447</span><span class="p">,</span>  <span class="mf">1.7845</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1493</span><span class="p">],</span>
         <span class="p">...,</span>
         <span class="p">[</span><span class="o">-</span><span class="mf">2.1073</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2149</span><span class="p">,</span> <span class="o">-</span><span class="mf">4.8630</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mf">0.8238</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5833</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2066</span><span class="p">],</span>
         <span class="p">[</span> <span class="mf">2.1747</span><span class="p">,</span>  <span class="mf">3.2924</span><span class="p">,</span>  <span class="mf">6.5808</span><span class="p">,</span>  <span class="p">...,</span> <span class="o">-</span><span class="mf">0.2926</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2511</span><span class="p">,</span>  <span class="mf">2.6996</span><span class="p">],</span>
         <span class="p">[</span><span class="o">-</span><span class="mf">2.8362</span><span class="p">,</span>  <span class="mf">2.8700</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.9729</span><span class="p">,</span>  <span class="p">...,</span> <span class="o">-</span><span class="mf">4.9913</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3616</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1708</span><span class="p">]],</span>
        <span class="n">grad_fn</span><span class="o">=&lt;</span><span class="n">MmBackward0</span><span class="o">&gt;</span><span class="p">)</span>
<span class="n">torch</span><span class="p">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">1024</span><span class="p">,</span> <span class="mi">1024</span><span class="p">]))</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">max_seq</span><span class="p">,</span> <span class="n">max_relative_position</span> <span class="o">=</span> <span class="mi">1024</span><span class="p">,</span> <span class="mi">512</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">q_index</span><span class="p">,</span> <span class="n">k_index</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="n">max_seq</span><span class="p">),</span> <span class="n">torch</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">max_relative_position</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">q_index</span><span class="p">,</span> <span class="n">k_index</span>
<span class="p">(</span><span class="n">tensor</span><span class="p">([</span>   <span class="mi">0</span><span class="p">,</span>    <span class="mi">1</span><span class="p">,</span>    <span class="mi">2</span><span class="p">,</span>  <span class="p">...,</span> <span class="mi">1021</span><span class="p">,</span> <span class="mi">1022</span><span class="p">,</span> <span class="mi">1023</span><span class="p">]),</span>
 <span class="n">tensor</span><span class="p">([</span>   <span class="mi">0</span><span class="p">,</span>    <span class="mi">1</span><span class="p">,</span>    <span class="mi">2</span><span class="p">,</span>  <span class="p">...,</span> <span class="mi">1021</span><span class="p">,</span> <span class="mi">1022</span><span class="p">,</span> <span class="mi">1023</span><span class="p">]))</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">tmp_pos</span> <span class="o">=</span> <span class="n">q_index</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="o">-</span> <span class="n">k_index</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">rel_pos_matrix</span> <span class="o">=</span> <span class="n">tmp_pos</span> <span class="o">+</span> <span class="n">max_relative_position</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">rel_pos_matrix</span>
<span class="n">tensor</span><span class="p">([[</span> <span class="mi">512</span><span class="p">,</span>  <span class="mi">511</span><span class="p">,</span>  <span class="mi">510</span><span class="p">,</span>  <span class="p">...,</span> <span class="o">-</span><span class="mi">509</span><span class="p">,</span> <span class="o">-</span><span class="mi">510</span><span class="p">,</span> <span class="o">-</span><span class="mi">511</span><span class="p">],</span>
        <span class="p">[</span> <span class="mi">513</span><span class="p">,</span>  <span class="mi">512</span><span class="p">,</span>  <span class="mi">511</span><span class="p">,</span>  <span class="p">...,</span> <span class="o">-</span><span class="mi">508</span><span class="p">,</span> <span class="o">-</span><span class="mi">509</span><span class="p">,</span> <span class="o">-</span><span class="mi">510</span><span class="p">],</span>
        <span class="p">[</span> <span class="mi">514</span><span class="p">,</span>  <span class="mi">513</span><span class="p">,</span>  <span class="mi">512</span><span class="p">,</span>  <span class="p">...,</span> <span class="o">-</span><span class="mi">507</span><span class="p">,</span> <span class="o">-</span><span class="mi">508</span><span class="p">,</span> <span class="o">-</span><span class="mi">509</span><span class="p">],</span>
        <span class="p">...,</span>
        <span class="p">[</span><span class="mi">1533</span><span class="p">,</span> <span class="mi">1532</span><span class="p">,</span> <span class="mi">1531</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mi">512</span><span class="p">,</span>  <span class="mi">511</span><span class="p">,</span>  <span class="mi">510</span><span class="p">],</span>
        <span class="p">[</span><span class="mi">1534</span><span class="p">,</span> <span class="mi">1533</span><span class="p">,</span> <span class="mi">1532</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mi">513</span><span class="p">,</span>  <span class="mi">512</span><span class="p">,</span>  <span class="mi">511</span><span class="p">],</span>
        <span class="p">[</span><span class="mi">1535</span><span class="p">,</span> <span class="mi">1534</span><span class="p">,</span> <span class="mi">1533</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mi">514</span><span class="p">,</span>  <span class="mi">513</span><span class="p">,</span>  <span class="mi">512</span><span class="p">]])</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">rel_pos_matrix</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">rel_pos_matrix</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="o">*</span><span class="n">max_relative_position</span> <span class="o">-</span> <span class="mi">1</span><span class="p">).</span><span class="n">repeat</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">tmp_c2p</span> <span class="o">=</span> <span class="n">tmp_c2p</span><span class="p">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">rel_pos_matrix</span><span class="p">,</span> <span class="n">rel_pos_matrix</span><span class="p">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">tmp_c2p</span><span class="p">.</span><span class="n">shape</span> 
<span class="p">(</span><span class="n">tensor</span><span class="p">([[[</span> <span class="mi">512</span><span class="p">,</span>  <span class="mi">511</span><span class="p">,</span>  <span class="mi">510</span><span class="p">,</span>  <span class="p">...,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">],</span>
          <span class="p">[</span> <span class="mi">513</span><span class="p">,</span>  <span class="mi">512</span><span class="p">,</span>  <span class="mi">511</span><span class="p">,</span>  <span class="p">...,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">],</span>
          <span class="p">[</span> <span class="mi">514</span><span class="p">,</span>  <span class="mi">513</span><span class="p">,</span>  <span class="mi">512</span><span class="p">,</span>  <span class="p">...,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">],</span>
          <span class="p">...,</span>
          <span class="p">[</span><span class="mi">1023</span><span class="p">,</span> <span class="mi">1023</span><span class="p">,</span> <span class="mi">1023</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mi">512</span><span class="p">,</span>  <span class="mi">511</span><span class="p">,</span>  <span class="mi">510</span><span class="p">],</span>
          <span class="p">[</span><span class="mi">1023</span><span class="p">,</span> <span class="mi">1023</span><span class="p">,</span> <span class="mi">1023</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mi">513</span><span class="p">,</span>  <span class="mi">512</span><span class="p">,</span>  <span class="mi">511</span><span class="p">],</span>
          <span class="p">[</span><span class="mi">1023</span><span class="p">,</span> <span class="mi">1023</span><span class="p">,</span> <span class="mi">1023</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mi">514</span><span class="p">,</span>  <span class="mi">513</span><span class="p">,</span>  <span class="mi">512</span><span class="p">]],</span>
 
         <span class="p">[[</span> <span class="mi">512</span><span class="p">,</span>  <span class="mi">511</span><span class="p">,</span>  <span class="mi">510</span><span class="p">,</span>  <span class="p">...,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">],</span>
          <span class="p">[</span> <span class="mi">513</span><span class="p">,</span>  <span class="mi">512</span><span class="p">,</span>  <span class="mi">511</span><span class="p">,</span>  <span class="p">...,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">],</span>
          <span class="p">[</span> <span class="mi">514</span><span class="p">,</span>  <span class="mi">513</span><span class="p">,</span>  <span class="mi">512</span><span class="p">,</span>  <span class="p">...,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">],</span>
          <span class="p">...,</span>
          <span class="p">[</span><span class="mi">1023</span><span class="p">,</span> <span class="mi">1023</span><span class="p">,</span> <span class="mi">1023</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mi">512</span><span class="p">,</span>  <span class="mi">511</span><span class="p">,</span>  <span class="mi">510</span><span class="p">],</span>
          <span class="p">[</span><span class="mi">1023</span><span class="p">,</span> <span class="mi">1023</span><span class="p">,</span> <span class="mi">1023</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mi">513</span><span class="p">,</span>  <span class="mi">512</span><span class="p">,</span>  <span class="mi">511</span><span class="p">],</span>
          <span class="p">[</span><span class="mi">1023</span><span class="p">,</span> <span class="mi">1023</span><span class="p">,</span> <span class="mi">1023</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mi">514</span><span class="p">,</span>  <span class="mi">513</span><span class="p">,</span>  <span class="mi">512</span><span class="p">]],</span>
<span class="n">torch</span><span class="p">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1024</span><span class="p">,</span> <span class="mi">1024</span><span class="p">]),</span>
<span class="n">torch</span><span class="p">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1024</span><span class="p">,</span> <span class="mi">1024</span><span class="p">]))</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">gather</span><span class="p">(</span><span class="n">tmp_c2p</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">rel_pos_matrix</span><span class="p">)</span>
<span class="n">tensor</span><span class="p">([[[</span><span class="o">-</span><span class="mf">0.8579</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2178</span><span class="p">,</span>  <span class="mf">1.6323</span><span class="p">,</span>  <span class="p">...,</span> <span class="o">-</span><span class="mf">2.6477</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.6477</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.6477</span><span class="p">],</span>
         <span class="p">[</span> <span class="mf">1.1601</span><span class="p">,</span>  <span class="mf">2.1752</span><span class="p">,</span>  <span class="mf">0.7187</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mf">0.0662</span><span class="p">,</span>  <span class="mf">0.0662</span><span class="p">,</span>  <span class="mf">0.0662</span><span class="p">],</span>
         <span class="p">[</span> <span class="mf">3.4379</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2573</span><span class="p">,</span>  <span class="mf">0.1375</span><span class="p">,</span>  <span class="p">...,</span> <span class="o">-</span><span class="mf">0.5010</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5010</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5010</span><span class="p">],</span>
         <span class="p">...,</span>
         <span class="p">[</span><span class="o">-</span><span class="mf">1.2066</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2066</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2066</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mf">0.5943</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5169</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.0820</span><span class="p">],</span>
         <span class="p">[</span> <span class="mf">2.6996</span><span class="p">,</span>  <span class="mf">2.6996</span><span class="p">,</span>  <span class="mf">2.6996</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mf">0.2014</span><span class="p">,</span>  <span class="mf">1.1458</span><span class="p">,</span>  <span class="mf">3.2626</span><span class="p">],</span>
         <span class="p">[</span><span class="o">-</span><span class="mf">0.1708</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1708</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1708</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mf">1.9955</span><span class="p">,</span>  <span class="mf">4.1549</span><span class="p">,</span>  <span class="mf">2.6356</span><span class="p">]],</span>
</code></pre></div></div>

<p>위 코드는 <code class="language-plaintext highlighter-rouge">DeBERTa</code> 의 <code class="language-plaintext highlighter-rouge">Disentangled Self-Attention</code>을 구현한 코드의 일부분이다. 자세한 원리는 <code class="language-plaintext highlighter-rouge">DeBERTa</code> 논문 리뷰 포스팅에서 확인하면 되고, 우리가 지금 주목할 부분은 바로 <code class="language-plaintext highlighter-rouge">tmp_c2p</code>, <code class="language-plaintext highlighter-rouge">rel_pos_matrix</code> 그리고 마지막 줄에 위치한 <code class="language-plaintext highlighter-rouge">torch.gather</code> 다. <code class="language-plaintext highlighter-rouge">[10, 1024, 1024]</code> 모양을 가진 대상 텐서 <code class="language-plaintext highlighter-rouge">tmp_c2p</code> 에서 내가 원하는 원소만 추출하려는 상황인데, 추출해야할 원소의 인덱스 값이 담긴 텐서를 <code class="language-plaintext highlighter-rouge">rel_pos_matrix</code> 로 정의했다. <code class="language-plaintext highlighter-rouge">rel_pos_matrix</code> 의 차원은 <code class="language-plaintext highlighter-rouge">[10, 1024, 1024]</code>로 <code class="language-plaintext highlighter-rouge">tmp_c2p</code>와 동일하다. 참고로 추출해야 하는 차원 방향은 가로 방향(두 번째 1024)이다.</p>

<p>이제 <code class="language-plaintext highlighter-rouge">torch.gather</code>의 동작을 살펴보자. 우리가 현재 추출하고 싶은 대상은 3차원 텐서의 가로 방향(두 번째 1024, 텐서의 행 벡터), 즉 <code class="language-plaintext highlighter-rouge">2 * max_sequence_length</code> 를 의미하는 차원 방향의 원소다. 따라서 <code class="language-plaintext highlighter-rouge">dim=-1</code>으로 설정해준다. 이제 메서드가 의도대로 적용되었는지 확인해보자. <code class="language-plaintext highlighter-rouge">rel_pos_matrix</code> 의 0번 배치, 0번째 시퀀스의 가장 마지막 차원의 값은 <code class="language-plaintext highlighter-rouge">0</code>으로 초기화 되어 있다. 다시 말해, 대상 텐서의 대상 차원에서 0번째 인덱스에 해당하는 값을 가져오라는 의미를 담고 있다. 그렇다면 <code class="language-plaintext highlighter-rouge">torch.gather</code> 실행 결과가 <code class="language-plaintext highlighter-rouge">tmp_c2p</code>의 0번 배치, 0번째 시퀀스의 0번째 차원 값과 일치하는지 확인해보자. 둘 다 <code class="language-plaintext highlighter-rouge">-2.6477</code>, <code class="language-plaintext highlighter-rouge">-2.6477</code> 으로 같은 값을 나타내고 있다. 따라서 우리 의도대로 잘 실행되었다는 사실을 알 수 있다.</p>

<h3 id="torchtriu-torchtril"><code class="language-plaintext highlighter-rouge">👩‍👩‍👧‍👦 torch.triu, torch.tril</code></h3>

<p>각각 입력 텐서를 <code class="language-plaintext highlighter-rouge">상삼각행렬</code>, <code class="language-plaintext highlighter-rouge">하삼각행렬</code>로 만든다. <code class="language-plaintext highlighter-rouge">triu</code>나 <code class="language-plaintext highlighter-rouge">tril</code>은 사실 뒤집으면 같은 결과를 반환하기 때문에 <code class="language-plaintext highlighter-rouge">tril</code>을 기준으로 설명을 하겠다. 메서드의 매개변수는 다음과 같다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># torch.triu, tril params
</span><span class="n">upper_tri_matrix</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">triu</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span> <span class="n">diagonal</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
<span class="n">lower_tri_matrix</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tril</span><span class="p">(</span><span class="n">input_tensors</span><span class="p">,</span> <span class="n">diagonal</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">diagonal</code> 에 주목해보자. 양수를 전달하면 주대각성분에서 해당하는 값만큼 떨어진 곳의 대각성분까지 그 값을 살려둔다. 한편 음수를 전달하면 주대각성분을 포함해 주어진 값만큼 떨어진 곳까지의 대각성분을 모두 0으로 만들어버린다. 기본은 0으로 설정되어 있으며, 이는 주대각성분부터 왼쪽 하단의 원소를 모두 살려두겠다는 의미가 된다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># torch.tril usage example
</span><span class="o">&gt;&gt;&gt;</span> <span class="n">lm_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tril</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">ones</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">lm_mask</span>
<span class="mi">1</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span>
<span class="mi">1</span> <span class="mi">1</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span>
<span class="mi">1</span> <span class="mi">1</span> <span class="mi">1</span> <span class="mi">0</span> <span class="mi">0</span>
<span class="mi">1</span> <span class="mi">1</span> <span class="mi">1</span> <span class="mi">1</span> <span class="mi">0</span>
</code></pre></div></div>

<p>두 메서드는 선형대수학이 필요한 다양한 분야에서 사용되는데, 필자의 경우, <code class="language-plaintext highlighter-rouge">GPT</code>처럼 <code class="language-plaintext highlighter-rouge">Transformer</code>의 <code class="language-plaintext highlighter-rouge">Decoder</code> 를 사용하는 모델을 빌드할 때 가장 많이 사용했던 것 같다. <code class="language-plaintext highlighter-rouge">Decoder</code>를 사용하는 모델은 대부분 구조상 <code class="language-plaintext highlighter-rouge">Language Modeling</code>을 위해서 <code class="language-plaintext highlighter-rouge">Masked Multi-Head Self-Attention Block</code>을 사용하는데 이 때 미래 시점의 토큰 임베딩 값에 마스킹을 해주기 위해 <code class="language-plaintext highlighter-rouge">torch.tril</code> 을 사용하게 되니 참고하자.</p>

<h3 id="torchtensormasked_fill"><code class="language-plaintext highlighter-rouge">👩‍👩‍👧‍👦 torch.Tensor.masked_fill</code></h3>

<p>사용자가 지정한 값에 해당되는 원소를 모두 마스킹 처리해주는 메서드다. 먼저 매개변수를 확인해보자.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># torch.Tensor.masked_fill params
</span><span class="n">input_tensors</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">6</span><span class="p">]])</span>
<span class="n">input_tensors</span><span class="p">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">mask</span><span class="p">:</span> <span class="n">BoolTensor</span><span class="p">,</span> <span class="n">value</span><span class="p">:</span> <span class="nb">float</span><span class="p">)</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">masked_fill</code> 은 텐서 객체의 내부 <code class="language-plaintext highlighter-rouge">attribute</code> 로 정의되기 때문에 해당 메서드를 사용하고 싶다면 먼저 마스킹 대상 텐서를 만들어야 한다. 텐서를 정의했다면 텐서 객체의 <code class="language-plaintext highlighter-rouge">attributes</code> 접근을 통해 <code class="language-plaintext highlighter-rouge">masked_fill()</code> 을 호출한 뒤, 필요한 매개변수를 전달해주는 방식으로 사용하면 된다.</p>

<p><code class="language-plaintext highlighter-rouge">mask</code> 매개변수에는 마스킹 텐서를 전달해야 하는데, 이 때 내부 원소는 모두 <code class="language-plaintext highlighter-rouge">boolean</code>이어야 하고 텐서의 형태는 대상 텐서와 동일해야 한다(완전히 같을 필요는 없고, 브로드 캐스팅만 가능하면 상관 없음).</p>

<p><code class="language-plaintext highlighter-rouge">value</code> 매개변수에는 마스킹 대상 원소들에 일괄적으로 적용해주고 싶은 값을 전달한다. 이게 말로만 들으면 이해하기 쉽지 않다. 아래 사용 예시를 함께 첨부했으니 참고 바란다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># torch.masked_fill usage
</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">lm_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tril</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">ones</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">lm_mask</span>
<span class="mi">1</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span>
<span class="mi">1</span> <span class="mi">1</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span>
<span class="mi">1</span> <span class="mi">1</span> <span class="mi">1</span> <span class="mi">0</span> <span class="mi">0</span>
<span class="mi">1</span> <span class="mi">1</span> <span class="mi">1</span> <span class="mi">1</span> <span class="mi">0</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">attention_matrix</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">))</span> <span class="o">/</span> <span class="n">dot_scale</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">attention_matrix</span>
<span class="mf">1.22</span> <span class="mf">2.1</span> <span class="mf">3.4</span> <span class="mf">1.2</span> <span class="mf">1.1</span>
<span class="mf">1.22</span> <span class="mf">2.1</span> <span class="mf">3.4</span> <span class="mf">9.9</span> <span class="mf">9.9</span>
<span class="mf">1.22</span> <span class="mf">2.1</span> <span class="mf">3.4</span> <span class="mf">9.9</span> <span class="mf">9.9</span>
<span class="mf">1.22</span> <span class="mf">2.1</span> <span class="mf">3.4</span> <span class="mf">9.9</span> <span class="mf">9.9</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">attention_matrix</span> <span class="o">=</span> <span class="n">attention_matrix</span><span class="p">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">lm_mask</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="s">'-inf'</span><span class="p">))</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">attention_matrix</span>
<span class="mf">1.22</span> <span class="o">-</span><span class="n">inf</span> <span class="o">-</span><span class="n">inf</span> <span class="o">-</span><span class="n">inf</span> <span class="o">-</span><span class="n">inf</span>
<span class="mf">1.22</span> <span class="mf">2.1</span> <span class="o">-</span><span class="n">inf</span> <span class="o">-</span><span class="n">inf</span> <span class="o">-</span><span class="n">inf</span>
<span class="mf">1.22</span> <span class="mf">2.1</span> <span class="mf">3.4</span> <span class="o">-</span><span class="n">inf</span> <span class="o">-</span><span class="n">inf</span>
<span class="mf">1.22</span> <span class="mf">2.1</span> <span class="mf">3.4</span> <span class="mf">9.9</span> <span class="o">-</span><span class="n">inf</span>
</code></pre></div></div>

<h3 id="️torchclone"><code class="language-plaintext highlighter-rouge">🗂️ torch.clone</code></h3>

<p><code class="language-plaintext highlighter-rouge">inputs</code> 인자로 전달한 텐서를 복사하는 파이토치 내장 메서드다.  사용법은 아래와 같다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="s">""" torch.clone """</span>
<span class="n">torch</span><span class="p">.</span><span class="n">clone</span><span class="p">(</span>
    <span class="nb">input</span><span class="p">,</span><span class="err"> </span>
    <span class="o">*</span><span class="p">,</span>
   <span class="err"> </span><span class="n">memory_format</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">preserve_format</span>
<span class="p">)</span><span class="err"> → </span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span>
</code></pre></div></div>

<p>딥러닝 파이프라인을 만들다 보면 많이 사용하게 되는 기본적인 메서드인데, 이렇게 따로 정리하게 된 이유가 있다. 입력된 텐서를 그대로 복사한다는 특성 때문에 사용시 주의해야 할 점이 있기 때문이다. 해당 메서드를 사용하기 전에 반드시 입력할 텐서가 현재 어느 디바이스(CPU, GPU) 위에 있는지, 그리고 해당 텐서가 계산 그래프를 가지고 있는지를 <strong>반드시</strong> 파악해야 한다.</p>

<p>필자는 ELECTRA 모델을 직접 구현하는 과정에서 <code class="language-plaintext highlighter-rouge">clone()</code> 메서드를 사용했는데, Generator 모델의 결과 로짓을 Discriminator의 입력으로 변환해주기 위함이었다. 그 과정에서 Generator가 반환한 로짓을 그대로 <code class="language-plaintext highlighter-rouge">clone</code>한 뒤, 입력을 만들어 주었고 그 결과 아래와 같은 에러를 마주했다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">RuntimeError</span><span class="p">:</span> <span class="n">one</span> <span class="n">of</span> <span class="n">the</span> <span class="n">variables</span> <span class="n">needed</span> <span class="k">for</span> <span class="n">gradient</span> <span class="n">computation</span> <span class="n">has</span> <span class="n">been</span> <span class="n">modified</span> <span class="n">by</span> <span class="n">an</span> <span class="n">inplace</span> <span class="n">operation</span><span class="p">:</span> <span class="p">[</span>
<span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">LongTensor</span> <span class="p">[</span><span class="mi">8</span><span class="p">,</span> <span class="mi">511</span><span class="p">]]</span> <span class="ow">is</span> <span class="n">at</span> <span class="n">version</span> <span class="mi">1</span><span class="p">;</span> <span class="n">expected</span> <span class="n">version</span> <span class="mi">0</span> 
<span class="n">instead</span><span class="p">.</span> <span class="n">Hint</span><span class="p">:</span> <span class="n">the</span> <span class="n">backtrace</span> <span class="n">further</span> <span class="n">above</span> <span class="n">shows</span> <span class="n">the</span> <span class="n">operation</span> <span class="n">that</span> <span class="n">failed</span> <span class="n">to</span> <span class="n">compute</span> <span class="n">its</span> <span class="n">gradient</span><span class="p">.</span> 
<span class="n">The</span> <span class="n">variable</span> <span class="ow">in</span> <span class="n">question</span> <span class="n">was</span> <span class="n">changed</span> <span class="ow">in</span> <span class="n">there</span> <span class="ow">or</span> <span class="n">anywhere</span> <span class="n">later</span><span class="p">.</span> <span class="n">Good</span> <span class="n">luck</span><span class="err">!</span>
</code></pre></div></div>

<p>에러 로그를 자세히 읽어보면 텐서 버전의 변경으로 인해 그라디언트 계산이 불가하다는 내용이 담겨있다. 구글링해봐도 잘 안나와서 포기하려던 찰라에 우연히 <code class="language-plaintext highlighter-rouge">torch.clone()</code> 메서드의 정확한 사용법이 궁금해 공식 Docs를 읽게 되었고, 거기서 엄청난 사실을 발견했다. <code class="language-plaintext highlighter-rouge">clone()</code> 메서드가 입력된 텐서의 현재 디바이스 위치에 똑같이 복사될 것이란 예상은 했지만, 입력 텐서의 계산그래프까지 복사될 것이란 생각은 전혀 하지 못했기 때문이다. 그래서 위와 같은 에러를 마주하지 않으려면, <code class="language-plaintext highlighter-rouge">clone()</code>을 호출할 때 뒤에 반드시 <code class="language-plaintext highlighter-rouge">detach()</code>를 함께 호출해줘야 한다.</p>

<p><code class="language-plaintext highlighter-rouge">clone()</code> 메서드는 입력된 텐서의 모든 것을 복사한다는 점을 반드시 기억하자.</p>]]></content><author><name>qcqced</name><email>qcqced123@gmail.com</email></author><category term="Framework &amp; Library" /><category term="Pytorch" /><category term="Tensor" /><category term="Linear Algebra" /><summary type="html"><![CDATA[파이토치에서 자주 사용하는 텐서 인덱싱 관련 메서드 모음]]></summary></entry><entry><title type="html">🔢 Product &amp;amp; Quotient Rule: 곱의 미분, 몫의 미분</title><link href="http://localhost:4000/optimization-theory/product_quotient_rule" rel="alternate" type="text/html" title="🔢 Product &amp;amp; Quotient Rule: 곱의 미분, 몫의 미분" /><published>2024-01-08T00:00:00+09:00</published><updated>2024-01-08T23:00:00+09:00</updated><id>http://localhost:4000/optimization-theory/product_quotient_rule</id><content type="html" xml:base="http://localhost:4000/optimization-theory/product_quotient_rule"><![CDATA[<p>곱의 미분, 몫의 미분은 함수가 곱의 꼴 형태 $f(x)g(x)$ 혹은 분수 꼴 형태 $\frac{f(x)}{g(x)}$를 가지고 있을 때 도함수를 구하는 방법이다. 고등학교 미적분 시간(17~18학번 기준)에 배운적이 있지만, 합성함수 미분법과 더불어 단순 암기의 폐해로 까먹기 좋은 미분법들이다. 크로스 엔트로피, 소프트맥스 미분에 쓰이므로 합성함수 미분법과 마찬가지로 딥러닝, 머신러닝에서 매우 중요하다.</p>

<h3 id="️product-rule"><code class="language-plaintext highlighter-rouge">✖️ Product Rule</code></h3>

<p>몫의 미분은 곱의 미분의 원리를 이해하면 자동으로 알 수 있기 때문에 곱의 미분부터 살펴보겠다. 먼저 아래와 같이 곱의 형태를 가지는 함수 $p(x)$가 있다고 가정해보자.</p>

\[p(x) = f(x)g(x)\ \ \ (0)\]

<p>우변의 두 항을 분리하기 전에 도함수의 정의를 이용해 도함수 $p’(x)$를 구하면 다음과 같다.</p>

\[p'(x) = \lim_{h -&gt; 0} \frac{p(x+h) - p(x)}{h}\ \ \ (1)\]

<p>이제 다시 $p(x+h),\  p(x)$에 $f(x)g(x)$를 대입해보자.</p>

\[p'(x) = \lim_{h -&gt; 0} \frac{p(x+h) - p(x)}{h} = \lim_{h-&gt;0} \frac{f(x+h)g(x+h) - f(x)(g(x)}{h}\ \ \ (2)\]

<p>이 지점에서 우리가 뭘하려고 지금 이렇게 수식을 전개하고 있는지 상기할 필요가 있다. 우리는 곱의 형태를 갖는 함수의 <code class="language-plaintext highlighter-rouge">도함수</code>를 구하고 싶은 것이다. (1)번처럼 함수에 대입하는 입력값을 뺀 결과가 $h$가 되도록 말이다. (1)과 같은 꼴을 만들어주기 위해 약간의 트릭을 쓸 필요가 있다. 사용할 트릭은 대수학에서 정말 빈번하게 사용되므로 잘 기억하고 있는게 좋다. 바로 $A-A = 0$이라는 것을 이용하는 것이다. 이게 무슨 말인가는 아래 수식을 보면 알 수 있다.</p>

\[p'(x) = \lim_{h-&gt;0} \frac{f(x+h)g(x+h) - f(x)g(x+h) + f(x)g(x+h) - f(x)g(x)}{h}\ \ \ (3)\]

<p>(3)번 수식은 (2)번 수식의 분자에 $- f(x)g(x+h) + f(x)g(x+h)$만 추가된 형태다. 두항을 더하면 0이 되기 때문에 사실 (2)번과 (3)번은 같은 수식이라고 볼 수 있는 것이다. 그래서 두항을 추가해도 전혀 문제가 없다. 이제 우리가 익숙한 도함수 정의를 만족하는 항들이 직관적으로 보인다.</p>

\[p'(x) = f'(x)g(x) + f(x)g'(x)\ \ \ (4)\]

<p>따라서 수식을 정리하면 결국 곱의 형태를 갖는 함수의 도함수는 (4)번과 같은 공식을 갖게 되는 것이다. 한편, 곱의 미분법은 (0)번 수식을 직사각형의 넓이라고 간주하면 좀 더 직관적으로 이해할 수 있다. 따라서 직사각형 넓이 $p(x)$에 대한 도함수 $p’(x)$는 넓이의 순간 변화율로 해석할 수 있다.</p>

<p align="center">
<img src="/assets/images/optimization/product_rule_rectangle.jpeg" alt="곱의 미분의 직관적 해석" class="align-center image-caption" width="50%&quot;, height=&quot;30%" />
<strong><em><a href="https://blog.naver.com/sodong212/220924875183">곱의 미분의 직관적 해석</a></em></strong>
</p>

<p>우변의 왼쪽항이 직사각형의 아래 부분의 증가하는 영역이 되고, 우측항이 회색이 칠해진 영역이 되는 것이다. 그렇다면 그림의 우측 하단에 위치한 작은 사각형의 넓이는 어떻게 처리해줘야 할까?? 곱의 미분 공식에는 해당 영역을 반영하는 항이 전혀 없다. 그 이유는 영역의 넓이가 너무나 작아서 근사치로 간주하고 무시해도 될 정도라서 그렇다. 해당 사각형의 가로 길이는 $g’(h)$, 세로 길이는 $f’(h)$가 된다. 도함수의 정의를 다시 떠올려보면 $h$는 0의 극한으로 근사하기 때문에 두 항의 곱인 영역의 넓이 역시 0에 매우 근접하게 된다. 따라서 고려할 필요 없이 무시해도 된다.</p>

<h3 id="quotient-rule"><code class="language-plaintext highlighter-rouge">➗ Quotient Rule</code></h3>

<p>몫의 미분은 곱의 미분 공식을 이용하고 나서 남은 지저분한 수식만 잘 정리하면 된다. 다음과 같이 분수 꼴 형태의 함수 $q(x)$가 있다고 가정해보자.</p>

\[q(x) = \frac{f(x)}{g(x)} \ \ \ (0)\]

<p>곱의 미분 공식을 이용하기 위해 분수 꼴의 함수를 다시 곱의 형태로 바꿔보자.</p>

\[f(x) = q(x)g(x) \ \ \ (1)\]

<p>이제 공식을 이용해 좌변에 대한 도함수를 구해보자.</p>

\[f'(x) = q'(x)g(x) + q(x)g'(x)\ \ \  (2)\]

<p>우리가 구하고 싶은 것은 분수 꼴을 갖는 함수 $q(x)$의 도함수 $q’(x)$이다. 따라서 (2)번 수식을 $q’(x)$에 대해서 정리해야 한다.</p>

\[q'(x) = \frac{f'(x) - q(x)g'(x)}{g(x)}\ \ \ (3)\]

<p>(3)번 수식을 예쁘게 정리하기 위해 공통 분모 $\frac{1}{g(x)}$를 앞으로 빼주고, $q(x)$에 (0)번 수식을 대입해 정리하면 몫의 미분법 공식, (5)번이 도출된다.</p>

\[q'(x) = \frac{1}{g(x)}(\frac{f'(x)g(x)-f(x)g'(x)}{g(x)})\ \ \ (4) \\
q'(x) = \frac{f'(x)g(x)-f(x)g'(x)}{g(x)^2}\ \ \ (5)\]

<p>이렇게 곱의 미분법, 몫의 미분법의 공식이 유도되는 과정을 모두 살펴보았다.</p>]]></content><author><name>qcqced</name><email>qcqced123@gmail.com</email></author><category term="Optimization Theory" /><category term="Calculus" /><category term="Product Rule" /><category term="Quotient Rule" /><summary type="html"><![CDATA[곱의 미분, 몫의 미분 공식 유도]]></summary></entry><entry><title type="html">📈 Chain Rule: 합성함수 미분법</title><link href="http://localhost:4000/optimization-theory/chain-rule" rel="alternate" type="text/html" title="📈 Chain Rule: 합성함수 미분법" /><published>2024-01-08T00:00:00+09:00</published><updated>2024-01-08T23:00:00+09:00</updated><id>http://localhost:4000/optimization-theory/chain_rule</id><content type="html" xml:base="http://localhost:4000/optimization-theory/chain-rule"><![CDATA[<p><code class="language-plaintext highlighter-rouge">Chain Rule</code> 이라고 불리기도 하는 합성함수 미분법은 미적분학에서 특히나 중요한 개념 중 하나다. 근래에는 신경망을 활용한 딥러닝이 주목받으면서 그 중요성이 더욱 부각되고 있다. 신경망 모델은 쉽게 생각하면 정말 많은 1차함수와 여러 활성함수를 합성한 것과 같기 때문이다. 따라서 오차 역전을 통해 가중치를 최적화 하는 과정을 정확히 이해하려면 합성함수 미분법에 대한 이해는 필수적이다.</p>

<h3 id="concept"><code class="language-plaintext highlighter-rouge">💡 Concept</code></h3>

<p>합성함수 미분을 이해하기 전에 먼저 도함수 표기법에 대해서 익숙해질 필요가 있다. 도함수 표기법은 크게 뉴턴 표기법과 라이프니츠 표기법으로 나뉜다. 아래 표기된 수식을 보자. (1)번 수식이 뉴턴 표기법, (2)번이 라이프니츠의 표기법이다.</p>

\[y'= 2x\ \ \ \ (1) \\
f'(x) = 2x\ \ \ (2)\]

<p>이제 아래와 같은 합성함수가 있다고 가정해보자. 도함수의 정의에 따라서 해당 함수에 대한 도함수는 다음과 같이 표현할 수 있을 것이다.</p>

\[y = g(f(x))\ \ \ (3) \\
y' = {g(f(x))}' = \frac{dy}{dx}\ \ \ (4)\]

<p>위에 작성한 수식들은 잠시 잊고 이제 $f’(x)$ 를 먼저 생각해보자. $f(x)$에 대한 도함수 $f’(x)$는 다음과 같이 나타낼 수 있다.</p>

\[u = f(x) \\
u' = f'(x) = \frac{du}{dx} \\\]

<p>그렇다면 함수 $g$에 대한 도함수는 어떻게 나타낼 수 있을까?? 답은 바로 치환을 이용하는 것이다. 위에서 우리는 $f(x)$가 $u$와 같다고 정의했다. 이것을 이용해 도함수 $g’$는 다음과 같이 나타낼 수 있겠다.</p>

\[y' = g'(u) = \frac{dy}{du} \\\]

<p>눈치가 빠르다면 벌써 왜 합성함수 미분법을 <code class="language-plaintext highlighter-rouge">Chain Rule</code>이라고 부르는지 깨닫게 되었을 것이다. (4)번 수식의 우변은 사실 분자와 분모에 위치한 $du$가 약분된 꼴이라고 볼 수 있다. 이것을 뉴턴 표기법으로 나타내면 아래와 같다.</p>

\[y' = \frac{dy}{du}•\frac{du}{dx}\]

<p>뉴턴 표기법은 직관적이지 않기 때문에 라이프니츠 표기법으로 다시 정리하면 다음과 같다.</p>

\[y' = g'(f(x))•f'(x)\]

<p>아마 고등학교 때는 합성함수 미분 공식을 겉미분•속미분이라는 명칭으로 처음 접했을 것이다. 안에 감싸져 있는 함수를 미분해서 밖으로 빼낸다 해서 속미분이라 부르고, 다시 속은 냅두고 밖의 둘러져 있는 함수만 미분한다해서 겉미분이라 칭하는데, 이렇게 단순하게 외우기보다는 공식이 도출되는 흐름을 이해하는 것이 훨씬 오래 기억에 남는다. 부끄럽지만 필자가 바로 그러했다. 대학교에 입학하고 수도 없이 합성함수 미분을 해야 했지만 사용할 때마다 까먹어서 인터넷을 찾아보거나 교과서를 뒤적뒤적 했던 기억이 있다. 이글을 읽는 독자들은 나와 같은 실수를 반복하지 않기를 바라며 포스팅을 마친다.</p>]]></content><author><name>qcqced</name><email>qcqced123@gmail.com</email></author><category term="Optimization Theory" /><category term="Calculus" /><summary type="html"><![CDATA[합성함수 미분법 공식 유도]]></summary></entry></feed>