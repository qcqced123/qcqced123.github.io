<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2024-04-01T18:18:26+09:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">AI/Business Study Log</title><subtitle>NLP, Marketing</subtitle><author><name>qcqced</name><email>qcqced123@gmail.com</email></author><entry><title type="html">🔪 [LoRA] Low-Rank Adaptation of Large Language Models</title><link href="http://localhost:4000/nlp/lora" rel="alternate" type="text/html" title="🔪 [LoRA] Low-Rank Adaptation of Large Language Models" /><published>2024-03-28T00:00:00+09:00</published><updated>2024-03-29T02:00:00+09:00</updated><id>http://localhost:4000/nlp/LoRA</id><content type="html" xml:base="http://localhost:4000/nlp/lora"><![CDATA[<h3 id="overview"><code class="language-plaintext highlighter-rouge">🔭 Overview</code></h3>

<p align="center">
<img src="/assets/images/lora/lora.png" alt="LoRA" class="align-center image-caption" width="50%&quot;, height=&quot;50%" />
<strong><em><a href="https://arxiv.org/abs/2106.09685">LoRA</a></em></strong>
</p>

<p>LoRA는 2021년 MS 연구진이 발표한 논문으로 원본(Full 파인튜닝)과 거의 유사한 성능(심지어 일부 벤치마크는 더 높음)으로 LLM 파인튜닝에 필요한 GPU 메모리를 획기적으로 줄이는데 성공해 주목을 받았다. 커뮤니티에서 <code class="language-plaintext highlighter-rouge">LoRA is All You Need</code> 라는 별명까지 얻으며 그 인기를 구가하고 있다.</p>

<p><code class="language-plaintext highlighter-rouge">DistilBERT</code> 리뷰에서도 살펴보았듯, BERT와 GPT의 등장 이후, 모든 NLP 도메인에서 비약적인 성능 향상이 이뤄줬음에도 불구하고, NLP용 딥러닝 모델을 실생활에 활용하기에는 너무 큰 리소스 요구량과 레이턴시가 발목을 잡았다. 하지만 <code class="language-plaintext highlighter-rouge">LoRA</code> 발표 이후, 파인튜닝 시점에 훈련해야 하는 파라미터 수가 현저히 줄어들면서 모델의 체크포인트 용량이 기하급수적으로 감소했다. 덕분에 요구 GPU VRAM이 현저히 낮아져, 리소스 제한 때문에 서빙하지 못하는 경우가 많이 사라졌다. 그래서 오늘날 <code class="language-plaintext highlighter-rouge">Mixed Precision</code>, <code class="language-plaintext highlighter-rouge">Quantization</code>과 함께 모델 경량•최적화 분야에서 가장 중요한 주제로 떠오르고 있다.</p>

<p>내용을 살펴보기전, <code class="language-plaintext highlighter-rouge">LoRA</code> 는 이미 사전학습을 완료한 모델을 파인튜닝할 때 사용해야함을 다시 한 번 명심하자. 이번 포스팅에서는 두가지를 집중적으로 다룰 것이다.</p>

<p><strong>1) 모델 크기 줄인 방법, 2) 크기를 줄이면서도 비슷한 성능을 낼 수 있었던 이유</strong></p>

<h3 id="concept-low-rank-adaptation"><code class="language-plaintext highlighter-rouge">🤔 Concept: Low-Rank Adaptation</code></h3>

\[h = W_0x + \Delta Wx =  W_0x + BAx\]

<p>아이디어는 상당히 간단하다. 사전학습을 마치고 수렴된 상태의 가중치 행렬을 의미하는 $W_0$과 새로운 가중치 행렬 $\Delta W$에 모두 입력을 통과시킨다. 그리고 나온 결과를 더해 다음층의 입력으로 사용한다. 오히려 새로운 가중치 행렬을 추가해 파인튜닝을 하는데 어떻게 훈련해야 하는 파라미터 수를 줄일 수 있었을까??</p>

<p>그 비밀은 <code class="language-plaintext highlighter-rouge">Freeze(Stop Gradient, require_grad=False)</code>와 <code class="language-plaintext highlighter-rouge">Matrix Factorization</code>에 숨어 있다. 먼저 사전 훈련된 가중치 행렬에 <code class="language-plaintext highlighter-rouge">Freeze(Stop Gradient, require_grad=False)</code> 를 적용해 그라디언트가 흐르지 않도록 한다. 이렇게 하면 파인튜닝 과정에서 가중치가 업데이트 되지 않아 사전 학습에서 습득한 지식을 유지할 수 있을 뿐만 아니라, 학습을 위해 그라디언트를 저장할 필요가 없어져 파인튜닝 때 필요한 GPU VRAM을 획기적으로 줄일 수 있다.</p>

<p>처음에 사전학습 가중치를 통과한 값과 새로운 가중치 행렬 $\Delta W$를 통과한 값을 서로 더한다고 언급했다. 그렇다면, 두 결과 행렬의 행렬 크기가 동일해야 한다는 것이다. 어떻게 기존보다 사이즈는 줄이면서 결과 행렬의 크기는 동일하게 만들어줄 수 있을까?? 바로 Low Rank value $r$을 도입해 Matrix Factorization 을 한다.</p>

\[W_{d \times d} = \begin{bmatrix}
w_{1,1} &amp; w_{1,2} &amp; \cdots &amp; w_{1,d} \\
w_{2,1} &amp; w_{2,2} &amp; \cdots &amp; w_{2,d} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
w_{d,1} &amp; w_{d,2} &amp; \cdots &amp; w_{d,d}
\end{bmatrix}\]

<p>행렬 곱셈(matrix multiplication)을 다시 한 번 상기해보자. <code class="language-plaintext highlighter-rouge">MxN</code> 의 크기를 갖는 행렬에 <code class="language-plaintext highlighter-rouge">NxK</code>의 크기를 갖는 행렬을 곱해주면 <code class="language-plaintext highlighter-rouge">MxK</code> 의 크기를 갖는 행렬을 만들어줄 수 있다. 마찬가지다. <code class="language-plaintext highlighter-rouge">dxd</code> 크기인 사전학습의 가중치 행렬 $W_{d \times d}$과 크기를 맞추기 위해, <code class="language-plaintext highlighter-rouge">dxd</code> 짜리 행렬을 각각 <code class="language-plaintext highlighter-rouge">dxr</code>, <code class="language-plaintext highlighter-rouge">rxd</code> 의 크기를 갖는 두 행렬 $B, A$로 분해한다. 이 때, 행렬 $B$의 열차원과 행렬 $A$차원의 행차원 크기를 표현하는 $r$에 바로 Low Rank value $r$을 대입하면 된다.</p>

\[\Delta W_{d \times d} = B_{d \times r}\ A_{r \times d} = \begin{bmatrix}
w_{1,1} &amp; w_{1,2} &amp; w_{1,r} \\
w_{2,1} &amp; w_{2,2} &amp; w_{2,r} \\
\vdots &amp; \vdots &amp; \vdots \\
w_{d,1} &amp; w_{d,2} &amp; w_{d,r}
\end{bmatrix}\begin{bmatrix}
w_{1,1} &amp; w_{2,1} &amp; w_{d,1} \\
w_{1,2} &amp; w_{2,2} &amp; w_{d,2} \\
\vdots &amp; \vdots &amp; \vdots \\
w_{1,r} &amp; w_{2,r} &amp; w_{d,r}
\end{bmatrix}\]

<p>$r=3$이라고 가정하고 <code class="language-plaintext highlighter-rouge">768x768</code> 짜리 기존 가중치 행렬 $W$과 <code class="language-plaintext highlighter-rouge">768x3</code>, <code class="language-plaintext highlighter-rouge">3x768</code>의 크기를 갖는 $\Delta W = BA$의 파라미터 개수를 비교해보자. 계산해보면 전자는 <code class="language-plaintext highlighter-rouge">589,824</code>개, 후자는 <code class="language-plaintext highlighter-rouge">4608</code>개가 된다. 정확하게 <code class="language-plaintext highlighter-rouge">128</code>배 차이가 난다. 트랜스포머 모델 속에는 행렬 $W$과 같은 크기를 갖는 가중치 행렬이 단일 인코더 내부, 하나의 어텐션 레이어만 해도 4개($W_q, W_k, W_v, W_o$)가 있다. <code class="language-plaintext highlighter-rouge">BERT-base</code> 모델을 기준으로 보면, 해당 모델이 <code class="language-plaintext highlighter-rouge">12</code>개의 인코더로 구성되어 있으니까 총 <code class="language-plaintext highlighter-rouge">48</code>개의 가중치 행렬이 있고, 어림잡아도 <code class="language-plaintext highlighter-rouge">48*128</code>배의 학습 파라미터 감소 효과를 낼 수 있다. 모델의 레이어가 많으면 많을수록 더 좋은 효율을 보인다.</p>

<p align="center">
<img src="/assets/images/lora/cuda_memory.png" alt="Resnet50 Memeory Type in GPU" class="align-center image-caption" width="100%&quot;, height=&quot;50%" />
<strong><em><a href="https://pytorch.org/blog/understanding-gpu-memory-1/">Resnet50 Memeory Type in GPU</a></em></strong>
</p>

<p>위 그림은 파이토치 공식 블로그에서 퍼온 자료로, 학습 때 ResNet50의 GPU VRAM 점유율 추이는 물론 모델의 개별 구성요소의 메모리 비율까지 자세히 보여준다. 먼저 <code class="language-plaintext highlighter-rouge">Parameter</code>와 <code class="language-plaintext highlighter-rouge">Optimizer State</code>를 보자. <code class="language-plaintext highlighter-rouge">Parameter</code> 는 모델에서 훈련을 통해 업데이트가 필요한 모든 구성 요소를 말한다. <code class="language-plaintext highlighter-rouge">Freeze</code>, <code class="language-plaintext highlighter-rouge">require_grad=False</code> <code class="language-plaintext highlighter-rouge">@torch.no_grad()</code> , <code class="language-plaintext highlighter-rouge">torch.register_buffer()</code> 의 영향을 받지 않은 모델 내부의 모든 텐서라고 보면 된다.</p>

<p>한편, <code class="language-plaintext highlighter-rouge">Optimizer State</code> 는 옵티마이저의 최적화 수행에 필요한 모든 정보들을 의미하는데, 예를 들어 업데이트 될 텐서의 메타 정보, 여러 하이퍼파라미터 값 같은 것들이 담겨 있다.</p>

<p>이 두 요소가 모델의 GPU VRAM을 차지하는 비율이 상당히 크다. 하지만 두 요소 모두 파라미터 개수에 비례하므로 <code class="language-plaintext highlighter-rouge">LoRA</code> 적용으로 파라미터 개수를 줄이면, GPU VRAM을 획기적으로 줄일 수 있다.</p>

<p>또한 파이토치는 역전파 수행을 위해 그라디언트를 파라미터와 동일한 모양(shape)을 갖는 텐서로 저장된다는 점을 감안하면, 기존의 Full-Rank 텐서 대신 Low-Rank 텐서를 학습에 이용함으로서 그라디언트 텐서의 크기 역시 획기적으로 줄일 수 있겠다.</p>

<p>트랜스포머 계열의 모델들이 ResNet 대비 압도적으로 파라미터 개수가 많기 때문에 <code class="language-plaintext highlighter-rouge">LoRA</code>를 적용한다면 훨씬 큰 효과를 볼 수 있을 것이다.</p>

<figure class="half">
  <a href="https://arxiv.org/abs/2106.09685"><img src="/assets/images/lora/Encoder_LoRA.png" title="Encoder LoRA Result" /></a>
  <a href="https://arxiv.org/abs/2106.09685"><img src="/assets/images/lora/Decoder_LoRA.png" title="Decoder LoRA Result" /></a>
</figure>

<p>왼쪽 그림은 논문에서 제시한, <code class="language-plaintext highlighter-rouge">BERT</code> 계열의 <code class="language-plaintext highlighter-rouge">LM</code>에 <code class="language-plaintext highlighter-rouge">LoRA</code>를 적용한 파인튜닝 결과다. 표의 <code class="language-plaintext highlighter-rouge">FT</code> 가 일반적인 파인튜닝 방법에 의해 나온 결과다. 엎치락뒤치락하면서 거의 비슷한 양상을 보인다. 벤치마크 평균 성능은 <code class="language-plaintext highlighter-rouge">LoRA</code>가 더 높다. 아마, 적당히 성능 차이를 보여주기 위해 취사선택된 벤치마크일 가능성이 높지만, 그래도 상당히 유의미한 결과라고 생각한다. 우측은 <code class="language-plaintext highlighter-rouge">GPT2</code>에 <code class="language-plaintext highlighter-rouge">LoRA</code>를 적용한 결과다. 마찬가지로, 엇비슷한 성능 추이를 보여준다.</p>

<p>지금까지 <code class="language-plaintext highlighter-rouge">LoRA</code> 가 제시하는 방법론이 어떻게 획기적으로 학습 파라미터를 줄이고 나아가 모델이 차지하는 <code class="language-plaintext highlighter-rouge">GPU VRAM</code> 크기를 감소시켰는지 알아 보았다. 이제 <code class="language-plaintext highlighter-rouge">LoRA</code>를 적용해도 일반적인 파인튜닝 방법과 비슷한 성능을 유지할 수 있었는지 그 결과에 대해 해석해보자. 논문의 <code class="language-plaintext highlighter-rouge">Chapter 7. UNDERSTANDIGN THE LOW-RANK UPDATES</code> 내용에 해당된다. 해당 파트는 3가지 인사이트를 제시한다.</p>

<h4 id="inisght-1-apply-to-lora-wq-wv-or-wq-wk-wv-wo"><code class="language-plaintext highlighter-rouge">💡 Inisght 1. Apply to LoRA (Wq, Wv) or (Wq, Wk, Wv, Wo)</code></h4>

<p align="center">
<img src="/assets/images/lora/applying.png" alt="Which Matrix is the BEST" class="align-center image-caption" width="100%&quot;, height=&quot;50%" />
<strong><em><a href="https://arxiv.org/abs/2106.09685">Which Matrix is the BEST</a></em></strong>
</p>

<p>필자는 논문을 읽는 내내, <code class="language-plaintext highlighter-rouge">‘그래서 어떤 가중치 행렬에 적용해야 할까?? 모든 가중치 행렬에 적용해도 되는걸까??’</code>하는 의문을 갖고 있었다. 근데 마침 저자들이 이러한 의문들을 에상한 듯, 위와 같이 적용 가중치 행렬에 따른 벤치마크 성능 결과를 표로 정리해주었다. 모델은 <code class="language-plaintext highlighter-rouge">GPT3</code>을 사용했다고 논문에서 밝히고 있다.</p>

<p>보이는 것과 같이, ($W_q, W_v$) 혹은 ($W_q, W_k, W_v, W_o$)에 <code class="language-plaintext highlighter-rouge">LoRA</code>를 적용하는게 가장 좋은 벤치마크 성능을 보여준다. 주목할 점은 랭크가 가장 낮으면서, 가장 많은 가중치 행렬에 <code class="language-plaintext highlighter-rouge">LoRA</code>를 적용하는게 가장 성능이 좋다는 것이다. 실험결과 제시 이외에 다른 증명이나 인사이트 제시가 없는게 아쉽지만, 이를 통해 다음과 같은 사실들을 떠올려 보았다.</p>

<ul>
  <li><strong>1) <code class="language-plaintext highlighter-rouge">FT</code> 문제 해결에 필요한 문맥 정보들이 <code class="language-plaintext highlighter-rouge">쿼리</code>, <code class="language-plaintext highlighter-rouge">키</code>, <code class="language-plaintext highlighter-rouge">벨류</code> 행렬에 적절히 분산</strong>
    <ul>
      <li><strong>세가지 가중치 행렬이 모두 유의미한 문맥 표현을 학습</strong></li>
    </ul>
  </li>
  <li><strong>2) 낮은 랭크로도 충분히, <code class="language-plaintext highlighter-rouge">FT</code>에 필요한 임베딩 추출 가능</strong>
    <ul>
      <li><strong>그만큼, 사전 학습에서 포착할 수 있는 임베딩이 풍부하며 일반화 능력이 좋다고 판단할 수 있음</strong>
        <ul>
          <li><strong>사전 학습 단계에서 최대한 깊게 많이 학습시킬수록 FT 단계가 간소화 될 수 있지 않을까??</strong></li>
          <li><strong>다만, 사전학습과 파인튜닝 사이의 괴리가 큰 경우라면??</strong></li>
          <li><strong>사전 학습은 영어로, 파인튜닝은 한국어 데이터 세트로 하는 경우라면??</strong></li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p>마침 주석에 <code class="language-plaintext highlighter-rouge">However, we do not expect a small r to work for every task or dataset. Consider the following thought experiment: if the downstream task were in a different language than the one used for pre-training, retraining the entire model</code> 이라는 언급이 있는 것으로 보아, 랭크 값은 되도록 낮은 값을 선정하되, 사전학습과 파인 튜닝의 괴리가 심하다고 판단되는 경우, 높은 랭크값과 실험 결과 비교를 통해 적절한 값을 선정해야겠다.</p>

<h4 id="inisght-2-낮은-랭크로도-충분"><code class="language-plaintext highlighter-rouge">💡 Inisght 2. 낮은 랭크로도 충분</code></h4>

<p align="center">
<img src="/assets/images/lora/insight2.png" alt="Insight 2" class="align-center image-caption" width="70%&quot;, height=&quot;50%" />
<strong><em><a href="https://arxiv.org/abs/2106.09685">Insight 2</a></em></strong>
</p>

<p>낮은 랭크로도 충분히, <code class="language-plaintext highlighter-rouge">FT</code>에 필요한 임베딩 추출 가능하다는 것을 좀 더 구체적인 실험으로 증명하고 있다. 그래프 $y$축과 $x$축은 각각 $A_r = 8$, $A_r = 64$인 (텐서 모양 <code class="language-plaintext highlighter-rouge">[r, dim_model]</code>) 가중치 행렬을 <code class="language-plaintext highlighter-rouge">SVD</code>하여 얻은 <code class="language-plaintext highlighter-rouge">right-singular matrix</code> 에서 <code class="language-plaintext highlighter-rouge">top-i(1 ≤ i ≤ 8)</code>, <code class="language-plaintext highlighter-rouge">top-j (1 ≤ j ≤ 64)</code>개의 특이값을 추출한 뒤, <code class="language-plaintext highlighter-rouge">Grassmann Distance</code>를 거리 매트릭으로 이용해 부분 공간 사이의 유사도를 측정한 결과다.</p>

<p>한편, 왜 <code class="language-plaintext highlighter-rouge">right-singular matrix</code> 일까 다시 한 번 생각해봤다. 전체 벡터 공간에서 <code class="language-plaintext highlighter-rouge">top-i(1 ≤ i ≤ 8)</code>, <code class="language-plaintext highlighter-rouge">top-j (1 ≤ j ≤ 64)</code>개의 특이값을 뽑아내 서로 비교하려면, 두 행렬 $A_{r = 8}$, $A_{r = 64}$이 같은 부분 공간에서 정의 되어야 한다. <code class="language-plaintext highlighter-rouge">SVD</code> 정의상, 왼쪽 특이벡터는 각각 <code class="language-plaintext highlighter-rouge">8x8</code>, <code class="language-plaintext highlighter-rouge">64x64</code>차원이 되어 비교하기 어렵다. 한편, 오른쪽 벡터는 두 행렬 모두 <code class="language-plaintext highlighter-rouge">dxd</code>로 정의된다. 만약 행렬 $A$ 대신 $B$를 사용하고 싶다면 왼쪽 특이 벡터를 사용하면 된다.</p>

<p>오랜지 색에 가까울수록 서로 겹치는 정보가 많다는 의미를 갖는데, 사용한 인코더 위치와 상관없이 $A_{r = 8}$의 <code class="language-plaintext highlighter-rouge">top</code> 열벡터일수록, $A_{r = 64}$의 나머지 열벡터들과 높은 유사도(오랜지색에 가까움)을 기록하고 있다.(헷갈리니까 왼쪽 두 개 그래프만 보는게 낫다). 그리고 $A_{r = 8}$의 <code class="language-plaintext highlighter-rouge">bottom</code> 열벡터일수록, 거무죽죽한 색깔을 가지며 $A_{r = 64}$의 나머지 열벡터들과 낮은 유사도를 보인다.</p>

<p><strong>결국 알고봤더니 사전학습을 충분히 수행한 모델의 경우, 파인튜닝 <code class="language-plaintext highlighter-rouge">Task</code>에 대해 적응시키는데 필요한 공간은 소수, 굳이 전체 공간을 학습 파라미터로 두고 파인튜닝해봐야 대부분의 열벡터는 쓰잘데기 없는 표현을 인코딩하는데 쓰이고 있었다고 볼 수 있겠다.</strong></p>

<p>물론 여기서도 주의할 점은, <code class="language-plaintext highlighter-rouge">GPT3</code>의 사전학습과 궤가 비슷한 <code class="language-plaintext highlighter-rouge">WikiSQL</code>, <code class="language-plaintext highlighter-rouge">MNLU</code>에 대해 파인튜닝한 결과라는 점이다. 다국어로 구성된 데이터 세트를 활용하게 되면, 이 결과가 어떻게 바뀔지 모른다.</p>

<p><code class="language-plaintext highlighter-rouge">Grassmann Distance</code> 는 선형 부분공간(linear subspace) 간의 거리를 측정하는 데 사용되는 개념이라고 하는데, 여기서 이것까지 다루면 포스팅 길이가 너무 길어질 것 같아서, 나중에 다른 포스트에서 다루도록 하겠다.</p>

<h4 id="inisght-3-w--delta-w"><code class="language-plaintext highlighter-rouge">💡 Inisght 3. w ≠ delta w</code></h4>

<p align="center">
<img src="/assets/images/lora/insight3.png" alt="Insight 3" class="align-center image-caption" width="100%&quot;, height=&quot;50%" />
<strong><em><a href="https://arxiv.org/abs/2106.09685">Insight 3</a></em></strong>
</p>

<p>사전 학습한 가중치 행렬 $W$과 <code class="language-plaintext highlighter-rouge">LoRA</code> 의 $\Delta W$가 서로 얼마나 유사한지, 실험적으로 증명하고 있다. 논문에서 제공한 실험 방식을 정리하면 다음과 같다.</p>

<ul>
  <li>1) 사전 학습으로 수렴된 쿼리 행렬, $W_q$를 <code class="language-plaintext highlighter-rouge">task-specific</code>한 공간($U^T, V^T$: $\Delta W$의 <code class="language-plaintext highlighter-rouge">Top-r</code>개의 <code class="language-plaintext highlighter-rouge">Left, Right-Singular Vector</code>)으로 투영</li>
  <li>2) LoRA에 의해 수렴된 델타 쿼리행렬, $\Delta W_q$은 이미 행렬 전반에 <code class="language-plaintext highlighter-rouge">task-specific</code>한 정보를 담고 있음.
    <ul>
      <li>그래서 <code class="language-plaintext highlighter-rouge">top-r</code> 추출하지 않고 전체에 대해서 <code class="language-plaintext highlighter-rouge">프로베니우스 놈</code> 구하기</li>
    </ul>
  </li>
  <li>3) 1번 스탭에서 구한 투영 행렬, $U^TW_qV^T$에 대해 <code class="language-plaintext highlighter-rouge">프로베니우스 놈</code> 계산</li>
  <li>4) 2번/3번 수행: <code class="language-plaintext highlighter-rouge">task-specific</code>한 공간을 <code class="language-plaintext highlighter-rouge">LoRA</code>가 사전 학습 가중치에 비해 얼마나 많이 강조했는지 나타내는 지표
    <ul>
      <li>논문에서는 <code class="language-plaintext highlighter-rouge">Feature Amplication Factor</code> 라고 정의</li>
    </ul>
  </li>
</ul>

<p><code class="language-plaintext highlighter-rouge">프로베니우스 놈</code>은 기하학적으로 행렬의 크기, 즉 <code class="language-plaintext highlighter-rouge">선형변환</code>의 크기를 의미한다. 그래서 곧 <code class="language-plaintext highlighter-rouge">Feature Amplication Factor</code>가 행렬의 크기/행렬의 크기를 나타내는 지표가 되고, 분자와 분모의 행렬은 모두 <code class="language-plaintext highlighter-rouge">task-specific</code>한 공간으로의 변환 크기를 의미하기 때문에, 같은 특징을 분자($\Delta W$)가 분모($W$)에 비해서 얼마나 더 강조하는지를 뜻하게 된다. 띠리서 <code class="language-plaintext highlighter-rouge">factor</code> 값이 클수록 <code class="language-plaintext highlighter-rouge">LoRA</code>가 사전 학습에서 강조하지 않았던 특징을 더욱 강조한다고 해석할 수 있게 된다. 이제 다시 표를 분석해보자.</p>

<p><code class="language-plaintext highlighter-rouge">Low Rank value</code> $r=4$일 때, <code class="language-plaintext highlighter-rouge">Feature Amplication Factor</code>의 분모는 <code class="language-plaintext highlighter-rouge">0.32</code>, 분자는 <code class="language-plaintext highlighter-rouge">6.91</code>이 된다. 따라서 <code class="language-plaintext highlighter-rouge">factor</code> 값은 대략 <code class="language-plaintext highlighter-rouge">21.5</code>가 된다. 다시 말해 <code class="language-plaintext highlighter-rouge">GPT3</code>의 48번째 레이어의 경우, <code class="language-plaintext highlighter-rouge">FT</code> 적응에 필요한 <code class="language-plaintext highlighter-rouge">task-specific</code>한 공간을 <code class="language-plaintext highlighter-rouge">LoRA</code>가 <code class="language-plaintext highlighter-rouge">사전학습 쿼리 행렬</code>보다 <code class="language-plaintext highlighter-rouge">21.5</code>배 강조하고 있다는 것이다.</p>

<p><code class="language-plaintext highlighter-rouge">Low Rank value</code> $r=64$일 때는 <code class="language-plaintext highlighter-rouge">factor</code>가 대략 <code class="language-plaintext highlighter-rouge">1.9</code>가 된다. $r=4$일 때보다 <code class="language-plaintext highlighter-rouge">factor</code> 값이 현저히 낮은 이유는 <code class="language-plaintext highlighter-rouge">Insight 2</code>의 결과(낮은 랭크로도 충분히 <code class="language-plaintext highlighter-rouge">FT</code>의 <code class="language-plaintext highlighter-rouge">task-specific</code> 정보 표현 가능)와 일맥상통한다고 볼 수 있다.</p>

<p>처음 읽었을 때 이 부분에 대한 해석이 너무 난해해, 저자들이 깃허브에 공개한, <code class="language-plaintext highlighter-rouge">RoBERTa</code>를 LoRA와 함께 <code class="language-plaintext highlighter-rouge">MRPC</code> 벤치마크에 파인튜닝한 가중치를 불러와 똑같은 방식으로 실험을 진행해봤다. 먼저 전체 실험 방식을 요약하면 다음과 같다.</p>

<ul>
  <li>1) <code class="language-plaintext highlighter-rouge">Huggingface Hub</code>에서 <code class="language-plaintext highlighter-rouge">RoBERTa-base</code>의 사전학습 가중치 불러오기</li>
  <li>2) <code class="language-plaintext highlighter-rouge">LoRA official github</code>에서 <code class="language-plaintext highlighter-rouge">roberta_base_lora_mrpc.bin</code> 불러오기</li>
  <li>3) <code class="language-plaintext highlighter-rouge">1,2</code>번에서 모두 <code class="language-plaintext highlighter-rouge">6</code>번째 <code class="language-plaintext highlighter-rouge">인코더 레이어</code>의 <code class="language-plaintext highlighter-rouge">쿼리 행렬</code>에 대한 가중치 추출</li>
  <li>4) 이하 나머지 과정은 위에 논문의 실험 방식을 따름</li>
</ul>

<p>전체 과정을 코드로 정리하면 다음과 같다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="s">""" Insight 3 Experiment Code Exanple """</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModel</span><span class="p">,</span> <span class="n">AutoConfig</span>

<span class="s">""" LoRA 결과 해석 재현 """</span>
<span class="n">pt_config</span> <span class="o">=</span> <span class="n">AutoConfig</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s">'FacebookAI/roberta-base'</span><span class="p">)</span>  
<span class="n">pt_model</span> <span class="o">=</span> <span class="n">AutoModel</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">(</span> <span class="c1"># pretrained model
</span>    <span class="s">'roberta-base'</span><span class="p">,</span>
    <span class="n">config</span><span class="o">=</span><span class="n">pt_config</span>
<span class="p">)</span>

<span class="n">lora_checkpoint</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="s">'model/roberta_base_lora_mrpc.bin'</span><span class="p">,</span> <span class="n">map_location</span><span class="o">=</span><span class="s">'cpu'</span><span class="p">)</span>
<span class="n">lora_checkpoint</span>

<span class="s">""" Select Wq in 6-th encoder layer """</span>
<span class="n">pt_wq</span><span class="p">,</span> <span class="n">lora_a</span><span class="p">,</span> <span class="n">lora_b</span> <span class="o">=</span> <span class="n">pt_model</span><span class="p">.</span><span class="n">encoder</span><span class="p">.</span><span class="n">layer</span><span class="p">[</span><span class="mi">6</span><span class="p">].</span><span class="n">attention</span><span class="p">.</span><span class="bp">self</span><span class="p">.</span><span class="n">query</span><span class="p">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">lora_checkpoint</span><span class="p">[</span><span class="s">'roberta.encoder.layer.6.attention.self.query.lora_A'</span><span class="p">],</span> <span class="n">lora_checkpoint</span><span class="p">[</span><span class="s">'roberta.encoder.layer.6.attention.self.query.lora_B'</span><span class="p">]</span>
<span class="n">delta_wq</span> <span class="o">=</span> <span class="n">lora_b</span> <span class="o">@</span> <span class="n">lora_a</span>
<span class="n">pt_wq</span><span class="p">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">lora_a</span><span class="p">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">lora_b</span><span class="p">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">delta_wq</span><span class="p">.</span><span class="n">shape</span>

<span class="o">&gt;&gt;&gt;</span> <span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">768</span><span class="p">,</span> <span class="mi">768</span><span class="p">]),</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">8</span><span class="p">,</span> <span class="mi">768</span><span class="p">]),</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">768</span><span class="p">,</span> <span class="mi">8</span><span class="p">]),</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">768</span><span class="p">,</span> <span class="mi">768</span><span class="p">]))</span>

<span class="s">""" Let's SVD, select top-r singular vector, 분자  """</span>
<span class="n">U</span><span class="p">,</span> <span class="n">S</span><span class="p">,</span> <span class="n">V</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">svd</span><span class="p">(</span><span class="n">delta_wq</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Delta W U: </span><span class="si">{</span><span class="n">U</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Delta W S: </span><span class="si">{</span><span class="n">S</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Delta W V: </span><span class="si">{</span><span class="n">V</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">Delta</span> <span class="n">W</span> <span class="n">U</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">768</span><span class="p">,</span> <span class="mi">768</span><span class="p">])</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">Delta</span> <span class="n">W</span> <span class="n">S</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">768</span><span class="p">])</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">Delta</span> <span class="n">W</span> <span class="n">V</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">768</span><span class="p">,</span> <span class="mi">768</span><span class="p">])</span>

<span class="n">r</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">r_U</span><span class="p">,</span> <span class="n">r_V</span> <span class="o">=</span> <span class="n">U</span><span class="p">[:,</span> <span class="p">:</span><span class="n">r</span><span class="p">],</span> <span class="n">V</span><span class="p">[:</span><span class="n">r</span><span class="p">,</span> <span class="p">:]</span>
<span class="n">result1</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">r_U</span><span class="p">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">pt_wq</span><span class="p">,</span> <span class="n">r_V</span><span class="p">.</span><span class="n">T</span><span class="p">)</span>
<span class="n">fwq_norm</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">norm</span><span class="p">(</span><span class="n">result1</span><span class="p">)</span>  <span class="c1"># 분자값
</span><span class="n">result1</span><span class="p">,</span> <span class="n">fwq_norm</span>

<span class="o">&gt;&gt;&gt;</span> <span class="p">(</span><span class="n">tensor</span><span class="p">([[</span><span class="o">-</span><span class="mf">0.0441</span><span class="p">,</span>  <span class="mf">0.0447</span><span class="p">,</span>  <span class="mf">0.0323</span><span class="p">,</span>  <span class="mf">0.0963</span><span class="p">],</span>
         <span class="p">[</span><span class="o">-</span><span class="mf">0.0038</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0412</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0903</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0949</span><span class="p">],</span>
         <span class="p">[</span><span class="o">-</span><span class="mf">0.0314</span><span class="p">,</span>  <span class="mf">0.1003</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0599</span><span class="p">,</span>  <span class="mf">0.0023</span><span class="p">],</span>
         <span class="p">[</span><span class="o">-</span><span class="mf">0.0222</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1090</span><span class="p">,</span>  <span class="mf">0.0315</span><span class="p">,</span>  <span class="mf">0.0575</span><span class="p">]],</span> <span class="n">grad_fn</span><span class="o">=&lt;</span><span class="n">MmBackward0</span><span class="o">&gt;</span><span class="p">),</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">tensor</span><span class="p">(</span><span class="mf">0.2539</span><span class="p">,</span> <span class="n">grad_fn</span><span class="o">=&lt;</span><span class="n">LinalgVectorNormBackward0</span><span class="o">&gt;</span><span class="p">))</span>

<span class="s">""" 분모 """</span>
<span class="n">fdwq_norm</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">norm</span><span class="p">(</span><span class="n">delta_wq</span><span class="p">)</span>  <span class="c1"># 분모값
</span><span class="n">fdwq_norm</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">tensor</span><span class="p">(</span><span class="mf">5.0820</span><span class="p">)</span>

<span class="s">"""결과: Feature Amplication Factor """</span>
<span class="n">fdwq_norm</span> <span class="o">/</span> <span class="n">fwq_norm</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">tensor</span><span class="p">(</span><span class="mf">20.0170</span><span class="p">,</span> <span class="n">grad_fn</span><span class="o">=&lt;</span><span class="n">DivBackward0</span><span class="o">&gt;</span><span class="p">)</span>

</code></pre></div></div>

<h3 id="implementation-by-pytorch"><code class="language-plaintext highlighter-rouge">👩‍💻 Implementation by Pytorch</code></h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">Tensor</span>

<span class="k">class</span> <span class="nc">LoRA</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="s">""" class module for Low-Rank adaptation of LLM SFT
    This module return result of "BAx*(a/r)" in mathematical expression in official paper

    Args:
        dim: dimension of input tensor
        rank: rank of tensor, which is hyperparameter for LoRA
        alpha: hyperparameter for LoRA, trainable parameter, which is initialized by rank value

    Math:
        h = W0x + ∆Wx = W0x + BAx*(a/r)

    References:
        https://arxiv.org/abs/2106.09685
        https://pytorch.org/blog/understanding-gpu-memory-1/
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">rank</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">a</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">dim</span><span class="p">))</span>  <span class="c1"># init by random Gaussian distribution (normal distribution)
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">b</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">rank</span><span class="p">))</span>  <span class="c1"># init by zero
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">alpha</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">rank</span><span class="p">))</span> <span class="o">/</span> <span class="n">rank</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">b</span> <span class="o">@</span> <span class="bp">self</span><span class="p">.</span><span class="n">a</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">alpha</span>

</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">LoRA</code> 객체 구현 자체는 매우 간단하다. 중요한 점은 사전 학습 모델에 <code class="language-plaintext highlighter-rouge">LoRA</code> 객체를 모델 전체 코드에 적용하는게 어렵다는 것이다. 아래 코드처럼,</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="s">""" before MHA """</span>

<span class="n">q</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">fc_q</span><span class="p">(</span><span class="n">x</span><span class="p">).</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="bp">self</span><span class="p">.</span><span class="n">num_attention_heads</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">dim_head</span><span class="p">).</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">).</span><span class="n">contiguous</span><span class="p">()</span>
<span class="n">k</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">fc_k</span><span class="p">(</span><span class="n">x</span><span class="p">).</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="bp">self</span><span class="p">.</span><span class="n">num_attention_heads</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">dim_head</span><span class="p">).</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">).</span><span class="n">contiguous</span><span class="p">()</span>
<span class="n">v</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">fc_v</span><span class="p">(</span><span class="n">x</span><span class="p">).</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="bp">self</span><span class="p">.</span><span class="n">num_attention_heads</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">dim_head</span><span class="p">).</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">).</span><span class="n">contiguous</span><span class="p">()</span>

<span class="n">attention_output</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">fc_concat</span><span class="p">(</span><span class="n">attention_matrix</span><span class="p">)</span>

<span class="s">""" after MHA """</span>
<span class="bp">self</span><span class="p">.</span><span class="n">lora_q</span> <span class="o">=</span> <span class="n">lora</span><span class="p">()</span>
<span class="bp">self</span><span class="p">.</span><span class="n">lora_k</span> <span class="o">=</span> <span class="n">lora</span><span class="p">()</span>
<span class="bp">self</span><span class="p">.</span><span class="n">lora_v</span> <span class="o">=</span> <span class="n">lora</span><span class="p">()</span>
<span class="bp">self</span><span class="p">.</span><span class="n">lora_o</span> <span class="o">=</span> <span class="n">lora</span><span class="p">()</span>

<span class="n">q</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">fc_q</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">lora_q</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># freeze + trainable
</span><span class="n">q</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="bp">self</span><span class="p">.</span><span class="n">num_attention_heads</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">dim_head</span><span class="p">).</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">).</span><span class="n">contiguous</span><span class="p">()</span>

<span class="n">k</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">fc_k</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">lora_k</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># freeze + trainable
</span><span class="n">k</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="bp">self</span><span class="p">.</span><span class="n">num_attention_heads</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">dim_head</span><span class="p">).</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">).</span><span class="n">contiguous</span><span class="p">()</span>

<span class="n">v</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">fc_v</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">lora_v</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># freeze + trainable
</span><span class="n">v</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="bp">self</span><span class="p">.</span><span class="n">num_attention_heads</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">dim_head</span><span class="p">).</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">).</span><span class="n">contiguous</span><span class="p">()</span>

<span class="n">attention_output</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">fc_concat</span><span class="p">(</span><span class="n">attention_matrix</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">lora_o</span><span class="p">(</span><span class="n">attention_matrix</span><span class="p">)</span> <span class="c1"># freeze + trainable
</span>
</code></pre></div></div>

<p>선형 투영된 쿼리, 키, 벨류 행렬과 각각의 <code class="language-plaintext highlighter-rouge">LoRA</code> 객체를 더해줄 수만 있다면 매우 간단하게 해결될 문제지만, 사전학습 모델의 <code class="language-plaintext highlighter-rouge">Multi-Head Attention</code>객체를 처음부터 저런식으로 정의해야만 가능한 일이다. 필자가 작성한 모델 코드를 비롯해 대부분의 오픈소스로 풀려있는 트랜스포머 모델들은 저런식으로 작성되어 있지 않다. 따라서 다른 방법을 떠올려하는데, 당장은 너무 복잡한 작업이 될 것 같아(실험 어플리케이션 구조를 뒤엎어야 가능할 것으로 예측) 일단은 여기서 마무리하려고 한다. 만약 <code class="language-plaintext highlighter-rouge">LoRA</code>를 사전 학습 모델에 적용해 파인튜닝을 해보고 싶다면, <code class="language-plaintext highlighter-rouge">Huggingface</code>의 <code class="language-plaintext highlighter-rouge">PEFT</code> 라이브러리를 이용해보자. <code class="language-plaintext highlighter-rouge">Hugginface</code>의 <code class="language-plaintext highlighter-rouge">Automodel</code>, <code class="language-plaintext highlighter-rouge">Trainer</code> 객체와 유연하게 연동이 가능하다. 아래에 <code class="language-plaintext highlighter-rouge">PEFT</code> 공식 문서에서 참고한 <code class="language-plaintext highlighter-rouge">Usage Example</code> 코드를 첨부했으니 참고 부탁바란다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="s">""" PEFT LoRA Usage Example

Reference:
		https://github.com/huggingface/peft/blob/main/src/peft/tuners/lora/model.py
"""</span>
<span class="o">&gt;&gt;&gt;</span> <span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForSeq2SeqLM</span>
<span class="o">&gt;&gt;&gt;</span> <span class="kn">from</span> <span class="nn">peft</span> <span class="kn">import</span> <span class="n">LoraModel</span><span class="p">,</span> <span class="n">LoraConfig</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">config</span> <span class="o">=</span> <span class="n">LoraConfig</span><span class="p">(</span>
<span class="p">...</span>     <span class="n">task_type</span><span class="o">=</span><span class="s">"SEQ_2_SEQ_LM"</span><span class="p">,</span>
<span class="p">...</span>     <span class="n">r</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>  <span class="c1"># rank value in official paper
</span><span class="p">...</span>     <span class="n">lora_alpha</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>  <span class="c1"># alpha value in official paper
</span><span class="p">...</span>     <span class="n">target_modules</span><span class="o">=</span><span class="p">[</span><span class="s">"q"</span><span class="p">,</span> <span class="s">"v"</span><span class="p">],</span>
<span class="p">...</span>     <span class="n">lora_dropout</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span>
<span class="p">...</span> <span class="p">)</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForSeq2SeqLM</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s">"t5-base"</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">lora_model</span> <span class="o">=</span> <span class="n">LoraModel</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">config</span><span class="p">,</span> <span class="s">"default"</span><span class="p">)</span>

<span class="o">&gt;&gt;&gt;</span> <span class="kn">import</span> <span class="nn">torch</span>
<span class="o">&gt;&gt;&gt;</span> <span class="kn">import</span> <span class="nn">transformers</span>
<span class="o">&gt;&gt;&gt;</span> <span class="kn">from</span> <span class="nn">peft</span> <span class="kn">import</span> <span class="n">LoraConfig</span><span class="p">,</span> <span class="n">PeftModel</span><span class="p">,</span> <span class="n">get_peft_model</span><span class="p">,</span> <span class="n">prepare_model_for_kbit_training</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">rank</span> <span class="o">=</span> <span class="p">...</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">target_modules</span> <span class="o">=</span> <span class="p">[</span><span class="s">"q_proj"</span><span class="p">,</span> <span class="s">"k_proj"</span><span class="p">,</span> <span class="s">"v_proj"</span><span class="p">,</span> <span class="s">"out_proj"</span><span class="p">,</span> <span class="s">"fc_in"</span><span class="p">,</span> <span class="s">"fc_out"</span><span class="p">,</span> <span class="s">"wte"</span><span class="p">]</span>  <span class="c1"># target for projection matrix, MLP
</span><span class="o">&gt;&gt;&gt;</span> <span class="n">config</span> <span class="o">=</span> <span class="n">LoraConfig</span><span class="p">(</span>
<span class="p">...</span>     <span class="n">r</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">lora_alpha</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">target_modules</span><span class="o">=</span><span class="n">target_modules</span><span class="p">,</span> <span class="n">lora_dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="s">"none"</span><span class="p">,</span> <span class="n">task_type</span><span class="o">=</span><span class="s">"CAUSAL_LM"</span>
<span class="p">...</span> <span class="p">)</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">quantization_config</span> <span class="o">=</span> <span class="n">transformers</span><span class="p">.</span><span class="n">BitsAndBytesConfig</span><span class="p">(</span><span class="n">load_in_8bit</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">transformers</span><span class="p">.</span><span class="n">AutoTokenizer</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">(</span>
<span class="p">...</span>     <span class="s">"kakaobrain/kogpt"</span><span class="p">,</span>
<span class="p">...</span>     <span class="n">revision</span><span class="o">=</span><span class="s">"KoGPT6B-ryan1.5b-float16"</span><span class="p">,</span>  <span class="c1"># or float32 version: revision=KoGPT6B-ryan1.5b
</span><span class="p">...</span>     <span class="n">bos_token</span><span class="o">=</span><span class="s">"[BOS]"</span><span class="p">,</span>
<span class="p">...</span>     <span class="n">eos_token</span><span class="o">=</span><span class="s">"[EOS]"</span><span class="p">,</span>
<span class="p">...</span>     <span class="n">unk_token</span><span class="o">=</span><span class="s">"[UNK]"</span><span class="p">,</span>
<span class="p">...</span>     <span class="n">pad_token</span><span class="o">=</span><span class="s">"[PAD]"</span><span class="p">,</span>
<span class="p">...</span>     <span class="n">mask_token</span><span class="o">=</span><span class="s">"[MASK]"</span><span class="p">,</span>
<span class="p">...</span> <span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">model</span> <span class="o">=</span> <span class="n">transformers</span><span class="p">.</span><span class="n">GPTJForCausalLM</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">(</span>
<span class="p">...</span>     <span class="s">"kakaobrain/kogpt"</span><span class="p">,</span>
<span class="p">...</span>     <span class="n">revision</span><span class="o">=</span><span class="s">"KoGPT6B-ryan1.5b-float16"</span><span class="p">,</span>  <span class="c1"># or float32 version: revision=KoGPT6B-ryan1.5b
</span><span class="p">...</span>     <span class="n">pad_token_id</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">.</span><span class="n">eos_token_id</span><span class="p">,</span>
<span class="p">...</span>     <span class="n">use_cache</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
<span class="p">...</span>     <span class="n">device_map</span><span class="o">=</span><span class="p">{</span><span class="s">""</span><span class="p">:</span> <span class="n">rank</span><span class="p">},</span>
<span class="p">...</span>     <span class="n">torch_dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float16</span><span class="p">,</span>
<span class="p">...</span>     <span class="n">quantization_config</span><span class="o">=</span><span class="n">quantization_config</span><span class="p">,</span>
<span class="p">...</span> <span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">model</span> <span class="o">=</span> <span class="n">prepare_model_for_kbit_training</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">lora_model</span> <span class="o">=</span> <span class="n">get_peft_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">config</span><span class="p">)</span> 
</code></pre></div></div>]]></content><author><name>qcqced</name><email>qcqced123@gmail.com</email></author><category term="NLP" /><category term="Natural Language Process" /><category term="LoRA" /><category term="Low-Rank Adaptation" /><category term="Fine-Tune" /><category term="Optimization" /><category term="Pytorch" /><category term="Huggingface" /><category term="PEFT" /><summary type="html"><![CDATA[LoRA Official Paper Review with Pytorch Implementation]]></summary></entry><entry><title type="html">👨⏰🐍 [Python] 시간복잡도 1</title><link href="http://localhost:4000/python/time_complexity1" rel="alternate" type="text/html" title="👨⏰🐍 [Python] 시간복잡도 1" /><published>2024-03-26T00:00:00+09:00</published><updated>2023-10-15T02:00:00+09:00</updated><id>http://localhost:4000/python/python-time_complexity</id><content type="html" xml:base="http://localhost:4000/python/time_complexity1"><![CDATA[<h3 id="memeory"><code class="language-plaintext highlighter-rouge">Memeory</code></h3>

<ul>
  <li><strong>1) 2</strong>32 ⇒ 4GB**</li>
  <li><strong>2) 2</strong>16 ⇒ 64MB**</li>
</ul>

<h3 id="time"><code class="language-plaintext highlighter-rouge">Time</code></h3>

<p>구체적인 성능은 플랫폼의 하드웨어에 따라서 달라지겠지만, 일반적으로 1초에 1억번 정도 계산할 수 있다고 가정하고 알고리즘의 시간 복잡도를 계산하면 된다. 즉, 어떤 문제의 시간 제한이 2초라면, 2억번 이하의 계산을 하는 알고리즘의 경우는 통과로 처리된다는 것이다.</p>

<p>시간 복잡도는 원래 데이터 개수에 따라서 달라지는 연산 횟수의 추이를 말한다. 하지만 많은 사람들은 편의상 데이터 개수와 연산 횟수를 동치로 간주하고 시간 복잡도를 계산하고 있다. 이러한 접근이 틀렸다고 보기 힘든 이유는 실전에서 정확하게 시간 복잡도를 계산하는 것이 불가능하기 때문이다. 애초에 시간 복잡도의 목적은 내가 세운 알고리즘의 대략적인 성능을 알아보기 위함이자, 문제 조건에 맞지 않는 경우의 수를 미리 처내기 위함이다. 이러한 관점에 따라서 어떤 문제의 시간 제한이 2초, 내가 세운 알고리즘이 O(N)의 시간복잡도를 갖는 상황을 가정해보자. 만약 문제에서 주어지는 데이터의 양이 2억개 이하라면 시간 초과에 걸리지 않고 통과할 것이다. 한편, 문제에서 주어지는 데이터의 양이 10억개라면, 선형 시간 복잡도 알고리즘으로는 다시 태어나도 절대 통과할 수 없다.</p>

<table>
  <thead>
    <tr>
      <th>시간 복잡도</th>
      <th>최대 데이터 길이 (1초)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>O(log N)</td>
      <td>1억**2</td>
    </tr>
    <tr>
      <td>O(N)</td>
      <td>1억</td>
    </tr>
    <tr>
      <td>O(N**2)</td>
      <td>1만</td>
    </tr>
    <tr>
      <td>O(N log N)</td>
      <td>30만</td>
    </tr>
    <tr>
      <td>O(N**3)</td>
      <td>500</td>
    </tr>
  </tbody>
</table>

<p>이제 파이썬 내장 라이브러리의 시간 복잡도에 대해서 정리해보자. 파이썬의 가장 큰 장점은 타 언어 대비 내장 라이브러리가 많다는 점이다. 그래서 다른 언어로 구현할 때보다 내장 메서드를 사용하는 경우가 빈번하기 때문에 미리 해당 함수들의 시간 복잡도에 대해서 숙지하고 있어야 한다.</p>

<table>
  <thead>
    <tr>
      <th>시간 복잡도</th>
      <th>최대 데이터 길이 (1초)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>O(1)</td>
      <td>모든 인덱싱 활용 연산(자료구조 길이 등)</td>
    </tr>
    <tr>
      <td>O(N)</td>
      <td>전체 자료 구조 할당, 비교, 복사, 탐색</td>
    </tr>
    <tr>
      <td>O(N log N)</td>
      <td>정렬(sort(), sorted())</td>
    </tr>
  </tbody>
</table>

<p>자료구조에 따른 내장 메서드가 너무 많아서, 최대한 일반화된 표현으로 작성했다. 여기서 한 가지 주목할 점은 자료구조 길이를 구하는 <code class="language-plaintext highlighter-rouge">len()</code>의 시간 복잡도가 상수 시간이라는 점이다. 파이썬 <code class="language-plaintext highlighter-rouge">mutable</code> 객체들은 모두 내부값이 변경될 것을 감안해 <code class="language-plaintext highlighter-rouge">resize()</code> 연산을 객체 내부 메서드에 가지고 있고, 자료구조 내부에 길이 정보값을 항상 내장하고 있다. 그래서 <code class="language-plaintext highlighter-rouge">len()</code>, <code class="language-plaintext highlighter-rouge">__len__()</code> 을 호출하면 자료구조를 순회하면서 길이를 세는게 아니라, 길이 정보값을 저장하고 있는 위치의 배열값을 반환한다. 따라서 상수 시간에 해결이 가능하다. 따라서 파이썬 <code class="language-plaintext highlighter-rouge">mutable</code> 객체의 크기는 우리 생각보다 좀 더 크다는 것을 명심하자.</p>]]></content><author><name>qcqced</name><email>qcqced123@gmail.com</email></author><category term="Python" /><category term="Python" /><category term="Function" /><category term="Argument" /><category term="mutable" /><category term="CS" /><summary type="html"><![CDATA[시간 복잡도에 대한 이해]]></summary></entry><entry><title type="html">👨⏰🐍 [Python] 시간복잡도 2</title><link href="http://localhost:4000/python/time_complexity2" rel="alternate" type="text/html" title="👨⏰🐍 [Python] 시간복잡도 2" /><published>2024-03-26T00:00:00+09:00</published><updated>2024-03-27T02:00:00+09:00</updated><id>http://localhost:4000/python/python-time_complexity2</id><content type="html" xml:base="http://localhost:4000/python/time_complexity2"><![CDATA[<h3 id="theme-1-입력-모듈"><code class="language-plaintext highlighter-rouge">Theme 1. 입력 모듈</code></h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="s">""" Compare to Input module """</span>
<span class="kn">import</span> <span class="nn">sys</span>

<span class="n">N</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">inputs</span><span class="p">())</span>
<span class="n">K</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">sys</span><span class="p">.</span><span class="n">stdin</span><span class="p">.</span><span class="n">readline</span><span class="p">())</span>
</code></pre></div></div>

<p>파이썬에서 사용자로부터 입력을 받는 모듈은 보통 <code class="language-plaintext highlighter-rouge">inputs()</code>, <code class="language-plaintext highlighter-rouge">sys.stdin.readline()</code> 을 사용한다. <code class="language-plaintext highlighter-rouge">inputs()</code> 는 입력 받는 데이터의 길이가 길고, 많아질수록 입력 효율이 떨어지는 단점이 있다. 그래서 이를 보완하기 위해 대부분의 코딩테스트 환경에서 입력을 받을 때는 <code class="language-plaintext highlighter-rouge">sys.stdin.readline()</code> 를 사용하는 것이 좋다. 어차피 테스트 케이스가 매우 길고 많기 때문에 일반적으로는 후자가 훨씬 효율적이다.</p>

<p>하지만 만약, 문제 조건에서 주어지는 입력 데이터의 개수와 그 형태가 적고 단순한 편이라면 전자의 사용도 고려해야 한다. 후자는 기본적으로 <code class="language-plaintext highlighter-rouge">sys</code> 모듈 내부에 내장된 메서드라서 <code class="language-plaintext highlighter-rouge">sys</code> 모듈을 불러와야 한다. 모듈을 <code class="language-plaintext highlighter-rouge">import</code> 하는 시간도 꽤나 걸리기 때문에, 데이터가 단순하다면 전자 사용도 고려해보자.</p>

<h3 id="theme-2-선언할당재구성-횟수-줄이기"><code class="language-plaintext highlighter-rouge">Theme 2. 선언/할당/재구성 횟수 줄이기</code></h3>

<p>파이썬은 자료구조 사용을 위해 명시적으로 메모리를 할당하고 해제하는 과정이 생략되어 있어서, 이 부분에 대한 인지를 하지 못하는 경우가 많다. 하지만 C/C++/JAVA 혹은 운영체제 공부를 해봤다면 알 것이다. 메모리 할당을 요구하는 일이 얼마나 시간을 많이 잡아 먹는 일인지 말이다. 프로그래머가 작성한 코드에서 메모리 할당을 요구한다면, 컴파일러/인터프리터는 운영체제 커널에 메모리 할당을 요구해야 한다. 그 다음 커널은 다시 가장 깊숙한 곳까지 내려가 메모리 할당을 하드웨어에 요청한 뒤, 작업 결과를 컴파일러에 전달해야 한다. 따라서 최대한 <code class="language-plaintext highlighter-rouge">할당</code>과 <code class="language-plaintext highlighter-rouge">재구성</code>하는 시간을 줄여야 효율적으로 코드를 작성할 수 있다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="s">""" 1. 리스트 곱셈 """</span>

<span class="n">arr1</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">)]</span>
<span class="n">arr2</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="mi">100</span>
</code></pre></div></div>

<p>arr1, arr2는 모두 길이가 100이고 모든 원소값이 0인 같은 배열(리스트)을 가지게 된다. 하지만, 실행 시간은 어느쪽이 더 빠를까?? 후자가 당연히 더 빠르다. 후자는 <code class="language-plaintext highlighter-rouge">재구성(resize)</code>연산 없이 한 번에 할당이 마무리 되기 때문이다. 반면 전자는 <code class="language-plaintext highlighter-rouge">선언</code>과 <code class="language-plaintext highlighter-rouge">할당</code> 및 <code class="language-plaintext highlighter-rouge">재구성</code> 시점이 서로 다르다. 그래서 일정 횟수마다 <code class="language-plaintext highlighter-rouge">재구성</code> 연산이 필요해진다. 앞서도 언급했듯, 커널에 메모리를 요구하는 횟수가 작아지는게 실행 시간에 유리하다. 따라서 명백히 후자의 실행 시간이 더 빠르다.</p>

<p>히지만, 리스트 곱셈을 사용할 때 반드시 주의해야할 점이 있다. 방금 살펴본 예시는 1차원 배열에 리스트 곱셈을 하기 때문에, 이후 해당 배열을 조작해도 문제가 발생하지 않는다. 하지만, 2차원 배열을 리스트 곱셈하여 선언하면 어떻게 될까??</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">arr1</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]]</span><span class="o">*</span><span class="mi">5</span>
<span class="n">arr2</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">)]</span>

<span class="c1"># Question 1.
</span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">):</span>
	<span class="n">arr1</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>

<span class="c1"># Question 2.
</span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">):</span>
	<span class="n">arr2</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
</code></pre></div></div>

<p>1번과 2번의 케이스에 대한 출력을 예측해보자. 어떻게 나올까??</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Question 1.
</span><span class="o">&gt;&gt;&gt;</span> <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">]]</span>

<span class="c1"># Question 2.
</span><span class="o">&gt;&gt;&gt;</span> <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]]</span>
</code></pre></div></div>

<p>얼핏 보기에는 똑같은 결과가 나와야 한다고 생각하기 쉽다. 하지만 1차원 배열의 경우와 명백히 다른점이 있다. 바로, 리스트의 원소가 리스트라는 것이다. 리스트는 <code class="language-plaintext highlighter-rouge">mutable</code> 객체로서, <code class="language-plaintext highlighter-rouge">resize</code> 연산을 가지고 있어서 별도의 조작을 해도 객체의 주소값이 변경되지 않는다. 다시 말해 메모리 복사가 발생하지 않는다.</p>

<p>하지만 1차원 배열의 경우는 원소가 <code class="language-plaintext highlighter-rouge">immutable</code> 객체인 정수다. 따라서 초기에 모든 원소가 같은 주소를 공유하고 있어도, 순차적으로 조작이 발생하면서 메모리에 새로운 값이 복사되고 서로 다른 주소를 갖게 되기 때문에 문제가 없는 것이다. 이러한 차이 때문에 이차원 배열의 경우 리스트 복사를 사용하게 되면, 모든 원소가 같은 주소를 계속해서 공유하기 때문에 모두 같은 값을 갖게 된다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Question 1. memory id
</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="mi">140226858532160</span>
<span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span>
<span class="mi">140226858532160</span>
<span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span>
<span class="mi">140226858532160</span>
<span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">4</span><span class="p">]</span>
<span class="mi">140226858532160</span>
<span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">]</span>
<span class="mi">140226858532160</span>

<span class="c1"># Question 2. memory id
</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="mi">140226858828416</span>
<span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="mi">140226858785472</span>
<span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="mi">140226858828160</span>
<span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="mi">140226858828480</span>
<span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="mi">140226858830016</span>
</code></pre></div></div>

<p>개별 원소에 순차적으로 접근할 때, 원소의 주소를 출력한 결과다. <code class="language-plaintext highlighter-rouge">Question 1</code> 은 서로 다른 인덱스의 원소에 접근해도 계속해서 동일한 주소 위치를 참조하고 있음을 알 수 있다.</p>

<p>한편, <code class="language-plaintext highlighter-rouge">Question 2</code> 의 경우, 처음부터 서로 다른 주소값을 갖는 리스트를 원소로 할당했기 때문에 서로 다른 주소값을 갖게 된다.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="s">""" 2. 튜플 사용 """</span>

<span class="n">tdy</span><span class="p">,</span> <span class="n">tdx</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># tuple
</span><span class="n">ldy</span><span class="p">,</span> <span class="n">ldx</span> <span class="o">=</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>  <span class="c1"># list
</span></code></pre></div></div>

<p>지금까지 설명한 내용과 같은 맥락에서, 배열 내부의 구성요소에 변경이 없는게 확실시 되는 경우, 배열을 튜플로 선언하는 것이 좋다. 위에 선언된 배열은 그래프 탐색 문제에서 상하좌우 방향 탐색을 구현할 때 자주 사용하는 방법이다. 상하좌우 방향을 표현하는게 목적이라서, 해당 배열은 미래에 변경될 일이 없다고 보장할 수 있다. 이런 경우에는 습관적으로 리스트를 사용하지말고, 튜플을 쓰자.</p>

<p>튜플이 리스트보다 나은 이유는 <code class="language-plaintext highlighter-rouge">immutable</code> 객체라는 점이다. <code class="language-plaintext highlighter-rouge">immutable</code> 객체는 변경되지 않는다는 것을 전제로 하기 때문에 <code class="language-plaintext highlighter-rouge">resize</code> 메서드가 객체에 없다. 이에 따라서 관련된 메타 정보도 저장하고 있을 필요가 사라진다. 그래서 리스트 보다 가볍고 빠른 계산이 가능하다. 또한 튜플은 20이하의 크기를 갖는 객체는 <code class="language-plaintext highlighter-rouge">래퍼런스 카운트</code>가 0이라도 <code class="language-plaintext highlighter-rouge">Python GC</code>가 메모리를 회수하지 않는다. 대신 메모리 상에 저장하고 있다가, 같은 크기의 튜플이 다시 한 번 선언될 때, 이것을 재활용한다.</p>

<p>리스트와 튜플의 전체 생애주기를 비교해보면, 튜플의 커널&amp;하드웨어 호출횟수가 리스트보다 훨씬 적어진다. 그래서 재구성이 필요없는 경우는 반드시 튜플로 선언하는게 이득이라고 볼 수 있다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="s">""" 문자열 합치기 """</span>

<span class="n">arr1</span><span class="p">,</span> <span class="n">arr2</span> <span class="o">=</span> <span class="s">'I am a boy, '</span><span class="p">,</span> <span class="s">'you are a girl'</span>

<span class="n">src</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="n">time</span><span class="p">()</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">arr1</span> <span class="o">+</span> <span class="n">arr2</span>
<span class="n">end</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="n">time</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Execution Time: </span><span class="si">{</span><span class="n">src</span><span class="o">-</span><span class="n">end</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Result: </span><span class="si">{</span><span class="n">result</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>

<span class="n">src</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="n">time</span><span class="p">()</span>
<span class="n">result2</span> <span class="o">=</span> <span class="s">''</span><span class="p">.</span><span class="n">join</span><span class="p">([</span><span class="n">arr1</span><span class="p">,</span> <span class="n">arr2</span><span class="p">])</span>
<span class="n">end</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="n">time</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Execution Time: </span><span class="si">{</span><span class="n">src</span><span class="o">-</span><span class="n">end</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Result: </span><span class="si">{</span><span class="n">result2</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>

<span class="n">Execution</span> <span class="n">Time</span><span class="p">:</span> <span class="o">-</span><span class="mf">3.600120544433594e-05</span>
<span class="n">Result</span><span class="p">:</span> <span class="n">I</span> <span class="n">am</span> <span class="n">a</span> <span class="n">boy</span><span class="p">,</span> <span class="n">you</span> <span class="n">are</span> <span class="n">a</span> <span class="n">girl</span>
<span class="n">Execution</span> <span class="n">Time</span><span class="p">:</span> <span class="o">-</span><span class="mf">3.504753112792969e-05</span>
<span class="n">Result</span><span class="p">:</span> <span class="n">I</span> <span class="n">am</span> <span class="n">a</span> <span class="n">boy</span><span class="p">,</span> <span class="n">you</span> <span class="n">are</span> <span class="n">a</span> <span class="n">girl</span>
</code></pre></div></div>

<p>문자열은 파이썬에서 <code class="language-plaintext highlighter-rouge">immutable</code> 객체로 간주한다. 따라서 일반 연산자(+, *)를 사용해 문자열을 수정할 경우, 여타 <code class="language-plaintext highlighter-rouge">immutable</code> 객체처럼 새로운 메모리 할당이 발생한다. 이를 방지하기 위해 파이썬 내부적으로 최적화 되어 있는 <code class="language-plaintext highlighter-rouge">str.join()</code> 을 사용해 문자열을 조작하도록 하자.</p>

<h3 id="theme-3-컴프리헨션제너레이터"><code class="language-plaintext highlighter-rouge">Theme 3. 컴프리헨션/제너레이터</code></h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">src</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="n">time</span><span class="p">()</span>
<span class="n">arr1</span> <span class="o">=</span> <span class="p">[</span><span class="n">i</span><span class="o">**</span><span class="mi">2</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100000</span><span class="p">)]</span>  <span class="c1"># list comprehension
</span><span class="n">end</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="n">time</span><span class="p">()</span>

<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Execution Time: </span><span class="si">{</span><span class="n">src</span><span class="o">-</span><span class="n">end</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Memory Size: </span><span class="si">{</span><span class="n">sys</span><span class="p">.</span><span class="n">getsizeof</span><span class="p">(</span><span class="n">arr1</span><span class="p">)</span><span class="si">}</span><span class="s"> byte"</span><span class="p">)</span>
<span class="c1"># print(f"Result: {arr1}")
</span>
<span class="n">src</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="n">time</span><span class="p">()</span>
<span class="n">arr2</span> <span class="o">=</span> <span class="p">(</span><span class="n">i</span><span class="o">**</span><span class="mi">2</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100000</span><span class="p">))</span>  <span class="c1"># init of generator
</span><span class="n">end</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="n">time</span><span class="p">()</span>

<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Execution Time: </span><span class="si">{</span><span class="n">src</span><span class="o">-</span><span class="n">end</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Memory Size: </span><span class="si">{</span><span class="n">sys</span><span class="p">.</span><span class="n">getsizeof</span><span class="p">(</span><span class="n">arr2</span><span class="p">)</span><span class="si">}</span><span class="s"> byte"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Result: </span><span class="si">{</span><span class="n">arr2</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>

<span class="n">Execution</span> <span class="n">Time</span><span class="p">:</span> <span class="o">-</span><span class="mf">0.023138046264648438</span>
<span class="n">Memory</span> <span class="n">Size</span><span class="p">:</span> <span class="mi">800984</span> <span class="n">byte</span>
<span class="n">Execution</span> <span class="n">Time</span><span class="p">:</span> <span class="o">-</span><span class="mf">5.2928924560546875e-05</span>
<span class="n">Memory</span> <span class="n">Size</span><span class="p">:</span> <span class="mi">112</span> <span class="n">byte</span>
<span class="n">Result</span><span class="p">:</span> <span class="o">&lt;</span><span class="n">generator</span> <span class="nb">object</span> <span class="o">&lt;</span><span class="n">genexpr</span><span class="o">&gt;</span> <span class="n">at</span> <span class="mh">0x1058c3f20</span><span class="o">&gt;</span>
</code></pre></div></div>

<p>속도, 필요한 메모리 크기 모두 제너레이터가 압도적으로 효율적인 모습이다. 제너레이터의 경우, 실제로는 모든 연산값을 갖고 있는게 아니기 때문이다. 그래서 인덱싱, 슬라이싱등 파이썬 배열의 여러 연산을 활용할 수 없다. 따라서 상황에 따라서 컴프리헨션과 제너레이터를 사용하면 된다. 만약 인덱싱, 슬라이싱 연산처럼 일부에 접근하는 연산을 이용해야 한다면 컴프리헨션을, sum()처럼 전체 배열 단위 연산 혹은 단순히 순차적으로 원소 하나 하나에 접근해 무엇인가 해야 하는 상황(range() 등)이라면 제너레이터를 활용하자.</p>]]></content><author><name>qcqced</name><email>qcqced123@gmail.com</email></author><category term="Python" /><category term="Python" /><category term="Time Complexity" /><category term="CS" /><summary type="html"><![CDATA[시간 복잡도 줄이기]]></summary></entry><entry><title type="html">🌆 [Linear Attention] Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention</title><link href="http://localhost:4000/nlp/linear_attention" rel="alternate" type="text/html" title="🌆 [Linear Attention] Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention" /><published>2024-03-14T00:00:00+09:00</published><updated>2024-03-15T02:00:00+09:00</updated><id>http://localhost:4000/nlp/linear_attention</id><content type="html" xml:base="http://localhost:4000/nlp/linear_attention"><![CDATA[<h3 id="overview"><code class="language-plaintext highlighter-rouge">🔭 Overview</code></h3>

<p><code class="language-plaintext highlighter-rouge">DistilBERT</code> 는 허깅 페이스 연구진이 2019년 발표한 BERT의 변형으로서, On-Device Ai 개발을 목표로 경량화에 초점을 맞춘 모델이다. GPT, BERT의 등장 이후, NLP 분야에서 비약적인 성능 향상이 이뤄졌음에도 불구하고, 터무니 없는 모델 사이즈와 컴퓨팅 리소스 요구로 인해 실생활 적용 같은 활용성은 여전히 해결해야할 문제로 남아 있었다. Google에서 발표한 초기 <code class="language-plaintext highlighter-rouge">BERT-base-uncased</code> 만 해도 파라미터가 1억 1천만개 수준에 달한다.</p>

<p>이를 다양한 비즈니스 요구 상황에 적용할 수 있으려면 최소한 8GB 이상의 가속기 전용 RAM 공간을 요구로 한다. 오늘날 개인용 PC 혹은 서버 컴퓨터의 경우, 8GB 이상의 VRAM이 달린 GPU가 일반적으로 탑재되기 때문에 크게 문제 될 것 없는 요구사항이지만, On-Device 환경에서는 이야기가 달라진다. 최신 하이엔드 스마트폰인 Galaxy S24 Ultra, iPhone 15 Pro의 경우 12GB, 8GB의 램 용량을 보유하고 있다. 그마저도 대부분의 온디바이스 환경은 SoC 구조를 채택하고 있기 때문에 전용 가속기가 온전히 저 모든 램 공간을 활용할 수 없다.</p>

<p>따라서 온디바이스에 Ai를 적용하기 위해서는 획기적인 모델 경량화가 필요한 상황이고 그 출발점이 된 연구가 바로 <code class="language-plaintext highlighter-rouge">DistilBERT</code>다. 로컬 디바이스 환경에서도 언어 모델을 활용하기 위해 허깅 페이스 연구진은 지식 증류 기법을 활용해 인코더 기반 언어 모델의 파라미터를 획기적으로 줄이는데 성공한다.</p>

<p>정리하자면, <code class="language-plaintext highlighter-rouge">DistilBERT</code> 모델은 기존 BERT의 구조적 측면 개선이 아닌, 사전학습 방법 특히 경량화에 초점을 맞춘 시도라고 볼 수 있다. 따라서 어떤 모델이더라도, 인코더 언어 모델이라면 모두 <code class="language-plaintext highlighter-rouge">DistilBERT</code> 구조를 사용할 수 있으며, 기존 논문에서는 원본 BERT 구조를 사용했다. 이번 포스팅에서도 BERT 구조에 대한 설명 대신, <code class="language-plaintext highlighter-rouge">DistilBERT</code>의 사전 학습 방법론인 <code class="language-plaintext highlighter-rouge">Knowledge Distillation</code>에 대해서만 다루려고 한다.</p>

<h3 id="knowledge-distillations"><code class="language-plaintext highlighter-rouge">🌆 Knowledge Distillations</code></h3>

\[\min_{\theta}\sum_{x \in X} \alpha \mathcal{L}_{\text{KL}}(x, \theta) + \beta \mathcal{L}_{\text{MLM}}(x, \theta) + \gamma \mathcal{L}_{\text{Cos}}(x, \theta)\]

<p><code class="language-plaintext highlighter-rouge">DistilBERT</code>는 Teacher-Student Architecture를 차용해 상대적으로 작은 파라미터 사이즈를 갖는 <code class="language-plaintext highlighter-rouge">Student</code> 모델에게 <code class="language-plaintext highlighter-rouge">Teacher</code>의 지식을 전수하는 것을 목표로 한다. 따라서 <code class="language-plaintext highlighter-rouge">Teacher</code> 모델은 이미 사전 학습을 마치고 수렴된 상태의 가중치를 갖고 있는 모델을 사용해야 한다. 더불어 Teacher 모델은 구조만 기존 BERT를 따르되, 사전 학습 방식은 RoBERTa의 방식과 동일(NSP 제거, Dynamic Masking 적용)하게 훈련되어야 한다.</p>

<p>한편, <code class="language-plaintext highlighter-rouge">Student</code> 모델은 <code class="language-plaintext highlighter-rouge">Teacher</code>의 60%정도 파라미터 사이즈를 갖도록 축소하여 사용한다. 이 때 축소는 모델의 <code class="language-plaintext highlighter-rouge">depth</code>(레이어 개수)에만 적용하는데, 연구진에 따르면 <code class="language-plaintext highlighter-rouge">width</code>(은닉층 크기)는 축소를 적용해도 연산 효율이 증가하지 않는다고 한다. 정리하면 <code class="language-plaintext highlighter-rouge">Teacher</code> 모델의 <code class="language-plaintext highlighter-rouge">레이어 개수*0.6</code>의 개수만큼 인코더를 쌓으면 된다는 것이다.</p>

<p>그리고 최대한 <code class="language-plaintext highlighter-rouge">Teacher</code>의 지식을 전수해야 하기 때문에, 데이터는 <code class="language-plaintext highlighter-rouge">Teacher</code> 를 수렴시킨 것과 동일한 세트를 이용해야 한다. 이 때, Teacher 모델은 이미 MLE 방식으로 훈련이 된 상태라서 로짓이 단일 토큰 하나 쪽으로 쏠려 있을 가능성이 매우 높다. 이는 <code class="language-plaintext highlighter-rouge">Student</code> 모델의 일반화 능력에 악영향을 미칠 가능성이 높다. 따라서 Temperature 변수 $T$ 도입해 소프트 맥스(로짓)의 분포를 평탄화 한다. 이렇게 하면, <code class="language-plaintext highlighter-rouge">argmax()</code> 가 아닌 다른 토큰 표현에 대해서도 <code class="language-plaintext highlighter-rouge">Student</code> 모델이 지식을 습득할 수 있어서 풍부한 문맥을 학습하고 일반화 능력을 높이는데 도움이 된다. 이를 <code class="language-plaintext highlighter-rouge">암흑 지식(Dark Knowledge)</code> 을 활용한다고 표현한다. Temperature 변수 $T$ 도입한 소프트맥스 함수 수식은 아래와 같다.</p>

\[\text{softmax}(x_i) = \frac{e^{\frac{x_i}{\tau}}}{\sum_{j} e^{\frac{x_j}{\tau}}}\]

<p>수식상 변수 $T$의 값을 1이상으로 세팅해야 평탄화를 할 수 있다. 따라서 연구진은 $T =2$ 로 두고 사전 학습을 진행했다(논문에 공개안됨, GitHub에 있음). 이번 파트 맨 처음에 등장한 수식을 다시 보자. 결국 <code class="language-plaintext highlighter-rouge">DisilBERT</code>의 목적함수는 3가지 손실의 가중합으로 구성된다. 이제부터는 개별 손실에 대해서 자세히 살펴보자.</p>

<h4 id="distillation-loss-kl-divergence-loss"><code class="language-plaintext highlighter-rouge">🌆 Distillation Loss: KL-Divergence Loss</code></h4>

\[\text{KL-Divergence}(P || Q) = \sum_{i} P(i) \log \frac{P(i)}{Q(i)}\]

<p>증류 손실로 사용되는 <code class="language-plaintext highlighter-rouge">KL-Divergence Loss</code>는 두 확률 분포 간의 차이를 측정하는 지표 중 하나다. 주로 확률 분포 P와 Q 사이의 차이를 나타내는데, 개별 요소의 확률값 차이가 클수록 합산값은 커져 손실이 커지게 된다. 반대로 두 분포의 개별 요소 확률값 차이가 작다면 당연히, 두 분포가 유사하다는 의미이므로 손실 역시 작아지게 된다. 일반적으로 <code class="language-plaintext highlighter-rouge">KL-Divergence Loss</code> 에서 확률분포 $P$ 가 이상적인 확률 분포를, $Q$ 가 모델이 예측한 확률분포를 의미한다. 따라서 <code class="language-plaintext highlighter-rouge">DistilBERT</code>의 경우 확률분포 $P$ 자리에는 <code class="language-plaintext highlighter-rouge">Teacher</code> 모델의 소프트맥스 분포가, $Q$ 에는 <code class="language-plaintext highlighter-rouge">Student</code> 모델의 소프트맥스 분포가 대입되면 된다. 이 때 두 확률분포 모두, 암흑 지식 획득을 위해 소프트맥스 평탄화를 적용한 결과를 사용한다. 논문에서, 선생 모델 예측에 평탄화를 적용한 것을 <code class="language-plaintext highlighter-rouge">소프트 라벨</code>, 학생 모델의 것에 적용한 결과는 <code class="language-plaintext highlighter-rouge">소프트 예측</code>이라고 부른다.</p>

<h4 id="student-loss-mlm-loss"><code class="language-plaintext highlighter-rouge">🌆 Student Loss: MLM Loss</code></h4>

\[\mathcal{L}_{\text{MLM}} = - \sum_{i=1}^{N} \sum_{j=1}^{L} \mathbb{1}_{m_{ij}} \log \text{softmax}(x_{ij})\]

<p>학생 손실은 말그대로 기본적인 MLM 손실을 말한다. 정확한 손실값 계산을 위해서 학생의 소프트맥스 분포에 평탄화를 적용하지 않는다. 이를 논문에서는 <code class="language-plaintext highlighter-rouge">하드 예측</code>이라고 부른다. 라벨 역시 <code class="language-plaintext highlighter-rouge">Teacher</code>로부터 나온 것이 아닌 원래 MLM 수행에 사용되는 마스킹 라벨을 사용한다.</p>

<h4 id="cosine-embedding-loss-contrastive-loss-by-cosine-similarity"><code class="language-plaintext highlighter-rouge">🌆 Cosine Embedding Loss: Contrastive Loss by cosine similarity</code></h4>

\[\mathcal{L}_{\text{COS}}(x,y) = \begin{cases} 1 - \cos(x_1, x_2), &amp; \text{if } y = 1 \\ \max(0, \cos(x_1, x_2) - \text{margin}), &amp; \text{if } y = -1 \end{cases}\]

<p><code class="language-plaintext highlighter-rouge">Teacher</code> 모델과 <code class="language-plaintext highlighter-rouge">Student</code> 모델의 마지막 인코더 모델이 출력하는 은닉값에 대한 <code class="language-plaintext highlighter-rouge">Contrastive Loss</code>를 의미한다. 이 때 <code class="language-plaintext highlighter-rouge">Distance Metric</code>은 코사인 유사도를 사용한다. 그래서 코사인 임베딩 손실이라고 논문에서 정의하는 것으로 추정된다. 위 수식을 최적화하는 것을 목적으로 한다. 이 때 라벨은 <code class="language-plaintext highlighter-rouge">[BS, Seq_len]</code>의 크기를 갖되, 모든 원소는 1이 되도록 만든다. 이유는 간단하다. <code class="language-plaintext highlighter-rouge">Student</code> 모델의 은닉값이 <code class="language-plaintext highlighter-rouge">Teacher</code> 모델의 것과 최대한 비슷해지도록 만드는게 우리 목적이기 때문이다.</p>

<h3 id="implementation-by-pytorch"><code class="language-plaintext highlighter-rouge">👩‍💻 Implementation by Pytorch</code></h3>
<p>논문의 내용과 오피셜로 공개된 코드를 종합하여 파이토치로 <code class="language-plaintext highlighter-rouge">DistilBERT</code>를 구현해봤다. 논문에 포함된 아이디어를 이해하는데는 역시 어렵지 않았지만, 페이퍼에 hyper-param 테이블이 따로 제시되어 있지 않아 공개된 코드를 안 볼수가 없었다.</p>

<p>전체 모델 구조 대한 코드는 <strong><a href="https://github.com/qcqced123/model_study">여기 링크</a></strong>를 통해 참고바란다.</p>

<h4 id="knowledge-distillation-pipeline"><code class="language-plaintext highlighter-rouge">👩‍💻 Knowledge Distillation Pipeline</code></h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">train_val_fn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">loader_train</span><span class="p">,</span> <span class="n">model</span><span class="p">:</span> <span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">criterion</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">],</span> <span class="n">optimizer</span><span class="p">,</span><span class="n">scheduler</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]:</span>
    <span class="s">""" Function for train loop with validation for each batch*N Steps
    DistillBERT has three loss:

        1) distillation loss, calculated by soft targets &amp; soft predictions
            (nn.KLDIVLoss(reduction='batchmean'))

        2) student loss, calculated by hard targets &amp; hard predictions
            (nn.CrossEntropyLoss(reduction='mean')), same as pure MLM Loss

        3) cosine similarity loss, calculated by student &amp; teacher logit similarity
            (nn.CosineEmbeddingLoss(reduction='mean')), similar as contrastive loss

    Those 3 losses are summed jointly and then backward to student model
    """</span>
    <span class="n">scaler</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">amp</span><span class="p">.</span><span class="n">GradScaler</span><span class="p">(</span><span class="n">enabled</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">amp_scaler</span><span class="p">)</span>
    <span class="n">model</span><span class="p">.</span><span class="n">train</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">step</span><span class="p">,</span> <span class="n">batch</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">tqdm</span><span class="p">(</span><span class="n">loader_train</span><span class="p">)):</span>
        <span class="n">optimizer</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">(</span><span class="n">set_to_none</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="n">inputs</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="s">'input_ids'</span><span class="p">].</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">device</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="n">labels</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="s">'labels'</span><span class="p">].</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">device</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="n">padding_mask</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="s">'padding_mask'</span><span class="p">].</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">device</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

        <span class="n">mask</span> <span class="o">=</span> <span class="n">padding_mask</span><span class="p">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">).</span><span class="n">expand</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">dim_model</span><span class="p">)</span>  <span class="c1"># for hidden states dim
</span>        <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">t_hidden_state</span><span class="p">,</span> <span class="n">soft_target</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">teacher_fw</span><span class="p">(</span>
                <span class="n">inputs</span><span class="o">=</span><span class="n">inputs</span><span class="p">,</span>
                <span class="n">padding_mask</span><span class="o">=</span><span class="n">padding_mask</span><span class="p">,</span>
                <span class="n">mask</span><span class="o">=</span><span class="n">mask</span>
            <span class="p">)</span>  <span class="c1"># teacher model's pred =&gt; hard logit
</span>
        <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">amp</span><span class="p">.</span><span class="n">autocast</span><span class="p">(</span><span class="n">enabled</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">amp_scaler</span><span class="p">):</span>
            <span class="n">s_hidden_state</span><span class="p">,</span> <span class="n">s_logit</span><span class="p">,</span> <span class="n">soft_pred</span><span class="p">,</span> <span class="n">c_labels</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">student_fw</span><span class="p">(</span>
                <span class="n">inputs</span><span class="o">=</span><span class="n">inputs</span><span class="p">,</span>
                <span class="n">padding_mask</span><span class="o">=</span><span class="n">padding_mask</span><span class="p">,</span>
                <span class="n">mask</span><span class="o">=</span><span class="n">mask</span>
            <span class="p">)</span>
            <span class="n">d_loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">[</span><span class="s">"KLDivLoss"</span><span class="p">](</span><span class="n">soft_pred</span><span class="p">.</span><span class="n">log</span><span class="p">(),</span> <span class="n">soft_target</span><span class="p">)</span>  <span class="c1"># nn.KLDIVLoss
</span>            <span class="n">s_loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">[</span><span class="s">"CrossEntropyLoss"</span><span class="p">](</span><span class="n">s_logit</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">vocab_size</span><span class="p">),</span> <span class="n">labels</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>  <span class="c1"># nn.CrossEntropyLoss
</span>            <span class="n">c_loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">[</span><span class="s">"CosineEmbeddingLoss"</span><span class="p">](</span><span class="n">s_hidden_state</span><span class="p">,</span> <span class="n">t_hidden_state</span><span class="p">,</span> <span class="n">c_labels</span><span class="p">)</span>  <span class="c1"># nn.CosineEmbeddingLoss
</span>            <span class="n">loss</span> <span class="o">=</span> <span class="n">d_loss</span><span class="o">*</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">alpha_distillation</span> <span class="o">+</span> <span class="n">s_loss</span><span class="o">*</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">alpha_student</span> <span class="o">+</span> <span class="n">c_loss</span><span class="o">*</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">alpha_cosine</span>  <span class="c1"># linear combination loss
</span>
        <span class="n">scaler</span><span class="p">.</span><span class="n">scale</span><span class="p">(</span><span class="n">loss</span><span class="p">).</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">scaler</span><span class="p">.</span><span class="n">step</span><span class="p">(</span><span class="n">optimizer</span><span class="p">)</span>
        <span class="n">scaler</span><span class="p">.</span><span class="n">update</span><span class="p">()</span>
        <span class="n">scheduler</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>
</code></pre></div></div>

<h4 id="knowledge-distillation-model"><code class="language-plaintext highlighter-rouge">👩‍💻 Knowledge Distillation Model</code></h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">DistillationKnowledge</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">AbstractTask</span><span class="p">):</span>
    <span class="s">""" Custom Task Module for Knowledge Distillation by DistilBERT Style Architecture
    DistilBERT Style Architecture is Teacher-Student Framework for Knowledge Distillation,

    And then they have 3 objective functions:
        1) distillation loss, calculated by soft targets &amp; soft predictions
            (nn.KLDIVLoss(reduction='batchmean'))
        2) student loss, calculated by hard targets &amp; hard predictions
            (nn.CrossEntropyLoss(reduction='mean')), same as pure MLM Loss
        3) cosine similarity loss, calculated by student &amp; teacher logit similarity
            (nn.CosineEmbeddingLoss(reduction='mean')), similar as contrastive loss

    References:
        https://arxiv.org/pdf/1910.01108.pdf
        https://github.com/huggingface/transformers/blob/main/examples/research_projects/distillation/distiller.py
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">cfg</span><span class="p">:</span> <span class="n">CFG</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">DistillationKnowledge</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span> <span class="o">=</span> <span class="n">CFG</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">DistilBERT</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">,</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">select_model</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">_init_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">teacher_load_pretrained</span><span class="p">:</span>  <span class="c1"># for teacher model
</span>            <span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">teacher</span><span class="p">.</span><span class="n">load_state_dict</span><span class="p">(</span>
                <span class="n">torch</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="n">cfg</span><span class="p">.</span><span class="n">checkpoint_dir</span> <span class="o">+</span> <span class="n">cfg</span><span class="p">.</span><span class="n">teacher_state_dict</span><span class="p">),</span>
                <span class="n">strict</span><span class="o">=</span><span class="bp">False</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">student_load_pretrained</span><span class="p">:</span>  <span class="c1"># for student model
</span>            <span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">student</span><span class="p">.</span><span class="n">load_state_dict</span><span class="p">(</span>
                <span class="n">torch</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="n">cfg</span><span class="p">.</span><span class="n">checkpoint_dir</span> <span class="o">+</span> <span class="n">cfg</span><span class="p">.</span><span class="n">student_state_dict</span><span class="p">),</span>
                <span class="n">strict</span><span class="o">=</span><span class="bp">True</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">freeze</span><span class="p">:</span>
            <span class="n">freeze</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">teacher</span><span class="p">)</span>
            <span class="n">freeze</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">mlm_head</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">gradient_checkpoint</span><span class="p">:</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">gradient_checkpointing_enable</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">teacher_fw</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">inputs</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">padding_mask</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">mask</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
        <span class="n">is_valid</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="bp">False</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
        <span class="s">""" teacher forward pass to make soft target, last_hidden_state for distillation loss """</span>
        <span class="c1"># 1) make soft target
</span>        <span class="n">temperature</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="k">if</span> <span class="n">is_valid</span> <span class="k">else</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">temperature</span>
        <span class="n">last_hidden_state</span><span class="p">,</span> <span class="n">t_logit</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">teacher_fw</span><span class="p">(</span>
            <span class="n">inputs</span><span class="p">,</span>
            <span class="n">padding_mask</span><span class="p">,</span>
            <span class="n">attention_mask</span>
        <span class="p">)</span>
        <span class="n">last_hidden_state</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">masked_select</span><span class="p">(</span><span class="n">last_hidden_state</span><span class="p">,</span> <span class="o">~</span><span class="n">mask</span><span class="p">)</span>  <span class="c1"># for inverse select
</span>        <span class="n">last_hidden_state</span> <span class="o">=</span> <span class="n">last_hidden_state</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">dim_model</span><span class="p">)</span>  <span class="c1"># flatten last_hidden_state
</span>        <span class="n">soft_target</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="n">softmax</span><span class="p">(</span>
            <span class="n">t_logit</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">vocab_size</span><span class="p">)</span> <span class="o">/</span> <span class="n">temperature</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span>  <span class="c1"># flatten softmax distribution
</span>            <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span>
        <span class="p">)</span>  <span class="c1"># [bs* seq, vocab_size]
</span>        <span class="k">return</span> <span class="n">last_hidden_state</span><span class="p">,</span> <span class="n">soft_target</span>

    <span class="k">def</span> <span class="nf">student_fw</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">inputs</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">padding_mask</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">mask</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
        <span class="n">is_valid</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="bp">False</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
        <span class="s">""" student forward pass to make soft prediction, hard prediction for student loss """</span>
        <span class="n">temperature</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="k">if</span> <span class="n">is_valid</span> <span class="k">else</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">temperature</span>
        <span class="n">last_hidden_state</span><span class="p">,</span> <span class="n">s_logit</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">teacher_fw</span><span class="p">(</span>
            <span class="n">inputs</span><span class="p">,</span>
            <span class="n">padding_mask</span><span class="p">,</span>
            <span class="n">attention_mask</span>
        <span class="p">)</span>
        <span class="n">last_hidden_state</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">masked_select</span><span class="p">(</span><span class="n">last_hidden_state</span><span class="p">,</span> <span class="o">~</span><span class="n">mask</span><span class="p">)</span>  <span class="c1"># for inverse select
</span>        <span class="n">last_hidden_state</span> <span class="o">=</span> <span class="n">last_hidden_state</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">dim_model</span><span class="p">)</span>  <span class="c1"># flatten last_hidden_state
</span>        <span class="n">c_labels</span> <span class="o">=</span> <span class="n">last_hidden_state</span><span class="p">.</span><span class="n">new</span><span class="p">(</span><span class="n">last_hidden_state</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)).</span><span class="n">fill_</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">soft_pred</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="n">softmax</span><span class="p">(</span>
            <span class="n">s_logit</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">vocab_size</span><span class="p">)</span> <span class="o">/</span> <span class="n">temperature</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span>  <span class="c1"># flatten softmax distribution
</span>            <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">last_hidden_state</span><span class="p">,</span> <span class="n">s_logit</span><span class="p">,</span> <span class="n">soft_pred</span><span class="p">,</span> <span class="n">c_labels</span>
</code></pre></div></div>

<h4 id="distilbert-model"><code class="language-plaintext highlighter-rouge">👩‍💻 DistilBERT Model</code></h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">DistilBERT</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">AbstractModel</span><span class="p">):</span>
    <span class="s">""" Main class for DistilBERT Style Model, Teacher-Student Framework
    for Knowledge Distillation aim to lighter Large Scale LLM model. This model have 3 objective functions:

        1) distillation loss, calculated by soft targets &amp; soft predictions
            (nn.KLDIVLoss(reduction='batchmean'))

        2) student loss, calculated by hard targets &amp; hard predictions
            (nn.CrossEntropyLoss(reduction='mean')), same as pure MLM Loss

        3) cosine similarity loss, calculated by student &amp; teacher logit similarity
            (nn.CosineEmbeddingLoss(reduction='mean')), similar as contrastive loss

    soft targets &amp; soft predictions are meaning that logit are passed through softmax function applied with temperature T
    temperature T aim to flatten softmax layer distribution for making "Dark Knowledge" from teacher model

    hard targets &amp; hard predictions are meaning that logit are passed through softmax function without temperature T
    hard targets are same as just simple labels from MLM Collator returns for calculating cross entropy loss

    cosine similarity loss is calculated by cosine similarity between student &amp; teacher
    in official repo, they mask padding tokens for calculating cosine similarity, target for this task is 1
    cosine similarity is calculated by nn.CosineSimilarity() function, values are range to [-1, 1]

    you can select any other backbone model architecture for Teacher &amp; Student Model for knowledge distillation
    but, in original paper, BERT is used for Teacher Model &amp; Student
    and you must select pretrained model for Teacher Model, because Teacher Model is used for knowledge distillation,
    which is containing pretrained mlm head

    Do not pass gradient backward to teacher model!!
    (teacher model must be frozen or register_buffer to model or use no_grad() context manager)

    Args:
        cfg: configuration.CFG
        model_func: make model instance in runtime from config.json

    References:
        https://arxiv.org/pdf/1910.01108.pdf
        https://github.com/huggingface/transformers/blob/main/examples/research_projects/distillation/distiller.py
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">cfg</span><span class="p">:</span> <span class="n">CFG</span><span class="p">,</span> <span class="n">model_func</span><span class="p">:</span> <span class="n">Callable</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">DistilBERT</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span> <span class="o">=</span> <span class="n">cfg</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">teacher</span> <span class="o">=</span> <span class="n">model_func</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">teacher_num_layers</span><span class="p">)</span>  <span class="c1"># must be loading pretrained model containing mlm head
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">mlm_head</span> <span class="o">=</span> <span class="n">MLMHead</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">)</span>  <span class="c1"># must be loading pretrained model's mlm head
</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">student</span> <span class="o">=</span> <span class="n">model_func</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">student_num_layers</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">s_mlm_head</span> <span class="o">=</span> <span class="n">MLMHead</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">teacher_fw</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">inputs</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">padding_mask</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
        <span class="s">""" forward pass for teacher model
        """</span>
        <span class="n">last_hidden_state</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">teacher</span><span class="p">(</span>
            <span class="n">inputs</span><span class="p">,</span>
            <span class="n">padding_mask</span><span class="p">,</span>
            <span class="n">attention_mask</span>
        <span class="p">)</span>
        <span class="n">t_logit</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">mlm_head</span><span class="p">(</span><span class="n">last_hidden_state</span><span class="p">)</span>  <span class="c1"># hard logit =&gt; to make soft logit
</span>        <span class="k">return</span> <span class="n">last_hidden_state</span><span class="p">,</span> <span class="n">t_logit</span>

    <span class="k">def</span> <span class="nf">student_fw</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">inputs</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">padding_mask</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="bp">None</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
        <span class="s">""" forward pass for student model
        """</span>
        <span class="n">last_hidden_state</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">student</span><span class="p">(</span>
            <span class="n">inputs</span><span class="p">,</span>
            <span class="n">padding_mask</span><span class="p">,</span>
            <span class="n">attention_mask</span>
        <span class="p">)</span>
        <span class="n">s_logit</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">s_mlm_head</span><span class="p">(</span><span class="n">last_hidden_state</span><span class="p">)</span>  <span class="c1"># hard logit =&gt; to make soft logit
</span>        <span class="k">return</span> <span class="n">last_hidden_state</span><span class="p">,</span> <span class="n">s_logit</span>
</code></pre></div></div>]]></content><author><name>qcqced</name><email>qcqced123@gmail.com</email></author><category term="NLP" /><category term="Natural Language Process" /><category term="Linear-Attention" /><category term="Transformer" /><category term="BERT" /><category term="Kernel Trick" /><category term="Self-Attention" /><category term="Pytorch" /><summary type="html"><![CDATA[Linear Attention Official Paper Review with Pytorch Implementation]]></summary></entry><entry><title type="html">🎡 [Roformer] RoFormer: Enhanced Transformer with Rotary Position Embedding</title><link href="http://localhost:4000/nlp/roformer" rel="alternate" type="text/html" title="🎡 [Roformer] RoFormer: Enhanced Transformer with Rotary Position Embedding" /><published>2024-03-11T00:00:00+09:00</published><updated>2024-03-12T02:00:00+09:00</updated><id>http://localhost:4000/nlp/roformer</id><content type="html" xml:base="http://localhost:4000/nlp/roformer"><![CDATA[<h3 id="overview"><code class="language-plaintext highlighter-rouge">🔭 Overview</code></h3>

<p><code class="language-plaintext highlighter-rouge">Roformer</code>는 2021년에 발표된 트랜스포머 모델의 변형으로, <code class="language-plaintext highlighter-rouge">RoPE(Rotary Position Embedding)</code>이라는 새로운 위치 정보 포착 방식을 제안했다. 근래 유명한 오픈소스 LLM 모델들(GPT-Neo, LLaMA)의 위치 정보 포착 방식으로 채택 되어 주목을 받고 있다. <code class="language-plaintext highlighter-rouge">RoPE</code> 기법에 대해 살펴보기 전에 일단, 관련 분야의 연구 동향 및 위치 정보의 개념에 대해 간단하게 살펴보고 넘어가려 한다.</p>

<h3 id="-absolute-position-vs-relative-position"><code class="language-plaintext highlighter-rouge">🤔 Absolute Position vs Relative Position</code></h3>

<p>트랜스포머가 성공을 거둘 수 있었던 이유는 전체 시퀀스를 병렬적으로 한 번에 처리하되, 시퀀스 발생 순서 정보를 행렬합 방식으로 인코딩해줬기 때문이다. 이 분야에 대한 연구 동향은 크게 <code class="language-plaintext highlighter-rouge">Absolute Position</code>, <code class="language-plaintext highlighter-rouge">Relative Position</code> 방식으로 분화된다.</p>

<p><code class="language-plaintext highlighter-rouge">Absolute Position</code>은 주어진 시퀀스의 길이를 측정한 뒤, 나열된 순서 그대로 <code class="language-plaintext highlighter-rouge">forward</code>하게 <code class="language-plaintext highlighter-rouge">0</code>부터 <code class="language-plaintext highlighter-rouge">길이-1</code>의 번호를 개별 토큰에 할당한다. 다시 말해, 단어가 시퀀스에서 발생한 순서를 수학적으로 표현해 모델에 주입한다는 의미가 된다.</p>

<p>한편, <code class="language-plaintext highlighter-rouge">Relative Position</code>은 시퀀스 내부 토큰 사이의 위치 관계 표현을 통해 토큰 사이의 <code class="language-plaintext highlighter-rouge">relation</code>을 <code class="language-plaintext highlighter-rouge">pairwise</code>하게 학습하는 위치 임베딩 기법을 말한다. 일반적으로 상대 위치 관계는 서로 다른 두 토큰의 시퀀스 인덱스 값의 차를 이용해 나타낸다. 포착하는 문맥 정보는 예시와 함깨 설명하겠다. 예시는 예전 DeBERTa 논문에서 나왔던 것을 활용했다. 딥러닝이라는 단어는 영어로 <code class="language-plaintext highlighter-rouge">Deep Learning</code> 이다. 두 단어를 합쳐놓고 보면 <code class="language-plaintext highlighter-rouge">신경망을 사용하는 머신러닝 기법의 한 종류</code>라는 의미를 갖겠지만, 따로 따로 보면 <code class="language-plaintext highlighter-rouge">깊은</code>, <code class="language-plaintext highlighter-rouge">배움</code>이라는 개별적인 의미로 나뉜다.</p>

<ul>
  <li><strong><code class="language-plaintext highlighter-rouge">1) The Deep Learning is the Best Technique in Computer Science</code></strong></li>
  <li><strong><code class="language-plaintext highlighter-rouge">2) I’m learning how to swim in the deep ocean</code></strong></li>
</ul>

<p><code class="language-plaintext highlighter-rouge">Deep</code>과 <code class="language-plaintext highlighter-rouge">Learning</code>의 상대적인 거리에 주목하면서 두 문장을 해석해보자. 첫 번째 문장에서 두 단어는 이웃하게 위치해 <code class="language-plaintext highlighter-rouge">신경망을 사용하는 머신러닝 기법의 한 종류</code> 라는 의미를 만들어내고 있다. 한편 두 번째 문장에서 두 단어는 띄어쓰기 기준 5개의 토큰만큼 떨어져 위치해 각각 <code class="language-plaintext highlighter-rouge">배움</code>, <code class="language-plaintext highlighter-rouge">깊은</code> 이라는 의미를 만들어 내고 있다. 이처럼 개별 토큰 사이의 위치 관계에 따라서 파생되는 문맥적 정보를 포착하려는 의도로 설계된 기법이 바로 <code class="language-plaintext highlighter-rouge">Relative Position Embedding</code> 이다.</p>

<h3 id="-word-context-vs-relative-position-vs-absolute-position"><strong><code class="language-plaintext highlighter-rouge">🤔 Word Context vs Relative Position vs Absolute Position</code></strong></h3>

<p align="center">
<img src="/assets/images/deberta/line_people.png" alt="줄 서있는 사람들" class="align-center image-caption" width="40%&quot;, height=&quot;50%" />
<strong><em><a href="https://kr.freepik.com/premium-photo/people-standing-in-line-during-airport-check-in_8754408.htm">줄 서있는 사람들</a></em></strong>
</p>

<p>지금까지 <code class="language-plaintext highlighter-rouge">Relative Position Embedding</code>이 무엇이고, 도대체 어떤 문맥 정보를 포착한다는 것인지 알아봤다. 필자의 설명이 매끄럽지 못하기도 하고 예시를 텍스트로 들고 있어서 직관적으로 <code class="language-plaintext highlighter-rouge">word context</code>는 무엇인지, <code class="language-plaintext highlighter-rouge">Position</code> 정보와는 뭐가 다른지, 두 가지 <code class="language-plaintext highlighter-rouge">Position</code> 정보는 뭐가 어떻게 다른지 와닿지 않는 분들이 많으실 것 같다. 그래서 최대한 직관적인 예시를 통해 세가지 정보의 차이점을 설명해보려 한다.</p>

<p>사람 5명이 공항 체크인을 위해 서 있다. 모두 왼쪽을 보고 있는 것을 보아 왼쪽에 키가 제일 작은 여자가 가장 앞줄이라고 볼 수 있겠다. 우리는 줄 서있는 순서대로 5명의 사람에게 번호를 부여할 것이다. 편의상 0번부터 시작해 4번까지 번호를 주겠다. 1번에 해당하는 사람은 누구인가??  바로 줄의 2번째에 서있는 여자다. 그럼 2번에 해당하는 사람은 누구인가?? 사진 속 줄의 가장 중간에 있는 남자가 2번이다. 이렇게 그룹 단위(전체 줄)에서 개개인에 일련의 번호를 부여해 위치를 표현하는 방법이 바로 <code class="language-plaintext highlighter-rouge">Absolute Position Embedding</code>이다.</p>

<p>한편, 다시 2번 사람에게 주목해보자. 우리는 2번 남자를 전체 줄에서 가운데 위치한 사람이 아니라, 검정색 양복과 구두를 신고 손에 쥔 무언가를 응시하고 있는 사람이라고 표현할 수도 있다. 이것이 바로 토큰의 의미 정보를 담은 <code class="language-plaintext highlighter-rouge">word context</code>에 해당한다.</p>

<p>마지막으로 <code class="language-plaintext highlighter-rouge">Relative Position Embedding</code> 방식으로 2번 남자를 표현해보자. 오른손으로는 커피를 들고 다른 손으로는 캐리어를 잡고 있으며 검정색 하이힐과 베이지색 바지를 입은 <strong>1번 여자의 뒤에 있는 사람</strong>, 회색 양복과 검은 뿔테 안경을 쓰고 한 손에는 캐리어를 잡고 있는 <strong>4번 여자의 앞에 있는 사람</strong>, 검정색 자켓과 청바지를 입고 한 손에는 회색 코트를 들고 있는 줄의 <strong>맨 앞 여자로부터 2번째 뒤에 서있는 사람</strong>, 턱수염이 길고 머리가 긴 편이며 파란색 가디건을 입고 초록색과 검정색이 혼합된 가방을 왼쪽으로 메고 있는 <strong>남자로부터 2번째 앞에 있는 사람.</strong></p>

<p>이처럼 표현하는게 바로 <code class="language-plaintext highlighter-rouge">Relative Position Embedding</code>에 대응된다고 볼 수 있다. 이제 위치 임베딩에 대해서 살펴봤으니, 논문에서 제시하는 내용에 대해서 알아보자.</p>

<h3 id="️-previous-work-relative-position-embedding"><strong><code class="language-plaintext highlighter-rouge">🗂️ Previous Work: Relative Position Embedding</code></strong></h3>

<p>미리 말하자면, <code class="language-plaintext highlighter-rouge">RoPE</code>는 위치 정보 중에서 상대 위치를 포착한다. 그래서 저자는 그들의 방법론을 소개하기 전에 먼저, 이전 연구들의 상대 위치 포착 방식에 대해서 소개하고 있다. 간단히 살펴보자.</p>

\[q^T_mk_n = x^T_mW^T_qW_kx_n + x^T_mW^T_qW_kp_n + p^T_mW^T_qW_kx_n + p^T_mW^T_qW_kp_n\ \ \ (1) \\\]

\[q^T_m k_n = x^T_m W^T_q W_k x_n + x^T_m W^T_q {W_k} \tilde{x}_{m-n} + \widetilde{p}_{m-n} W^T_q W_k x_n \ \ \ (2)\]

<p>(1)번 수식은 <code class="language-plaintext highlighter-rouge">Transformer-XL</code> 논문에서 제시된 <code class="language-plaintext highlighter-rouge">Cross Attention</code> 수식이다. 위치 정보를 담아내는 항을 따로 만들고 쿼리, 키에 대응되는 항과 곱하고 있다. (2)번 수식은 <code class="language-plaintext highlighter-rouge">DeBERTa</code> 모델에서 제시된 <code class="language-plaintext highlighter-rouge">Disentangled Attention</code> 이다. (1)과 구성의 차이는 있지만 역시, 위치 정보를 담아내는 항을 억지로 만들고 그것들을 쿼리 혹은 키와 곱하여 위치 정보를 담아낸 뒤, 모두 합하여 어텐션 행렬을 만들어 내고 있다.</p>

<p>정리하면, 기존 연구들은 상대 위치를 포착하기 위해 별도의 포지션 행렬을 만들고, 이리저리 곱하고, 다시 그것들을 모두 합하여 어텐션 행렬을 만들고 있는 것이다. 기존 연구들이 제시하는 방법론들의 공통된 문제는 학습해야 할 파라미터 수가 늘어나 모델 사이즈도 커지고, 학습시간도 늘어난다는 것이다.</p>

<h3 id="rope"><code class="language-plaintext highlighter-rouge">🎡 RoPE</code></h3>

\[f_{q,k}(x_m, m)= \left( \begin{array}{cc}\cos(m\theta) &amp; \sin(m\theta) \\-\sin(m\theta) &amp; \cos(m\theta)\end{array} \right)

\left( \begin{array}{cc}W^{(11)}_{q,k} &amp; W^{(12)}_{q,k} \\W^{(21)}_{q,k} &amp; W^{(22)}_{q,k} \end{array} \right) 

\left( \begin{array}{cc}x_m^{(1)} \\x_m^{(2)} \end{array} \right)\]

<p>등식의 좌변은 <code class="language-plaintext highlighter-rouge">word embedding</code>을 선형 투영 시켜 얻은 <code class="language-plaintext highlighter-rouge">query</code>, <code class="language-plaintext highlighter-rouge">key</code> 벡터에 <code class="language-plaintext highlighter-rouge">Rotary Position Embedding</code> 값을 추가한 결과 값을 뜻한다. 우변의 수식이 상당히 복잡해 보이나, 실상은 매우 간단하다. 선형 투영으로 얻은 <code class="language-plaintext highlighter-rouge">query</code>, <code class="language-plaintext highlighter-rouge">key</code> 벡터에 좌측의 괴랄하게 생긴 행렬을 곱해주겠다는 것이다. 좌측의 행렬은 대학교 선형대수 시간에 스치듯 지나갔던 <code class="language-plaintext highlighter-rouge">Transformation Matrix(회전 행렬)</code>이다. $m$은 $m$-th 토큰을 의미하는데, 세타가 뭔지는 모르겠지만 일단 토큰의 인덱스 값에 따라서, 주어진 워드 임베딩 벡터를 회전시키겠다는 것이다. 지금 살펴본 예시는 은닉층 크기가 2차원인 단순한 벡터였다. 실제 모델에 사용하는 차원(384, 512, 768, …)으로 확장하기 전에 세타의 정체에 대해 알아보자.</p>

\[\Theta = \left\{ \theta_i = 10000^{ -{2(i-1)}/{d}}, \quad i \in \left[1, 2, \ldots, \frac{d}{2}\right] \right\}\]

<p>$\theta$의 정체는 바로 주기함수 였다. 퓨어한 트랜스포머에서 <code class="language-plaintext highlighter-rouge">Absolute Position Encoding</code>을 위해 <code class="language-plaintext highlighter-rouge">Sinusoidal</code> 함수를 사용한 것과 같은 이치라고 생각하면 된다. 즉 $\theta$는 <code class="language-plaintext highlighter-rouge">word embedding</code> 벡터가 가진 은닉층 차원 방향 인덱스에 따라서 달라진다. 여기에 시퀀스 길이 차원 방향의 인덱스 값을 따로 곱해주기 때문에 그 유일성을 보장할 수 있다.</p>

<p>이제 전체 RoPE를 이해하는데 필요한 재료 준비는 모두 끝났다. 이제 실제 차원으로 확장해보자.</p>

\[fq,k(x_m,m)=R^d_{Θ,m}W_{q,k}x_m \\\]

<p>행렬 $R^d_{Θ,m}$은 아래와 같은 행렬을 말하는데,</p>

\[R^d_{Θ,m} = \begin{bmatrix}
\cos(m\theta_1) &amp; -\sin(m\theta_1) &amp; 0 &amp; 0 &amp; \cdots &amp; 0 &amp; 0 \\
\sin(m\theta_1) &amp; \cos(m\theta_1) &amp; 0 &amp; 0 &amp; \cdots &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; \cos(m\theta_2) &amp; -\sin(m\theta_2) &amp; \cdots &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; \sin(m\theta_2) &amp; \cos(m\theta_2) &amp; \cdots &amp; 0 &amp; 0 \\
\vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots &amp; \vdots \\
0 &amp; 0 &amp; 0 &amp; 0 &amp; \cdots &amp; \cos(m\theta_{d/2}) &amp; -\sin(m\theta_{d/2}) \\
0 &amp; 0 &amp; 0 &amp; 0 &amp; \cdots &amp; \sin(m\theta_{d/2}) &amp; \cos(m\theta_{d/2})
\end{bmatrix}\]

<p>토큰의 인덱스와 모델의 은닉차원 인덱스에 따라서 행렬의 원소값이 결정됨을 알 수 있다. 이제 다시 (3)번 수식의 의미를 생각해보자. 단어 임베딩을 쿼리, 키 행렬로 선형 투영한 뒤 (4)번 수식을 곱한다. 순수한 회전행렬을 쿼리, 키 벡터에 곱하기 때문에 벡터의 크기를 유지한채, 방향만 바꿔줄 수 있다는 장점이 있다.</p>

<p>이전의 연구들은 포지션 정보를 가지고 있는 행렬을 단어 벡터에 더하기 때문에 벡터의 방향은 물론 크기 역시 왜곡된다. 물론 단어 벡터와 포지션 벡터가 서로 성격이 다른 정보라는 점을 고려하면 모델의 은닉층처럼 고차원 공간에서 서로 직교할 확률이 매우 높기 때문에, 서로 학습에 영향을 미칠 가능성은 낮다. 하지만 확률적인 접근일 뿐더러, 단어 벡터의 크기가 왜곡된다는 점이 층을 거듭할수록 영향을 미칠지 알 수 없다.</p>

<p>RoPE 방식의 또다른 장점은 곱하는 것만으로도, 상대 위치 정보를 인코딩 해줄 수 있다는 점이다. 이전 연구들은 대부분 절대 위치 혹은 상대 위치 하나만을 선택해 단어 임베딩에 정보를 추가해주는 경우가 대다수 였다. DeBERTa의 경우에만, Task 레이어 근처(레이어 후반부)에 가서 절대 위치를 더해 상대 위치가 갖는 단점을 보완하려는 시도를 했다. DeBERTa가 여러 방면에서 상당히 좋은 성능을 거둬서 그렇지, 마지막 레이어 근처에 가서 절대 위치를 더해주는게 사실 자연스럽다고 생각되지는 않는다. 그런데 RoPE는 회전 행렬을 곱하는 것만으로도 절대 위치와 상대 위치 모두 인코딩이 가능하다. 어떻게 그럴까??</p>

<p>일단 RoPE 선형 투영된 쿼리, 키 행렬에 각각 회전행렬을 곱한다. 곱하는 과정에서 이미 토큰의 인덱스 값에 따라서 서로 다른 포지션 값이 단어 임베딩에 곱해지게 된다. 이것으로 일단 절대 위치 정보를 추가해줄 수 있다. 그리고 잘 알다시피, 쿼리와 키의 내적을 수행한다. 쿼리와 키의 내적을 각각 단어 임베딩, 선형 투영, 회전행렬 항으로 나눠서 식을 풀어 쓰면 아래와 같다.</p>

\[q^T_mk_n=(R^d_{Θ,m}W_{q}x_m)^T(R^d_{Θ,n}W_{k}x_n) \ \ \ (5)\]

<p>수식을 전개하면 자연스레,</p>

\[x^TW_qR^d_{Θ,n-m}W_kx_n \ \ \ (6)\]

<p>(6)번 수식처럼 된다. 행렬 $R^d_{Θ,n-m}$의 원소는 아래처럼,</p>

\[\cos(m\theta_1)*\cos(n\theta_1) - \sin(m\theta_1)*\sin(n\theta_1) \\\]

<p>토큰 인덱스를 의미하는 $m,n$에 대한 수식으로 표현된다. 따라서 자연스럽게 상대 위치를 포착할 수 있게 된다. 상당히 자연스럽게 서로 다른 두 위치 정보를 인코딩하는게 가능하며, 추가적으로 다른 항을 만들어 어텐션 행렬을 계산하지 않기 때문에 메모리를 좀 더 효율적으로 사용 가능하다.</p>

<p>한편, 토큰의 상대 위치를 포착하는 방식은 자신과 상대적 거리가 멀어질수록 의미적 연관성이나 관계성이 떨어진다는 점을 전제로 한다. 즉, 서로 거리가 먼 토큰일수록 쿼리와 키벡터의 내적값이 0에 가까워져야 한다는 것이다. 저자 역시 이점을 언급하며 <code class="language-plaintext highlighter-rouge">RoPE</code> 방식이 <code class="language-plaintext highlighter-rouge">Long-Term Decay</code> 속성을 갖고 있다고 주장한다.</p>

<p align="center">
<img src="/assets/images/roformer/roformer_long_term.png" alt="Long-Term Decay" class="align-center image-caption" width="70%&quot;, height=&quot;50%" />
<strong><em><a href="https://arxiv.org/abs/2104.09864">Long-Term Decay</a></em></strong>
</p>

<p><code class="language-plaintext highlighter-rouge">Appendix</code>에서 수학적으로 증명까지 제시하고 있으나, 필자의 수학 실력이 얕아서 제시된 과정이 이해가 가질 않는다. 추후에 관련 내용은 추가하도록 하겠다. 일단 Relative Upper Bound가 정확히 무엇을 말하는지 모르겠지만(논문에 제대로 언급 x, 추측하건데, 의미적 연관성을 나타내는 지표 같음, 아마 내적값으로 추정), 제시된 그래프를 보면 서로 상대적 거리가 멀어질수록 해당 지표가 확연히 감소하는 추세를 보인다.</p>

<p>마지막으로 논문에서 밝히길 (4), (5)번 수식의 형태로 RoPE를 만드는 것은 연산 효율이 떨어진다고 한다. 그래서 <code class="language-plaintext highlighter-rouge">Appendix</code>에서 효율적으로 연산하는 수식을 다시 제시하고 있다.</p>

\[R^d_{Θ,m}x = \begin{bmatrix}
x_1 \\
x_2 \\
x_3 \\
x_4 \\
... \\
x_{d-1} \\
x_{d} \\
\end{bmatrix} \otimes \begin{bmatrix} 
cos (m\theta_1) \\
cos (m\theta_1) \\
cos (m\theta_2) \\
cos (m\theta_2) \\
... \\
cos (m\theta_{d/2}) \\
cos (m\theta_{d/2}) \\
\end{bmatrix} + 
\begin{bmatrix}
-x_2 \\
x_1 \\
-x_4 \\
x_3 \\
... \\
-x_{d-1} \\
x_d \\
\end{bmatrix} \otimes  \begin{bmatrix}
\sin(m\theta_1) \\
\sin(m\theta_1) \\
\sin(m\theta_2) \\
\sin(m\theta_2) \\
\vdots \\
\sin(m\theta_{d/2}) \\
\sin(m\theta_{d/2})

\end{bmatrix}\]

<p>수식 (4), (5)번 형태 그대로 구현하려면, 크기가 <code class="language-plaintext highlighter-rouge">[seq_len, dim_head, dim_head]</code>인 텐서를 계속 가지고 있어야 한다. 이는 상당히 메모리를 낭비하게 된다. 아래 그림은 필자가 (4), (5)번 형태 그대로 구현한 뒤, MLM 학습을 돌리던 모습이다.</p>

<p align="center">
<img src="/assets/images/roformer/body_ver.png" alt="body ver result" class="align-center image-caption" width="100%&quot;, height=&quot;70%" />
<strong><em>[body ver result]</em></strong>
</p>

<p>11시간 40분으로 훈련 시간이 예측되는걸 볼 수 있다. 물론, 이러한 결과가 나온 이유는 \(R^d_{Θ,m}x\)이 차지하는 메모리 크기가 커지면서, GPU 상에 한 번에 올릴 수가 없어져 배치마다 루프를 돌려서 RoPE를 개별 쿼리, 키에 곱해주는 방식을 선택했기 때문이다. 이제 Appendix에서 제시한 방법대로 RoPE를 구현하면,</p>

<p align="center">
<img src="/assets/images/roformer/appendix_ver.png" alt="appendix ver result" class="align-center image-caption" width="100%&quot;, height=&quot;70%" />
<strong><em>[appendix ver result]</em></strong>
</p>

<p>이렇게 4시간으로 시간이 드라마틱하게 줄어들었다. 이 방법은 또한 $R^d_{Θ,m}$를 <code class="language-plaintext highlighter-rouge">[seq_len, dim_head]</code> 크기를 갖는 텐서를 사용하면 되기 때문에, 이전 방식보다 훨씬 메모리도 덜 차지한다. 이 방식은 배치 차원으로 루프를 돌릴 필요가 없어져 훈련시간도 대폭 단축되는 것이다.</p>

<h3 id="-rope-with-linear-attention"><code class="language-plaintext highlighter-rouge">📏 RoPE with linear attention</code></h3>

<p>저자는 퓨어한 <code class="language-plaintext highlighter-rouge">full attention</code> 대신 <strong><a href="https://arxiv.org/abs/2006.16236">&lt;Transformers are RNNs: Fast Autoregressive Transformers with linear attention&gt;</a></strong> 논문에서 제시된 <code class="language-plaintext highlighter-rouge">linear attention</code> 을 사용했다고 밝히고 있다.</p>

<p>하지만, <code class="language-plaintext highlighter-rouge">linear attention</code> 의 경우 디코더의 <code class="language-plaintext highlighter-rouge">CLM</code> 수행에 어울리는 방식으로, <code class="language-plaintext highlighter-rouge">NLU</code>를 위한 인코더에는 적합하지 않다. 해당 논문에서도 모델의 벤치마크 결과를 모두 <code class="language-plaintext highlighter-rouge">NLG</code>에 대해서만 제시한다. 그리고 필자가 직접 구현해 <code class="language-plaintext highlighter-rouge">MLM</code>을 수행해본 결과<strong>(<a href="https://wandb.ai/qcqced/MaskedLanguageModel/runs/63ogmndi?nw=nwuserqcqced">실험 결과 링크</a>)</strong> 정확도가 상당히 낮게 나오는 것을 알 수 있다. 물론 애초에 해당 방식은 트랜스포머를 <code class="language-plaintext highlighter-rouge">RNN</code>처럼 시간 차원에 대해서 학습하는 경우를 상정하고 만들었기 떄문에 <code class="language-plaintext highlighter-rouge">linear attention</code> 을 BERT 같은 인코더 모델에 그대로 사용하는게 애초에 안 맞을 수 있다. 하지만 허깅 페이스의 <code class="language-plaintext highlighter-rouge">roformer</code> 코드를 보면 역시, <code class="language-plaintext highlighter-rouge">linear attention</code> 대신 <code class="language-plaintext highlighter-rouge">full attention</code>에 <code class="language-plaintext highlighter-rouge">RoPE</code>를 통합하는 방식으로 구현했다. 따라서 필자 역시 <code class="language-plaintext highlighter-rouge">full attention</code>을 기준으로 모델을 구현했음을 밝힌다.</p>

<h3 id="implementation-by-pytorch"><code class="language-plaintext highlighter-rouge">👩‍💻 Implementation by Pytorch</code></h3>

<p>논문의 내용과 오피셜로 공개된 코드를 종합하여 파이토치로 <code class="language-plaintext highlighter-rouge">Roformer</code>를 구현해봤다. 다만, <code class="language-plaintext highlighter-rouge">linear attention</code> 대신 <code class="language-plaintext highlighter-rouge">full attention</code>을 사용했고 오직 인코더 부분만 구현했음을 밝힌다.</p>

<p>한편, 필자가 직접 구현한 RoPE를 코드도 있으나, GPU 연산 최적화까지는 실패해 대신 허깅페이스의 구현체를 참고했음을 밝힌다. 시간이 될 때, 직접 구현했던 RoPE 코드도 함꼐 첨부하겠다. 그리고 이번 포스팅에서는 RoPE를 구현하는 방법에 대해서만 다루고, 나머지 구현에 대한 설명은 생략하려 한다. 전체 모델 구조 대한 코드는 <strong><a href="https://github.com/qcqced123/model_study">여기 링크</a></strong>를 통해 참고바란다.</p>

<h4 id="-rotary-position-embedding"><code class="language-plaintext highlighter-rouge">🎡 Rotary Position Embedding</code></h4>

<p><code class="language-plaintext highlighter-rouge">_init_weight()</code>의 <code class="language-plaintext highlighter-rouge">position_enc</code>를 주목해보자. <code class="language-plaintext highlighter-rouge">position_enc</code>는 <code class="language-plaintext highlighter-rouge">position</code>과 <code class="language-plaintext highlighter-rouge">dim</code>을 인자로 받아 <code class="language-plaintext highlighter-rouge">position</code>과 <code class="language-plaintext highlighter-rouge">dim</code>에 따라서 <code class="language-plaintext highlighter-rouge">position_enc</code>를 만들어내는데, 이것이 바로 <code class="language-plaintext highlighter-rouge">RoPE</code>의 핵심이다. 해당 코드 라인이 정확하게 $m\theta_d$을 계산하게 된다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">RoFormerSinusoidalPositionalEmbedding</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Embedding</span><span class="p">):</span>
    <span class="s">""" This module produces sinusoidal positional embeddings of any length
    Original Source code from Huggingface's RoFormer model, which is the most optimized way to create positional embedding

    Args:
        max_seq: max sequence length of model
        dim_head: dimension of each attention head's hidden states

    Returns:
        Tensor -&gt; torch.Size([seq_len, dim_head])

    References:
        https://arxiv.org/abs/2104.09864  # RoFormer: Enhanced Transformer with Rotary Position Embedding
        https://github.com/huggingface/transformers/blob/main/src/transformers/models/roformer/modeling_roformer.py#L323
    """</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">max_seq</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">dim_head</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">(</span><span class="n">max_seq</span><span class="p">,</span> <span class="n">dim_head</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">weight</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">_init_weight</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">weight</span><span class="p">)</span>

    <span class="o">@</span><span class="nb">staticmethod</span>
    <span class="k">def</span> <span class="nf">_init_weight</span><span class="p">(</span><span class="n">out</span><span class="p">:</span> <span class="n">nn</span><span class="p">.</span><span class="n">Parameter</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">nn</span><span class="p">.</span><span class="n">Parameter</span><span class="p">:</span>
        <span class="s">"""
        Identical to the XLM create_sinusoidal_embeddings except features are not interleaved. The cos features are in
        the 2nd half of the vector. [dim // 2:]
        """</span>
        <span class="n">n_pos</span><span class="p">,</span> <span class="n">dim</span> <span class="o">=</span> <span class="n">out</span><span class="p">.</span><span class="n">shape</span>
        <span class="n">position_enc</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span>
            <span class="p">[[</span><span class="n">pos</span> <span class="o">/</span> <span class="n">np</span><span class="p">.</span><span class="n">power</span><span class="p">(</span><span class="mi">10000</span><span class="p">,</span> <span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">j</span> <span class="o">//</span> <span class="mi">2</span><span class="p">)</span> <span class="o">/</span> <span class="n">dim</span><span class="p">)</span> <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">dim</span><span class="p">)]</span> <span class="k">for</span> <span class="n">pos</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_pos</span><span class="p">)]</span>
        <span class="p">)</span>  <span class="c1"># m * theta
</span>        <span class="n">out</span><span class="p">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="bp">False</span>  <span class="c1"># set early to avoid an error in pytorch-1.8+
</span>        <span class="n">sentinel</span> <span class="o">=</span> <span class="n">dim</span> <span class="o">//</span> <span class="mi">2</span> <span class="k">if</span> <span class="n">dim</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">==</span> <span class="mi">0</span> <span class="k">else</span> <span class="p">(</span><span class="n">dim</span> <span class="o">//</span> <span class="mi">2</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>
        <span class="n">out</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">:</span><span class="n">sentinel</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">sin</span><span class="p">(</span><span class="n">position_enc</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">::</span><span class="mi">2</span><span class="p">]))</span>
        <span class="n">out</span><span class="p">[:,</span> <span class="n">sentinel</span><span class="p">:]</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">cos</span><span class="p">(</span><span class="n">position_enc</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">::</span><span class="mi">2</span><span class="p">]))</span>
        <span class="n">out</span><span class="p">.</span><span class="n">detach_</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">out</span>

    <span class="o">@</span><span class="n">torch</span><span class="p">.</span><span class="n">no_grad</span><span class="p">()</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">past_key_values_length</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="n">positions</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span>
            <span class="n">past_key_values_length</span><span class="p">,</span> <span class="n">past_key_values_length</span> <span class="o">+</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">long</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">weight</span><span class="p">.</span><span class="n">device</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">().</span><span class="n">forward</span><span class="p">(</span><span class="n">positions</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">Embedding</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="s">""" Class module for Roformer Embedding, word embedding &amp; rotary positional encoding
    This module has option =&gt; whether or not to use ALBERT Style Factorized Embedding

    Args:
        cfg: configuration.py

    References:
        https://arxiv.org/abs/1706.03762
        https://arxiv.org/pdf/1810.04805.pdf
        https://arxiv.org/abs/2006.16236
        https://arxiv.org/abs/2104.09864  # RoFormer: Enhanced Transformer with Rotary Position Embedding
        https://github.com/huggingface/transformers/blob/main/src/transformers/models/roformer/modeling_roformer.py
        https://github.com/idiap/fast-transformers/blob/master/fast_transformers/attention/linear_attention.py
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">cfg</span><span class="p">:</span> <span class="n">CFG</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Embedding</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span> <span class="o">=</span> <span class="n">cfg</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">batch_size</span> <span class="o">=</span> <span class="n">cfg</span><span class="p">.</span><span class="n">batch_size</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">max_seq</span> <span class="o">=</span> <span class="n">cfg</span><span class="p">.</span><span class="n">max_seq</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span> <span class="o">=</span> <span class="n">cfg</span><span class="p">.</span><span class="n">dim_model</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">word_embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Embedding</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">cfg</span><span class="p">.</span><span class="n">tokenizer</span><span class="p">),</span> <span class="n">cfg</span><span class="p">.</span><span class="n">dim_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">layer_norm1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">cfg</span><span class="p">.</span><span class="n">dim_model</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="n">cfg</span><span class="p">.</span><span class="n">layer_norm_eps</span><span class="p">)</span>  <span class="c1"># for word embedding
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">hidden_dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">cfg</span><span class="p">.</span><span class="n">hidden_dropout_prob</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">rotary_pos_encoding</span> <span class="o">=</span> <span class="n">RoFormerSinusoidalPositionalEmbedding</span><span class="p">(</span>
            <span class="n">cfg</span><span class="p">.</span><span class="n">max_seq</span><span class="p">,</span>
            <span class="n">cfg</span><span class="p">.</span><span class="n">dim_model</span> <span class="o">//</span> <span class="n">cfg</span><span class="p">.</span><span class="n">num_attention_heads</span>
        <span class="p">)</span>

        <span class="c1"># ALBERT Style Factorized Embedding
</span>        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">is_mf_embedding</span><span class="p">:</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">word_embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Embedding</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">cfg</span><span class="p">.</span><span class="n">tokenizer</span><span class="p">),</span> <span class="nb">int</span><span class="p">(</span><span class="n">cfg</span><span class="p">.</span><span class="n">dim_model</span><span class="o">/</span><span class="mi">6</span><span class="p">))</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">projector</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">cfg</span><span class="p">.</span><span class="n">dim_model</span><span class="o">/</span><span class="mi">6</span><span class="p">),</span> <span class="n">cfg</span><span class="p">.</span><span class="n">dim_model</span><span class="p">)</span>  <span class="c1"># project to original hidden dim
</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">nn</span><span class="p">.</span><span class="n">Embedding</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">is_mf_embedding</span><span class="p">:</span>
            <span class="n">word_embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">hidden_dropout</span><span class="p">(</span>
                <span class="bp">self</span><span class="p">.</span><span class="n">layer_norm1</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">projector</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">word_embedding</span><span class="p">(</span><span class="n">inputs</span><span class="p">)))</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">word_embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">hidden_dropout</span><span class="p">(</span>
                <span class="bp">self</span><span class="p">.</span><span class="n">layer_norm1</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">word_embedding</span><span class="p">(</span><span class="n">inputs</span><span class="p">))</span>
            <span class="p">)</span>
        <span class="n">rotary_pos_enc</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">rotary_pos_encoding</span><span class="p">(</span><span class="n">inputs</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
        <span class="k">return</span> <span class="n">word_embeddings</span><span class="p">,</span> <span class="n">rotary_pos_enc</span>

</code></pre></div></div>

<h4 id="-integrated-rope-into-full-attentionscaled-dot-product-attention"><code class="language-plaintext highlighter-rouge">🔨 Integrated RoPE into Full Attention(scaled dot-product attention)</code></h4>

<p><code class="language-plaintext highlighter-rouge">RoPE</code>를 적용하는 <code class="language-plaintext highlighter-rouge">Full Attention</code>의 구현 순서는 다음과 같다. 먼저, 단어 임베딩을 쿼리, 키, 벨류 행렬로 선형 투영한다. 이 때 RoPE를 곱해주기 위헤 <code class="language-plaintext highlighter-rouge">apply_rotary_position_embeddings()</code>에 인자로 쿼리, 키 행렬을 전달한다. 이 때 반드시 벨류 행렬은 단어 임베딩으로부터 선형 투영된 상태를 유지해야함을 기억하자. <code class="language-plaintext highlighter-rouge">apply_rotary_position_embeddings()</code>는 <code class="language-plaintext highlighter-rouge">RoPE</code>가 곱해진 쿼리, 키 행렬을 반환한다. 이후 과정은 퓨어한 <code class="language-plaintext highlighter-rouge">full attention</code>과 동일하다.</p>

<p>인자로 들어가는 텐서들의 모양은 주석을 참고 바란다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">apply_rotary_position_embeddings</span><span class="p">(</span><span class="n">sinusoidal_pos</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">query_layer</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">key_layer</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">value_layer</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="bp">None</span><span class="p">):</span>
    <span class="s">""" Apply rotary position encoding to query, key layer
    Original Source code from Huggingface's RoFormer model, which is the most optimized way to create positional embedding

    You can find mathematical proof in official paper's Appendix

    Args:
        sinusoidal_pos: sinusoidal positional encoding, shape [batch(None), num_dim(None), seq_len, dim_head]
        query_layer: query matrix, shape (batch_size, num_head, seq_len, dim_head)
        key_layer: key matrix, shape (batch_size, num_head, seq_len, dim_head)
        value_layer: value matrix, shape (batch_size, num_head, seq_len, dim_head)

    References:
        https://arxiv.org/abs/2104.09864  # RoFormer: Enhanced Transformer with Rotary Position Embedding
        https://github.com/huggingface/transformers/blob/main/src/transformers/models/roformer/modeling_roformer.py#L323
    """</span>
    <span class="n">sin</span><span class="p">,</span> <span class="n">cos</span> <span class="o">=</span> <span class="n">sinusoidal_pos</span><span class="p">.</span><span class="n">chunk</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># select two element of index values
</span>    <span class="n">sin_pos</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">stack</span><span class="p">([</span><span class="n">sin</span><span class="p">,</span> <span class="n">sin</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">).</span><span class="n">reshape_as</span><span class="p">(</span><span class="n">sinusoidal_pos</span><span class="p">)</span>

    <span class="n">cos_pos</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">stack</span><span class="p">([</span><span class="n">cos</span><span class="p">,</span> <span class="n">cos</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">).</span><span class="n">reshape_as</span><span class="p">(</span><span class="n">sinusoidal_pos</span><span class="p">)</span>
    <span class="n">rotate_half_query_layer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">stack</span><span class="p">([</span><span class="o">-</span><span class="n">query_layer</span><span class="p">[...,</span> <span class="mi">1</span><span class="p">::</span><span class="mi">2</span><span class="p">],</span> <span class="n">query_layer</span><span class="p">[...,</span> <span class="p">::</span><span class="mi">2</span><span class="p">]],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">).</span><span class="n">reshape_as</span><span class="p">(</span>
        <span class="n">query_layer</span>
    <span class="p">)</span>

    <span class="c1"># mathematical expression from Appendix in official repo
</span>    <span class="n">query_layer</span> <span class="o">=</span> <span class="n">query_layer</span> <span class="o">*</span> <span class="n">cos_pos</span> <span class="o">+</span> <span class="n">rotate_half_query_layer</span> <span class="o">*</span> <span class="n">sin_pos</span>
    <span class="n">rotate_half_key_layer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">stack</span><span class="p">([</span><span class="o">-</span><span class="n">key_layer</span><span class="p">[...,</span> <span class="mi">1</span><span class="p">::</span><span class="mi">2</span><span class="p">],</span> <span class="n">key_layer</span><span class="p">[...,</span> <span class="p">::</span><span class="mi">2</span><span class="p">]],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">).</span><span class="n">reshape_as</span><span class="p">(</span><span class="n">key_layer</span><span class="p">)</span>
    <span class="n">key_layer</span> <span class="o">=</span> <span class="n">key_layer</span> <span class="o">*</span> <span class="n">cos_pos</span> <span class="o">+</span> <span class="n">rotate_half_key_layer</span> <span class="o">*</span> <span class="n">sin_pos</span>

    <span class="k">if</span> <span class="n">value_layer</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>  <span class="c1"># In official, they don't use value_layer
</span>        <span class="n">rotate_half_value_layer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">stack</span><span class="p">([</span><span class="o">-</span><span class="n">value_layer</span><span class="p">[...,</span> <span class="mi">1</span><span class="p">::</span><span class="mi">2</span><span class="p">],</span> <span class="n">value_layer</span><span class="p">[...,</span> <span class="p">::</span><span class="mi">2</span><span class="p">]],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">).</span><span class="n">reshape_as</span><span class="p">(</span>
            <span class="n">value_layer</span>
        <span class="p">)</span>
        <span class="n">value_layer</span> <span class="o">=</span> <span class="n">value_layer</span> <span class="o">*</span> <span class="n">cos_pos</span> <span class="o">+</span> <span class="n">rotate_half_value_layer</span> <span class="o">*</span> <span class="n">sin_pos</span>
        <span class="k">return</span> <span class="n">query_layer</span><span class="p">,</span> <span class="n">key_layer</span><span class="p">,</span> <span class="n">value_layer</span>
    <span class="k">return</span> <span class="n">query_layer</span><span class="p">,</span> <span class="n">key_layer</span>

<span class="k">class</span> <span class="nc">MultiHeadAttention</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">dim_model</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1024</span><span class="p">,</span>
        <span class="n">num_attention_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">16</span><span class="p">,</span>
        <span class="n">dim_head</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">64</span><span class="p">,</span>
        <span class="n">kernel</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s">'softmax'</span><span class="p">,</span>
        <span class="n">attention_dropout_prob</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MultiHeadAttention</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span> <span class="o">=</span> <span class="n">dim_model</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">num_attention_heads</span> <span class="o">=</span> <span class="n">num_attention_heads</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dim_head</span> <span class="o">=</span> <span class="n">dim_head</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">fc_q</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">fc_k</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">fc_v</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">fc_concat</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">apply_rope</span> <span class="o">=</span> <span class="n">apply_rotary_position_embeddings</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">attention</span> <span class="o">=</span> <span class="n">scaled_dot_product_attention</span> <span class="k">if</span> <span class="n">kernel</span> <span class="o">==</span> <span class="s">'softmax'</span> <span class="k">else</span> <span class="n">linear_attention</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">attention_dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">attention_dropout_prob</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dot_scale</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">dim_head</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">))</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">kernel</span> <span class="o">=</span> <span class="n">kernel</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">eps</span> <span class="o">=</span> <span class="mf">1e-6</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">rotary_pos_enc</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">padding_mask</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="bp">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="s">""" x is already passed nn.Layernorm, already multiplied with rotary position encoding """</span>
        <span class="k">assert</span> <span class="n">x</span><span class="p">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">3</span><span class="p">,</span> <span class="sa">f</span><span class="s">'Expected (batch, seq, hidden) got </span><span class="si">{</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="s">'</span>

        <span class="c1"># size: bs, seq, nums head, dim head, linear projection
</span>        <span class="n">q</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">fc_q</span><span class="p">(</span><span class="n">x</span><span class="p">).</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="bp">self</span><span class="p">.</span><span class="n">num_attention_heads</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">dim_head</span><span class="p">).</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">).</span><span class="n">contiguous</span><span class="p">()</span>
        <span class="n">k</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">fc_k</span><span class="p">(</span><span class="n">x</span><span class="p">).</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="bp">self</span><span class="p">.</span><span class="n">num_attention_heads</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">dim_head</span><span class="p">).</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">).</span><span class="n">contiguous</span><span class="p">()</span>
        <span class="n">v</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">fc_v</span><span class="p">(</span><span class="n">x</span><span class="p">).</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="bp">self</span><span class="p">.</span><span class="n">num_attention_heads</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">dim_head</span><span class="p">).</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">).</span><span class="n">contiguous</span><span class="p">()</span>

        <span class="c1"># multiple word embedding, rotary position encoding
</span>        <span class="n">rotary_q</span><span class="p">,</span> <span class="n">rotary_k</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">apply_rope</span><span class="p">(</span><span class="n">rotary_pos_enc</span><span class="p">,</span> <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span>

        <span class="n">attention_matrix</span> <span class="o">=</span> <span class="bp">None</span>
        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">kernel</span> <span class="o">==</span> <span class="s">'elu'</span><span class="p">:</span>
            <span class="n">attention_matrix</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">attention</span><span class="p">(</span>
                <span class="n">rotary_q</span><span class="p">,</span>
                <span class="n">rotary_k</span><span class="p">,</span>
                <span class="n">v</span><span class="p">,</span>
                <span class="bp">self</span><span class="p">.</span><span class="n">kernel</span><span class="p">,</span>
                <span class="bp">self</span><span class="p">.</span><span class="n">eps</span><span class="p">,</span>
                <span class="bp">self</span><span class="p">.</span><span class="n">attention_dropout</span><span class="p">,</span>
                <span class="n">padding_mask</span><span class="p">,</span>
                <span class="n">attention_mask</span>
            <span class="p">)</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="p">.</span><span class="n">kernel</span> <span class="o">==</span> <span class="s">'softmax'</span><span class="p">:</span>  <span class="c1"># pure self-attention
</span>            <span class="n">attention_matrix</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">attention</span><span class="p">(</span>
                <span class="n">rotary_q</span><span class="p">,</span>
                <span class="n">rotary_k</span><span class="p">,</span>
                <span class="n">v</span><span class="p">,</span>
                <span class="bp">self</span><span class="p">.</span><span class="n">dot_scale</span><span class="p">,</span>
                <span class="bp">self</span><span class="p">.</span><span class="n">attention_dropout</span><span class="p">,</span>
                <span class="n">padding_mask</span><span class="p">,</span>
                <span class="n">attention_mask</span>
            <span class="p">)</span>

        <span class="n">attention_output</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">fc_concat</span><span class="p">(</span><span class="n">attention_matrix</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">attention_output</span>
</code></pre></div></div>]]></content><author><name>qcqced</name><email>qcqced123@gmail.com</email></author><category term="NLP" /><category term="Natural Language Process" /><category term="Roformer" /><category term="Transformation Matrix" /><category term="Complex Space" /><category term="Self-Attention" /><category term="Linear-Attention" /><category term="Pytorch" /><summary type="html"><![CDATA[Roformer Official Paper Review with Pytorch Implementation]]></summary></entry><entry><title type="html">👮 [ELECTRA] Pre-training Text Encoders as Discriminators Rather Than Generators</title><link href="http://localhost:4000/nlp/electra" rel="alternate" type="text/html" title="👮 [ELECTRA] Pre-training Text Encoders as Discriminators Rather Than Generators" /><published>2024-03-11T00:00:00+09:00</published><updated>2024-03-12T02:00:00+09:00</updated><id>http://localhost:4000/nlp/electra</id><content type="html" xml:base="http://localhost:4000/nlp/electra"><![CDATA[<h3 id="overview"><code class="language-plaintext highlighter-rouge">🔭 Overview</code></h3>

<p><code class="language-plaintext highlighter-rouge">ELECTRA</code>는 2020년 Google에서 처음 발표한 모델로, GAN(Generative Adversarial Networks) Style 아키텍처를 NLP에 적용한 것이 특징이다. 새로운 구조 차용에 맞춰서 <code class="language-plaintext highlighter-rouge">RTD(Replace Token Dection)</code> Task를 고안에 사전 학습으로 사용했다. 모든 아이디어는 기존 MLM(Masked Language Model)을 사전학습 방법론으로 사용하는 인코더 언어 모델(BERT 계열)의 단점으로부터 출발한다.</p>

<p><strong>[MLM 단점]</strong></p>
<ul>
  <li>1) 사전학습과 파인튜닝 사이 불일치
    <ul>
      <li>파인튜닝 때 Masking Task가 없음</li>
    </ul>
  </li>
  <li>2) 연산량 대비 학습량은 적은편
    <ul>
      <li>전체 시퀀스의 15%만 마스킹 활용(15%만 학습)</li>
      <li>전역 어텐션의 시공간 복잡도 고려하면 상당히 비효율적인 수치
        <ul>
          <li>시퀀스길이 ** 2의 복잡도</li>
          <li>Vocab Size만큼의 차원을 갖는 소프트맥스 계산 반복</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p>그래서 MLM은 활용하되, 파인튜닝과 괴리는 크지 않은 목적함수를 설계함으로서 입력된 전체 시퀀스에 대해서 모델이 학습하여 연산량 대비 학습량을 늘리고자 했던게 바로 ELECTRA 모델이다.</p>

<p>정리하자면, ELECTRA 모델은 기존 BERT의 구조적 측면 개선이 아닌, 사전학습 방법에 대한 개선 시도라고 볼 수 있다. 따라서 어떤 모델이더라도, 인코더 언어 모델이라면 모두 ELECTRA 구조를 사용할 수 있으며, 기존 논문에서는 원본 BERT 구조를 사용했다. 그래서 본 포스팅에서도 BERT에 대한 설명 없이 RTD에 대해서만 다루려고 한다.</p>

<h3 id="rtd-new-pre-train-task"><code class="language-plaintext highlighter-rouge">👮 RTD: New Pre-train Task</code></h3>

<p align="center">
<img src="/assets/images/electra/electra.png" alt="RTD Task" class="align-center image-caption" width="100%&quot;, height=&quot;100%" />
<strong><em><a href="https://arxiv.org/abs/2003.10555">RTD Task</a></em></strong>
</p>

<p>RTD의 아이디어는 간단하다. 생성자(Generator)가 출력으로 내놓은 토큰 시퀀스에 대해서 판별자(Discriminator)가 개별 토큰들이 원본인지 아닌지를 판정(이진 분류)하도록 만든다. 생성자는 기존의 MLM을 그대로 수행하고, 판별자는 생성자의 예측에 대해 진짜인지 가짜인지 분류하는 식이다.</p>

<p>위 그림을 예시로 살펴보자. 모델에 입력으로 <code class="language-plaintext highlighter-rouge">the chef cooked the meal</code>라는 시퀀스 준다. 그러면 MLM 규칙에 따라서 15%의 토큰이 무작위로 선택된다. 그래서 <code class="language-plaintext highlighter-rouge">the</code>, <code class="language-plaintext highlighter-rouge">cooked</code>가 마스킹 되었다. 이제 생성자는 마스킹 토큰에 대해 <code class="language-plaintext highlighter-rouge">the</code>, <code class="language-plaintext highlighter-rouge">ate</code>라는 결과를 내놓는다. 그래서 최종적으로 생성자가 반환하는 시퀀스는 <code class="language-plaintext highlighter-rouge">the chef ate the meal</code>이 된다. 이제 생성자가 반환한 시퀀스를 판별자에 입력으로 대입한다. 판별자는 개별 토큰들이 원본인지 아닌지를 판정해 결과를 출력한다.</p>

<p>이러한 구조 및 사전학습 방식의 장점은 판별자가 MLM 학습에 따른 지식을 생성자로부터 전수 받는 동시에 전체 시퀀스에 대해서 학습할 기회가 생긴다는 것이다. 시퀀스 내부 모든 토큰에 대해서 예측을 수행하고 손실을 계산할 수 있기 때문에 같은 크기의 시퀀스를 사용해도 기존 MLM 대비 더 풍부한 문맥 정보를 모델이 포착할 수 있게 된다. 또한 판별자를 파인튜닝의 BackBone으로 사용하면, 판별자의 사전학습은 결국 마스킹 없이 모든 시퀀스를 활용한 이진 분류라고 볼 수 있기 때문에, 사전학습과 파인튜닝 사이의 괴리도 상당히 많이 줄어들게 된다.</p>

<h3 id="architecture"><code class="language-plaintext highlighter-rouge">🌟 Architecture</code></h3>

<p align="center">
<img src="/assets/images/electra/electra_experiment.png" alt="Model Architecture" class="align-center image-caption" width="100%&quot;, height=&quot;100%" />
<strong><em><a href="https://arxiv.org/abs/2003.10555">Model Architecture</a></em></strong>
</p>

<p>저자는 위와 같은 실험 결과를 바탕으로, 생성자의 width (은닉층) 크기가 판별자보다 작도록 모델 크기를 세팅하는게 가장 효율적이라고 주장한다. 제시된 그래프는 생성자와 판별자의 크기 변화 대비 파인튜닝 성능의 추이를 나타낸다. 생성자의 width 크기가 256, 판별자의 width 크기가 768일 때 가장 점수가 높다. depth(레이어 개수)에 대한 언급은 따로 없지만, 저자에 의해 공개된 Hyper-Param 테이블을 보면 은닉층의 크기만 줄이고, 레이어 개수는 생성자와 판별자가 같은 것으로 추정된다.</p>

<p>추가로, 생성자와 판별자가 임베딩 층을 서로 공유하는게 가장 높은 성능을 낸다고 주장한다. 오른쪽 그래프 추이를 보면 같은 연산량이라면, 임베딩 공유(파란색 실선) 방식이 가장 높은 파인튜닝 성능을 보여준다는 것을 알 수 있다. 따라서 단어 임베딩, 절대 위치 임베딩을 서로 공유하도록 설계한다. 대신 생성자 은닉층의 크기가 더 작은게 유리하다고 언급했기 때문에, 이것을 실제로 구현하려면 임베딩 층으로부터 나온 결과값을 생성자의 은닉층 차원으로 선형 투영해줘야 한다. 그래서 생성자의 임베딩 층과 인코더 사이에 linear layer가 추가되어야 한다.</p>

\[\min_{\theta_G, \theta_D}\sum_{x \in X} \mathcal{L}_{\text{MLM}}(x, \theta_G) + \lambda \mathcal{L}_{\text{Disc}}(x, \theta_D)\]

<p>따라서, 지금까지 살펴본 모든 내용을 종합해보면 ELECTRA의 목적함수는 다음 수식과 같다. 생성자의 MLM 손실과 판별자의 이진 분류 손실을 더해서 모델에 오차 역전해주면 되는데, 특이한 점은 판별자의 손실에 상수값인 람다가 곱해진다는 것이다. 실제 모델을 구현하고 사전학습을 해보면, 데이터의 양이나 모델 크기 혹은 종류에 따라 달라지겠지만 두 손실 사이의 스케일의 차이가 10배정도 차이 나게 된다. 두 손실의 스케일을 맞춰주는 동시에, 임베딩층의 학습이 판별자의 손실을 줄이는데 더 집중하도록 만들기 위해 도입한 것으로 추정된다. 논문과 코드를 보면 저자는 $\lambda=50$ 으로 두고 학습하고 있다.</p>

<h3 id="implementation-by-pytorch"><code class="language-plaintext highlighter-rouge">👩‍💻 Implementation by Pytorch</code></h3>

<p>논문의 내용과 저자가 직접 공개한 코드를 종합하여 파이토치로 ELECTRA를 구현해봤다. 두 개의 서로 다른 모델을 같은 스탭에서 학습시켜야 하기 때문에, 제시된 내용에 비해 실제 구현은 매우 까다로운 편이었다. 본 포스팅에서는 ELECTRA 모델 구조를 비롯해 RTD 학습 파이프라인 구성에 필수적인 요소 몇 가지에 대해서만 설명하려 한다. 전체 구조에 대한 코드는 <strong><a href="https://github.com/qcqced123/model_study">여기 링크</a></strong>를 통해 참고 부탁드린다.</p>

<p>ELECTRA의 사전 학습인 RTD의 학습 파이프라인을 구현한 코드를 본 뒤, 세부 구성 요소들에 대해서 살펴보자.</p>

<h4 id="-rtd-trainer-method"><strong><code class="language-plaintext highlighter-rouge">🌆 RTD trainer method</code></strong></h4>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">train_val_fn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">loader_train</span><span class="p">,</span> <span class="n">model</span><span class="p">:</span> <span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">criterion</span><span class="p">:</span> <span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">scheduler</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]:</span>
  <span class="n">scaler</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">amp</span><span class="p">.</span><span class="n">GradScaler</span><span class="p">(</span><span class="n">enabled</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">amp_scaler</span><span class="p">)</span>
  <span class="n">model</span><span class="p">.</span><span class="n">train</span><span class="p">()</span>
  <span class="k">for</span> <span class="n">step</span><span class="p">,</span> <span class="n">batch</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">tqdm</span><span class="p">(</span><span class="n">loader_train</span><span class="p">)):</span>
      <span class="n">optimizer</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">(</span><span class="n">set_to_none</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
      <span class="n">inputs</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="s">'input_ids'</span><span class="p">].</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">device</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
      <span class="n">labels</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="s">'labels'</span><span class="p">].</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">device</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>  
      <span class="n">padding_mask</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="s">'padding_mask'</span><span class="p">].</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">device</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>  

      <span class="n">mask_labels</span> <span class="o">=</span> <span class="bp">None</span>
      <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">rtd_masking</span> <span class="o">==</span> <span class="s">'SpanBoundaryObjective'</span><span class="p">:</span>
          <span class="n">mask_labels</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="s">'mask_labels'</span><span class="p">].</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">device</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

      <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">amp</span><span class="p">.</span><span class="n">autocast</span><span class="p">(</span><span class="n">enabled</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">amp_scaler</span><span class="p">):</span>
          <span class="n">g_logit</span><span class="p">,</span> <span class="n">d_inputs</span><span class="p">,</span> <span class="n">d_labels</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">generator_fw</span><span class="p">(</span>
              <span class="n">inputs</span><span class="p">,</span>
              <span class="n">labels</span><span class="p">,</span>
              <span class="n">padding_mask</span><span class="p">,</span>
              <span class="n">mask_labels</span>
          <span class="p">)</span>
          <span class="n">d_logit</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">discriminator_fw</span><span class="p">(</span>
              <span class="n">d_inputs</span><span class="p">,</span>
              <span class="n">padding_mask</span>
          <span class="p">)</span>
          <span class="n">g_loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">g_logit</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">vocab_size</span><span class="p">),</span> <span class="n">labels</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>
          <span class="n">d_loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">d_logit</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">d_labels</span><span class="p">)</span>
          <span class="n">loss</span> <span class="o">=</span> <span class="n">g_loss</span> <span class="o">+</span> <span class="n">d_loss</span><span class="o">*</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">discriminator_lambda</span>

      <span class="n">scaler</span><span class="p">.</span><span class="n">scale</span><span class="p">(</span><span class="n">loss</span><span class="p">).</span><span class="n">backward</span><span class="p">()</span>
      <span class="n">scaler</span><span class="p">.</span><span class="n">step</span><span class="p">(</span><span class="n">optimizer</span><span class="p">)</span>
      <span class="n">scaler</span><span class="p">.</span><span class="n">update</span><span class="p">()</span>
      <span class="n">scheduler</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>
</code></pre></div></div>
<p>데이터로더로부터 받은 입력들을 생성자에 넣고 MLM 예측 결과, RTD 수행을 위해 필요한 새로운 라벨값을 반환 받는다. 그리고 이것을 다시 판별자의 입력으로 사용하고, 판별자의 예측 결과를 반환받아 서로 다른 두 모델에 대한 가중 손실합산을 구한 뒤, 옵티마이저에 보내고 최적화를 수행한다. 이 때, 처음에 데이터로더가 반환하는 입력 시퀀스와 라벨은 MLM의 그것과 동일하다,</p>

<p>구현하면서 가장 어려웠던게, 옵티마이저 및 스케줄러의 구성이었다. 두 개의 모델을 같은 스탭에서 학습시키는 경험이 처음이라서 처음에 모델 개수만큼 옵티마이저와 스케줄러 객체를 만들어줘야 한다고 생각했다. 특히 두 모델의 스케일이 다르기 때문에 서로 다른 옵티마이저, 스케줄러 도입으로 각기 다른 학습률을 적용하는게 정확할 것이라 생각했다.</p>

<p>하지만, 옵티마이저를 두 개 사용하는 것은 매우 많은 메모리를 차지할 뿐더러 논문에서 공개한 하이퍼파라미터 테이블을 보면 두 모델에 같은 학습률을 적용하고 있는 것을 알 수 있었다. 따라서 그에 맞게 같은 옵티마이저, 스케줄러를 사용해 동시에 두 모델이 학습되도록 파이프라인을 만들게 되었다.</p>

<p>추가로, 공개된 오피셜 코드 역시 단일 옵티마이저 및 스케줄러를 사용하는 것을 확인했다.</p>

<h4 id="-electra-module"><strong><code class="language-plaintext highlighter-rouge">🌆 ELECTRA Module</code></strong></h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="nn">experiment.models.abstract_model</span> <span class="kn">import</span> <span class="n">AbstractModel</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">Tensor</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">Callable</span>
<span class="kn">from</span> <span class="nn">einops.layers.torch</span> <span class="kn">import</span> <span class="n">Rearrange</span>
<span class="kn">from</span> <span class="nn">experiment.tuner.mlm</span> <span class="kn">import</span> <span class="n">MLMHead</span>
<span class="kn">from</span> <span class="nn">experiment.tuner.sbo</span> <span class="kn">import</span> <span class="n">SBOHead</span>
<span class="kn">from</span> <span class="nn">experiment.tuner.rtd</span> <span class="kn">import</span> <span class="n">get_discriminator_input</span><span class="p">,</span> <span class="n">RTDHead</span>
<span class="kn">from</span> <span class="nn">configuration</span> <span class="kn">import</span> <span class="n">CFG</span>

<span class="k">class</span> <span class="nc">ELECTRA</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">AbstractModel</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">cfg</span><span class="p">:</span> <span class="n">CFG</span><span class="p">,</span> <span class="n">model_func</span><span class="p">:</span> <span class="n">Callable</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">ELECTRA</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span> <span class="o">=</span> <span class="n">cfg</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">generator</span> <span class="o">=</span> <span class="n">model_func</span><span class="p">(</span><span class="n">cfg</span><span class="p">.</span><span class="n">generator_num_layers</span><span class="p">)</span>  <span class="c1"># init generator
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">mlm_head</span> <span class="o">=</span> <span class="n">MLMHead</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">rtd_masking</span> <span class="o">==</span> <span class="s">'SpanBoundaryObjective'</span><span class="p">:</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">mlm_head</span> <span class="o">=</span> <span class="n">SBOHead</span><span class="p">(</span>
                <span class="n">cfg</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">,</span>
                <span class="n">is_concatenate</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">is_concatenate</span><span class="p">,</span>
                <span class="n">max_span_length</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">max_span_length</span>
            <span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">discriminator</span> <span class="o">=</span> <span class="n">model_func</span><span class="p">(</span><span class="n">cfg</span><span class="p">.</span><span class="n">discriminator_num_layers</span><span class="p">)</span>  <span class="c1"># init generator
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">rtd_head</span> <span class="o">=</span> <span class="n">RTDHead</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">share_embed_method</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">share_embed_method</span>  <span class="c1"># instance, es, gdes
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">share_embedding</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">share_embedding</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="k">def</span> <span class="nf">discriminator_hook</span><span class="p">(</span><span class="n">module</span><span class="p">:</span> <span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">,</span> <span class="o">*</span><span class="n">inputs</span><span class="p">):</span>
            <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">share_embed_method</span> <span class="o">==</span> <span class="s">'instance'</span><span class="p">:</span>  <span class="c1"># Instance Sharing
</span>                <span class="bp">self</span><span class="p">.</span><span class="n">discriminator</span><span class="p">.</span><span class="n">embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">generator</span><span class="p">.</span><span class="n">embeddings</span>

            <span class="k">elif</span> <span class="bp">self</span><span class="p">.</span><span class="n">share_embed_method</span> <span class="o">==</span> <span class="s">'ES'</span><span class="p">:</span>  <span class="c1"># ES (Embedding Sharing)
</span>                <span class="bp">self</span><span class="p">.</span><span class="n">discriminator</span><span class="p">.</span><span class="n">embeddings</span><span class="p">.</span><span class="n">word_embedding</span><span class="p">.</span><span class="n">weight</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">generator</span><span class="p">.</span><span class="n">embeddings</span><span class="p">.</span><span class="n">word_embedding</span><span class="p">.</span><span class="n">weight</span>
                <span class="bp">self</span><span class="p">.</span><span class="n">discriminator</span><span class="p">.</span><span class="n">embeddings</span><span class="p">.</span><span class="n">abs_pos_emb</span><span class="p">.</span><span class="n">weight</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">generator</span><span class="p">.</span><span class="n">embeddings</span><span class="p">.</span><span class="n">abs_pos_emb</span><span class="p">.</span><span class="n">weight</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">discriminator</span><span class="p">.</span><span class="n">register_forward_pre_hook</span><span class="p">(</span><span class="n">discriminator_hook</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">generator_fw</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span>
            <span class="n">inputs</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
            <span class="n">labels</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
            <span class="n">padding_mask</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
            <span class="n">mask_labels</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
            <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="bp">None</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
        <span class="n">g_last_hidden_states</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">generator</span><span class="p">(</span>
            <span class="n">inputs</span><span class="p">,</span>
            <span class="n">padding_mask</span><span class="p">,</span>
            <span class="n">attention_mask</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">rtd_masking</span> <span class="o">==</span> <span class="s">'MaskedLanguageModel'</span><span class="p">:</span>
            <span class="n">g_logit</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">mlm_head</span><span class="p">(</span>
                <span class="n">g_last_hidden_states</span>
            <span class="p">)</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">rtd_masking</span> <span class="o">==</span> <span class="s">'SpanBoundaryObjective'</span><span class="p">:</span>
            <span class="n">g_logit</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">mlm_head</span><span class="p">(</span>
                <span class="n">g_last_hidden_states</span><span class="p">,</span>
                <span class="n">mask_labels</span>
            <span class="p">)</span>
        <span class="n">pred</span> <span class="o">=</span> <span class="n">g_logit</span><span class="p">.</span><span class="n">clone</span><span class="p">().</span><span class="n">detach</span><span class="p">()</span>
        <span class="n">d_inputs</span><span class="p">,</span> <span class="n">d_labels</span> <span class="o">=</span> <span class="n">get_discriminator_input</span><span class="p">(</span>
            <span class="n">inputs</span><span class="p">,</span>
            <span class="n">labels</span><span class="p">,</span>
            <span class="n">pred</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">g_logit</span><span class="p">,</span> <span class="n">d_inputs</span><span class="p">,</span> <span class="n">d_labels</span>

    <span class="k">def</span> <span class="nf">discriminator_fw</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span>
            <span class="n">inputs</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
            <span class="n">padding_mask</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
            <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="bp">None</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="n">d_last_hidden_states</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">discriminator</span><span class="p">(</span>
            <span class="n">inputs</span><span class="p">,</span>
            <span class="n">padding_mask</span><span class="p">,</span>
            <span class="n">attention_mask</span>
        <span class="p">)</span>
        <span class="n">d_logit</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">rtd_head</span><span class="p">(</span>
            <span class="n">d_last_hidden_states</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">d_logit</span>

</code></pre></div></div>
<p>ELECTRA 모델 객체는 크게 임배딩 레이어 공유, 생성자 포워드, 판별자 포워드 파트로 나뉜다. 먼저 임베딩 레이어 공유는 크게 두 가지 방식으로 구현 가능하다. 하나는 임베딩 레이어 인스턴스 자체를 공유하는 방식으로, 생성자와 판별자의 스케일이 동일할 때 사용할 수 있다. 나머지는 단어 임베딩, 포지션 임베딩의 가중치 행렬만 공유하는 방식으로, 서로 스케일이 달라도 사용할 수 있다. 논문에서 제시하는 가장 효율적인 방법은 후자이며, 판별자의 임베딩 행렬이 생성자의 임베딩 행렬의 주소를 가리키도록 함으로서 구현 가능하다.</p>

<h4 id="-rtd-input-making"><strong><code class="language-plaintext highlighter-rouge">🌆 RTD Input Making</code></strong></h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">Tensor</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Tuple</span>
<span class="kn">from</span> <span class="nn">configuration</span> <span class="kn">import</span> <span class="n">CFG</span>

<span class="k">def</span> <span class="nf">get_discriminator_input</span><span class="p">(</span><span class="n">inputs</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">labels</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">pred</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
    <span class="s">""" Post Processing for Replaced Token Detection Task
    1) get index of the highest probability of [MASK] token in pred tensor
    2) convert [MASK] token to prediction token
    3) make label for Discriminator

    Args:
        inputs: pure inputs from tokenizing by tokenizer
        labels: labels for masked language modeling
        pred: prediction tensor from Generator

    returns:
        d_inputs: torch.Tensor, shape of [Batch, Sequence], for Discriminator inputs
        d_labels: torch.Tensor, shape of [Sequence], for Discriminator labels
    """</span>
    <span class="c1"># 1) flatten pred to 2D Tensor
</span>    <span class="n">d_inputs</span><span class="p">,</span> <span class="n">d_labels</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">.</span><span class="n">clone</span><span class="p">().</span><span class="n">detach</span><span class="p">().</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="bp">None</span>  <span class="c1"># detach to prevent back-propagation
</span>    <span class="n">flat_pred</span><span class="p">,</span> <span class="n">flat_label</span> <span class="o">=</span> <span class="n">pred</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">pred</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)),</span> <span class="n">labels</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># (batch * sequence, vocab_size)
</span>
    <span class="c1"># 2) get index of the highest probability of [MASK] token
</span>    <span class="n">pred_token_idx</span><span class="p">,</span> <span class="n">mlm_mask_idx</span> <span class="o">=</span> <span class="n">flat_pred</span><span class="p">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">),</span> <span class="n">torch</span><span class="p">.</span><span class="n">where</span><span class="p">(</span><span class="n">flat_label</span> <span class="o">!=</span> <span class="o">-</span><span class="mi">100</span><span class="p">)</span>
    <span class="n">pred_tokens</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">index_select</span><span class="p">(</span><span class="n">pred_token_idx</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">mlm_mask_idx</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

    <span class="c1"># 3) convert [MASK] token to prediction token
</span>    <span class="n">d_inputs</span><span class="p">[</span><span class="n">mlm_mask_idx</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span> <span class="o">=</span> <span class="n">pred_tokens</span>

    <span class="c1"># 4) make label for Discriminator
</span>    <span class="n">original_tokens</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">.</span><span class="n">clone</span><span class="p">().</span><span class="n">detach</span><span class="p">().</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">original_tokens</span><span class="p">[</span><span class="n">mlm_mask_idx</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span> <span class="o">=</span> <span class="n">flat_label</span><span class="p">[</span><span class="n">mlm_mask_idx</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span>
    <span class="n">d_labels</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">eq</span><span class="p">(</span><span class="n">original_tokens</span><span class="p">,</span> <span class="n">d_inputs</span><span class="p">).</span><span class="nb">long</span><span class="p">()</span>
    <span class="n">d_inputs</span> <span class="o">=</span> <span class="n">d_inputs</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="n">pred</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># covert to [batch, sequence]
</span>    <span class="k">return</span> <span class="n">d_inputs</span><span class="p">,</span> <span class="n">d_labels</span>
</code></pre></div></div>
<p>이제 마지막으로 판별자의 입력을 만드는 알고리즘에 대한 코드를 보자. 알고리즘은 다음과 같다.</p>
<ul>
  <li>1) 개별 마스킹 토큰에 대한 예측 토큰 구하기
    <ul>
      <li>로짓을 실제 토큰 인덱스로 변환</li>
    </ul>
  </li>
  <li>2) 모든 마스킹 부분에 예측 토큰들로 대체</li>
  <li>3) 기존 입력과 2번으로 만들어진 시퀀스 비교해 라벨 생성
    <ul>
      <li>서로 같으면 0</li>
      <li>서로 다르면 1
이렇게 만들어진 새로운 입력 시퀀스와 라벨을 ELECTRA 모델 인스턴스의 판별자 포워드 메서드에 인자로 전달하면 된다.</li>
    </ul>
  </li>
</ul>

<h3 id="-future-work-읽고-구현하면서-느낀점--개선방향"><code class="language-plaintext highlighter-rouge">🌟 Future Work (읽고 구현하면서 느낀점 &amp; 개선방향)</code></h3>

<p>이렇게 ELECTRA 모델에 대한 구현까지 살펴봤다. 논문을 읽고 구현하면서 가장 의문스러웠던 부분은 임베딩 공유 방법이었다. 수학적으로 엄밀하게 계산하고 따져보지 못했지만, 직관적으로도 생성자의 MLM과 판별자의 RTD는 서로 성격이 상당히 다른 사전 학습 방법론이라는 것을 알 수 있다. 그렇다면 단순히 단어, 포지션 임베딩을 공유하는 경우 학습 방향성이 달라서 간섭이 발생하고 모델이 수렴하지 못할 여지가 생긴다. 이러한 <code class="language-plaintext highlighter-rouge">줄다리기 현상(tag-of-war)</code>을 어떻게 해결할 수 있을까에 대한 고민이 더 필요하다고 생각한다.</p>

<p>그래서 다음 포스팅에서는 이러한 줄다리기 현상을 해결하고자한 논문인 <strong><a href="https://arxiv.org/abs/2111.09543">&lt;DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing&gt;</a></strong>을 리뷰해보고자 한다.</p>]]></content><author><name>qcqced</name><email>qcqced123@gmail.com</email></author><category term="NLP" /><category term="Natural Language Process" /><category term="ELECTRA" /><category term="BERT" /><category term="GAN" /><category term="Transformer" /><category term="Self-Attention" /><category term="Pytorch" /><summary type="html"><![CDATA[ELECTRA Official Paper Review with Pytorch Implementation]]></summary></entry><entry><title type="html">🧑‍🏫 [DistilBERT] DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter</title><link href="http://localhost:4000/nlp/distilbert" rel="alternate" type="text/html" title="🧑‍🏫 [DistilBERT] DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter" /><published>2024-03-11T00:00:00+09:00</published><updated>2024-03-12T02:00:00+09:00</updated><id>http://localhost:4000/nlp/distilbert</id><content type="html" xml:base="http://localhost:4000/nlp/distilbert"><![CDATA[<h3 id="overview"><code class="language-plaintext highlighter-rouge">🔭 Overview</code></h3>

<p><code class="language-plaintext highlighter-rouge">DistilBERT</code> 는 허깅 페이스 연구진이 2019년 발표한 BERT의 변형으로서, On-Device Ai 개발을 목표로 경량화에 초점을 맞춘 모델이다. GPT, BERT의 등장 이후, NLP 분야에서 비약적인 성능 향상이 이뤄졌음에도 불구하고, 터무니 없는 모델 사이즈와 컴퓨팅 리소스 요구로 인해 실생활 적용 같은 활용성은 여전히 해결해야할 문제로 남아 있었다. Google에서 발표한 초기 <code class="language-plaintext highlighter-rouge">BERT-base-uncased</code> 만 해도 파라미터가 1억 1천만개 수준에 달한다.</p>

<p>이를 다양한 비즈니스 요구 상황에 적용할 수 있으려면 최소한 8GB 이상의 가속기 전용 RAM 공간을 요구로 한다. 오늘날 개인용 PC 혹은 서버 컴퓨터의 경우, 8GB 이상의 VRAM이 달린 GPU가 일반적으로 탑재되기 때문에 크게 문제 될 것 없는 요구사항이지만, On-Device 환경에서는 이야기가 달라진다. 최신 하이엔드 스마트폰인 Galaxy S24 Ultra, iPhone 15 Pro의 경우 12GB, 8GB의 램 용량을 보유하고 있다. 그마저도 대부분의 온디바이스 환경은 SoC 구조를 채택하고 있기 때문에 전용 가속기가 온전히 저 모든 램 공간을 활용할 수 없다.</p>

<p>따라서 온디바이스에 Ai를 적용하기 위해서는 획기적인 모델 경량화가 필요한 상황이고 그 출발점이 된 연구가 바로 <code class="language-plaintext highlighter-rouge">DistilBERT</code>다. 로컬 디바이스 환경에서도 언어 모델을 활용하기 위해 허깅 페이스 연구진은 지식 증류 기법을 활용해 인코더 기반 언어 모델의 파라미터를 획기적으로 줄이는데 성공한다.</p>

<p>정리하자면, <code class="language-plaintext highlighter-rouge">DistilBERT</code> 모델은 기존 BERT의 구조적 측면 개선이 아닌, 사전학습 방법 특히 경량화에 초점을 맞춘 시도라고 볼 수 있다. 따라서 어떤 모델이더라도, 인코더 언어 모델이라면 모두 <code class="language-plaintext highlighter-rouge">DistilBERT</code> 구조를 사용할 수 있으며, 기존 논문에서는 원본 BERT 구조를 사용했다. 이번 포스팅에서도 BERT 구조에 대한 설명 대신, <code class="language-plaintext highlighter-rouge">DistilBERT</code>의 사전 학습 방법론인 <code class="language-plaintext highlighter-rouge">Knowledge Distillation</code>에 대해서만 다루려고 한다.</p>

<h3 id="knowledge-distillations"><code class="language-plaintext highlighter-rouge">📲 Knowledge Distillations</code></h3>

\[\min_{\theta}\sum_{x \in X} \alpha \mathcal{L}_{\text{KL}}(x, \theta) + \beta \mathcal{L}_{\text{MLM}}(x, \theta) + \gamma \mathcal{L}_{\text{Cos}}(x, \theta)\]

<p><code class="language-plaintext highlighter-rouge">DistilBERT</code>는 Teacher-Student Architecture를 차용해 상대적으로 작은 파라미터 사이즈를 갖는 <code class="language-plaintext highlighter-rouge">Student</code> 모델에게 <code class="language-plaintext highlighter-rouge">Teacher</code>의 지식을 전수하는 것을 목표로 한다. 따라서 <code class="language-plaintext highlighter-rouge">Teacher</code> 모델은 이미 사전 학습을 마치고 수렴된 상태의 가중치를 갖고 있는 모델을 사용해야 한다. 더불어 Teacher 모델은 구조만 기존 BERT를 따르되, 사전 학습 방식은 RoBERTa의 방식과 동일(NSP 제거, Dynamic Masking 적용)하게 훈련되어야 한다.</p>

<p>한편, <code class="language-plaintext highlighter-rouge">Student</code> 모델은 <code class="language-plaintext highlighter-rouge">Teacher</code>의 60%정도 파라미터 사이즈를 갖도록 축소하여 사용한다. 이 때 축소는 모델의 <code class="language-plaintext highlighter-rouge">depth</code>(레이어 개수)에만 적용하는데, 연구진에 따르면 <code class="language-plaintext highlighter-rouge">width</code>(은닉층 크기)는 축소를 적용해도 연산 효율이 증가하지 않는다고 한다. 정리하면 <code class="language-plaintext highlighter-rouge">Teacher</code> 모델의 <code class="language-plaintext highlighter-rouge">레이어 개수*0.6</code>의 개수만큼 인코더를 쌓으면 된다는 것이다.</p>

<p>그리고 최대한 <code class="language-plaintext highlighter-rouge">Teacher</code>의 지식을 전수해야 하기 때문에, 데이터는 <code class="language-plaintext highlighter-rouge">Teacher</code> 를 수렴시킨 것과 동일한 세트를 이용해야 한다. 이 때, Teacher 모델은 이미 MLE 방식으로 훈련이 된 상태라서 로짓이 단일 토큰 하나 쪽으로 쏠려 있을 가능성이 매우 높다. 이는 <code class="language-plaintext highlighter-rouge">Student</code> 모델의 일반화 능력에 악영향을 미칠 가능성이 높다. 따라서 Temperature 변수 $T$ 도입해 소프트 맥스(로짓)의 분포를 평탄화 한다. 이렇게 하면, <code class="language-plaintext highlighter-rouge">argmax()</code> 가 아닌 다른 토큰 표현에 대해서도 <code class="language-plaintext highlighter-rouge">Student</code> 모델이 지식을 습득할 수 있어서 풍부한 문맥을 학습하고 일반화 능력을 높이는데 도움이 된다. 이를 <code class="language-plaintext highlighter-rouge">암흑 지식(Dark Knowledge)</code> 을 활용한다고 표현한다. Temperature 변수 $T$ 도입한 소프트맥스 함수 수식은 아래와 같다.</p>

\[\text{softmax}(x_i) = \frac{e^{\frac{x_i}{\tau}}}{\sum_{j} e^{\frac{x_j}{\tau}}}\]

<p>수식상 변수 $T$의 값을 1이상으로 세팅해야 평탄화를 할 수 있다. 따라서 연구진은 $T =2$ 로 두고 사전 학습을 진행했다(논문에 공개안됨, GitHub에 있음). 이번 파트 맨 처음에 등장한 수식을 다시 보자. 결국 <code class="language-plaintext highlighter-rouge">DisilBERT</code>의 목적함수는 3가지 손실의 가중합으로 구성된다. 이제부터는 개별 손실에 대해서 자세히 살펴보자.</p>

<h4 id="distillation-loss-kl-divergence-loss"><code class="language-plaintext highlighter-rouge">🪢 Distillation Loss: KL-Divergence Loss</code></h4>

\[\text{KL-Divergence}(P || Q) = \sum_{i} P(i) \log \frac{P(i)}{Q(i)}\]

<p>증류 손실로 사용되는 <code class="language-plaintext highlighter-rouge">KL-Divergence Loss</code>는 두 확률 분포 간의 차이를 측정하는 지표 중 하나다. 주로 확률 분포 P와 Q 사이의 차이를 나타내는데, 개별 요소의 확률값 차이가 클수록 합산값은 커져 손실이 커지게 된다. 반대로 두 분포의 개별 요소 확률값 차이가 작다면 당연히, 두 분포가 유사하다는 의미이므로 손실 역시 작아지게 된다. 일반적으로 <code class="language-plaintext highlighter-rouge">KL-Divergence Loss</code> 에서 확률분포 $P$ 가 이상적인 확률 분포를, $Q$ 가 모델이 예측한 확률분포를 의미한다. 따라서 <code class="language-plaintext highlighter-rouge">DistilBERT</code>의 경우 확률분포 $P$ 자리에는 <code class="language-plaintext highlighter-rouge">Teacher</code> 모델의 소프트맥스 분포가, $Q$ 에는 <code class="language-plaintext highlighter-rouge">Student</code> 모델의 소프트맥스 분포가 대입되면 된다. 이 때 두 확률분포 모두, 암흑 지식 획득을 위해 소프트맥스 평탄화를 적용한 결과를 사용한다. 논문에서, 선생 모델 예측에 평탄화를 적용한 것을 <code class="language-plaintext highlighter-rouge">소프트 라벨</code>, 학생 모델의 것에 적용한 결과는 <code class="language-plaintext highlighter-rouge">소프트 예측</code>이라고 부른다.</p>

<h4 id="student-loss-mlm-loss"><code class="language-plaintext highlighter-rouge">🧑‍🎓 Student Loss: MLM Loss</code></h4>

\[\mathcal{L}_{\text{MLM}} = - \sum_{i=1}^{N} \sum_{j=1}^{L} \mathbb{1}_{m_{ij}} \log \text{softmax}(x_{ij})\]

<p>학생 손실은 말그대로 기본적인 MLM 손실을 말한다. 정확한 손실값 계산을 위해서 학생의 소프트맥스 분포에 평탄화를 적용하지 않는다. 이를 논문에서는 <code class="language-plaintext highlighter-rouge">하드 예측</code>이라고 부른다. 라벨 역시 <code class="language-plaintext highlighter-rouge">Teacher</code>로부터 나온 것이 아닌 원래 MLM 수행에 사용되는 마스킹 라벨을 사용한다.</p>

<h4 id="cosine-embedding-loss-contrastive-loss-by-cosine-similarity"><code class="language-plaintext highlighter-rouge">🌆 Cosine Embedding Loss: Contrastive Loss by cosine similarity</code></h4>

\[\mathcal{L}_{\text{COS}}(x,y) = \begin{cases} 1 - \cos(x_1, x_2), &amp; \text{if } y = 1 \\ \max(0, \cos(x_1, x_2) - \text{margin}), &amp; \text{if } y = -1 \end{cases}\]

<p><code class="language-plaintext highlighter-rouge">Teacher</code> 모델과 <code class="language-plaintext highlighter-rouge">Student</code> 모델의 마지막 인코더 모델이 출력하는 은닉값에 대한 <code class="language-plaintext highlighter-rouge">Contrastive Loss</code>를 의미한다. 이 때 <code class="language-plaintext highlighter-rouge">Distance Metric</code>은 코사인 유사도를 사용한다. 그래서 코사인 임베딩 손실이라고 논문에서 정의하는 것으로 추정된다. 위 수식을 최적화하는 것을 목적으로 한다. 이 때 라벨은 <code class="language-plaintext highlighter-rouge">[BS, Seq_len]</code>의 크기를 갖되, 모든 원소는 1이 되도록 만든다. 이유는 간단하다. <code class="language-plaintext highlighter-rouge">Student</code> 모델의 은닉값이 <code class="language-plaintext highlighter-rouge">Teacher</code> 모델의 것과 최대한 비슷해지도록 만드는게 우리 목적이기 때문이다.</p>

<h3 id="implementation-by-pytorch"><code class="language-plaintext highlighter-rouge">👩‍💻 Implementation by Pytorch</code></h3>
<p>논문의 내용과 오피셜로 공개된 코드를 종합하여 파이토치로 <code class="language-plaintext highlighter-rouge">DistilBERT</code>를 구현해봤다. 논문에 포함된 아이디어를 이해하는데는 역시 어렵지 않았지만, 페이퍼에 hyper-param 테이블이 따로 제시되어 있지 않아 공개된 코드를 안 볼수가 없었다.</p>

<p>전체 모델 구조 대한 코드는 <strong><a href="https://github.com/qcqced123/model_study">여기 링크</a></strong>를 통해 참고바란다.</p>

<h4 id="knowledge-distillation-pipeline"><code class="language-plaintext highlighter-rouge">👩‍💻 Knowledge Distillation Pipeline</code></h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">train_val_fn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">loader_train</span><span class="p">,</span> <span class="n">model</span><span class="p">:</span> <span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">criterion</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">],</span> <span class="n">optimizer</span><span class="p">,</span><span class="n">scheduler</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]:</span>
    <span class="s">""" Function for train loop with validation for each batch*N Steps
    DistillBERT has three loss:

        1) distillation loss, calculated by soft targets &amp; soft predictions
            (nn.KLDIVLoss(reduction='batchmean'))

        2) student loss, calculated by hard targets &amp; hard predictions
            (nn.CrossEntropyLoss(reduction='mean')), same as pure MLM Loss

        3) cosine similarity loss, calculated by student &amp; teacher logit similarity
            (nn.CosineEmbeddingLoss(reduction='mean')), similar as contrastive loss

    Those 3 losses are summed jointly and then backward to student model
    """</span>
    <span class="n">scaler</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">amp</span><span class="p">.</span><span class="n">GradScaler</span><span class="p">(</span><span class="n">enabled</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">amp_scaler</span><span class="p">)</span>
    <span class="n">model</span><span class="p">.</span><span class="n">train</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">step</span><span class="p">,</span> <span class="n">batch</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">tqdm</span><span class="p">(</span><span class="n">loader_train</span><span class="p">)):</span>
        <span class="n">optimizer</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">(</span><span class="n">set_to_none</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="n">inputs</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="s">'input_ids'</span><span class="p">].</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">device</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="n">labels</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="s">'labels'</span><span class="p">].</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">device</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="n">padding_mask</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="s">'padding_mask'</span><span class="p">].</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">device</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

        <span class="n">mask</span> <span class="o">=</span> <span class="n">padding_mask</span><span class="p">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">).</span><span class="n">expand</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">dim_model</span><span class="p">)</span>  <span class="c1"># for hidden states dim
</span>        <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">t_hidden_state</span><span class="p">,</span> <span class="n">soft_target</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">teacher_fw</span><span class="p">(</span>
                <span class="n">inputs</span><span class="o">=</span><span class="n">inputs</span><span class="p">,</span>
                <span class="n">padding_mask</span><span class="o">=</span><span class="n">padding_mask</span><span class="p">,</span>
                <span class="n">mask</span><span class="o">=</span><span class="n">mask</span>
            <span class="p">)</span>  <span class="c1"># teacher model's pred =&gt; hard logit
</span>
        <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">amp</span><span class="p">.</span><span class="n">autocast</span><span class="p">(</span><span class="n">enabled</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">amp_scaler</span><span class="p">):</span>
            <span class="n">s_hidden_state</span><span class="p">,</span> <span class="n">s_logit</span><span class="p">,</span> <span class="n">soft_pred</span><span class="p">,</span> <span class="n">c_labels</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">student_fw</span><span class="p">(</span>
                <span class="n">inputs</span><span class="o">=</span><span class="n">inputs</span><span class="p">,</span>
                <span class="n">padding_mask</span><span class="o">=</span><span class="n">padding_mask</span><span class="p">,</span>
                <span class="n">mask</span><span class="o">=</span><span class="n">mask</span>
            <span class="p">)</span>
            <span class="n">d_loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">[</span><span class="s">"KLDivLoss"</span><span class="p">](</span><span class="n">soft_pred</span><span class="p">.</span><span class="n">log</span><span class="p">(),</span> <span class="n">soft_target</span><span class="p">)</span>  <span class="c1"># nn.KLDIVLoss
</span>            <span class="n">s_loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">[</span><span class="s">"CrossEntropyLoss"</span><span class="p">](</span><span class="n">s_logit</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">vocab_size</span><span class="p">),</span> <span class="n">labels</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>  <span class="c1"># nn.CrossEntropyLoss
</span>            <span class="n">c_loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">[</span><span class="s">"CosineEmbeddingLoss"</span><span class="p">](</span><span class="n">s_hidden_state</span><span class="p">,</span> <span class="n">t_hidden_state</span><span class="p">,</span> <span class="n">c_labels</span><span class="p">)</span>  <span class="c1"># nn.CosineEmbeddingLoss
</span>            <span class="n">loss</span> <span class="o">=</span> <span class="n">d_loss</span><span class="o">*</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">alpha_distillation</span> <span class="o">+</span> <span class="n">s_loss</span><span class="o">*</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">alpha_student</span> <span class="o">+</span> <span class="n">c_loss</span><span class="o">*</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">alpha_cosine</span>  <span class="c1"># linear combination loss
</span>
        <span class="n">scaler</span><span class="p">.</span><span class="n">scale</span><span class="p">(</span><span class="n">loss</span><span class="p">).</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">scaler</span><span class="p">.</span><span class="n">step</span><span class="p">(</span><span class="n">optimizer</span><span class="p">)</span>
        <span class="n">scaler</span><span class="p">.</span><span class="n">update</span><span class="p">()</span>
        <span class="n">scheduler</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>
</code></pre></div></div>

<h4 id="knowledge-distillation-model"><code class="language-plaintext highlighter-rouge">👩‍💻 Knowledge Distillation Model</code></h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">DistillationKnowledge</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">AbstractTask</span><span class="p">):</span>
    <span class="s">""" Custom Task Module for Knowledge Distillation by DistilBERT Style Architecture
    DistilBERT Style Architecture is Teacher-Student Framework for Knowledge Distillation,

    And then they have 3 objective functions:
        1) distillation loss, calculated by soft targets &amp; soft predictions
            (nn.KLDIVLoss(reduction='batchmean'))
        2) student loss, calculated by hard targets &amp; hard predictions
            (nn.CrossEntropyLoss(reduction='mean')), same as pure MLM Loss
        3) cosine similarity loss, calculated by student &amp; teacher logit similarity
            (nn.CosineEmbeddingLoss(reduction='mean')), similar as contrastive loss

    References:
        https://arxiv.org/pdf/1910.01108.pdf
        https://github.com/huggingface/transformers/blob/main/examples/research_projects/distillation/distiller.py
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">cfg</span><span class="p">:</span> <span class="n">CFG</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">DistillationKnowledge</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span> <span class="o">=</span> <span class="n">CFG</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">DistilBERT</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">,</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">select_model</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">_init_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">teacher_load_pretrained</span><span class="p">:</span>  <span class="c1"># for teacher model
</span>            <span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">teacher</span><span class="p">.</span><span class="n">load_state_dict</span><span class="p">(</span>
                <span class="n">torch</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="n">cfg</span><span class="p">.</span><span class="n">checkpoint_dir</span> <span class="o">+</span> <span class="n">cfg</span><span class="p">.</span><span class="n">teacher_state_dict</span><span class="p">),</span>
                <span class="n">strict</span><span class="o">=</span><span class="bp">False</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">student_load_pretrained</span><span class="p">:</span>  <span class="c1"># for student model
</span>            <span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">student</span><span class="p">.</span><span class="n">load_state_dict</span><span class="p">(</span>
                <span class="n">torch</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="n">cfg</span><span class="p">.</span><span class="n">checkpoint_dir</span> <span class="o">+</span> <span class="n">cfg</span><span class="p">.</span><span class="n">student_state_dict</span><span class="p">),</span>
                <span class="n">strict</span><span class="o">=</span><span class="bp">True</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">freeze</span><span class="p">:</span>
            <span class="n">freeze</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">teacher</span><span class="p">)</span>
            <span class="n">freeze</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">mlm_head</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">gradient_checkpoint</span><span class="p">:</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">gradient_checkpointing_enable</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">teacher_fw</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">inputs</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">padding_mask</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">mask</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
        <span class="n">is_valid</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="bp">False</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
        <span class="s">""" teacher forward pass to make soft target, last_hidden_state for distillation loss """</span>
        <span class="c1"># 1) make soft target
</span>        <span class="n">temperature</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="k">if</span> <span class="n">is_valid</span> <span class="k">else</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">temperature</span>
        <span class="n">last_hidden_state</span><span class="p">,</span> <span class="n">t_logit</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">teacher_fw</span><span class="p">(</span>
            <span class="n">inputs</span><span class="p">,</span>
            <span class="n">padding_mask</span><span class="p">,</span>
            <span class="n">attention_mask</span>
        <span class="p">)</span>
        <span class="n">last_hidden_state</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">masked_select</span><span class="p">(</span><span class="n">last_hidden_state</span><span class="p">,</span> <span class="o">~</span><span class="n">mask</span><span class="p">)</span>  <span class="c1"># for inverse select
</span>        <span class="n">last_hidden_state</span> <span class="o">=</span> <span class="n">last_hidden_state</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">dim_model</span><span class="p">)</span>  <span class="c1"># flatten last_hidden_state
</span>        <span class="n">soft_target</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="n">softmax</span><span class="p">(</span>
            <span class="n">t_logit</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">vocab_size</span><span class="p">)</span> <span class="o">/</span> <span class="n">temperature</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span>  <span class="c1"># flatten softmax distribution
</span>            <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span>
        <span class="p">)</span>  <span class="c1"># [bs* seq, vocab_size]
</span>        <span class="k">return</span> <span class="n">last_hidden_state</span><span class="p">,</span> <span class="n">soft_target</span>

    <span class="k">def</span> <span class="nf">student_fw</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">inputs</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">padding_mask</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">mask</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
        <span class="n">is_valid</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="bp">False</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
        <span class="s">""" student forward pass to make soft prediction, hard prediction for student loss """</span>
        <span class="n">temperature</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="k">if</span> <span class="n">is_valid</span> <span class="k">else</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">temperature</span>
        <span class="n">last_hidden_state</span><span class="p">,</span> <span class="n">s_logit</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">teacher_fw</span><span class="p">(</span>
            <span class="n">inputs</span><span class="p">,</span>
            <span class="n">padding_mask</span><span class="p">,</span>
            <span class="n">attention_mask</span>
        <span class="p">)</span>
        <span class="n">last_hidden_state</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">masked_select</span><span class="p">(</span><span class="n">last_hidden_state</span><span class="p">,</span> <span class="o">~</span><span class="n">mask</span><span class="p">)</span>  <span class="c1"># for inverse select
</span>        <span class="n">last_hidden_state</span> <span class="o">=</span> <span class="n">last_hidden_state</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">dim_model</span><span class="p">)</span>  <span class="c1"># flatten last_hidden_state
</span>        <span class="n">c_labels</span> <span class="o">=</span> <span class="n">last_hidden_state</span><span class="p">.</span><span class="n">new</span><span class="p">(</span><span class="n">last_hidden_state</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)).</span><span class="n">fill_</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">soft_pred</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="n">softmax</span><span class="p">(</span>
            <span class="n">s_logit</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">vocab_size</span><span class="p">)</span> <span class="o">/</span> <span class="n">temperature</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span>  <span class="c1"># flatten softmax distribution
</span>            <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">last_hidden_state</span><span class="p">,</span> <span class="n">s_logit</span><span class="p">,</span> <span class="n">soft_pred</span><span class="p">,</span> <span class="n">c_labels</span>
</code></pre></div></div>

<h4 id="distilbert-model"><code class="language-plaintext highlighter-rouge">👩‍💻 DistilBERT Model</code></h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">DistilBERT</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">AbstractModel</span><span class="p">):</span>
    <span class="s">""" Main class for DistilBERT Style Model, Teacher-Student Framework
    for Knowledge Distillation aim to lighter Large Scale LLM model. This model have 3 objective functions:

        1) distillation loss, calculated by soft targets &amp; soft predictions
            (nn.KLDIVLoss(reduction='batchmean'))

        2) student loss, calculated by hard targets &amp; hard predictions
            (nn.CrossEntropyLoss(reduction='mean')), same as pure MLM Loss

        3) cosine similarity loss, calculated by student &amp; teacher logit similarity
            (nn.CosineEmbeddingLoss(reduction='mean')), similar as contrastive loss

    soft targets &amp; soft predictions are meaning that logit are passed through softmax function applied with temperature T
    temperature T aim to flatten softmax layer distribution for making "Dark Knowledge" from teacher model

    hard targets &amp; hard predictions are meaning that logit are passed through softmax function without temperature T
    hard targets are same as just simple labels from MLM Collator returns for calculating cross entropy loss

    cosine similarity loss is calculated by cosine similarity between student &amp; teacher
    in official repo, they mask padding tokens for calculating cosine similarity, target for this task is 1
    cosine similarity is calculated by nn.CosineSimilarity() function, values are range to [-1, 1]

    you can select any other backbone model architecture for Teacher &amp; Student Model for knowledge distillation
    but, in original paper, BERT is used for Teacher Model &amp; Student
    and you must select pretrained model for Teacher Model, because Teacher Model is used for knowledge distillation,
    which is containing pretrained mlm head

    Do not pass gradient backward to teacher model!!
    (teacher model must be frozen or register_buffer to model or use no_grad() context manager)

    Args:
        cfg: configuration.CFG
        model_func: make model instance in runtime from config.json

    References:
        https://arxiv.org/pdf/1910.01108.pdf
        https://github.com/huggingface/transformers/blob/main/examples/research_projects/distillation/distiller.py
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">cfg</span><span class="p">:</span> <span class="n">CFG</span><span class="p">,</span> <span class="n">model_func</span><span class="p">:</span> <span class="n">Callable</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">DistilBERT</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span> <span class="o">=</span> <span class="n">cfg</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">teacher</span> <span class="o">=</span> <span class="n">model_func</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">teacher_num_layers</span><span class="p">)</span>  <span class="c1"># must be loading pretrained model containing mlm head
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">mlm_head</span> <span class="o">=</span> <span class="n">MLMHead</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">)</span>  <span class="c1"># must be loading pretrained model's mlm head
</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">student</span> <span class="o">=</span> <span class="n">model_func</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">student_num_layers</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">s_mlm_head</span> <span class="o">=</span> <span class="n">MLMHead</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">teacher_fw</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">inputs</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">padding_mask</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
        <span class="s">""" forward pass for teacher model
        """</span>
        <span class="n">last_hidden_state</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">teacher</span><span class="p">(</span>
            <span class="n">inputs</span><span class="p">,</span>
            <span class="n">padding_mask</span><span class="p">,</span>
            <span class="n">attention_mask</span>
        <span class="p">)</span>
        <span class="n">t_logit</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">mlm_head</span><span class="p">(</span><span class="n">last_hidden_state</span><span class="p">)</span>  <span class="c1"># hard logit =&gt; to make soft logit
</span>        <span class="k">return</span> <span class="n">last_hidden_state</span><span class="p">,</span> <span class="n">t_logit</span>

    <span class="k">def</span> <span class="nf">student_fw</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">inputs</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">padding_mask</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="bp">None</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
        <span class="s">""" forward pass for student model
        """</span>
        <span class="n">last_hidden_state</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">student</span><span class="p">(</span>
            <span class="n">inputs</span><span class="p">,</span>
            <span class="n">padding_mask</span><span class="p">,</span>
            <span class="n">attention_mask</span>
        <span class="p">)</span>
        <span class="n">s_logit</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">s_mlm_head</span><span class="p">(</span><span class="n">last_hidden_state</span><span class="p">)</span>  <span class="c1"># hard logit =&gt; to make soft logit
</span>        <span class="k">return</span> <span class="n">last_hidden_state</span><span class="p">,</span> <span class="n">s_logit</span>
</code></pre></div></div>]]></content><author><name>qcqced</name><email>qcqced123@gmail.com</email></author><category term="NLP" /><category term="Natural Language Process" /><category term="DistilBERT" /><category term="BERT" /><category term="Self-Attention" /><category term="Pytorch" /><summary type="html"><![CDATA[DistilBERT Official Paper Review with Pytorch Implementation]]></summary></entry><entry><title type="html">🪢 [DeBERTa-V3] DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing</title><link href="http://localhost:4000/nlp/deberta_v3" rel="alternate" type="text/html" title="🪢 [DeBERTa-V3] DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing" /><published>2024-03-11T00:00:00+09:00</published><updated>2024-03-12T02:00:00+09:00</updated><id>http://localhost:4000/nlp/deberta-v3</id><content type="html" xml:base="http://localhost:4000/nlp/deberta_v3"><![CDATA[<h3 id="overview"><code class="language-plaintext highlighter-rouge">🔭 Overview</code></h3>

<p>2021년 Microsoft에서 공개한 <code class="language-plaintext highlighter-rouge">DeBERTa-V3</code>은 기존 DeBERTa의 모델 구조는 그대로 유지하되, ELECTRA의 Generator-Discriminator 구조를 차용하여 전작 대비 성능을 향상 시킨 모델이다. ELECTRA에서 BackBone 모델로 BERT 대신 <code class="language-plaintext highlighter-rouge">DeBERTa을</code> 사용했다고 생각하면 된다. 거기에 더해 ELECTRA의 <code class="language-plaintext highlighter-rouge">Tug-of-War</code> 현상을 방지하기 위해 새로운 임베딩 공유 기법인 <code class="language-plaintext highlighter-rouge">GDES(Gradient Disentagnled Embedding Sharing)</code>방법을 제시했다.</p>

<p>이번 포스팅에서는 구현 코드와 함께 GDES에 대해서만 살펴보려 한다. ELECTRA, DeBERTa에 대해 궁금하다면 이전 포스팅을, 전체 구조에 대한 코드는 <strong><a href="https://github.com/qcqced123/model_study">여기 링크</a></strong>를 통해 확인 가능하다.</p>

<h3 id="gdes-gradient-disentangled-embedding-sharing"><code class="language-plaintext highlighter-rouge">🪢GDES: Gradient Disentangled Embedding Sharing</code></h3>

<p align="center">
<img src="/assets/images/deberta_v3/deberta_v3.png" alt="GDES" class="align-center image-caption" width="100%&quot;, height=&quot;100%" />
<strong><em><a href="https://arxiv.org/abs/2111.09543">GDES</a></em></strong>
</p>

<p>그림의 (a)가 기존 ELECTRA의 가중치 공유 방식, (c)가 GDES에 해당된다. 그림 속 모식도와 설명이 좀 복잡해 보이지만 아이디어는 매우 간단하다.</p>

<p>생성자와 판별자가 서로 포워드 패스 시점에는 단어, 위치 임베딩을 공유하되, 백워드 패스 시점에서는 공유되지 못하도록 하여, 판별자의 학습 결과에 의해 생성자의 단어 임베딩, 위치 임베딩이 업데이트 되지 못하도록 하지는 것이다. 오직 생성자의 MLM 학습에 의해서만 단어 및 위치 임베딩이 업데이트 되어야 한다.</p>

\[E_{D} = \text{sg}(E_{G}) + E_{\Delta}\]

<p>필자가 추정하기로는 <code class="language-plaintext highlighter-rouge">Skip-Connection</code>에서 영감을 받지 않았나 싶은 이 수식은, 생성자의 임베딩에 잔차값들을 더해 판별자의 임베딩 행렬이 RTD에 최적화 되도록 설계 되었다. 여기서 <code class="language-plaintext highlighter-rouge">sg()</code> 는 <code class="language-plaintext highlighter-rouge">stop gradient</code>를 의미한다. 다시 말해, 생성자의 임베딩 가중치를 판별자 학습에 사용하되, 해당 시점에서는 계산 그래프 작성을 중단시켜 판별자의 학습 결과(이진 분류 손실)가 생성자의 임베딩 가중치에 영향을 미치지 못하도록 한 것이다.</p>

<p>이러한 아이디어는 실제로 어떻게 코드로 구현해야할까, 아래 코드와 함께 살펴보자.</p>

<h3 id="implementation-by-pytorch"><code class="language-plaintext highlighter-rouge">👩‍💻 Implementation by Pytorch</code></h3>

<p>ELECTRA 모듈 <strong><code class="language-plaintext highlighter-rouge">__init__</code></strong>의 <strong><code class="language-plaintext highlighter-rouge">share_embed_method</code></strong>에 따라 브랜치가 발생하는 구간과, 아래 <strong><code class="language-plaintext highlighter-rouge">share_embedding()</code></strong> 메서드에 주목해보자.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="nn">experiment.models.abstract_model</span> <span class="kn">import</span> <span class="n">AbstractModel</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">Tensor</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">Callable</span>
<span class="kn">from</span> <span class="nn">einops.layers.torch</span> <span class="kn">import</span> <span class="n">Rearrange</span>
<span class="kn">from</span> <span class="nn">experiment.tuner.mlm</span> <span class="kn">import</span> <span class="n">MLMHead</span>
<span class="kn">from</span> <span class="nn">experiment.tuner.sbo</span> <span class="kn">import</span> <span class="n">SBOHead</span>
<span class="kn">from</span> <span class="nn">experiment.tuner.rtd</span> <span class="kn">import</span> <span class="n">get_discriminator_input</span><span class="p">,</span> <span class="n">RTDHead</span>
<span class="kn">from</span> <span class="nn">configuration</span> <span class="kn">import</span> <span class="n">CFG</span>


<span class="k">class</span> <span class="nc">ELECTRA</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">AbstractModel</span><span class="p">):</span>
    <span class="s">""" If you want to use pure ELECTRA, you should set share_embedding = ES
    elif you want to use ELECTRA with GDES, you should set share_embedding = GDES
    GDES is new approach of embedding sharing method from DeBERTa-V3 paper

    Args:
        cfg: configuration.CFG
        model_func: make model instance in runtime from config.json

    Var:
        cfg: configuration.CFG
        generator: Generator, which is used for generating replaced tokens for RTD
                   should select backbone model ex) BERT, RoBERTa, DeBERTa, ...
        discriminator: Discriminator, which is used for detecting replaced tokens for RTD
                       should select backbone model ex) BERT, RoBERTa, DeBERTa, ...
        share_embedding: whether or not to share embedding layer (word &amp; pos) between Generator &amp; Discriminator
        self.word_bias: Delta_E in paper
        self.abs_pos_bias: Delta_E in paper
        self.rel_pos_bias: Delta_E in paper

    References:
        https://arxiv.org/pdf/2003.10555.pdf
        https://arxiv.org/pdf/2111.09543.pdf
        https://github.com/google-research/electra
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">cfg</span><span class="p">:</span> <span class="n">CFG</span><span class="p">,</span> <span class="n">model_func</span><span class="p">:</span> <span class="n">Callable</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">ELECTRA</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span> <span class="o">=</span> <span class="n">cfg</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">generator</span> <span class="o">=</span> <span class="n">model_func</span><span class="p">(</span><span class="n">cfg</span><span class="p">.</span><span class="n">generator_num_layers</span><span class="p">)</span>  <span class="c1"># init generator
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">mlm_head</span> <span class="o">=</span> <span class="n">MLMHead</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">rtd_masking</span> <span class="o">==</span> <span class="s">'SpanBoundaryObjective'</span><span class="p">:</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">mlm_head</span> <span class="o">=</span> <span class="n">SBOHead</span><span class="p">(</span>
                <span class="n">cfg</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">,</span>
                <span class="n">is_concatenate</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">is_concatenate</span><span class="p">,</span>
                <span class="n">max_span_length</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">max_span_length</span>
            <span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">discriminator</span> <span class="o">=</span> <span class="n">model_func</span><span class="p">(</span><span class="n">cfg</span><span class="p">.</span><span class="n">discriminator_num_layers</span><span class="p">)</span>  <span class="c1"># init generator
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">rtd_head</span> <span class="o">=</span> <span class="n">RTDHead</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">share_embed_method</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">share_embed_method</span>  <span class="c1"># instance, es, gdes
</span>        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">share_embed_method</span> <span class="o">==</span> <span class="s">'GDES'</span><span class="p">:</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">word_bias</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Parameter</span><span class="p">(</span>
                <span class="n">torch</span><span class="p">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">discriminator</span><span class="p">.</span><span class="n">embeddings</span><span class="p">.</span><span class="n">word_embedding</span><span class="p">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">abs_pos_bias</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Parameter</span><span class="p">(</span>
                <span class="n">torch</span><span class="p">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">discriminator</span><span class="p">.</span><span class="n">embeddings</span><span class="p">.</span><span class="n">abs_pos_emb</span><span class="p">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>
            <span class="p">)</span>
            <span class="nb">delattr</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">discriminator</span><span class="p">.</span><span class="n">embeddings</span><span class="p">.</span><span class="n">word_embedding</span><span class="p">,</span> <span class="s">'weight'</span><span class="p">)</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">discriminator</span><span class="p">.</span><span class="n">embeddings</span><span class="p">.</span><span class="n">word_embedding</span><span class="p">.</span><span class="n">register_parameter</span><span class="p">(</span><span class="s">'_weight'</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">word_bias</span><span class="p">)</span>

            <span class="nb">delattr</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">discriminator</span><span class="p">.</span><span class="n">embeddings</span><span class="p">.</span><span class="n">abs_pos_emb</span><span class="p">,</span> <span class="s">'weight'</span><span class="p">)</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">discriminator</span><span class="p">.</span><span class="n">embeddings</span><span class="p">.</span><span class="n">abs_pos_emb</span><span class="p">.</span><span class="n">register_parameter</span><span class="p">(</span><span class="s">'_weight'</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">abs_pos_bias</span><span class="p">)</span>

            <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">model_name</span> <span class="o">==</span> <span class="s">'DeBERTa'</span><span class="p">:</span>
                <span class="bp">self</span><span class="p">.</span><span class="n">rel_pos_bias</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Parameter</span><span class="p">(</span>
                    <span class="n">torch</span><span class="p">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">discriminator</span><span class="p">.</span><span class="n">embeddings</span><span class="p">.</span><span class="n">rel_pos_emb</span><span class="p">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>
                <span class="p">)</span>
                <span class="nb">delattr</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">discriminator</span><span class="p">.</span><span class="n">embeddings</span><span class="p">.</span><span class="n">rel_pos_emb</span><span class="p">,</span> <span class="s">'weight'</span><span class="p">)</span>
                <span class="bp">self</span><span class="p">.</span><span class="n">discriminator</span><span class="p">.</span><span class="n">embeddings</span><span class="p">.</span><span class="n">rel_pos_emb</span><span class="p">.</span><span class="n">register_parameter</span><span class="p">(</span><span class="s">'_weight'</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">rel_pos_emb</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">share_embedding</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">share_embedding</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="k">def</span> <span class="nf">discriminator_hook</span><span class="p">(</span><span class="n">module</span><span class="p">:</span> <span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">,</span> <span class="o">*</span><span class="n">inputs</span><span class="p">):</span>
            <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">share_embed_method</span> <span class="o">==</span> <span class="s">'instance'</span><span class="p">:</span>  <span class="c1"># Instance Sharing
</span>                <span class="bp">self</span><span class="p">.</span><span class="n">discriminator</span><span class="p">.</span><span class="n">embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">generator</span><span class="p">.</span><span class="n">embeddings</span>

            <span class="k">elif</span> <span class="bp">self</span><span class="p">.</span><span class="n">share_embed_method</span> <span class="o">==</span> <span class="s">'ES'</span><span class="p">:</span>  <span class="c1"># ES (Embedding Sharing)
</span>                <span class="bp">self</span><span class="p">.</span><span class="n">discriminator</span><span class="p">.</span><span class="n">embeddings</span><span class="p">.</span><span class="n">word_embedding</span><span class="p">.</span><span class="n">weight</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">generator</span><span class="p">.</span><span class="n">embeddings</span><span class="p">.</span><span class="n">word_embedding</span><span class="p">.</span><span class="n">weight</span>
                <span class="bp">self</span><span class="p">.</span><span class="n">discriminator</span><span class="p">.</span><span class="n">embeddings</span><span class="p">.</span><span class="n">abs_pos_emb</span><span class="p">.</span><span class="n">weight</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">generator</span><span class="p">.</span><span class="n">embeddings</span><span class="p">.</span><span class="n">abs_pos_emb</span><span class="p">.</span><span class="n">weight</span>
                <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">model_name</span> <span class="o">==</span> <span class="s">'DeBERTa'</span><span class="p">:</span>
                    <span class="bp">self</span><span class="p">.</span><span class="n">discriminator</span><span class="p">.</span><span class="n">embeddings</span><span class="p">.</span><span class="n">rel_pos_emb</span><span class="p">.</span><span class="n">weight</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">generator</span><span class="p">.</span><span class="n">embeddings</span><span class="p">.</span><span class="n">rel_pos_emb</span><span class="p">.</span><span class="n">weight</span>

            <span class="k">elif</span> <span class="bp">self</span><span class="p">.</span><span class="n">share_embed_method</span> <span class="o">==</span> <span class="s">'GDES'</span><span class="p">:</span>  <span class="c1"># GDES (Generator Discriminator Embedding Sharing)
</span>                <span class="n">g_w_emb</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">generator</span><span class="p">.</span><span class="n">embeddings</span><span class="p">.</span><span class="n">word_embedding</span>
                <span class="n">d_w_emb</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">discriminator</span><span class="p">.</span><span class="n">embeddings</span><span class="p">.</span><span class="n">word_embedding</span>
                <span class="bp">self</span><span class="p">.</span><span class="n">_set_param</span><span class="p">(</span><span class="n">d_w_emb</span><span class="p">,</span> <span class="s">'weight'</span><span class="p">,</span> <span class="n">g_w_emb</span><span class="p">.</span><span class="n">weight</span><span class="p">.</span><span class="n">detach</span><span class="p">()</span> <span class="o">+</span> <span class="n">d_w_emb</span><span class="p">.</span><span class="n">_weight</span><span class="p">)</span>
                <span class="n">g_p_emb</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">generator</span><span class="p">.</span><span class="n">embeddings</span><span class="p">.</span><span class="n">abs_pos_emb</span>
                <span class="n">d_p_emb</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">discriminator</span><span class="p">.</span><span class="n">embeddings</span><span class="p">.</span><span class="n">abs_pos_emb</span>
                <span class="bp">self</span><span class="p">.</span><span class="n">_set_param</span><span class="p">(</span><span class="n">d_p_emb</span><span class="p">,</span> <span class="s">'weight'</span><span class="p">,</span> <span class="n">g_p_emb</span><span class="p">.</span><span class="n">weight</span><span class="p">.</span><span class="n">detach</span><span class="p">()</span> <span class="o">+</span> <span class="n">d_p_emb</span><span class="p">.</span><span class="n">_weight</span><span class="p">)</span>

                <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">model_name</span> <span class="o">==</span> <span class="s">'DeBERTa'</span><span class="p">:</span>
                    <span class="n">g_rp_emb</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">generator</span><span class="p">.</span><span class="n">embeddings</span><span class="p">.</span><span class="n">rel_pos_emb</span>
                    <span class="n">d_rp_emb</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">discriminator</span><span class="p">.</span><span class="n">embeddings</span><span class="p">.</span><span class="n">rel_pos_emb</span>
                    <span class="bp">self</span><span class="p">.</span><span class="n">_set_param</span><span class="p">(</span><span class="n">d_rp_emb</span><span class="p">,</span> <span class="s">'weight'</span><span class="p">,</span> <span class="n">g_rp_emb</span><span class="p">.</span><span class="n">weight</span><span class="p">.</span><span class="n">detach</span><span class="p">()</span> <span class="o">+</span> <span class="n">d_rp_emb</span><span class="p">.</span><span class="n">_weight</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">discriminator</span><span class="p">.</span><span class="n">register_forward_pre_hook</span><span class="p">(</span><span class="n">discriminator_hook</span><span class="p">)</span>

    <span class="o">@</span><span class="nb">staticmethod</span>
    <span class="k">def</span> <span class="nf">_set_param</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">param_name</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="n">module</span><span class="p">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="n">param_name</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">generator_fw</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">labels</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">padding_mask</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">mask_labels</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="bp">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
        <span class="n">g_last_hidden_states</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">generator</span><span class="p">(</span>
            <span class="n">inputs</span><span class="p">,</span>
            <span class="n">padding_mask</span><span class="p">,</span>
            <span class="n">attention_mask</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">rtd_masking</span> <span class="o">==</span> <span class="s">'MaskedLanguageModel'</span><span class="p">:</span>
            <span class="n">g_logit</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">mlm_head</span><span class="p">(</span>
                <span class="n">g_last_hidden_states</span>
            <span class="p">)</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">rtd_masking</span> <span class="o">==</span> <span class="s">'SpanBoundaryObjective'</span><span class="p">:</span>
            <span class="n">g_logit</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">mlm_head</span><span class="p">(</span>
                <span class="n">g_last_hidden_states</span><span class="p">,</span>
                <span class="n">mask_labels</span>
            <span class="p">)</span>
        <span class="n">pred</span> <span class="o">=</span> <span class="n">g_logit</span><span class="p">.</span><span class="n">clone</span><span class="p">().</span><span class="n">detach</span><span class="p">()</span>
        <span class="n">d_inputs</span><span class="p">,</span> <span class="n">d_labels</span> <span class="o">=</span> <span class="n">get_discriminator_input</span><span class="p">(</span>
            <span class="n">inputs</span><span class="p">,</span>
            <span class="n">labels</span><span class="p">,</span>
            <span class="n">pred</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">g_logit</span><span class="p">,</span> <span class="n">d_inputs</span><span class="p">,</span> <span class="n">d_labels</span>

    <span class="k">def</span> <span class="nf">discriminator_fw</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">padding_mask</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span><span class="n">attention_mask</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="bp">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="n">d_last_hidden_states</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">discriminator</span><span class="p">(</span>
            <span class="n">inputs</span><span class="p">,</span>
            <span class="n">padding_mask</span><span class="p">,</span>
            <span class="n">attention_mask</span>
        <span class="p">)</span>
        <span class="n">d_logit</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">rtd_head</span><span class="p">(</span>
            <span class="n">d_last_hidden_states</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">d_logit</span>
</code></pre></div></div>
<p>먼저 <strong><code class="language-plaintext highlighter-rouge">__init__</code></strong>의 브랜치 구간을 살펴보자. <strong><code class="language-plaintext highlighter-rouge">word_bias</code></strong>, <strong><code class="language-plaintext highlighter-rouge">pos_bias</code></strong>를 만들어 <code class="language-plaintext highlighter-rouge">register_parameter</code>화를 하고 있다. 새롭게 생성되어 <strong><code class="language-plaintext highlighter-rouge">_weight</code></strong>이란 이름으로 생성자의 파라미터가 된 두 가중치가 바로 $E_{\Delta}$ 가 된다.</p>

<p>다음 <strong><code class="language-plaintext highlighter-rouge">share_embedding()</code></strong> 메서드를 보자. $E_{G}$ 에 <code class="language-plaintext highlighter-rouge">torch.detach()</code>를 사용해 수식의 <code class="language-plaintext highlighter-rouge">stop gradient</code> 효과를 적용한다. 그리고 두 가중치를 더하고, <code class="language-plaintext highlighter-rouge">torch.register_buffer</code>를 호출해 포워드 패스에 활용은 되지만 백워드 패스에 그라디언트가 해당 가중치를 업데이트 하지 못하도록 설정한다. 그리고 마지막에 <code class="language-plaintext highlighter-rouge">torch.register_forward_pre_hook</code>을 호출하는데, 그 이유는 $E_{G}$ 에 <code class="language-plaintext highlighter-rouge">torch.detach()</code> 를 사용했기 때문에 현재 판별자의 버퍼에 있는 $E_{G}$ 는 이전 시점의 생성자 MLM 손실에 의해 새롭게 업데이트 $E_{G}$ 가 아니다. 따라서 매번 판별자의 포워드 패스가 호출(시작)되는 시점에 업데이트 된 $E_{G}$ 를 반영해 RTD를 수행할 수 있도록 하기 위해 <code class="language-plaintext highlighter-rouge">register_forward_pre_hook</code> 를 사용했다.</p>

<h3 id="-gdes-experiment"><strong><code class="language-plaintext highlighter-rouge">🤔 GDES Experiment</code></strong></h3>

<p>GDES가 제대로 구현되었는지, 논문 주장대로 판별자 학습 결과가 간섭을 발생시키지 않는지 확인하기 위해 한가지 실험을 진행했다. 실험 내용은 이렇다. 만약 GDES가 의도대로 구현된게 맞다면, 인코더 모델의 MLM 학습 결과 추이와 ELECTRA의 생성자 학습 결과 추이 양상이 유사해야 한다. 만약 최적화 추세가 다르다면, 필자가 잘못 구현했거나, 저자의 주장과 다르게 간섭이 발생하는 것이라 볼 수 있을 것이다. Backbone을 DeBERTa로 두고 각각 학습을 진행했다. 모든 하이퍼 파라미터를 고정한 뒤, 학습 초반 120스탭에 대한 결과 추이를 비교해봤다.</p>

<p align="center">
<img src="/assets/images/deberta_v3/deberta_test.png" alt="DeBERTa MLM Result" class="align-center image-caption" width="100%&quot;, height=&quot;100%" />
<strong><em><a href="https://arxiv.org/abs/2111.09543">DeBERTa MLM Result</a></em></strong>
</p>

<p align="center">
<img src="/assets/images/deberta_v3/gdes_test.png" alt="GDES Result" class="align-center image-caption" width="100%&quot;, height=&quot;100%" />
<strong><em><a href="https://arxiv.org/abs/2111.09543">GDES Result</a></em></strong>
</p>

<p>미처 까먹고 <code class="language-plaintext highlighter-rouge">torch.backends.cudnn.deterministic = False</code>로 두고 실험을 진행하여, 생성자의 수렴이 좀 더 빨리 진행되는 양상을 보이고 있다. 아마도 생성자 학습을 할 때 <code class="language-plaintext highlighter-rouge">cudnn</code> 이 열심히 일을 한 것 같댜. 수렴 속도에는 차이가 조금 나지만, 최적화 되는 추세 자체는 동일한 것을 알 수 있다.</p>

<p>따라서 GDES를 사용하면 간섭이 발생하지 않아 <code class="language-plaintext highlighter-rouge">Tug-of-War</code> 현상을 방지할 수 있다. 다만, 실험이 다소 엄밀하지 못한 측면이 있다. 추후에 좀 더 엄밀한 증명을 할 수 있는 실험 방법을 생각해봐야겠다.</p>]]></content><author><name>qcqced</name><email>qcqced123@gmail.com</email></author><category term="NLP" /><category term="Natural Language Process" /><category term="DeBERTa-V3" /><category term="DeBERTa" /><category term="ELECTRA" /><category term="Weight Sharing" /><category term="GDES" /><category term="Pytorch" /><summary type="html"><![CDATA[DeBERTa-V3 Official Paper Review with Pytorch Implementation]]></summary></entry><entry><title type="html">🗂️[SpanBERT] SpanBERT: Improving Pre-training by Representing and Predicting Spans</title><link href="http://localhost:4000/nlp/spanbert" rel="alternate" type="text/html" title="🗂️[SpanBERT] SpanBERT: Improving Pre-training by Representing and Predicting Spans" /><published>2024-03-11T00:00:00+09:00</published><updated>2024-03-12T02:00:00+09:00</updated><id>http://localhost:4000/nlp/spanbert</id><content type="html" xml:base="http://localhost:4000/nlp/spanbert"><![CDATA[<h3 id="overview"><code class="language-plaintext highlighter-rouge">🔭 Overview</code></h3>

<p><code class="language-plaintext highlighter-rouge">SpanBERT</code>는 2020년 페이스북에서 발표한 BERT 계열 모델로, 새로운 방법론인 <code class="language-plaintext highlighter-rouge">SBO(Span Boundary Objective)</code>를 고안해 사전학습을 하여 기존 대비 높은 성능을 기록했다. 기존 <code class="language-plaintext highlighter-rouge">MLM</code>, <code class="language-plaintext highlighter-rouge">CLM</code>은 단일 토큰을 예측하는 방식을 사용하기 때문에 Word-Level Task에 아주 적합하지만 상대적으로 QA, Sentence-Similarity 같은 문장 단위 테스크에 그대로 활용하기에는 부족한 점이 있었다. 이러한 문제를 해결하기 위해 고안된 방법론이 바로 <code class="language-plaintext highlighter-rouge">SBO</code>다. <code class="language-plaintext highlighter-rouge">SBO</code>란, MLM과 비슷하지만, Span(절•구문) 단위로 마스킹하고 다시 Denoising을 하기 때문에, Sentence-Level Task에 속하는 Down-Stream Task를 위한 모델의 사전 훈련으로 적합하다.</p>

<p>정리하자면, <code class="language-plaintext highlighter-rouge">SpanBERT</code> 모델은 기존 BERT의 구조적 측면 개선이 아닌, 사전학습 방법에 대한 개선 시도라고 볼 수 있다. 따라서 어떤 모델이더라도, 인코더 언어 모델이라면 모두 <code class="language-plaintext highlighter-rouge">SpanBERT</code> 구조를 사용할 수 있으며, 기존 논문에서는 원본 BERT 구조를 사용했다. 그래서 본 포스팅에서도 BERT에 대한 설명 없이 SBO에 대해서만 다루려고 한다.</p>

<h3 id="sbo-span-boundary-objective"><code class="language-plaintext highlighter-rouge">📚 SBO: Span Boundary Objective</code></h3>

<p align="center">
<img src="/assets/images/spanbert/sbo.png" alt="SBO Task" class="align-center image-caption" width="100%&quot;, height=&quot;100%" />
<strong><em><a href="https://arxiv.org/abs/1907.10529">SBO Task</a></em></strong>
</p>

<p><strong>[SBO Algorithm Summary]</strong></p>
<ul>
  <li><strong>1) 연속된 범위의 Span 생성</strong>
    <ul>
      <li><strong>무작위로 Span의 양쪽 끝 토큰 지정 ($x_{4}, x_{9}$)</strong>
        <ul>
          <li><strong>$x_{5}$ to $x_{8}$ 은 스팬 내부 토큰</strong></li>
        </ul>
      </li>
      <li><strong>마스킹 예산 계산</strong>
        <ul>
          <li><strong>문장 당 마스킹 예산(합산 Span 길이)은 문장 길이의 15%</strong></li>
          <li><strong>예시 시퀀스 길이: 512</strong></li>
          <li><strong>마스킹 예산: 대략 75 = 512*0.15</strong></li>
        </ul>
      </li>
      <li><strong>기하 분포 사용해서 개별 스팬 길이 지정</strong>
        <ul>
          <li><strong>개별 스팬당 최대 길이 지정, 최대 10이 넘지 않도록 설정</strong></li>
          <li><strong>최대 스팬 합산 길이 도달까지 마스킹 반복</strong>
            <ul>
              <li><strong>남은 마스킹 예산 &lt; 현재 스팬 길이</strong>
                <ul>
                  <li><strong>남은 마스킹 예산을 현재 스팬 길이로 설정</strong></li>
                </ul>
              </li>
            </ul>
          </li>
        </ul>
      </li>
      <li><strong>따라서 Subword Tokenizing이 아니라 Whole Word Masking 단위 작업이 필요</strong></li>
    </ul>
  </li>
  <li><strong>2) 시작 토큰 기준, 상대 위치 계산</strong>
    <ul>
      <li><strong>스팬 내부 토큰의 상대 위치 임베딩 생성 및 계산</strong></li>
      <li><strong>시작토큰, 마지막토큰, 스팬 내부 토큰의 상대 위치 임베딩을 concat, 은닉 벡터 생성</strong></li>
      <li><strong>SpanHead에 은닉 벡터 통과시키기</strong></li>
    </ul>
  </li>
  <li><strong>3) SpanHead 출력값을 마스킹에 대한 예측 표현으로 사용</strong></li>
</ul>

<p>SBO의 아이디어 자체는 상당히 간단하다. 기존 MLM처럼 무작위로 시퀀스에서 아무 토큰이나 선택하는게 아니라, 주어진 문장에서 일정 길이의 연속된 토큰들을 한번에 선택해 마스킹 처리하여 학습하겠다는 것이다. 논문에서 제시한 SBO 알고리즘을 정리하면 아래와 같다.</p>

\[\begin{align*}
h_0 &amp;= [x_{s-1}; x_{e+1}; p_{i-s+1}] \\
h_1 &amp;= \text{LayerNorm}(\text{GeLU}(W_1 h_0)) \\
y_i &amp;= \text{LayerNorm}(\text{GeLU}(W_2 h_1))
\end{align*}\]

<p>위 그림을 예시로 알고리즘을 살펴보자. 먼저 주어진 스팬 길이에 맞게, 스팬의 시작과 끝 지점이 되는 토큰을 무작위로 선택한다. 그다음 시작 위치를 기준으로, 스팬 내부에 속하는 토큰들의 상대 위치 인덱스를 계산해준다. 그림 속 $x_{7}$ 토큰의 상대 위치 번호는 3이 된다. 미리 정의한 상대 위치 임베딩에서 행 인덱스가 3인 행벡터를 가져온다. 그 다음 양쪽 끝 벡터와 concat을 수행하여 $h_{0}$ 을 만든다. 그리고 미리 정의된 <code class="language-plaintext highlighter-rouge">SBOHead</code>에 통과시킨다. <code class="language-plaintext highlighter-rouge">SBOHead</code>에게 반환 받은 은닉 벡터값은 해당 위치의 마스킹에 대한 예측값($y_{i}$)으로 사용하고 이를 이용해 SBO 손실을 구한다. 지금까지 내용을 정리해 수식으로 표현하면 위와 같다.</p>

\[L(x_i) = L_{MLM}(x_i) + L_{SBO}(x_i)\]

<p><code class="language-plaintext highlighter-rouge">SpanBERT</code>의 목적함수는 SBO 손실 뿐만 아니라 기존 MLM 손실도 함꼐 포함되어 있다. 다만 MLM 손실을 구하기 위해 주어진 시퀀스에 대해 따로 마스킹을 하는 것은 아니고, SBO를 위해 적용했던 Span Masking을 그대로 활용한다. 대신 위의 SBO 수식의 $h_{0}$ 이 아니라, $x_{i-s+1}$ ($i-s+1$ 번째 토큰의 인코더 출력값)을 그대로 MLM 손실을 구하는데 사용한다. 정리하면, <code class="language-plaintext highlighter-rouge">SpanBERT</code>의 최종 손실은 위 수식과 같다. 한편, <code class="language-plaintext highlighter-rouge">ELECTRA</code> 때와는 다르게 두 손실의 스케일 차이가 거의 없어 따로 스케일 상수를 곱해주지는 않는 것 같다.</p>

<h3 id="implementation-by-pytorch"><code class="language-plaintext highlighter-rouge">👩‍💻 Implementation by Pytorch</code></h3>

<p>논문의 내용 종합하여 파이토치로 <code class="language-plaintext highlighter-rouge">SpanBERT</code>를 구현해봤다. 논문에 포함된 아이디어를 이해하는데는 어렵지 않았지만, 제한된 조건에 맞는 스팬을 찾고, 마스킹하는 과정을 실제 구현하는 것은 매우 까다로운 편이었다.
본 포스팅에서는 <code class="language-plaintext highlighter-rouge">SpanBERT</code>의 SBO 학습을 위한 입력 만들기, SBOHead에 대해서만 설명하려고 한다. <code class="language-plaintext highlighter-rouge">BERT</code>, <code class="language-plaintext highlighter-rouge">Whole World Masking</code>에 대해 궁금하다면 이전 포스팅을, 전체 모델 구조 대한 코드는 <strong><a href="https://github.com/qcqced123/model_study">여기 링크</a></strong>를 통해 참고바란다.</p>

<p>공개할 코드는 아직 완벽하게 벡터화를 적용하지 못해, GPU 병렬 연산에 최적화 되지 못한 점 양해 부탁한다. 빠른 시일 이내에 벡터화를 적용해서 다시 수정된 코드를 올리겠다.</p>

<h4 id="span-masking-algoritm"><code class="language-plaintext highlighter-rouge">👩‍💻 Span Masking Algoritm</code></h4>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">random</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="nn">torch.nn.utils.rnn</span> <span class="kn">import</span> <span class="n">pad_sequence</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">Tensor</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">List</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">Any</span>
<span class="kn">from</span> <span class="nn">..tuner.mlm</span> <span class="kn">import</span> <span class="n">WholeWordMaskingCollator</span>
<span class="kn">from</span> <span class="nn">configuration</span> <span class="kn">import</span> <span class="n">CFG</span>

<span class="n">BPE</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s">'RobertaTokenizerFast'</span><span class="p">,</span>
    <span class="s">'GPT2TokenizerFast'</span><span class="p">,</span>
<span class="p">]</span>

<span class="n">SPM</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s">'DebertaV2TokenizerFast'</span><span class="p">,</span>
    <span class="s">'DebertaTokenizerFast'</span><span class="p">,</span>
    <span class="s">'XLMRobertaTokenizerFast'</span><span class="p">,</span>
<span class="p">]</span>

<span class="n">WORDPIECE</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s">'BertTokenizerFast'</span><span class="p">,</span>
    <span class="s">'ElectraTokenizerFast'</span><span class="p">,</span>
<span class="p">]</span>

<span class="k">def</span> <span class="nf">random_non_negative_integer</span><span class="p">(</span><span class="n">max_value</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">random</span><span class="p">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">max_value</span><span class="p">)</span>

<span class="k">class</span> <span class="nc">SpanCollator</span><span class="p">(</span><span class="n">WholeWordMaskingCollator</span><span class="p">):</span>
    <span class="s">""" Custom Collator for Span Boundary Objective Task, which is used for span masking algorithm
    Span Masking is similar to Whole Word Masking, but it has some differences:
        1) Span Masking does not use 10% of selected token left &amp; 10% of selected token replaced other vocab token
            - just replace all selected token to [MASK] token
    Algorithm:
    1) Select 2 random tokens from input tokens for spanning
    2) Calculate relative position embedding for each token with 2 random tokens froms step 1.
    3) Calculate span boundary objective with 2 random tokens from step 1 &amp; pos embedding from step 2.
    Args:
        cfg: configuration.CFG
        masking_budget: masking budget for Span Masking
                        (default: 0.15 =&gt; Recommended by original paper)
        span_probability: probability of span length for Geometric Distribution
                         (default: 0.2 =&gt; Recommended by original paper)
        max_span_length: maximum span length of each span in one batch sequence
                         (default: 10 =&gt; Recommended by original paper)
    References:
        https://arxiv.org/pdf/1907.10529.pdf
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">cfg</span><span class="p">:</span> <span class="n">CFG</span><span class="p">,</span>
        <span class="n">masking_budget</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.15</span><span class="p">,</span>
        <span class="n">span_probability</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.2</span><span class="p">,</span>
        <span class="n">max_span_length</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">SpanCollator</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">(</span><span class="n">cfg</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span> <span class="o">=</span> <span class="n">cfg</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">tokenizer</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">tokenizer</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">masking_budget</span> <span class="o">=</span> <span class="n">masking_budget</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">span_probability</span> <span class="o">=</span> <span class="n">span_probability</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">max_span_length</span> <span class="o">=</span> <span class="n">max_span_length</span>

    <span class="k">def</span> <span class="nf">_whole_word_mask</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">input_tokens</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span>
        <span class="n">max_predictions</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">CFG</span><span class="p">.</span><span class="n">max_seq</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]:</span>
        <span class="s">"""
        0) apply Whole Word Masking Algorithm for make gathering original token index in natural language
        1) calculate number of convert into masking tokens with masking budget*len(input_tokens)
        2) define span length of this iteration
            - span length follow geometric distribution
            - span length is limited by max_span_length
        """</span>
        <span class="n">cand_indexes</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">token</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">input_tokens</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">token</span> <span class="o">==</span> <span class="s">"[CLS]"</span> <span class="ow">or</span> <span class="n">token</span> <span class="o">==</span> <span class="s">"[SEP]"</span><span class="p">:</span>
                <span class="k">continue</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">cand_indexes</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="mi">1</span> <span class="ow">and</span> <span class="bp">self</span><span class="p">.</span><span class="n">select_post_string</span><span class="p">(</span><span class="n">token</span><span class="p">):</span>  <span class="c1"># method from WholeWordMaskingCollator
</span>                <span class="n">cand_indexes</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">].</span><span class="n">append</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
            <span class="k">elif</span> <span class="bp">self</span><span class="p">.</span><span class="n">select_src_string</span><span class="p">(</span><span class="n">token</span><span class="p">):</span>  <span class="c1"># method from WholeWordMaskingCollator
</span>                <span class="n">cand_indexes</span><span class="p">.</span><span class="n">append</span><span class="p">([</span><span class="n">i</span><span class="p">])</span>

        <span class="n">l</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">input_tokens</span><span class="p">)</span>
        <span class="n">src_l</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">cand_indexes</span><span class="p">)</span>
        <span class="n">num_convert_tokens</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">masking_budget</span> <span class="o">*</span> <span class="n">l</span><span class="p">)</span>
        <span class="n">budget</span> <span class="o">=</span> <span class="n">num_convert_tokens</span>  <span class="c1"># int is immutable object, so do not copy manually
</span>        <span class="n">masked_lms</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">covered_indexes</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
        <span class="k">while</span> <span class="n">budget</span><span class="p">:</span>
            <span class="n">span_length</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">min</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">distributions</span><span class="p">.</span><span class="n">Geometric</span><span class="p">(</span><span class="n">probs</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">span_probability</span><span class="p">).</span><span class="n">sample</span><span class="p">())))</span>
            <span class="n">src_index</span> <span class="o">=</span> <span class="n">random_non_negative_integer</span><span class="p">(</span><span class="n">src_l</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">span_length</span> <span class="o">&gt;</span> <span class="n">budget</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">budget</span> <span class="o">&lt;</span> <span class="mi">5</span><span class="p">:</span>  <span class="c1"># Set the span length to budget to avoid a large number of iter if the remaining budget is too small
</span>                    <span class="n">span_length</span> <span class="o">=</span> <span class="n">budget</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">continue</span>
            <span class="k">if</span> <span class="n">cand_indexes</span><span class="p">[</span><span class="n">src_index</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">span_length</span> <span class="o">&gt;</span> <span class="n">l</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>  <span class="c1"># If the index of the last token in the span is outside the full sequence range
</span>                <span class="k">continue</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">cand_indexes</span><span class="p">[</span><span class="n">src_index</span><span class="p">])</span> <span class="o">&gt;</span> <span class="n">span_length</span><span class="p">:</span>  <span class="c1"># handling bad case: violating WWM algorithm at start
</span>                <span class="k">continue</span>
            <span class="n">span_token_index</span> <span class="o">=</span> <span class="n">cand_indexes</span><span class="p">[</span><span class="n">src_index</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>  <span class="c1"># init span token index: src token
</span>            <span class="k">while</span> <span class="n">span_length</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">span_length</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="k">break</span>
                <span class="k">if</span> <span class="n">span_token_index</span> <span class="ow">in</span> <span class="n">covered_indexes</span><span class="p">:</span> <span class="c1"># If it encounters an index that is already masked, it ends, and starts the next iteration
</span>                    <span class="k">break</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">covered_indexes</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">span_token_index</span><span class="p">)</span>
                    <span class="n">masked_lms</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">span_token_index</span><span class="p">)</span>
                    <span class="n">span_length</span> <span class="o">-=</span> <span class="mi">1</span>
                    <span class="n">budget</span> <span class="o">-=</span> <span class="mi">1</span>
                    <span class="n">span_token_index</span> <span class="o">+=</span> <span class="mi">1</span>
                    <span class="k">continue</span>

        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">covered_indexes</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">masked_lms</span><span class="p">):</span>
            <span class="k">raise</span> <span class="nb">ValueError</span><span class="p">(</span><span class="s">"Length of covered_indexes is not equal to length of masked_lms."</span><span class="p">)</span>
        <span class="n">mask_labels</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span> <span class="k">if</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">covered_indexes</span> <span class="k">else</span> <span class="mi">0</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">input_tokens</span><span class="p">))]</span>
        <span class="k">return</span> <span class="n">mask_labels</span>

    <span class="k">def</span> <span class="nf">get_mask_tokens</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">inputs</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">mask_labels</span><span class="p">:</span> <span class="n">Tensor</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
        <span class="s">""" Prepare masked tokens inputs/labels for Span Boundary Objective with MLM (15%),
        All of masked tokens (15%) are replaced by [MASK] token,
        Unlike BERT MLM which is replaced by random token or stay original token left
        """</span>
        <span class="n">labels</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">.</span><span class="n">clone</span><span class="p">()</span>
        <span class="n">probability_matrix</span> <span class="o">=</span> <span class="n">mask_labels</span>

        <span class="n">special_tokens_mask</span> <span class="o">=</span> <span class="p">[</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">tokenizer</span><span class="p">.</span><span class="n">get_special_tokens_mask</span><span class="p">(</span><span class="n">val</span><span class="p">,</span> <span class="n">already_has_special_tokens</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span> <span class="k">for</span> <span class="n">val</span> <span class="ow">in</span> <span class="n">labels</span><span class="p">.</span><span class="n">tolist</span><span class="p">()</span>
        <span class="p">]</span>
        <span class="n">probability_matrix</span><span class="p">.</span><span class="n">masked_fill_</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">special_tokens_mask</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">bool</span><span class="p">),</span> <span class="n">value</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">tokenizer</span><span class="p">.</span><span class="n">pad_token</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">padding_mask</span> <span class="o">=</span> <span class="n">labels</span><span class="p">.</span><span class="n">eq</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">tokenizer</span><span class="p">.</span><span class="n">pad_token_id</span><span class="p">)</span>
            <span class="n">probability_matrix</span><span class="p">.</span><span class="n">masked_fill_</span><span class="p">(</span><span class="n">padding_mask</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)</span>

        <span class="n">masked_indices</span> <span class="o">=</span> <span class="n">probability_matrix</span><span class="p">.</span><span class="nb">bool</span><span class="p">()</span>
        <span class="n">labels</span><span class="p">[</span><span class="o">~</span><span class="n">masked_indices</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mi">100</span>  <span class="c1"># We only compute loss on masked tokens
</span>        <span class="n">inputs</span><span class="p">[</span><span class="n">masked_indices</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">tokenizer</span><span class="p">.</span><span class="n">convert_tokens_to_ids</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">tokenizer</span><span class="p">.</span><span class="n">mask_token</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">labels</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batched</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]])</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">:</span>
        <span class="s">""" Abstract Method for Collator, you must implement this method in child class """</span>
        <span class="n">input_ids</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="s">"input_ids"</span><span class="p">])</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">batched</span><span class="p">]</span>
        <span class="n">padding_mask</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="p">.</span><span class="n">get_padding_mask</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">input_ids</span><span class="p">]</span>

        <span class="n">padding_mask</span> <span class="o">=</span> <span class="n">pad_sequence</span><span class="p">(</span><span class="n">padding_mask</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">padding_value</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="n">input_ids</span> <span class="o">=</span> <span class="n">pad_sequence</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">padding_value</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

        <span class="n">mask_labels</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">batched</span><span class="p">:</span>
            <span class="n">ref_tokens</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">input_id</span> <span class="ow">in</span> <span class="n">x</span><span class="p">[</span><span class="s">"input_ids"</span><span class="p">]:</span>
                <span class="n">token</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">tokenizer</span><span class="p">.</span><span class="n">_convert_id_to_token</span><span class="p">(</span><span class="n">input_id</span><span class="p">)</span>
                <span class="n">ref_tokens</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">token</span><span class="p">)</span>
            <span class="n">mask_labels</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">_whole_word_mask</span><span class="p">(</span><span class="n">ref_tokens</span><span class="p">))</span>

        <span class="n">mask_labels</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">mask_labels</span><span class="p">]</span>
        <span class="n">mask_labels</span> <span class="o">=</span> <span class="n">pad_sequence</span><span class="p">(</span><span class="n">mask_labels</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">padding_value</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">input_ids</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">get_mask_tokens</span><span class="p">(</span>
            <span class="n">input_ids</span><span class="p">,</span>
            <span class="n">mask_labels</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="p">{</span>
            <span class="s">"input_ids"</span><span class="p">:</span> <span class="n">input_ids</span><span class="p">,</span>
            <span class="s">"labels"</span><span class="p">:</span> <span class="n">labels</span><span class="p">,</span>
            <span class="s">"padding_mask"</span><span class="p">:</span> <span class="n">padding_mask</span><span class="p">,</span>
            <span class="s">"mask_labels"</span><span class="p">:</span> <span class="n">mask_labels</span>
        <span class="p">}</span>
</code></pre></div></div>

<h4 id="sbo-head"><code class="language-plaintext highlighter-rouge">👩‍💻 SBO Head</code></h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">SBOHead</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="s">""" Custom Head for Span Boundary Objective Task, this module return logit value for each token
    we use z for class logit, each Fully Connected Layer doesn't have bias term in original paper
    so we don't use bias term in this module =&gt; nn.Linear(bias=False)

    You must select option for matrix sum or concatenate with x_s-1, x_e+1, p_i-s+1
    If you select concatenate option, you must pass is_concatenate=True to cfg.is_concatenate, default is True
    
    Math:
        h_0 = [x_s-1;x_e+1;p_i-s+1]
        h_t = LayerNorm(GELU(W_0•h_0))
        z = LayerNorm(GELU(W_1•h_t))

    Args:
        cfg: configuration.CFG
        is_concatenate: option for matrix sum or concatenate with x_s-1, x_e+1, p_i-s+1, default True
        max_span_length: maximum span length of each span in one batch sequence
                         (default: 10 =&gt; Recommended by original paper)
    References:
        https://arxiv.org/pdf/1907.10529.pdf
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">cfg</span><span class="p">:</span> <span class="n">CFG</span><span class="p">,</span>
        <span class="n">is_concatenate</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="bp">True</span><span class="p">,</span>
        <span class="n">max_span_length</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">SBOHead</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span> <span class="o">=</span> <span class="n">cfg</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">is_concatenate</span> <span class="o">=</span> <span class="n">is_concatenate</span>  <span class="c1"># for matrix sum or concatenate with x_s-1, x_e+1, p_i-s+1
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">projector</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">dim_model</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">dim_model</span><span class="o">*</span><span class="mi">3</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>  <span class="c1"># for concatenate x_s-1, x_e+1, p_i-s+1
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">span_pos_emb</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">max_span_length</span><span class="o">*</span><span class="mi">3</span><span class="p">,</span> <span class="n">cfg</span><span class="p">.</span><span class="n">dim_model</span><span class="p">)</span>  <span class="c1"># size of dim_model is research topic
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">head</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">dim_model</span><span class="o">*</span><span class="mi">3</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">dim_ffn</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">GELU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">dim_ffn</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">dim_ffn</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">dim_model</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">GELU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">dim_model</span><span class="p">),</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">classifier</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">dim_model</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">cfg</span><span class="p">.</span><span class="n">vocab_size</span><span class="p">))</span>  <span class="c1"># for matching vocab size
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">classifier</span><span class="p">.</span><span class="n">bias</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">bias</span>

    <span class="o">@</span><span class="nb">staticmethod</span>
    <span class="k">def</span> <span class="nf">find_consecutive_groups</span><span class="p">(</span><span class="n">mask_labels</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">target_value</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">Dict</span><span class="p">]]:</span>
        <span class="s">""" Get the start and end positions of consecutive groups in tensor for the target value
        This method is used for SBO Objective Function, this version is not best performance to make span groups

        Args:
            mask_labels: masking tensor for span
            target_value: target value for finding consecutive groups
        """</span>
        <span class="n">all_consecutive_groups</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">mask_label</span> <span class="ow">in</span> <span class="n">mask_labels</span><span class="p">:</span>
            <span class="n">consecutive_groups</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="n">current_group</span> <span class="o">=</span> <span class="bp">None</span>
            <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">mask_label</span><span class="p">):</span>
                <span class="k">if</span> <span class="n">value</span> <span class="o">==</span> <span class="n">target_value</span><span class="p">:</span>
                    <span class="k">if</span> <span class="n">current_group</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
                        <span class="n">current_group</span> <span class="o">=</span> <span class="p">{</span><span class="s">"start"</span><span class="p">:</span> <span class="n">i</span><span class="p">,</span> <span class="s">"end"</span><span class="p">:</span> <span class="n">i</span><span class="p">}</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">current_group</span><span class="p">[</span><span class="s">"end"</span><span class="p">]</span> <span class="o">=</span> <span class="n">i</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">if</span> <span class="n">current_group</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
                        <span class="n">consecutive_groups</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">current_group</span><span class="p">)</span>
                        <span class="n">current_group</span> <span class="o">=</span> <span class="bp">None</span>
            <span class="k">if</span> <span class="n">current_group</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
                <span class="n">consecutive_groups</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">current_group</span><span class="p">)</span>
            <span class="n">all_consecutive_groups</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">consecutive_groups</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">all_consecutive_groups</span>

    <span class="k">def</span> <span class="nf">cal_span_emb</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">h</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">consecutive_groups</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">Dict</span><span class="p">]])</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="s">""" Calculate span embedding for each span in one batch sequence

        Args:
            h: hidden states, already passed through projection layer (dim*3)
            hidden_states: hidden states from encoder
            consecutive_groups: consecutive groups for each batch sequence
        """</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">batch</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">consecutive_groups</span><span class="p">):</span>  <span class="c1"># batch level
</span>            <span class="k">for</span> <span class="n">j</span><span class="p">,</span> <span class="n">span</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">batch</span><span class="p">):</span>  <span class="c1"># span level
</span>                <span class="n">start</span><span class="p">,</span> <span class="n">end</span> <span class="o">=</span> <span class="n">span</span><span class="p">[</span><span class="s">"start"</span><span class="p">],</span> <span class="n">span</span><span class="p">[</span><span class="s">"end"</span><span class="p">]</span>
                <span class="n">length</span> <span class="o">=</span> <span class="n">end</span> <span class="o">-</span> <span class="n">start</span> <span class="o">+</span> <span class="mi">1</span>
                <span class="n">idx</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="n">length</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>   <span class="c1"># .to(self.cfg.device)
</span>                <span class="n">context_s</span><span class="p">,</span> <span class="n">context_e</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">start</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="p">:],</span> <span class="n">hidden_states</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">end</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="p">:]</span>
                <span class="n">span_pos_emb</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">span_pos_emb</span><span class="p">(</span><span class="n">idx</span><span class="p">).</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">length</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                    <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">p_h</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">span_pos_emb</span><span class="p">):</span>  <span class="c1"># length of span_pos_emb == length of span of this iterations
</span>                        <span class="n">h</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">start</span><span class="o">+</span><span class="n">k</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cat</span><span class="p">([</span><span class="n">context_s</span><span class="p">,</span> <span class="n">p_h</span><span class="p">,</span> <span class="n">context_e</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">h</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">start</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cat</span><span class="p">([</span><span class="n">context_s</span><span class="p">,</span> <span class="n">span_pos_emb</span><span class="p">,</span> <span class="n">context_e</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">h</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">mask_labels</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="n">consecutive_groups</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">find_consecutive_groups</span><span class="p">(</span><span class="n">mask_labels</span><span class="p">)</span>  <span class="c1"># [batch, num_consecutive_groups]
</span>        <span class="n">h</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">projector</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>  <span class="c1"># [batch, seq, dim_model*3]
</span>        <span class="n">h_t</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">cal_span_emb</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">,</span> <span class="n">consecutive_groups</span><span class="p">)</span>
        <span class="n">z</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">head</span><span class="p">(</span><span class="n">h_t</span><span class="p">)</span>
        <span class="n">logit</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">classifier</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">logit</span>

</code></pre></div></div>]]></content><author><name>qcqced</name><email>qcqced123@gmail.com</email></author><category term="NLP" /><category term="Natural Language Process" /><category term="SpanBERT" /><category term="BERT" /><category term="Self-Attention" /><category term="Pytorch" /><summary type="html"><![CDATA[SpanBERT Official Paper Review with Pytorch Implementation]]></summary></entry><entry><title type="html">👩‍💻🎄 [baekjoon] 1987번: 알파벳</title><link href="http://localhost:4000/ps/baekjoon-1987" rel="alternate" type="text/html" title="👩‍💻🎄 [baekjoon] 1987번: 알파벳" /><published>2024-01-30T00:00:00+09:00</published><updated>2024-01-31T02:00:00+09:00</updated><id>http://localhost:4000/ps/baekjoon_1987</id><content type="html" xml:base="http://localhost:4000/ps/baekjoon-1987"><![CDATA[<h3 id="️solution"><strong><code class="language-plaintext highlighter-rouge">🖍️ solution</code></strong></h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">sys</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">List</span>

<span class="k">def</span> <span class="nf">backtracking</span><span class="p">(</span><span class="n">y</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">count</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">visit</span><span class="p">:</span> <span class="n">List</span><span class="p">,</span> <span class="n">graph</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">]):</span>
    <span class="k">global</span> <span class="n">result</span>
    <span class="n">visit</span><span class="p">[</span><span class="nb">ord</span><span class="p">(</span><span class="n">graph</span><span class="p">[</span><span class="n">y</span><span class="p">][</span><span class="n">x</span><span class="p">])</span> <span class="o">-</span> <span class="mi">65</span><span class="p">]</span> <span class="o">=</span> <span class="bp">True</span>
    <span class="n">result</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">count</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">):</span>
        <span class="n">ny</span><span class="p">,</span> <span class="n">nx</span> <span class="o">=</span> <span class="n">dy</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">y</span><span class="p">,</span> <span class="n">dx</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">x</span>
        <span class="k">if</span> <span class="o">-</span><span class="mi">1</span> <span class="o">&lt;</span> <span class="n">ny</span> <span class="o">&lt;</span> <span class="n">r</span> <span class="ow">and</span> <span class="o">-</span><span class="mi">1</span> <span class="o">&lt;</span> <span class="n">nx</span> <span class="o">&lt;</span> <span class="n">c</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">visit</span><span class="p">[</span><span class="nb">ord</span><span class="p">(</span><span class="n">graph</span><span class="p">[</span><span class="n">ny</span><span class="p">][</span><span class="n">nx</span><span class="p">])</span> <span class="o">-</span> <span class="mi">65</span><span class="p">]:</span>
            <span class="n">backtracking</span><span class="p">(</span><span class="n">ny</span><span class="p">,</span> <span class="n">nx</span><span class="p">,</span> <span class="n">count</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">visit</span><span class="p">,</span> <span class="n">graph</span><span class="p">)</span>
            <span class="n">visit</span><span class="p">[</span><span class="nb">ord</span><span class="p">(</span><span class="n">graph</span><span class="p">[</span><span class="n">ny</span><span class="p">][</span><span class="n">nx</span><span class="p">])</span> <span class="o">-</span> <span class="mi">65</span><span class="p">]</span> <span class="o">=</span> <span class="bp">False</span>

<span class="n">r</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="nb">map</span><span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="n">sys</span><span class="p">.</span><span class="n">stdin</span><span class="p">.</span><span class="n">readline</span><span class="p">().</span><span class="n">split</span><span class="p">())</span>

<span class="n">result</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
<span class="n">dy</span><span class="p">,</span> <span class="n">dx</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">grid</span><span class="p">,</span> <span class="n">visited</span> <span class="o">=</span> <span class="p">[</span><span class="nb">list</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="nb">str</span><span class="p">,</span> <span class="n">sys</span><span class="p">.</span><span class="n">stdin</span><span class="p">.</span><span class="n">readline</span><span class="p">().</span><span class="n">rstrip</span><span class="p">()))</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">r</span><span class="p">)],</span> <span class="p">[</span><span class="bp">False</span><span class="p">]</span> <span class="o">*</span> <span class="mi">26</span>
<span class="n">backtracking</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">visited</span><span class="p">,</span> <span class="n">grid</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="nb">max</span><span class="p">(</span><span class="n">result</span><span class="p">))</span>
</code></pre></div></div>

<h3 id="idea"><strong><code class="language-plaintext highlighter-rouge">💡 idea</code></strong></h3>

<ul>
  <li><strong>Back Tracking</strong></li>
  <li><strong>1) 방문 기록 배열 변경</strong>
    <ul>
      <li><strong>조건 중에서 경로에 알파벳 중복이 불가능하다는 점 이용</strong></li>
      <li><strong>전체 격자 사이즈와 동일한 배열 대신 알파벳 사이즈(26)만 선언</strong></li>
    </ul>
  </li>
</ul>

<p>일반적인 백트래킹 문제라고 볼 수 있다. 하지만 파이썬으로 해결하려는 경우 시간, 메모리 제한 때문에 빡센 코드 최적화가 필요하다. 격자 문제라서 <code class="language-plaintext highlighter-rouge">bfs</code> 선택도 가능한데 그렇다면 <code class="language-plaintext highlighter-rouge">python3</code>로도 해결가능하다. 한편, 일반적인 <code class="language-plaintext highlighter-rouge">dfs</code>라면 빡센 최적화를 통해 <code class="language-plaintext highlighter-rouge">pypy3</code>으로만 통과 가능하다.</p>

<p>문제를 리뷰하던 도중 일반적인 <code class="language-plaintext highlighter-rouge">dfs</code> 백트래킹 방식의 비효율성에 대해 고찰해봤다. 아래와 같은 입력이 있다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">IEFCJ</span>
<span class="n">FHFKC</span>
<span class="n">FFALF</span>
<span class="n">HFGCF</span>
<span class="n">HMCHH</span>
</code></pre></div></div>

<p>일반적인 백트래킹 알고리즘이 탐색하는 과정을 생각해보자. 빨간색으로 칠해진 글자를 <code class="language-plaintext highlighter-rouge">IFHE</code> 순서로 탐색했다면, 다음은 <code class="language-plaintext highlighter-rouge">F</code>를 탐색해 방문해도 되는지 여부를 판정할 것이다. 이미 <code class="language-plaintext highlighter-rouge">F</code>는 방문했기 때문에 아마도 스택 프레임 할당을 취소하면서, 결국에는 <code class="language-plaintext highlighter-rouge">I</code>까지 되돌아 갈 것이다.</p>

<p>그리고 다시 오른쪽에 있는 <code class="language-plaintext highlighter-rouge">E</code>를  방문한 뒤, <code class="language-plaintext highlighter-rouge">FCK</code> 순서로 방문하게 될 것이다. 이 때 들게 되는 의문은 바로 이렇다. 굳이 <code class="language-plaintext highlighter-rouge">I</code>까지 되돌아갔다가 탐색해야 할까?? 이미 <code class="language-plaintext highlighter-rouge">IE</code> 는 탐색이 가능한 경로라는 것을 우리는 충분히 알 수 있다. 따라서 <code class="language-plaintext highlighter-rouge">DP Tabulation</code> 개념을 차용한다면 훨씬 빠르게 풀이가 가능할 것이다.</p>

<p>경로의 유일성을 보장하면서 수정 가능한 자료구조가 필요하기 때문에 배열 대신 세트를 사용해보자. 세트에는 현재까지의 경로 그리고 해당 경로의 마지막 인덱스를 저장해줘야 한다. 같은 경로라고 할 지라도 서로 다른 인덱스에 의해 만들어졌을 가능성이 있기 때문이다. 이렇게 세트를 구성한 뒤, 하나씩 pop해서 경로를 얻어낸다. 그 다음 해당 경로로부터 파생되는 여러 잠재적 경로들을 모두 검사해 경로가 만들어질 수 있는지 여부를 판정하면 된다. 코드는 다음과 같다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">sys</span>

<span class="k">def</span> <span class="nf">dfs</span><span class="p">(</span><span class="n">y</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
    <span class="n">dp</span><span class="p">,</span> <span class="n">result</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(),</span> <span class="mi">0</span>
    <span class="n">dp</span><span class="p">.</span><span class="n">add</span><span class="p">((</span><span class="n">y</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">grid</span><span class="p">[</span><span class="n">y</span><span class="p">][</span><span class="n">x</span><span class="p">]))</span>
    <span class="k">while</span> <span class="n">dp</span><span class="p">:</span>
        <span class="n">vy</span><span class="p">,</span> <span class="n">vx</span><span class="p">,</span> <span class="n">path</span> <span class="o">=</span> <span class="n">dp</span><span class="p">.</span><span class="n">pop</span><span class="p">()</span>
        <span class="n">result</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">result</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">path</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">result</span> <span class="o">==</span> <span class="mi">26</span><span class="p">:</span>
            <span class="k">return</span> <span class="mi">26</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">):</span>
            <span class="n">ny</span><span class="p">,</span> <span class="n">nx</span> <span class="o">=</span> <span class="n">dy</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">vy</span><span class="p">,</span> <span class="n">dx</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">vx</span>
            <span class="k">if</span> <span class="o">-</span><span class="mi">1</span> <span class="o">&lt;</span> <span class="n">ny</span> <span class="o">&lt;</span> <span class="n">r</span> <span class="ow">and</span> <span class="o">-</span><span class="mi">1</span> <span class="o">&lt;</span> <span class="n">nx</span> <span class="o">&lt;</span> <span class="n">c</span> <span class="ow">and</span> <span class="n">grid</span><span class="p">[</span><span class="n">ny</span><span class="p">][</span><span class="n">nx</span><span class="p">]</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">path</span><span class="p">:</span>
                <span class="n">dp</span><span class="p">.</span><span class="n">add</span><span class="p">((</span><span class="n">ny</span><span class="p">,</span> <span class="n">nx</span><span class="p">,</span> <span class="n">grid</span><span class="p">[</span><span class="n">ny</span><span class="p">][</span><span class="n">nx</span><span class="p">]</span> <span class="o">+</span> <span class="n">path</span><span class="p">))</span>
                
    <span class="k">return</span> <span class="n">result</span>

<span class="n">r</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="nb">map</span><span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="n">sys</span><span class="p">.</span><span class="n">stdin</span><span class="p">.</span><span class="n">readline</span><span class="p">().</span><span class="n">split</span><span class="p">())</span>
<span class="n">dy</span><span class="p">,</span> <span class="n">dx</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">grid</span> <span class="o">=</span> <span class="p">[</span><span class="nb">list</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="nb">str</span><span class="p">,</span> <span class="n">sys</span><span class="p">.</span><span class="n">stdin</span><span class="p">.</span><span class="n">readline</span><span class="p">().</span><span class="n">rstrip</span><span class="p">()))</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">r</span><span class="p">)]</span>
<span class="k">print</span><span class="p">(</span><span class="n">dfs</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
</code></pre></div></div>

<p align="center">
<img src="/assets/images/ps/after.png" alt="Common BackTracking" class="align-center image-caption" width="100%&quot;, height=&quot;100%" />
<strong><em>Common BackTracking</em></strong>
</p>

<p align="center">
<img src="/assets/images/ps/before.png" alt="DP Tabulation BackTracking" class="align-center image-caption" width="100%&quot;, height=&quot;100%" />
<strong><em>DP Tabulation BackTracking</em></strong>
</p>

<p>위에는 개선이전 결과고 아래는 개선 이후 결과다. 비약적인 속도 상승하는 동시에 메모리 역시 3배나 덜 사용하는 모습이다. 세트에 있는 유니크한 경로들을 하나씩 꺼내는 방식을 선택했기 때문에 알고리즘 성능이 시드에 영향(<code class="language-plaintext highlighter-rouge">set.pop()</code>은 랜덤으로 원소 선택)을 받는다는 점만 감안한다면 매우 좋은 풀이라고 생각한다.</p>]]></content><author><name>qcqced</name><email>qcqced123@gmail.com</email></author><category term="Problem Solving" /><category term="Python" /><category term="Codeing Test" /><category term="Algorithm" /><category term="Baekjoon" /><category term="Graph" /><category term="DFS" /><category term="BackTracking" /><summary type="html"><![CDATA[백준 1987번: 알파벳]]></summary></entry></feed>