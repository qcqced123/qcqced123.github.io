<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2024-03-11T11:28:55+09:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">AI/Business Study Log</title><subtitle>NLP, Marketing</subtitle><author><name>qcqced</name><email>qcqced123@gmail.com</email></author><entry><title type="html">🌆 [ELECTRA] Pre-training Text Encoders as Discriminators Rather Than Generators</title><link href="http://localhost:4000/nlp/electra" rel="alternate" type="text/html" title="🌆 [ELECTRA] Pre-training Text Encoders as Discriminators Rather Than Generators" /><published>2024-03-11T00:00:00+09:00</published><updated>2024-03-12T02:00:00+09:00</updated><id>http://localhost:4000/nlp/electra</id><content type="html" xml:base="http://localhost:4000/nlp/electra"><![CDATA[<h3 id="overview"><code class="language-plaintext highlighter-rouge">🔭 Overview</code></h3>

<p><code class="language-plaintext highlighter-rouge">ELECTRA</code>는 2020년 Google에서 처음 발표한 모델로, GAN(Generative Adversarial Networks) Style 아키텍처를 NLP에 적용한 것이 특징이다. 새로운 구조 차용에 맞춰서 <code class="language-plaintext highlighter-rouge">RTD(Replace Token Dection)</code> Task를 고안에 사전 학습으로 사용했다. 모든 아이디어는 기존 MLM(Masked Language Model)을 사전학습 방법론으로 사용하는 인코더 언어 모델(BERT 계열)의 단점으로부터 출발한다.</p>

<p><strong>[MLM 단점]</strong></p>
<ul>
  <li>1) 사전학습과 파인튜닝 사이 불일치
    <ul>
      <li>파인튜닝 때 Masking Task가 없음</li>
    </ul>
  </li>
  <li>2) 연산량 대비 학습량은 적은편
    <ul>
      <li>전체 시퀀스의 15%만 마스킹 활용(15%만 학습)</li>
      <li>전역 어텐션의 시공간 복잡도 고려하면 상당히 비효율적인 수치
        <ul>
          <li>시퀀스길이 ** 2의 복잡도</li>
          <li>Vocab Size만큼의 차원을 갖는 소프트맥스 계산 반복</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p>그래서 MLM은 활용하되, 파인튜닝과 괴리는 크지 않은 목적함수를 설계함으로서 입력된 전체 시퀀스에 대해서 모델이 학습하여 연산량 대비 학습량을 늘리고자 했던게 바로 ELECTRA 모델이다.</p>

<p>정리하자면, ELECTRA 모델은 기존 BERT의 구조적 측면 개선이 아닌, 사전학습 방법에 대한 개선 시도라고 볼 수 있다. 따라서 어떤 모델이더라도, 인코더 언어 모델이라면 모두 ELECTRA 구조를 사용할 수 있으며, 기존 논문에서는 원본 BERT 구조를 사용했다. 그래서 본 포스팅에서도 BERT에 대한 설명 없이 RTD에 대해서만 다루려고 한다.</p>

<h3 id="rtd-new-pre-train-task"><code class="language-plaintext highlighter-rouge">🌆 RTD: New Pre-train Task</code></h3>

<p align="center">
<img src="/assets/images/electra/electra.png" alt="RTD Task" class="align-center image-caption" width="100%&quot;, height=&quot;100%" />
<strong><em><a href="https://arxiv.org/abs/2003.10555">RTD Task</a></em></strong>
</p>

<p>RTD의 아이디어는 간단하다. 생성자(Generator)가 출력으로 내놓은 토큰 시퀀스에 대해서 판별자(Discriminator)가 개별 토큰들이 원본인지 아닌지를 판정(이진 분류)하도록 만든다. 생성자는 기존의 MLM을 그대로 수행하고, 판별자는 생성자의 예측에 대해 진짜인지 가짜인지 분류하는 식이다.</p>

<p>위 그림을 예시로 살펴보자. 모델에 입력으로 <code class="language-plaintext highlighter-rouge">the chef cooked the meal</code>라는 시퀀스 준다. 그러면 MLM 규칙에 따라서 15%의 토큰이 무작위로 선택된다. 그래서 <code class="language-plaintext highlighter-rouge">the</code>, <code class="language-plaintext highlighter-rouge">cooked</code>가 마스킹 되었다. 이제 생성자는 마스킹 토큰에 대해 <code class="language-plaintext highlighter-rouge">the</code>, <code class="language-plaintext highlighter-rouge">ate</code>라는 결과를 내놓는다. 그래서 최종적으로 생성자가 반환하는 시퀀스는 <code class="language-plaintext highlighter-rouge">the chef ate the meal</code>이 된다. 이제 생성자가 반환한 시퀀스를 판별자에 입력으로 대입한다. 판별자는 개별 토큰들이 원본인지 아닌지를 판정해 결과를 출력한다.</p>

<p>이러한 구조 및 사전학습 방식의 장점은 판별자가 MLM 학습에 따른 지식을 생성자로부터 전수 받는 동시에 전체 시퀀스에 대해서 학습할 기회가 생긴다는 것이다. 시퀀스 내부 모든 토큰에 대해서 예측을 수행하고 손실을 계산할 수 있기 때문에 같은 크기의 시퀀스를 사용해도 기존 MLM 대비 더 풍부한 문맥 정보를 모델이 포착할 수 있게 된다. 또한 판별자를 파인튜닝의 BackBone으로 사용하면, 판별자의 사전학습은 결국 마스킹 없이 모든 시퀀스를 활용한 이진 분류라고 볼 수 있기 때문에, 사전학습과 파인튜닝 사이의 괴리도 상당히 많이 줄어들게 된다.</p>

<h3 id="architecture"><code class="language-plaintext highlighter-rouge">🌟 Architecture</code></h3>

<p align="center">
<img src="/assets/images/electra/electra_experiment.png" alt="Model Architecture" class="align-center image-caption" width="100%&quot;, height=&quot;100%" />
<strong><em><a href="https://arxiv.org/abs/2003.10555">Model Architecture</a></em></strong>
</p>

<p>저자는 위와 같은 실험 결과를 바탕으로, 생성자의 width (은닉층) 크기가 판별자보다 작도록 모델 크기를 세팅하는게 가장 효율적이라고 주장한다. 제시된 그래프는 생성자와 판별자의 크기 변화 대비 파인튜닝 성능의 추이를 나타낸다. 생성자의 width 크기가 256, 판별자의 width 크기가 768일 때 가장 점수가 높다. depth(레이어 개수)에 대한 언급은 따로 없지만, 저자에 의해 공개된 Hyper-Param 테이블을 보면 은닉층의 크기만 줄이고, 레이어 개수는 생성자와 판별자가 같은 것으로 추정된다.</p>

<p>추가로, 생성자와 판별자가 임베딩 층을 서로 공유하는게 가장 높은 성능을 낸다고 주장한다. 오른쪽 그래프 추이를 보면 같은 연산량이라면, 임베딩 공유(파란색 실선) 방식이 가장 높은 파인튜닝 성능을 보여준다는 것을 알 수 있다. 따라서 단어 임베딩, 절대 위치 임베딩을 서로 공유하도록 설계한다. 대신 생성자 은닉층의 크기가 더 작은게 유리하다고 언급했기 때문에, 이것을 실제로 구현하려면 임베딩 층으로부터 나온 결과값을 생성자의 은닉층 차원으로 선형 투영해줘야 한다. 그래서 생성자의 임베딩 층과 인코더 사이에 linear layer가 추가되어야 한다.</p>

\[\min_{\theta_G, \theta_D}\sum_{x \in X} \mathcal{L}_{\text{MLM}}(x, \theta_G) + \lambda \mathcal{L}_{\text{Disc}}(x, \theta_D)\]

<p>따라서, 지금까지 살펴본 모든 내용을 종합해보면 ELECTRA의 목적함수는 다음 수식과 같다. 생성자의 MLM 손실과 판별자의 이진 분류 손실을 더해서 모델에 오차 역전해주면 되는데, 특이한 점은 판별자의 손실에 상수값인 람다가 곱해진다는 것이다. 실제 모델을 구현하고 사전학습을 해보면, 데이터의 양이나 모델 크기 혹은 종류에 따라 달라지겠지만 두 손실 사이의 스케일의 차이가 10배정도 차이 나게 된다. 두 손실의 스케일을 맞춰주는 동시에, 임베딩층의 학습이 판별자의 손실을 줄이는데 더 집중하도록 만들기 위해 도입한 것으로 추정된다. 논문과 코드를 보면 저자는 $\lambda=50$ 으로 두고 학습하고 있다.</p>

<h3 id="implementation-by-pytorch"><code class="language-plaintext highlighter-rouge">👩‍💻 Implementation by Pytorch</code></h3>

<p>논문의 내용과 저자가 직접 공개한 코드를 종합하여 파이토치로 ELECTRA를 구현해봤다. 두 개의 서로 다른 모델을 같은 스탭에서 학습시켜야 하기 때문에, 제시된 내용에 비해 실제 구현은 매우 까다로운 편이었다. 본 포스팅에서는 ELECTRA 모델 구조를 비롯해 RTD 학습 파이프라인 구성에 필수적인 요소 몇 가지에 대해서만 설명하려 한다. 전체 구조에 대한 코드는 <strong><a href="https://github.com/qcqced123/model_study">여기 링크</a></strong>를 통해 참고 부탁드린다.</p>

<p>ELECTRA의 사전 학습인 RTD의 학습 파이프라인을 구현한 코드를 본 뒤, 세부 구성 요소들에 대해서 살펴보자.</p>

<h4 id="-rtd-trainer-method"><strong><code class="language-plaintext highlighter-rouge">🌆 RTD trainer method</code></strong></h4>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">train_val_fn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">loader_train</span><span class="p">,</span> <span class="n">model</span><span class="p">:</span> <span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">criterion</span><span class="p">:</span> <span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">scheduler</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]:</span>
  <span class="n">scaler</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">amp</span><span class="p">.</span><span class="n">GradScaler</span><span class="p">(</span><span class="n">enabled</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">amp_scaler</span><span class="p">)</span>
  <span class="n">model</span><span class="p">.</span><span class="n">train</span><span class="p">()</span>
  <span class="k">for</span> <span class="n">step</span><span class="p">,</span> <span class="n">batch</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">tqdm</span><span class="p">(</span><span class="n">loader_train</span><span class="p">)):</span>
      <span class="n">optimizer</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">(</span><span class="n">set_to_none</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
      <span class="n">inputs</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="s">'input_ids'</span><span class="p">].</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">device</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
      <span class="n">labels</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="s">'labels'</span><span class="p">].</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">device</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>  
      <span class="n">padding_mask</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="s">'padding_mask'</span><span class="p">].</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">device</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>  

      <span class="n">mask_labels</span> <span class="o">=</span> <span class="bp">None</span>
      <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">rtd_masking</span> <span class="o">==</span> <span class="s">'SpanBoundaryObjective'</span><span class="p">:</span>
          <span class="n">mask_labels</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="s">'mask_labels'</span><span class="p">].</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">device</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

      <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">amp</span><span class="p">.</span><span class="n">autocast</span><span class="p">(</span><span class="n">enabled</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">amp_scaler</span><span class="p">):</span>
          <span class="n">g_logit</span><span class="p">,</span> <span class="n">d_inputs</span><span class="p">,</span> <span class="n">d_labels</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">generator_fw</span><span class="p">(</span>
              <span class="n">inputs</span><span class="p">,</span>
              <span class="n">labels</span><span class="p">,</span>
              <span class="n">padding_mask</span><span class="p">,</span>
              <span class="n">mask_labels</span>
          <span class="p">)</span>
          <span class="n">d_logit</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">discriminator_fw</span><span class="p">(</span>
              <span class="n">d_inputs</span><span class="p">,</span>
              <span class="n">padding_mask</span>
          <span class="p">)</span>
          <span class="n">g_loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">g_logit</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">vocab_size</span><span class="p">),</span> <span class="n">labels</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>
          <span class="n">d_loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">d_logit</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">d_labels</span><span class="p">)</span>
          <span class="n">loss</span> <span class="o">=</span> <span class="n">g_loss</span> <span class="o">+</span> <span class="n">d_loss</span><span class="o">*</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">discriminator_lambda</span>

      <span class="n">scaler</span><span class="p">.</span><span class="n">scale</span><span class="p">(</span><span class="n">loss</span><span class="p">).</span><span class="n">backward</span><span class="p">()</span>
      <span class="n">scaler</span><span class="p">.</span><span class="n">step</span><span class="p">(</span><span class="n">optimizer</span><span class="p">)</span>
      <span class="n">scaler</span><span class="p">.</span><span class="n">update</span><span class="p">()</span>
      <span class="n">scheduler</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>
</code></pre></div></div>
<p>데이터로더로부터 받은 입력들을 생성자에 넣고 MLM 예측 결과, RTD 수행을 위해 필요한 새로운 라벨값을 반환 받는다. 그리고 이것을 다시 판별자의 입력으로 사용하고, 판별자의 예측 결과를 반환받아 서로 다른 두 모델에 대한 가중 손실합산을 구한 뒤, 옵티마이저에 보내고 최적화를 수행한다. 이 때, 처음에 데이터로더가 반환하는 입력 시퀀스와 라벨은 MLM의 그것과 동일하다,</p>

<p>구현하면서 가장 어려웠던게, 옵티마이저 및 스케줄러의 구성이었다. 두 개의 모델을 같은 스탭에서 학습시키는 경험이 처음이라서 처음에 모델 개수만큼 옵티마이저와 스케줄러 객체를 만들어줘야 한다고 생각했다. 특히 두 모델의 스케일이 다르기 때문에 서로 다른 옵티마이저, 스케줄러 도입으로 각기 다른 학습률을 적용하는게 정확할 것이라 생각했다.</p>

<p>하지만, 옵티마이저를 두 개 사용하는 것은 매우 많은 메모리를 차지할 뿐더러 논문에서 공개한 하이퍼파라미터 테이블을 보면 두 모델에 같은 학습률을 적용하고 있는 것을 알 수 있었다. 따라서 그에 맞게 같은 옵티마이저, 스케줄러를 사용해 동시에 두 모델이 학습되도록 파이프라인을 만들게 되었다.</p>

<p>추가로, 공개된 오피셜 코드 역시 단일 옵티마이저 및 스케줄러를 사용하는 것을 확인했다.</p>

<h4 id="-electra-module"><strong><code class="language-plaintext highlighter-rouge">🌆 ELECTRA Module</code></strong></h4>
<p>🌟 Architecture</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="nn">experiment.models.abstract_model</span> <span class="kn">import</span> <span class="n">AbstractModel</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">Tensor</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">Callable</span>
<span class="kn">from</span> <span class="nn">einops.layers.torch</span> <span class="kn">import</span> <span class="n">Rearrange</span>
<span class="kn">from</span> <span class="nn">experiment.tuner.mlm</span> <span class="kn">import</span> <span class="n">MLMHead</span>
<span class="kn">from</span> <span class="nn">experiment.tuner.sbo</span> <span class="kn">import</span> <span class="n">SBOHead</span>
<span class="kn">from</span> <span class="nn">experiment.tuner.rtd</span> <span class="kn">import</span> <span class="n">get_discriminator_input</span><span class="p">,</span> <span class="n">RTDHead</span>
<span class="kn">from</span> <span class="nn">configuration</span> <span class="kn">import</span> <span class="n">CFG</span>

<span class="k">class</span> <span class="nc">ELECTRA</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">AbstractModel</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">cfg</span><span class="p">:</span> <span class="n">CFG</span><span class="p">,</span> <span class="n">model_func</span><span class="p">:</span> <span class="n">Callable</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">ELECTRA</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span> <span class="o">=</span> <span class="n">cfg</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">generator</span> <span class="o">=</span> <span class="n">model_func</span><span class="p">(</span><span class="n">cfg</span><span class="p">.</span><span class="n">generator_num_layers</span><span class="p">)</span>  <span class="c1"># init generator
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">mlm_head</span> <span class="o">=</span> <span class="n">MLMHead</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">rtd_masking</span> <span class="o">==</span> <span class="s">'SpanBoundaryObjective'</span><span class="p">:</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">mlm_head</span> <span class="o">=</span> <span class="n">SBOHead</span><span class="p">(</span>
                <span class="n">cfg</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">,</span>
                <span class="n">is_concatenate</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">is_concatenate</span><span class="p">,</span>
                <span class="n">max_span_length</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">max_span_length</span>
            <span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">discriminator</span> <span class="o">=</span> <span class="n">model_func</span><span class="p">(</span><span class="n">cfg</span><span class="p">.</span><span class="n">discriminator_num_layers</span><span class="p">)</span>  <span class="c1"># init generator
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">rtd_head</span> <span class="o">=</span> <span class="n">RTDHead</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">share_embed_method</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">share_embed_method</span>  <span class="c1"># instance, es, gdes
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">share_embedding</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">share_embedding</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="k">def</span> <span class="nf">discriminator_hook</span><span class="p">(</span><span class="n">module</span><span class="p">:</span> <span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">,</span> <span class="o">*</span><span class="n">inputs</span><span class="p">):</span>
            <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">share_embed_method</span> <span class="o">==</span> <span class="s">'instance'</span><span class="p">:</span>  <span class="c1"># Instance Sharing
</span>                <span class="bp">self</span><span class="p">.</span><span class="n">discriminator</span><span class="p">.</span><span class="n">embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">generator</span><span class="p">.</span><span class="n">embeddings</span>

            <span class="k">elif</span> <span class="bp">self</span><span class="p">.</span><span class="n">share_embed_method</span> <span class="o">==</span> <span class="s">'ES'</span><span class="p">:</span>  <span class="c1"># ES (Embedding Sharing)
</span>                <span class="bp">self</span><span class="p">.</span><span class="n">discriminator</span><span class="p">.</span><span class="n">embeddings</span><span class="p">.</span><span class="n">word_embedding</span><span class="p">.</span><span class="n">weight</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">generator</span><span class="p">.</span><span class="n">embeddings</span><span class="p">.</span><span class="n">word_embedding</span><span class="p">.</span><span class="n">weight</span>
                <span class="bp">self</span><span class="p">.</span><span class="n">discriminator</span><span class="p">.</span><span class="n">embeddings</span><span class="p">.</span><span class="n">abs_pos_emb</span><span class="p">.</span><span class="n">weight</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">generator</span><span class="p">.</span><span class="n">embeddings</span><span class="p">.</span><span class="n">abs_pos_emb</span><span class="p">.</span><span class="n">weight</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">discriminator</span><span class="p">.</span><span class="n">register_forward_pre_hook</span><span class="p">(</span><span class="n">discriminator_hook</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">generator_fw</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span>
            <span class="n">inputs</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
            <span class="n">labels</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
            <span class="n">padding_mask</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
            <span class="n">mask_labels</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
            <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="bp">None</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
        <span class="n">g_last_hidden_states</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">generator</span><span class="p">(</span>
            <span class="n">inputs</span><span class="p">,</span>
            <span class="n">padding_mask</span><span class="p">,</span>
            <span class="n">attention_mask</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">rtd_masking</span> <span class="o">==</span> <span class="s">'MaskedLanguageModel'</span><span class="p">:</span>
            <span class="n">g_logit</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">mlm_head</span><span class="p">(</span>
                <span class="n">g_last_hidden_states</span>
            <span class="p">)</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">rtd_masking</span> <span class="o">==</span> <span class="s">'SpanBoundaryObjective'</span><span class="p">:</span>
            <span class="n">g_logit</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">mlm_head</span><span class="p">(</span>
                <span class="n">g_last_hidden_states</span><span class="p">,</span>
                <span class="n">mask_labels</span>
            <span class="p">)</span>
        <span class="n">pred</span> <span class="o">=</span> <span class="n">g_logit</span><span class="p">.</span><span class="n">clone</span><span class="p">().</span><span class="n">detach</span><span class="p">()</span>
        <span class="n">d_inputs</span><span class="p">,</span> <span class="n">d_labels</span> <span class="o">=</span> <span class="n">get_discriminator_input</span><span class="p">(</span>
            <span class="n">inputs</span><span class="p">,</span>
            <span class="n">labels</span><span class="p">,</span>
            <span class="n">pred</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">g_logit</span><span class="p">,</span> <span class="n">d_inputs</span><span class="p">,</span> <span class="n">d_labels</span>

    <span class="k">def</span> <span class="nf">discriminator_fw</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span>
            <span class="n">inputs</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
            <span class="n">padding_mask</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
            <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="bp">None</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="n">d_last_hidden_states</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">discriminator</span><span class="p">(</span>
            <span class="n">inputs</span><span class="p">,</span>
            <span class="n">padding_mask</span><span class="p">,</span>
            <span class="n">attention_mask</span>
        <span class="p">)</span>
        <span class="n">d_logit</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">rtd_head</span><span class="p">(</span>
            <span class="n">d_last_hidden_states</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">d_logit</span>

</code></pre></div></div>
<p>ELECTRA 모델 객체는 크게 임배딩 레이어 공유, 생성자 포워드, 판별자 포워드 파트로 나뉜다. 먼저 임베딩 레이어 공유는 크게 두 가지 방식으로 구현 가능하다. 하나는 임베딩 레이어 인스턴스 자체를 공유하는 방식으로, 생성자와 판별자의 스케일이 동일할 때 사용할 수 있다. 나머지는 단어 임베딩, 포지션 임베딩의 가중치 행렬만 공유하는 방식으로, 서로 스케일이 달라도 사용할 수 있다. 논문에서 제시하는 가장 효율적인 방법은 후자이며, 판별자의 임베딩 행렬이 생성자의 임베딩 행렬의 주소를 가리키도록 함으로서 구현 가능하다.</p>

<h4 id="-rtd-input-making"><strong><code class="language-plaintext highlighter-rouge">🌆 RTD Input Making</code></strong></h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">Tensor</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Tuple</span>
<span class="kn">from</span> <span class="nn">configuration</span> <span class="kn">import</span> <span class="n">CFG</span>

<span class="k">def</span> <span class="nf">get_discriminator_input</span><span class="p">(</span><span class="n">inputs</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">labels</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">pred</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
    <span class="s">""" Post Processing for Replaced Token Detection Task
    1) get index of the highest probability of [MASK] token in pred tensor
    2) convert [MASK] token to prediction token
    3) make label for Discriminator

    Args:
        inputs: pure inputs from tokenizing by tokenizer
        labels: labels for masked language modeling
        pred: prediction tensor from Generator

    returns:
        d_inputs: torch.Tensor, shape of [Batch, Sequence], for Discriminator inputs
        d_labels: torch.Tensor, shape of [Sequence], for Discriminator labels
    """</span>
    <span class="c1"># 1) flatten pred to 2D Tensor
</span>    <span class="n">d_inputs</span><span class="p">,</span> <span class="n">d_labels</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">.</span><span class="n">clone</span><span class="p">().</span><span class="n">detach</span><span class="p">().</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="bp">None</span>  <span class="c1"># detach to prevent back-propagation
</span>    <span class="n">flat_pred</span><span class="p">,</span> <span class="n">flat_label</span> <span class="o">=</span> <span class="n">pred</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">pred</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)),</span> <span class="n">labels</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># (batch * sequence, vocab_size)
</span>
    <span class="c1"># 2) get index of the highest probability of [MASK] token
</span>    <span class="n">pred_token_idx</span><span class="p">,</span> <span class="n">mlm_mask_idx</span> <span class="o">=</span> <span class="n">flat_pred</span><span class="p">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">),</span> <span class="n">torch</span><span class="p">.</span><span class="n">where</span><span class="p">(</span><span class="n">flat_label</span> <span class="o">!=</span> <span class="o">-</span><span class="mi">100</span><span class="p">)</span>
    <span class="n">pred_tokens</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">index_select</span><span class="p">(</span><span class="n">pred_token_idx</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">mlm_mask_idx</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

    <span class="c1"># 3) convert [MASK] token to prediction token
</span>    <span class="n">d_inputs</span><span class="p">[</span><span class="n">mlm_mask_idx</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span> <span class="o">=</span> <span class="n">pred_tokens</span>

    <span class="c1"># 4) make label for Discriminator
</span>    <span class="n">original_tokens</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">.</span><span class="n">clone</span><span class="p">().</span><span class="n">detach</span><span class="p">().</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">original_tokens</span><span class="p">[</span><span class="n">mlm_mask_idx</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span> <span class="o">=</span> <span class="n">flat_label</span><span class="p">[</span><span class="n">mlm_mask_idx</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span>
    <span class="n">d_labels</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">eq</span><span class="p">(</span><span class="n">original_tokens</span><span class="p">,</span> <span class="n">d_inputs</span><span class="p">).</span><span class="nb">long</span><span class="p">()</span>
    <span class="n">d_inputs</span> <span class="o">=</span> <span class="n">d_inputs</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="n">pred</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># covert to [batch, sequence]
</span>    <span class="k">return</span> <span class="n">d_inputs</span><span class="p">,</span> <span class="n">d_labels</span>
</code></pre></div></div>
<p>이제 마지막으로 판별자의 입력을 만드는 알고리즘에 대한 코드를 보자. 알고리즘은 다음과 같다.</p>
<ul>
  <li>1) 개별 마스킹 토큰에 대한 예측 토큰 구하기</li>
  <li>2) 모든 마스킹 부분에 예측 토큰들로 대체</li>
  <li>3) 기존 입력과 2번으로 만들어진 시퀀스 비교해 라벨 생성
    <ul>
      <li>서로 같으면 0</li>
      <li>서로 다르면 1
이렇게 만들어진 새로운 입력 시퀀스와 라벨을 ELECTRA 모델 인스턴스의 판별자 포워드 메서드에 인자로 전달하면 된다.</li>
    </ul>
  </li>
</ul>

<h3 id="-future-work-읽고-구현하면서-느낀점--개선방향"><code class="language-plaintext highlighter-rouge">🌟 Future Work (읽고 구현하면서 느낀점 &amp; 개선방향)</code></h3>

<p>이렇게 ELECTRA 모델에 대한 구현까지 살펴봤다. 논문을 읽고 구현하면서 가장 의문스러웠던 부분은 임베딩 공유 방법이었다. 수학적으로 엄밀하게 계산하고 따져보지 못했지만, 직관적으로도 생성자의 MLM과 판별자의 RTD는 서로 성격이 상당히 다른 사전 학습 방법론이라는 것을 알 수 있다. 그렇다면 단순히 단어, 포지션 임베딩을 공유하는 경우 학습 방향성이 달라서 간섭이 발생하고 모델이 수렴하지 못할 여지가 생긴다. 이러한 <code class="language-plaintext highlighter-rouge">줄다리기 현상(tag-of-war)</code>을 어떻게 해결할 수 있을까에 대한 고민이 더 필요하다고 생각한다.</p>

<p>그래서 다음 포스팅에서는 이러한 줄다리기 현상을 해결하고자한 논문인 <strong><a href="https://arxiv.org/abs/2111.09543">&lt;DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing&gt;</a></strong>을 리뷰해보고자 한다.</p>]]></content><author><name>qcqced</name><email>qcqced123@gmail.com</email></author><category term="NLP" /><category term="Natural Language Process" /><category term="ELECTRA" /><category term="BERT" /><category term="GAN" /><category term="Transformer" /><category term="Self-Attention" /><category term="Pytorch" /><summary type="html"><![CDATA[ELECTRA Official Paper Review with Pytorch Implementation]]></summary></entry><entry><title type="html">👩‍💻🎄 [baekjoon] 1987번: 알파벳</title><link href="http://localhost:4000/ps/baekjoon-1987" rel="alternate" type="text/html" title="👩‍💻🎄 [baekjoon] 1987번: 알파벳" /><published>2024-01-30T00:00:00+09:00</published><updated>2024-01-31T02:00:00+09:00</updated><id>http://localhost:4000/ps/baekjoon_1987</id><content type="html" xml:base="http://localhost:4000/ps/baekjoon-1987"><![CDATA[<h3 id="️solution"><strong><code class="language-plaintext highlighter-rouge">🖍️ solution</code></strong></h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">sys</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">List</span>

<span class="k">def</span> <span class="nf">backtracking</span><span class="p">(</span><span class="n">y</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">count</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">visit</span><span class="p">:</span> <span class="n">List</span><span class="p">,</span> <span class="n">graph</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">]):</span>
    <span class="k">global</span> <span class="n">result</span>
    <span class="n">visit</span><span class="p">[</span><span class="nb">ord</span><span class="p">(</span><span class="n">graph</span><span class="p">[</span><span class="n">y</span><span class="p">][</span><span class="n">x</span><span class="p">])</span> <span class="o">-</span> <span class="mi">65</span><span class="p">]</span> <span class="o">=</span> <span class="bp">True</span>
    <span class="n">result</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">count</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">):</span>
        <span class="n">ny</span><span class="p">,</span> <span class="n">nx</span> <span class="o">=</span> <span class="n">dy</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">y</span><span class="p">,</span> <span class="n">dx</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">x</span>
        <span class="k">if</span> <span class="o">-</span><span class="mi">1</span> <span class="o">&lt;</span> <span class="n">ny</span> <span class="o">&lt;</span> <span class="n">r</span> <span class="ow">and</span> <span class="o">-</span><span class="mi">1</span> <span class="o">&lt;</span> <span class="n">nx</span> <span class="o">&lt;</span> <span class="n">c</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">visit</span><span class="p">[</span><span class="nb">ord</span><span class="p">(</span><span class="n">graph</span><span class="p">[</span><span class="n">ny</span><span class="p">][</span><span class="n">nx</span><span class="p">])</span> <span class="o">-</span> <span class="mi">65</span><span class="p">]:</span>
            <span class="n">backtracking</span><span class="p">(</span><span class="n">ny</span><span class="p">,</span> <span class="n">nx</span><span class="p">,</span> <span class="n">count</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">visit</span><span class="p">,</span> <span class="n">graph</span><span class="p">)</span>
            <span class="n">visit</span><span class="p">[</span><span class="nb">ord</span><span class="p">(</span><span class="n">graph</span><span class="p">[</span><span class="n">ny</span><span class="p">][</span><span class="n">nx</span><span class="p">])</span> <span class="o">-</span> <span class="mi">65</span><span class="p">]</span> <span class="o">=</span> <span class="bp">False</span>

<span class="n">r</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="nb">map</span><span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="n">sys</span><span class="p">.</span><span class="n">stdin</span><span class="p">.</span><span class="n">readline</span><span class="p">().</span><span class="n">split</span><span class="p">())</span>

<span class="n">result</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
<span class="n">dy</span><span class="p">,</span> <span class="n">dx</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">grid</span><span class="p">,</span> <span class="n">visited</span> <span class="o">=</span> <span class="p">[</span><span class="nb">list</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="nb">str</span><span class="p">,</span> <span class="n">sys</span><span class="p">.</span><span class="n">stdin</span><span class="p">.</span><span class="n">readline</span><span class="p">().</span><span class="n">rstrip</span><span class="p">()))</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">r</span><span class="p">)],</span> <span class="p">[</span><span class="bp">False</span><span class="p">]</span> <span class="o">*</span> <span class="mi">26</span>
<span class="n">backtracking</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">visited</span><span class="p">,</span> <span class="n">grid</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="nb">max</span><span class="p">(</span><span class="n">result</span><span class="p">))</span>
</code></pre></div></div>

<h3 id="idea"><strong><code class="language-plaintext highlighter-rouge">💡 idea</code></strong></h3>

<ul>
  <li><strong>Back Tracking</strong></li>
  <li><strong>1) 방문 기록 배열 변경</strong>
    <ul>
      <li><strong>조건 중에서 경로에 알파벳 중복이 불가능하다는 점 이용</strong></li>
      <li><strong>전체 격자 사이즈와 동일한 배열 대신 알파벳 사이즈(26)만 선언</strong></li>
    </ul>
  </li>
</ul>

<p>일반적인 백트래킹 문제라고 볼 수 있다. 하지만 파이썬으로 해결하려는 경우 시간, 메모리 제한 때문에 빡센 코드 최적화가 필요하다. 격자 문제라서 <code class="language-plaintext highlighter-rouge">bfs</code> 선택도 가능한데 그렇다면 <code class="language-plaintext highlighter-rouge">python3</code>로도 해결가능하다. 한편, 일반적인 <code class="language-plaintext highlighter-rouge">dfs</code>라면 빡센 최적화를 통해 <code class="language-plaintext highlighter-rouge">pypy3</code>으로만 통과 가능하다.</p>

<p>문제를 리뷰하던 도중 일반적인 <code class="language-plaintext highlighter-rouge">dfs</code> 백트래킹 방식의 비효율성에 대해 고찰해봤다. 아래와 같은 입력이 있다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">IEFCJ</span>
<span class="n">FHFKC</span>
<span class="n">FFALF</span>
<span class="n">HFGCF</span>
<span class="n">HMCHH</span>
</code></pre></div></div>

<p>일반적인 백트래킹 알고리즘이 탐색하는 과정을 생각해보자. 빨간색으로 칠해진 글자를 <code class="language-plaintext highlighter-rouge">IFHE</code> 순서로 탐색했다면, 다음은 <code class="language-plaintext highlighter-rouge">F</code>를 탐색해 방문해도 되는지 여부를 판정할 것이다. 이미 <code class="language-plaintext highlighter-rouge">F</code>는 방문했기 때문에 아마도 스택 프레임 할당을 취소하면서, 결국에는 <code class="language-plaintext highlighter-rouge">I</code>까지 되돌아 갈 것이다.</p>

<p>그리고 다시 오른쪽에 있는 <code class="language-plaintext highlighter-rouge">E</code>를  방문한 뒤, <code class="language-plaintext highlighter-rouge">FCK</code> 순서로 방문하게 될 것이다. 이 때 들게 되는 의문은 바로 이렇다. 굳이 <code class="language-plaintext highlighter-rouge">I</code>까지 되돌아갔다가 탐색해야 할까?? 이미 <code class="language-plaintext highlighter-rouge">IE</code> 는 탐색이 가능한 경로라는 것을 우리는 충분히 알 수 있다. 따라서 <code class="language-plaintext highlighter-rouge">DP Tabulation</code> 개념을 차용한다면 훨씬 빠르게 풀이가 가능할 것이다.</p>

<p>경로의 유일성을 보장하면서 수정 가능한 자료구조가 필요하기 때문에 배열 대신 세트를 사용해보자. 세트에는 현재까지의 경로 그리고 해당 경로의 마지막 인덱스를 저장해줘야 한다. 같은 경로라고 할 지라도 서로 다른 인덱스에 의해 만들어졌을 가능성이 있기 때문이다. 이렇게 세트를 구성한 뒤, 하나씩 pop해서 경로를 얻어낸다. 그 다음 해당 경로로부터 파생되는 여러 잠재적 경로들을 모두 검사해 경로가 만들어질 수 있는지 여부를 판정하면 된다. 코드는 다음과 같다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">sys</span>

<span class="k">def</span> <span class="nf">dfs</span><span class="p">(</span><span class="n">y</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
    <span class="n">dp</span><span class="p">,</span> <span class="n">result</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(),</span> <span class="mi">0</span>
    <span class="n">dp</span><span class="p">.</span><span class="n">add</span><span class="p">((</span><span class="n">y</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">grid</span><span class="p">[</span><span class="n">y</span><span class="p">][</span><span class="n">x</span><span class="p">]))</span>
    <span class="k">while</span> <span class="n">dp</span><span class="p">:</span>
        <span class="n">vy</span><span class="p">,</span> <span class="n">vx</span><span class="p">,</span> <span class="n">path</span> <span class="o">=</span> <span class="n">dp</span><span class="p">.</span><span class="n">pop</span><span class="p">()</span>
        <span class="n">result</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">result</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">path</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">result</span> <span class="o">==</span> <span class="mi">26</span><span class="p">:</span>
            <span class="k">return</span> <span class="mi">26</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">):</span>
            <span class="n">ny</span><span class="p">,</span> <span class="n">nx</span> <span class="o">=</span> <span class="n">dy</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">vy</span><span class="p">,</span> <span class="n">dx</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">vx</span>
            <span class="k">if</span> <span class="o">-</span><span class="mi">1</span> <span class="o">&lt;</span> <span class="n">ny</span> <span class="o">&lt;</span> <span class="n">r</span> <span class="ow">and</span> <span class="o">-</span><span class="mi">1</span> <span class="o">&lt;</span> <span class="n">nx</span> <span class="o">&lt;</span> <span class="n">c</span> <span class="ow">and</span> <span class="n">grid</span><span class="p">[</span><span class="n">ny</span><span class="p">][</span><span class="n">nx</span><span class="p">]</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">path</span><span class="p">:</span>
                <span class="n">dp</span><span class="p">.</span><span class="n">add</span><span class="p">((</span><span class="n">ny</span><span class="p">,</span> <span class="n">nx</span><span class="p">,</span> <span class="n">grid</span><span class="p">[</span><span class="n">ny</span><span class="p">][</span><span class="n">nx</span><span class="p">]</span> <span class="o">+</span> <span class="n">path</span><span class="p">))</span>
                
    <span class="k">return</span> <span class="n">result</span>

<span class="n">r</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="nb">map</span><span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="n">sys</span><span class="p">.</span><span class="n">stdin</span><span class="p">.</span><span class="n">readline</span><span class="p">().</span><span class="n">split</span><span class="p">())</span>
<span class="n">dy</span><span class="p">,</span> <span class="n">dx</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">grid</span> <span class="o">=</span> <span class="p">[</span><span class="nb">list</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="nb">str</span><span class="p">,</span> <span class="n">sys</span><span class="p">.</span><span class="n">stdin</span><span class="p">.</span><span class="n">readline</span><span class="p">().</span><span class="n">rstrip</span><span class="p">()))</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">r</span><span class="p">)]</span>
<span class="k">print</span><span class="p">(</span><span class="n">dfs</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
</code></pre></div></div>

<p align="center">
<img src="/assets/images/ps/after.png" alt="Common BackTracking" class="align-center image-caption" width="100%&quot;, height=&quot;100%" />
<strong><em>Common BackTracking</em></strong>
</p>

<p align="center">
<img src="/assets/images/ps/before.png" alt="DP Tabulation BackTracking" class="align-center image-caption" width="100%&quot;, height=&quot;100%" />
<strong><em>DP Tabulation BackTracking</em></strong>
</p>

<p>위에는 개선이전 결과고 아래는 개선 이후 결과다. 비약적인 속도 상승하는 동시에 메모리 역시 3배나 덜 사용하는 모습이다. 세트에 있는 유니크한 경로들을 하나씩 꺼내는 방식을 선택했기 때문에 알고리즘 성능이 시드에 영향(<code class="language-plaintext highlighter-rouge">set.pop()</code>은 랜덤으로 원소 선택)을 받는다는 점만 감안한다면 매우 좋은 풀이라고 생각한다.</p>]]></content><author><name>qcqced</name><email>qcqced123@gmail.com</email></author><category term="Problem Solving" /><category term="Python" /><category term="Codeing Test" /><category term="Algorithm" /><category term="Baekjoon" /><category term="Graph" /><category term="DFS" /><category term="BackTracking" /><summary type="html"><![CDATA[백준 1987번: 알파벳]]></summary></entry><entry><title type="html">🔥 Pytorch Tensor Indexing 자주 사용하는 메서드 모음집</title><link href="http://localhost:4000/framework-library/torch-indexing-function" rel="alternate" type="text/html" title="🔥 Pytorch Tensor Indexing 자주 사용하는 메서드 모음집" /><published>2024-01-09T00:00:00+09:00</published><updated>2024-01-10T02:00:00+09:00</updated><id>http://localhost:4000/framework-library/Pytorch-Tensor-Indexing-Function</id><content type="html" xml:base="http://localhost:4000/framework-library/torch-indexing-function"><![CDATA[<p>파이토치에서 필자가 자주 사용하는 텐서 인덱싱 관련 메서드의 사용법 및 사용 예시를 한방에 정리한 포스트다. 메서드 하나당 하나의 포스트로 만들기에는 너무 길이가 짧다 생각해 한 페이지에 모두 넣게 되었다. 지속적으로 업데이트 될 예정이다. 또한 텐서 인덱싱 말고도 다른 주제로도 관련 메서드를 정리해 올릴 예정이니 많은 관심 부탁드린다.</p>

<h3 id="torchargmax"><code class="language-plaintext highlighter-rouge">🔎 torch.argmax</code></h3>

<p>입력 텐서에서 가장 큰 값을 갖고 있는 원소의 인덱스를 반환한다. 최대값을 찾을 차원을 지정해줄 수 있다. 아래 예시 코드를 확인해보자.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># torch.argmax params
</span><span class="n">torch</span><span class="p">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

<span class="c1"># torch.argmax example 1
</span><span class="n">test</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">29</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">45</span><span class="p">,</span><span class="mi">22</span><span class="p">,</span><span class="mi">3</span><span class="p">])</span>
<span class="n">torch</span><span class="p">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">test</span><span class="p">)</span>
<span class="n">torch</span><span class="p">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">test2</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="o">&lt;</span><span class="n">Result</span><span class="o">&gt;</span>
<span class="n">tensor</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>

<span class="c1"># torch.argmax example 2
</span><span class="n">test</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
                     <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">]])</span>

<span class="n">torch</span><span class="p">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">test</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="o">&lt;</span><span class="n">Result</span><span class="o">&gt;</span>
<span class="n">tensor</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>

<span class="c1"># torch.argmax example 3
</span><span class="n">test</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
                     <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">]])</span>

<span class="n">torch</span><span class="p">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">test</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">tensor</span><span class="p">([[</span><span class="mi">0</span><span class="p">],</span>
        <span class="p">[</span><span class="mi">2</span><span class="p">]])</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">dim</code> 매개변수에 원하는 차원을 입력하면 해당 차원 뷰에서 가장 큰 원소를 찾아 인덱스 값을 반환해줄 것이다. 이 때 <code class="language-plaintext highlighter-rouge">keepdim=True</code> 로 설정한다면 입력 차원에서 가장 큰 원소의 인덱스를 반환하되 원본 텐서의 차원과 동일한 형태로 출력해준다. <code class="language-plaintext highlighter-rouge">example 2</code> 의 경우 <code class="language-plaintext highlighter-rouge">dim=0</code> 라서 행이 누적된 방향으로 텐서를 바라봐야 한다. 행이 누적된 방향으로 텐서를 보게 되면 <code class="language-plaintext highlighter-rouge">tensor([[0, 1, 1]])</code>이 된다.</p>

<h3 id="torchstack"><code class="language-plaintext highlighter-rouge">📚 torch.stack</code></h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="s">"""
torch.stack
Args:
	tensors(sequence of Tensors): 텐서가 담긴 파이썬 시퀀스 객체
	dim(int): 추가할 차원 방향을 세팅, 기본값은 0
"""</span>
<span class="n">torch</span><span class="p">.</span><span class="n">stack</span><span class="p">(</span><span class="n">tensors</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</code></pre></div></div>

<p>매개변수로 주어진 파이썬 시퀀스 객체(리스트, 튜플)를 사용자가 지정한 새로운 차원에 쌓는 기능을 한다. 매개변수 <code class="language-plaintext highlighter-rouge">tensors</code> 는 텐서가 담긴 파이썬의 시퀀스 객체를 입력으로 받는다. <code class="language-plaintext highlighter-rouge">dim</code> 은 사용자가 텐서 적재를 하고 싶은 새로운 차원을 지정해주면 된다. 기본값은 0차원으로 지정 되어있으며, 텐서의 맨 앞차원이 새롭게 생기게 된다. <code class="language-plaintext highlighter-rouge">torch.stack</code> 은 기계학습, 특히 딥러닝에서 정말 자주 사용되기 때문에 사용법 및 사용상황을 익혀두면 도움이 된다. 예시를 통해 해당 메서드를 어떤 상황에서 어떻게 사용하는지 알아보자.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="s">""" torch.stack example """</span>

<span class="k">class</span> <span class="nc">Projector</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="s">"""
    Making projection matrix(Q, K, V) for each attention head
    When you call this class, it returns projection matrix of each attention head
    For example, if you call this class with 8 heads, it returns 8 set of projection matrices (Q, K, V)
    Args:
        num_heads: number of heads in MHA, default 8
        dim_head: dimension of each attention head, default 64
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span> <span class="n">dim_head</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">64</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Projector</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span> <span class="o">=</span> <span class="n">num_heads</span> <span class="o">*</span> <span class="n">dim_head</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dim_head</span> <span class="o">=</span> <span class="n">dim_head</span>

    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">fc_q</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">dim_head</span><span class="p">)</span>
        <span class="n">fc_k</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">dim_head</span><span class="p">)</span>
        <span class="n">fc_v</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">dim_head</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">fc_q</span><span class="p">,</span> <span class="n">fc_k</span><span class="p">,</span> <span class="n">fc_v</span>

<span class="n">num_heads</span> <span class="o">=</span> <span class="mi">8</span>
<span class="n">dim_head</span> <span class="o">=</span> <span class="mi">64</span>
<span class="n">projector</span> <span class="o">=</span> <span class="n">Projector</span><span class="p">(</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">dim_head</span><span class="p">)</span>  <span class="c1"># init instance
</span><span class="n">projector_list</span> <span class="o">=</span> <span class="p">[</span><span class="nb">list</span><span class="p">(</span><span class="n">projector</span><span class="p">())</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_heads</span><span class="p">)]</span>  <span class="c1"># call instance
</span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">512</span><span class="p">)</span> <span class="c1"># x.shape: [Batch_Size, Sequence_Length, Dim_model]
</span><span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">V</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[],</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_heads</span><span class="p">):</span>
    <span class="n">Q</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">projector_list</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">0</span><span class="p">](</span><span class="n">x</span><span class="p">))</span> <span class="c1"># [10, 512, 64]
</span>    <span class="n">K</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">projector_list</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">1</span><span class="p">](</span><span class="n">x</span><span class="p">))</span> <span class="c1"># [10, 512, 64]
</span>	  <span class="n">V</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">projector_list</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">2</span><span class="p">](</span><span class="n">x</span><span class="p">))</span> <span class="c1"># [10, 512, 64]
</span> 
<span class="n">Q</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">stack</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># Q.shape: [10, 8, 512, 64]
</span><span class="n">K</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">stack</span><span class="p">(</span><span class="n">K</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># K.shape: [10, 8, 512, 64]
</span><span class="n">V</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">stack</span><span class="p">(</span><span class="n">V</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># V.shape: [10, 8, 512, 64]
</span></code></pre></div></div>

<p>위 코드는 <code class="language-plaintext highlighter-rouge">Transformer</code> 의 <code class="language-plaintext highlighter-rouge">Multi-Head Attention</code> 구현체 일부를 발췌해온 것이다. <code class="language-plaintext highlighter-rouge">Multi-Head Attention</code> 은 개별 어텐션 해드별로 행렬 $Q, K, V$를 가져야 한다. 따라서 입력 임베딩을 개별 어텐션 헤드에 <code class="language-plaintext highlighter-rouge">Linear Combination</code> 해줘야 하는데 헤드 개수가 8개나 되기 때문에 개별적으로 <code class="language-plaintext highlighter-rouge">Projection Matrix</code> 를 선언해주는 것은 매우 비효율적이다. 따라서 객체  <code class="language-plaintext highlighter-rouge">Projector</code> 에 행렬 $Q, K, V$에 대한 <code class="language-plaintext highlighter-rouge">Projection Matrix</code> 를 정의해줬다. 이후 헤드 개수만큼 객체  <code class="language-plaintext highlighter-rouge">Projector</code> 를 호출해 리스트에 해드별 <code class="language-plaintext highlighter-rouge">Projection Matrix</code> 를 담아준다. 그 다음 <code class="language-plaintext highlighter-rouge">torch.stack</code>을 사용해 <code class="language-plaintext highlighter-rouge">Attention Head</code> 방향의 차원으로 리스트 내부 텐서들을 쌓아주면 된다.</p>

<h3 id="torcharange"><code class="language-plaintext highlighter-rouge">🔢 torch.arange</code></h3>

<p>사용자가 지정한 시작점부터 끝점까지 일정한 간격으로 텐서를 나열한다. Python의 내장 메서드 <code class="language-plaintext highlighter-rouge">range</code>와 동일한 역할을 하는데, 대신 텐서 그 결과를 텐서 구조체로 반환한다고 생각하면 되겠다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># torch.arange usage
</span><span class="n">torch</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="n">start</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">end</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
<span class="n">tensor</span><span class="p">([</span> <span class="mi">0</span><span class="p">,</span>  <span class="mi">1</span><span class="p">,</span>  <span class="mi">2</span><span class="p">,</span>  <span class="mi">3</span><span class="p">,</span>  <span class="mi">4</span><span class="p">])</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">tensor</span><span class="p">([</span> <span class="mi">1</span><span class="p">,</span>  <span class="mi">2</span><span class="p">,</span>  <span class="mi">3</span><span class="p">])</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">)</span>
<span class="n">tensor</span><span class="p">([</span> <span class="mf">1.0000</span><span class="p">,</span>  <span class="mf">1.5000</span><span class="p">,</span>  <span class="mf">2.0000</span><span class="p">])</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">step</code> 매개변수로 원소간 간격 조정을 할 수 있는데, 기본은 1로 지정 되어 있으니 참고하자. 필자의 경우에는 <code class="language-plaintext highlighter-rouge">nn.Embedding</code>의 입력 텐서를 만들 때 가장 많이 사용했다. <code class="language-plaintext highlighter-rouge">nn.Embedding</code> 의 경우 Input으로 <code class="language-plaintext highlighter-rouge">IntTensor</code>, <code class="language-plaintext highlighter-rouge">LongTensor</code>를 받게 되어 있으니 알아두자.</p>

<h3 id="torchrepeat"><code class="language-plaintext highlighter-rouge">🔁 torch.repeat</code></h3>

<p>입력값으로 주어진 텐서를 사용자가 지정한 반복 횟수만큼 특정 차원 방향으로 늘린다. 예를 들면 <code class="language-plaintext highlighter-rouge">[1,2,3] * 3</code>의 결과는 <code class="language-plaintext highlighter-rouge">[1, 2, 3, 1, 2, 3, 1, 2, 3]</code> 인데, 이것을 사용자가 지정한 반복 횟수만큼 특정 차원으로 수행하겠다는 것이다. 아래 사용 예제를 확인해보자.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># torch.repeat example
</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">x</span><span class="p">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">tensor</span><span class="p">([[</span> <span class="mi">1</span><span class="p">,</span>  <span class="mi">2</span><span class="p">,</span>  <span class="mi">3</span><span class="p">,</span>  <span class="mi">1</span><span class="p">,</span>  <span class="mi">2</span><span class="p">,</span>  <span class="mi">3</span><span class="p">],</span>
        <span class="p">[</span> <span class="mi">1</span><span class="p">,</span>  <span class="mi">2</span><span class="p">,</span>  <span class="mi">3</span><span class="p">,</span>  <span class="mi">1</span><span class="p">,</span>  <span class="mi">2</span><span class="p">,</span>  <span class="mi">3</span><span class="p">],</span>
        <span class="p">[</span> <span class="mi">1</span><span class="p">,</span>  <span class="mi">2</span><span class="p">,</span>  <span class="mi">3</span><span class="p">,</span>  <span class="mi">1</span><span class="p">,</span>  <span class="mi">2</span><span class="p">,</span>  <span class="mi">3</span><span class="p">],</span>
        <span class="p">[</span> <span class="mi">1</span><span class="p">,</span>  <span class="mi">2</span><span class="p">,</span>  <span class="mi">3</span><span class="p">,</span>  <span class="mi">1</span><span class="p">,</span>  <span class="mi">2</span><span class="p">,</span>  <span class="mi">3</span><span class="p">]])</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">x</span><span class="p">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">tensor</span><span class="p">([[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
         <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]],</span>

        <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
         <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]],</span>

        <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
         <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]],</span>

        <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
         <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]]])</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">x</span><span class="p">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">).</span><span class="n">size</span>
<span class="n">torch</span><span class="p">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">x</span><span class="p">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">tensor</span><span class="p">([[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
         <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]],</span>

        <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
         <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]],</span>

        <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
         <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]],</span>

        <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
         <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]]])</span>
</code></pre></div></div>

<p>$t$를 어떤 텐서 구조체 $x$의 최대 차원이라고 했을 , $x_t$를 가장 왼쪽에 넣고 가장 낮은 차원인 0차원에 대한 반복 횟수를 오른쪽 끝에 대입해서 사용하면 된다. (<code class="language-plaintext highlighter-rouge">torch.repeat(</code>$x_t, x_{t-1}, … x_2, x_1, x_0$<code class="language-plaintext highlighter-rouge">))</code>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># torch.arange &amp; torch.repeate usage example
</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">pos_x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">num_patches</span> <span class="o">+</span> <span class="mi">1</span><span class="p">).</span><span class="n">repeat</span><span class="p">(</span><span class="n">inputs</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]).</span><span class="n">to</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">pos_x</span><span class="p">.</span><span class="n">shape</span>
<span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">16</span><span class="p">,</span> <span class="mi">1025</span><span class="p">])</span>
</code></pre></div></div>

<p>필자의 경우, <code class="language-plaintext highlighter-rouge">position embedding</code>의 입력을 만들고 싶을 때 <code class="language-plaintext highlighter-rouge">torch.arange</code> 와 연계해 자주 사용 했던 것 같다. 위 코드를 참고하자.</p>

<h3 id="torchclamp"><code class="language-plaintext highlighter-rouge">🔬 torch.clamp</code></h3>

<p>입력 텐서의 원소값을 사용자가 지정한 최대•최소값 범위 이내로 제한하는 메서드다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># torch.clamp params
</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">clamp</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="nb">min</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span> <span class="err">→</span> <span class="n">Tensor</span>

<span class="c1"># torch.clamp usage example
</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">a</span>
<span class="n">tensor</span><span class="p">([</span><span class="o">-</span><span class="mf">1.7120</span><span class="p">,</span>  <span class="mf">0.1734</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0478</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0922</span><span class="p">])</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="nb">min</span><span class="o">=-</span><span class="mf">0.5</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">tensor</span><span class="p">([</span><span class="o">-</span><span class="mf">0.5000</span><span class="p">,</span>  <span class="mf">0.1734</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0478</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0922</span><span class="p">])</span>
</code></pre></div></div>

<p>입력된 텐서의 원소를 지정 최대•최소 설정값과 하나 하나 대조해서 텐서 내부의 모든 원소가 지정 범위 안에 들도록 만들어준다. <code class="language-plaintext highlighter-rouge">torch.clamp</code> 역시 다양한 상황에서 사용되는데, 필자의 경우 모델 레이어 중간에 제곱근이나 지수, 분수 혹은 각도 관련 연산이 들어가 <code class="language-plaintext highlighter-rouge">Backward Pass</code>에서 <code class="language-plaintext highlighter-rouge">NaN</code>이 발생할 수 있는 경우에 안전장치로 많이 사용하고 있다. (<a href="https://qcqced123.github.io/framework-library/backward-nan/">자세히 알고 싶다면 클릭</a>)</p>

<h3 id="torchgather"><code class="language-plaintext highlighter-rouge">👩‍👩‍👧‍👦 torch.gather</code></h3>

<p>텐서 객체 내부에서 원하는 인덱스에 위치한 원소만 추출하고 싶을 때 사용하면 매우 유용한 메서드다. 텐서 역시 <code class="language-plaintext highlighter-rouge">iterable</code> 객체라서 <code class="language-plaintext highlighter-rouge">loop</code> 를 사용해 접근하는 것이 직관적으로 보일 수 있으나, 통상적으로 텐서를 사용하는 상황이라면 객체의 차원이 어마무시 하기 때문에 루프로 접근해 관리하는 것은 매우 비효율적이다. 루프를 통해 접근하면 파이썬의 내장 리스트를 사용하는 것과 별반 다를게 없어지기 때문에, 텐서를 사용하는 메리트가 사라진다. 비교적 크지 않은 2~3차원의 텐서 정도라면 사용해도 크게 문제는 없을거라 생각하지만 그래도 코드의 일관성을 위해 <code class="language-plaintext highlighter-rouge">torch.gather</code> 사용을 권장한다. 이제 <code class="language-plaintext highlighter-rouge">torch.gather</code>의 사용법에 대해 알아보자.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># torch.gather params
</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">gather</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">sparse_grad</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">dim</code>과 <code class="language-plaintext highlighter-rouge">index</code>에 주목해보자. 먼저 <code class="language-plaintext highlighter-rouge">dim</code>은 사용자가 인덱싱을 적용하고 싶은 차원을 지정해주는 역할을 한다. <code class="language-plaintext highlighter-rouge">index</code> 매개변수로 전달하는 텐서 안에는 원소의 <code class="language-plaintext highlighter-rouge">‘인덱스’</code>를 의미하는 숫자들이 마구잡이로 담겨있는데, 해당 인덱스가 대상 텐서의 어느 차원을 가리킬 것인지를 컴퓨터에게 알려준다고 생각하면 된다. <code class="language-plaintext highlighter-rouge">index</code> 는 앞에서 설명했듯이 원소의 <code class="language-plaintext highlighter-rouge">‘인덱스’</code>를 의미하는 숫자들이 담긴 텐서를 입력으로 하는 매개변수다. 이 때 주의할 점은 대상 텐서(<code class="language-plaintext highlighter-rouge">input</code>)와 인덱스 텐서의 차원 형태가 반드시 동일해야 한다는 것이다. 역시 말로만 들으면 이해하기 힘드니 사용 예시를 함꼐 살펴보자.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># torch.gather usage example
</span><span class="o">&gt;&gt;&gt;</span> <span class="n">q</span><span class="p">,</span> <span class="n">kr</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1024</span><span class="p">,</span> <span class="mi">64</span><span class="p">),</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1024</span><span class="p">,</span> <span class="mi">64</span><span class="p">)</span> <span class="c1"># [batch, sequence, dim_head], [batch, 2*sequence, dim_head]
</span><span class="o">&gt;&gt;&gt;</span> <span class="n">tmp_c2p</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">kr</span><span class="p">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">))</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">tmp_c2p</span><span class="p">,</span> <span class="n">tmp_c2p</span><span class="p">.</span><span class="n">shape</span>
<span class="p">(</span><span class="n">tensor</span><span class="p">([[</span><span class="o">-</span><span class="mf">2.6477</span><span class="p">,</span> <span class="o">-</span><span class="mf">4.7478</span><span class="p">,</span> <span class="o">-</span><span class="mf">5.3250</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mf">1.6062</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.9717</span><span class="p">,</span>  <span class="mf">3.8004</span><span class="p">],</span>
         <span class="p">[</span> <span class="mf">0.0662</span><span class="p">,</span>  <span class="mf">1.5240</span><span class="p">,</span>  <span class="mf">0.1182</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mf">0.1653</span><span class="p">,</span>  <span class="mf">2.8476</span><span class="p">,</span>  <span class="mf">1.6337</span><span class="p">],</span>
         <span class="p">[</span><span class="o">-</span><span class="mf">0.5010</span><span class="p">,</span> <span class="o">-</span><span class="mf">4.2267</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.1179</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mf">1.1447</span><span class="p">,</span>  <span class="mf">1.7845</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1493</span><span class="p">],</span>
         <span class="p">...,</span>
         <span class="p">[</span><span class="o">-</span><span class="mf">2.1073</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2149</span><span class="p">,</span> <span class="o">-</span><span class="mf">4.8630</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mf">0.8238</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5833</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2066</span><span class="p">],</span>
         <span class="p">[</span> <span class="mf">2.1747</span><span class="p">,</span>  <span class="mf">3.2924</span><span class="p">,</span>  <span class="mf">6.5808</span><span class="p">,</span>  <span class="p">...,</span> <span class="o">-</span><span class="mf">0.2926</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2511</span><span class="p">,</span>  <span class="mf">2.6996</span><span class="p">],</span>
         <span class="p">[</span><span class="o">-</span><span class="mf">2.8362</span><span class="p">,</span>  <span class="mf">2.8700</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.9729</span><span class="p">,</span>  <span class="p">...,</span> <span class="o">-</span><span class="mf">4.9913</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3616</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1708</span><span class="p">]],</span>
        <span class="n">grad_fn</span><span class="o">=&lt;</span><span class="n">MmBackward0</span><span class="o">&gt;</span><span class="p">)</span>
<span class="n">torch</span><span class="p">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">1024</span><span class="p">,</span> <span class="mi">1024</span><span class="p">]))</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">max_seq</span><span class="p">,</span> <span class="n">max_relative_position</span> <span class="o">=</span> <span class="mi">1024</span><span class="p">,</span> <span class="mi">512</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">q_index</span><span class="p">,</span> <span class="n">k_index</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="n">max_seq</span><span class="p">),</span> <span class="n">torch</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">max_relative_position</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">q_index</span><span class="p">,</span> <span class="n">k_index</span>
<span class="p">(</span><span class="n">tensor</span><span class="p">([</span>   <span class="mi">0</span><span class="p">,</span>    <span class="mi">1</span><span class="p">,</span>    <span class="mi">2</span><span class="p">,</span>  <span class="p">...,</span> <span class="mi">1021</span><span class="p">,</span> <span class="mi">1022</span><span class="p">,</span> <span class="mi">1023</span><span class="p">]),</span>
 <span class="n">tensor</span><span class="p">([</span>   <span class="mi">0</span><span class="p">,</span>    <span class="mi">1</span><span class="p">,</span>    <span class="mi">2</span><span class="p">,</span>  <span class="p">...,</span> <span class="mi">1021</span><span class="p">,</span> <span class="mi">1022</span><span class="p">,</span> <span class="mi">1023</span><span class="p">]))</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">tmp_pos</span> <span class="o">=</span> <span class="n">q_index</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="o">-</span> <span class="n">k_index</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">rel_pos_matrix</span> <span class="o">=</span> <span class="n">tmp_pos</span> <span class="o">+</span> <span class="n">max_relative_position</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">rel_pos_matrix</span>
<span class="n">tensor</span><span class="p">([[</span> <span class="mi">512</span><span class="p">,</span>  <span class="mi">511</span><span class="p">,</span>  <span class="mi">510</span><span class="p">,</span>  <span class="p">...,</span> <span class="o">-</span><span class="mi">509</span><span class="p">,</span> <span class="o">-</span><span class="mi">510</span><span class="p">,</span> <span class="o">-</span><span class="mi">511</span><span class="p">],</span>
        <span class="p">[</span> <span class="mi">513</span><span class="p">,</span>  <span class="mi">512</span><span class="p">,</span>  <span class="mi">511</span><span class="p">,</span>  <span class="p">...,</span> <span class="o">-</span><span class="mi">508</span><span class="p">,</span> <span class="o">-</span><span class="mi">509</span><span class="p">,</span> <span class="o">-</span><span class="mi">510</span><span class="p">],</span>
        <span class="p">[</span> <span class="mi">514</span><span class="p">,</span>  <span class="mi">513</span><span class="p">,</span>  <span class="mi">512</span><span class="p">,</span>  <span class="p">...,</span> <span class="o">-</span><span class="mi">507</span><span class="p">,</span> <span class="o">-</span><span class="mi">508</span><span class="p">,</span> <span class="o">-</span><span class="mi">509</span><span class="p">],</span>
        <span class="p">...,</span>
        <span class="p">[</span><span class="mi">1533</span><span class="p">,</span> <span class="mi">1532</span><span class="p">,</span> <span class="mi">1531</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mi">512</span><span class="p">,</span>  <span class="mi">511</span><span class="p">,</span>  <span class="mi">510</span><span class="p">],</span>
        <span class="p">[</span><span class="mi">1534</span><span class="p">,</span> <span class="mi">1533</span><span class="p">,</span> <span class="mi">1532</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mi">513</span><span class="p">,</span>  <span class="mi">512</span><span class="p">,</span>  <span class="mi">511</span><span class="p">],</span>
        <span class="p">[</span><span class="mi">1535</span><span class="p">,</span> <span class="mi">1534</span><span class="p">,</span> <span class="mi">1533</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mi">514</span><span class="p">,</span>  <span class="mi">513</span><span class="p">,</span>  <span class="mi">512</span><span class="p">]])</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">rel_pos_matrix</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">rel_pos_matrix</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="o">*</span><span class="n">max_relative_position</span> <span class="o">-</span> <span class="mi">1</span><span class="p">).</span><span class="n">repeat</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">tmp_c2p</span> <span class="o">=</span> <span class="n">tmp_c2p</span><span class="p">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">rel_pos_matrix</span><span class="p">,</span> <span class="n">rel_pos_matrix</span><span class="p">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">tmp_c2p</span><span class="p">.</span><span class="n">shape</span> 
<span class="p">(</span><span class="n">tensor</span><span class="p">([[[</span> <span class="mi">512</span><span class="p">,</span>  <span class="mi">511</span><span class="p">,</span>  <span class="mi">510</span><span class="p">,</span>  <span class="p">...,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">],</span>
          <span class="p">[</span> <span class="mi">513</span><span class="p">,</span>  <span class="mi">512</span><span class="p">,</span>  <span class="mi">511</span><span class="p">,</span>  <span class="p">...,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">],</span>
          <span class="p">[</span> <span class="mi">514</span><span class="p">,</span>  <span class="mi">513</span><span class="p">,</span>  <span class="mi">512</span><span class="p">,</span>  <span class="p">...,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">],</span>
          <span class="p">...,</span>
          <span class="p">[</span><span class="mi">1023</span><span class="p">,</span> <span class="mi">1023</span><span class="p">,</span> <span class="mi">1023</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mi">512</span><span class="p">,</span>  <span class="mi">511</span><span class="p">,</span>  <span class="mi">510</span><span class="p">],</span>
          <span class="p">[</span><span class="mi">1023</span><span class="p">,</span> <span class="mi">1023</span><span class="p">,</span> <span class="mi">1023</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mi">513</span><span class="p">,</span>  <span class="mi">512</span><span class="p">,</span>  <span class="mi">511</span><span class="p">],</span>
          <span class="p">[</span><span class="mi">1023</span><span class="p">,</span> <span class="mi">1023</span><span class="p">,</span> <span class="mi">1023</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mi">514</span><span class="p">,</span>  <span class="mi">513</span><span class="p">,</span>  <span class="mi">512</span><span class="p">]],</span>
 
         <span class="p">[[</span> <span class="mi">512</span><span class="p">,</span>  <span class="mi">511</span><span class="p">,</span>  <span class="mi">510</span><span class="p">,</span>  <span class="p">...,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">],</span>
          <span class="p">[</span> <span class="mi">513</span><span class="p">,</span>  <span class="mi">512</span><span class="p">,</span>  <span class="mi">511</span><span class="p">,</span>  <span class="p">...,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">],</span>
          <span class="p">[</span> <span class="mi">514</span><span class="p">,</span>  <span class="mi">513</span><span class="p">,</span>  <span class="mi">512</span><span class="p">,</span>  <span class="p">...,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">],</span>
          <span class="p">...,</span>
          <span class="p">[</span><span class="mi">1023</span><span class="p">,</span> <span class="mi">1023</span><span class="p">,</span> <span class="mi">1023</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mi">512</span><span class="p">,</span>  <span class="mi">511</span><span class="p">,</span>  <span class="mi">510</span><span class="p">],</span>
          <span class="p">[</span><span class="mi">1023</span><span class="p">,</span> <span class="mi">1023</span><span class="p">,</span> <span class="mi">1023</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mi">513</span><span class="p">,</span>  <span class="mi">512</span><span class="p">,</span>  <span class="mi">511</span><span class="p">],</span>
          <span class="p">[</span><span class="mi">1023</span><span class="p">,</span> <span class="mi">1023</span><span class="p">,</span> <span class="mi">1023</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mi">514</span><span class="p">,</span>  <span class="mi">513</span><span class="p">,</span>  <span class="mi">512</span><span class="p">]],</span>
<span class="n">torch</span><span class="p">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1024</span><span class="p">,</span> <span class="mi">1024</span><span class="p">]),</span>
<span class="n">torch</span><span class="p">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1024</span><span class="p">,</span> <span class="mi">1024</span><span class="p">]))</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">gather</span><span class="p">(</span><span class="n">tmp_c2p</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">rel_pos_matrix</span><span class="p">)</span>
<span class="n">tensor</span><span class="p">([[[</span><span class="o">-</span><span class="mf">0.8579</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2178</span><span class="p">,</span>  <span class="mf">1.6323</span><span class="p">,</span>  <span class="p">...,</span> <span class="o">-</span><span class="mf">2.6477</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.6477</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.6477</span><span class="p">],</span>
         <span class="p">[</span> <span class="mf">1.1601</span><span class="p">,</span>  <span class="mf">2.1752</span><span class="p">,</span>  <span class="mf">0.7187</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mf">0.0662</span><span class="p">,</span>  <span class="mf">0.0662</span><span class="p">,</span>  <span class="mf">0.0662</span><span class="p">],</span>
         <span class="p">[</span> <span class="mf">3.4379</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2573</span><span class="p">,</span>  <span class="mf">0.1375</span><span class="p">,</span>  <span class="p">...,</span> <span class="o">-</span><span class="mf">0.5010</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5010</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5010</span><span class="p">],</span>
         <span class="p">...,</span>
         <span class="p">[</span><span class="o">-</span><span class="mf">1.2066</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2066</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2066</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mf">0.5943</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5169</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.0820</span><span class="p">],</span>
         <span class="p">[</span> <span class="mf">2.6996</span><span class="p">,</span>  <span class="mf">2.6996</span><span class="p">,</span>  <span class="mf">2.6996</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mf">0.2014</span><span class="p">,</span>  <span class="mf">1.1458</span><span class="p">,</span>  <span class="mf">3.2626</span><span class="p">],</span>
         <span class="p">[</span><span class="o">-</span><span class="mf">0.1708</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1708</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1708</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mf">1.9955</span><span class="p">,</span>  <span class="mf">4.1549</span><span class="p">,</span>  <span class="mf">2.6356</span><span class="p">]],</span>
</code></pre></div></div>

<p>위 코드는 <code class="language-plaintext highlighter-rouge">DeBERTa</code> 의 <code class="language-plaintext highlighter-rouge">Disentangled Self-Attention</code>을 구현한 코드의 일부분이다. 자세한 원리는 <code class="language-plaintext highlighter-rouge">DeBERTa</code> 논문 리뷰 포스팅에서 확인하면 되고, 우리가 지금 주목할 부분은 바로 <code class="language-plaintext highlighter-rouge">tmp_c2p</code>, <code class="language-plaintext highlighter-rouge">rel_pos_matrix</code> 그리고 마지막 줄에 위치한 <code class="language-plaintext highlighter-rouge">torch.gather</code> 다. <code class="language-plaintext highlighter-rouge">[10, 1024, 1024]</code> 모양을 가진 대상 텐서 <code class="language-plaintext highlighter-rouge">tmp_c2p</code> 에서 내가 원하는 원소만 추출하려는 상황인데, 추출해야할 원소의 인덱스 값이 담긴 텐서를 <code class="language-plaintext highlighter-rouge">rel_pos_matrix</code> 로 정의했다. <code class="language-plaintext highlighter-rouge">rel_pos_matrix</code> 의 차원은 <code class="language-plaintext highlighter-rouge">[10, 1024, 1024]</code>로 <code class="language-plaintext highlighter-rouge">tmp_c2p</code>와 동일하다. 참고로 추출해야 하는 차원 방향은 가로 방향(두 번째 1024)이다.</p>

<p>이제 <code class="language-plaintext highlighter-rouge">torch.gather</code>의 동작을 살펴보자. 우리가 현재 추출하고 싶은 대상은 3차원 텐서의 가로 방향(두 번째 1024, 텐서의 행 벡터), 즉 <code class="language-plaintext highlighter-rouge">2 * max_sequence_length</code> 를 의미하는 차원 방향의 원소다. 따라서 <code class="language-plaintext highlighter-rouge">dim=-1</code>으로 설정해준다. 이제 메서드가 의도대로 적용되었는지 확인해보자. <code class="language-plaintext highlighter-rouge">rel_pos_matrix</code> 의 0번 배치, 0번째 시퀀스의 가장 마지막 차원의 값은 <code class="language-plaintext highlighter-rouge">0</code>으로 초기화 되어 있다. 다시 말해, 대상 텐서의 대상 차원에서 0번째 인덱스에 해당하는 값을 가져오라는 의미를 담고 있다. 그렇다면 <code class="language-plaintext highlighter-rouge">torch.gather</code> 실행 결과가 <code class="language-plaintext highlighter-rouge">tmp_c2p</code>의 0번 배치, 0번째 시퀀스의 0번째 차원 값과 일치하는지 확인해보자. 둘 다 <code class="language-plaintext highlighter-rouge">-2.6477</code>, <code class="language-plaintext highlighter-rouge">-2.6477</code> 으로 같은 값을 나타내고 있다. 따라서 우리 의도대로 잘 실행되었다는 사실을 알 수 있다.</p>

<h3 id="torchtriu-torchtril"><code class="language-plaintext highlighter-rouge">👩‍👩‍👧‍👦 torch.triu, torch.tril</code></h3>

<p>각각 입력 텐서를 <code class="language-plaintext highlighter-rouge">상삼각행렬</code>, <code class="language-plaintext highlighter-rouge">하삼각행렬</code>로 만든다. <code class="language-plaintext highlighter-rouge">triu</code>나 <code class="language-plaintext highlighter-rouge">tril</code>은 사실 뒤집으면 같은 결과를 반환하기 때문에 <code class="language-plaintext highlighter-rouge">tril</code>을 기준으로 설명을 하겠다. 메서드의 매개변수는 다음과 같다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># torch.triu, tril params
</span><span class="n">upper_tri_matrix</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">triu</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span> <span class="n">diagonal</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
<span class="n">lower_tri_matrix</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tril</span><span class="p">(</span><span class="n">input_tensors</span><span class="p">,</span> <span class="n">diagonal</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">diagonal</code> 에 주목해보자. 양수를 전달하면 주대각성분에서 해당하는 값만큼 떨어진 곳의 대각성분까지 그 값을 살려둔다. 한편 음수를 전달하면 주대각성분을 포함해 주어진 값만큼 떨어진 곳까지의 대각성분을 모두 0으로 만들어버린다. 기본은 0으로 설정되어 있으며, 이는 주대각성분부터 왼쪽 하단의 원소를 모두 살려두겠다는 의미가 된다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># torch.tril usage example
</span><span class="o">&gt;&gt;&gt;</span> <span class="n">lm_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tril</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">ones</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">lm_mask</span>
<span class="mi">1</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span>
<span class="mi">1</span> <span class="mi">1</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span>
<span class="mi">1</span> <span class="mi">1</span> <span class="mi">1</span> <span class="mi">0</span> <span class="mi">0</span>
<span class="mi">1</span> <span class="mi">1</span> <span class="mi">1</span> <span class="mi">1</span> <span class="mi">0</span>
</code></pre></div></div>

<p>두 메서드는 선형대수학이 필요한 다양한 분야에서 사용되는데, 필자의 경우, <code class="language-plaintext highlighter-rouge">GPT</code>처럼 <code class="language-plaintext highlighter-rouge">Transformer</code>의 <code class="language-plaintext highlighter-rouge">Decoder</code> 를 사용하는 모델을 빌드할 때 가장 많이 사용했던 것 같다. <code class="language-plaintext highlighter-rouge">Decoder</code>를 사용하는 모델은 대부분 구조상 <code class="language-plaintext highlighter-rouge">Language Modeling</code>을 위해서 <code class="language-plaintext highlighter-rouge">Masked Multi-Head Self-Attention Block</code>을 사용하는데 이 때 미래 시점의 토큰 임베딩 값에 마스킹을 해주기 위해 <code class="language-plaintext highlighter-rouge">torch.tril</code> 을 사용하게 되니 참고하자.</p>

<h3 id="torchtensormasked_fill"><code class="language-plaintext highlighter-rouge">👩‍👩‍👧‍👦 torch.Tensor.masked_fill</code></h3>

<p>사용자가 지정한 값에 해당되는 원소를 모두 마스킹 처리해주는 메서드다. 먼저 매개변수를 확인해보자.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># torch.Tensor.masked_fill params
</span><span class="n">input_tensors</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">6</span><span class="p">]])</span>
<span class="n">input_tensors</span><span class="p">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">mask</span><span class="p">:</span> <span class="n">BoolTensor</span><span class="p">,</span> <span class="n">value</span><span class="p">:</span> <span class="nb">float</span><span class="p">)</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">masked_fill</code> 은 텐서 객체의 내부 <code class="language-plaintext highlighter-rouge">attribute</code> 로 정의되기 때문에 해당 메서드를 사용하고 싶다면 먼저 마스킹 대상 텐서를 만들어야 한다. 텐서를 정의했다면 텐서 객체의 <code class="language-plaintext highlighter-rouge">attributes</code> 접근을 통해 <code class="language-plaintext highlighter-rouge">masked_fill()</code> 을 호출한 뒤, 필요한 매개변수를 전달해주는 방식으로 사용하면 된다.</p>

<p><code class="language-plaintext highlighter-rouge">mask</code> 매개변수에는 마스킹 텐서를 전달해야 하는데, 이 때 내부 원소는 모두 <code class="language-plaintext highlighter-rouge">boolean</code>이어야 하고 텐서의 형태는 대상 텐서와 동일해야 한다(완전히 같을 필요는 없고, 브로드 캐스팅만 가능하면 상관 없음).</p>

<p><code class="language-plaintext highlighter-rouge">value</code> 매개변수에는 마스킹 대상 원소들에 일괄적으로 적용해주고 싶은 값을 전달한다. 이게 말로만 들으면 이해하기 쉽지 않다. 아래 사용 예시를 함께 첨부했으니 참고 바란다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># torch.masked_fill usage
</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">lm_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tril</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">ones</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">lm_mask</span>
<span class="mi">1</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span>
<span class="mi">1</span> <span class="mi">1</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span>
<span class="mi">1</span> <span class="mi">1</span> <span class="mi">1</span> <span class="mi">0</span> <span class="mi">0</span>
<span class="mi">1</span> <span class="mi">1</span> <span class="mi">1</span> <span class="mi">1</span> <span class="mi">0</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">attention_matrix</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">))</span> <span class="o">/</span> <span class="n">dot_scale</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">attention_matrix</span>
<span class="mf">1.22</span> <span class="mf">2.1</span> <span class="mf">3.4</span> <span class="mf">1.2</span> <span class="mf">1.1</span>
<span class="mf">1.22</span> <span class="mf">2.1</span> <span class="mf">3.4</span> <span class="mf">9.9</span> <span class="mf">9.9</span>
<span class="mf">1.22</span> <span class="mf">2.1</span> <span class="mf">3.4</span> <span class="mf">9.9</span> <span class="mf">9.9</span>
<span class="mf">1.22</span> <span class="mf">2.1</span> <span class="mf">3.4</span> <span class="mf">9.9</span> <span class="mf">9.9</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">attention_matrix</span> <span class="o">=</span> <span class="n">attention_matrix</span><span class="p">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">lm_mask</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="s">'-inf'</span><span class="p">))</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">attention_matrix</span>
<span class="mf">1.22</span> <span class="o">-</span><span class="n">inf</span> <span class="o">-</span><span class="n">inf</span> <span class="o">-</span><span class="n">inf</span> <span class="o">-</span><span class="n">inf</span>
<span class="mf">1.22</span> <span class="mf">2.1</span> <span class="o">-</span><span class="n">inf</span> <span class="o">-</span><span class="n">inf</span> <span class="o">-</span><span class="n">inf</span>
<span class="mf">1.22</span> <span class="mf">2.1</span> <span class="mf">3.4</span> <span class="o">-</span><span class="n">inf</span> <span class="o">-</span><span class="n">inf</span>
<span class="mf">1.22</span> <span class="mf">2.1</span> <span class="mf">3.4</span> <span class="mf">9.9</span> <span class="o">-</span><span class="n">inf</span>
</code></pre></div></div>

<h3 id="️torchclone"><code class="language-plaintext highlighter-rouge">🗂️ torch.clone</code></h3>

<p><code class="language-plaintext highlighter-rouge">inputs</code> 인자로 전달한 텐서를 복사하는 파이토치 내장 메서드다.  사용법은 아래와 같다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="s">""" torch.clone """</span>
<span class="n">torch</span><span class="p">.</span><span class="n">clone</span><span class="p">(</span>
    <span class="nb">input</span><span class="p">,</span><span class="err"> </span>
    <span class="o">*</span><span class="p">,</span>
   <span class="err"> </span><span class="n">memory_format</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">preserve_format</span>
<span class="p">)</span><span class="err"> → </span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span>
</code></pre></div></div>

<p>딥러닝 파이프라인을 만들다 보면 많이 사용하게 되는 기본적인 메서드인데, 이렇게 따로 정리하게 된 이유가 있다. 입력된 텐서를 그대로 복사한다는 특성 때문에 사용시 주의해야 할 점이 있기 때문이다. 해당 메서드를 사용하기 전에 반드시 입력할 텐서가 현재 어느 디바이스(CPU, GPU) 위에 있는지, 그리고 해당 텐서가 계산 그래프를 가지고 있는지를 <strong>반드시</strong> 파악해야 한다.</p>

<p>필자는 ELECTRA 모델을 직접 구현하는 과정에서 <code class="language-plaintext highlighter-rouge">clone()</code> 메서드를 사용했는데, Generator 모델의 결과 로짓을 Discriminator의 입력으로 변환해주기 위함이었다. 그 과정에서 Generator가 반환한 로짓을 그대로 <code class="language-plaintext highlighter-rouge">clone</code>한 뒤, 입력을 만들어 주었고 그 결과 아래와 같은 에러를 마주했다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">RuntimeError</span><span class="p">:</span> <span class="n">one</span> <span class="n">of</span> <span class="n">the</span> <span class="n">variables</span> <span class="n">needed</span> <span class="k">for</span> <span class="n">gradient</span> <span class="n">computation</span> <span class="n">has</span> <span class="n">been</span> <span class="n">modified</span> <span class="n">by</span> <span class="n">an</span> <span class="n">inplace</span> <span class="n">operation</span><span class="p">:</span> <span class="p">[</span>
<span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">LongTensor</span> <span class="p">[</span><span class="mi">8</span><span class="p">,</span> <span class="mi">511</span><span class="p">]]</span> <span class="ow">is</span> <span class="n">at</span> <span class="n">version</span> <span class="mi">1</span><span class="p">;</span> <span class="n">expected</span> <span class="n">version</span> <span class="mi">0</span> 
<span class="n">instead</span><span class="p">.</span> <span class="n">Hint</span><span class="p">:</span> <span class="n">the</span> <span class="n">backtrace</span> <span class="n">further</span> <span class="n">above</span> <span class="n">shows</span> <span class="n">the</span> <span class="n">operation</span> <span class="n">that</span> <span class="n">failed</span> <span class="n">to</span> <span class="n">compute</span> <span class="n">its</span> <span class="n">gradient</span><span class="p">.</span> 
<span class="n">The</span> <span class="n">variable</span> <span class="ow">in</span> <span class="n">question</span> <span class="n">was</span> <span class="n">changed</span> <span class="ow">in</span> <span class="n">there</span> <span class="ow">or</span> <span class="n">anywhere</span> <span class="n">later</span><span class="p">.</span> <span class="n">Good</span> <span class="n">luck</span><span class="err">!</span>
</code></pre></div></div>

<p>에러 로그를 자세히 읽어보면 텐서 버전의 변경으로 인해 그라디언트 계산이 불가하다는 내용이 담겨있다. 구글링해봐도 잘 안나와서 포기하려던 찰라에 우연히 <code class="language-plaintext highlighter-rouge">torch.clone()</code> 메서드의 정확한 사용법이 궁금해 공식 Docs를 읽게 되었고, 거기서 엄청난 사실을 발견했다. <code class="language-plaintext highlighter-rouge">clone()</code> 메서드가 입력된 텐서의 현재 디바이스 위치에 똑같이 복사될 것이란 예상은 했지만, 입력 텐서의 계산그래프까지 복사될 것이란 생각은 전혀 하지 못했기 때문이다. 그래서 위와 같은 에러를 마주하지 않으려면, <code class="language-plaintext highlighter-rouge">clone()</code>을 호출할 때 뒤에 반드시 <code class="language-plaintext highlighter-rouge">detach()</code>를 함께 호출해줘야 한다.</p>

<p><code class="language-plaintext highlighter-rouge">clone()</code> 메서드는 입력된 텐서의 모든 것을 복사한다는 점을 반드시 기억하자.</p>]]></content><author><name>qcqced</name><email>qcqced123@gmail.com</email></author><category term="Framework &amp; Library" /><category term="Pytorch" /><category term="Tensor" /><category term="Linear Algebra" /><summary type="html"><![CDATA[파이토치에서 자주 사용하는 텐서 인덱싱 관련 메서드 모음]]></summary></entry><entry><title type="html">📈 Chain Rule: 합성함수 미분법</title><link href="http://localhost:4000/optimization-theory/chain-rule" rel="alternate" type="text/html" title="📈 Chain Rule: 합성함수 미분법" /><published>2024-01-08T00:00:00+09:00</published><updated>2024-01-08T23:00:00+09:00</updated><id>http://localhost:4000/optimization-theory/chain_rule</id><content type="html" xml:base="http://localhost:4000/optimization-theory/chain-rule"><![CDATA[<p><code class="language-plaintext highlighter-rouge">Chain Rule</code> 이라고 불리기도 하는 합성함수 미분법은 미적분학에서 특히나 중요한 개념 중 하나다. 근래에는 신경망을 활용한 딥러닝이 주목받으면서 그 중요성이 더욱 부각되고 있다. 신경망 모델은 쉽게 생각하면 정말 많은 1차함수와 여러 활성함수를 합성한 것과 같기 때문이다. 따라서 오차 역전을 통해 가중치를 최적화 하는 과정을 정확히 이해하려면 합성함수 미분법에 대한 이해는 필수적이다.</p>

<h3 id="concept"><code class="language-plaintext highlighter-rouge">💡 Concept</code></h3>

<p>합성함수 미분을 이해하기 전에 먼저 도함수 표기법에 대해서 익숙해질 필요가 있다. 도함수 표기법은 크게 뉴턴 표기법과 라이프니츠 표기법으로 나뉜다. 아래 표기된 수식을 보자. (1)번 수식이 뉴턴 표기법, (2)번이 라이프니츠의 표기법이다.</p>

\[y'= 2x\ \ \ \ (1) \\
f'(x) = 2x\ \ \ (2)\]

<p>이제 아래와 같은 합성함수가 있다고 가정해보자. 도함수의 정의에 따라서 해당 함수에 대한 도함수는 다음과 같이 표현할 수 있을 것이다.</p>

\[y = g(f(x))\ \ \ (3) \\
y' = {g(f(x))}' = \frac{dy}{dx}\ \ \ (4)\]

<p>위에 작성한 수식들은 잠시 잊고 이제 $f’(x)$ 를 먼저 생각해보자. $f(x)$에 대한 도함수 $f’(x)$는 다음과 같이 나타낼 수 있다.</p>

\[u = f(x) \\
u' = f'(x) = \frac{du}{dx} \\\]

<p>그렇다면 함수 $g$에 대한 도함수는 어떻게 나타낼 수 있을까?? 답은 바로 치환을 이용하는 것이다. 위에서 우리는 $f(x)$가 $u$와 같다고 정의했다. 이것을 이용해 도함수 $g’$는 다음과 같이 나타낼 수 있겠다.</p>

\[y' = g'(u) = \frac{dy}{du} \\\]

<p>눈치가 빠르다면 벌써 왜 합성함수 미분법을 <code class="language-plaintext highlighter-rouge">Chain Rule</code>이라고 부르는지 깨닫게 되었을 것이다. (4)번 수식의 우변은 사실 분자와 분모에 위치한 $du$가 약분된 꼴이라고 볼 수 있다. 이것을 뉴턴 표기법으로 나타내면 아래와 같다.</p>

\[y' = \frac{dy}{du}•\frac{du}{dx}\]

<p>뉴턴 표기법은 직관적이지 않기 때문에 라이프니츠 표기법으로 다시 정리하면 다음과 같다.</p>

\[y' = g'(f(x))•f'(x)\]

<p>아마 고등학교 때는 합성함수 미분 공식을 겉미분•속미분이라는 명칭으로 처음 접했을 것이다. 안에 감싸져 있는 함수를 미분해서 밖으로 빼낸다 해서 속미분이라 부르고, 다시 속은 냅두고 밖의 둘러져 있는 함수만 미분한다해서 겉미분이라 칭하는데, 이렇게 단순하게 외우기보다는 공식이 도출되는 흐름을 이해하는 것이 훨씬 오래 기억에 남는다. 부끄럽지만 필자가 바로 그러했다. 대학교에 입학하고 수도 없이 합성함수 미분을 해야 했지만 사용할 때마다 까먹어서 인터넷을 찾아보거나 교과서를 뒤적뒤적 했던 기억이 있다. 이글을 읽는 독자들은 나와 같은 실수를 반복하지 않기를 바라며 포스팅을 마친다.</p>]]></content><author><name>qcqced</name><email>qcqced123@gmail.com</email></author><category term="Optimization Theory" /><category term="Calculus" /><summary type="html"><![CDATA[합성함수 미분법 공식 유도]]></summary></entry><entry><title type="html">🔢 Product &amp;amp; Quotient Rule: 곱의 미분, 몫의 미분</title><link href="http://localhost:4000/optimization-theory/product_quotient_rule" rel="alternate" type="text/html" title="🔢 Product &amp;amp; Quotient Rule: 곱의 미분, 몫의 미분" /><published>2024-01-08T00:00:00+09:00</published><updated>2024-01-08T23:00:00+09:00</updated><id>http://localhost:4000/optimization-theory/product_quotient_rule</id><content type="html" xml:base="http://localhost:4000/optimization-theory/product_quotient_rule"><![CDATA[<p>곱의 미분, 몫의 미분은 함수가 곱의 꼴 형태 $f(x)g(x)$ 혹은 분수 꼴 형태 $\frac{f(x)}{g(x)}$를 가지고 있을 때 도함수를 구하는 방법이다. 고등학교 미적분 시간(17~18학번 기준)에 배운적이 있지만, 합성함수 미분법과 더불어 단순 암기의 폐해로 까먹기 좋은 미분법들이다. 크로스 엔트로피, 소프트맥스 미분에 쓰이므로 합성함수 미분법과 마찬가지로 딥러닝, 머신러닝에서 매우 중요하다.</p>

<h3 id="️product-rule"><code class="language-plaintext highlighter-rouge">✖️ Product Rule</code></h3>

<p>몫의 미분은 곱의 미분의 원리를 이해하면 자동으로 알 수 있기 때문에 곱의 미분부터 살펴보겠다. 먼저 아래와 같이 곱의 형태를 가지는 함수 $p(x)$가 있다고 가정해보자.</p>

\[p(x) = f(x)g(x)\ \ \ (0)\]

<p>우변의 두 항을 분리하기 전에 도함수의 정의를 이용해 도함수 $p’(x)$를 구하면 다음과 같다.</p>

\[p'(x) = \lim_{h -&gt; 0} \frac{p(x+h) - p(x)}{h}\ \ \ (1)\]

<p>이제 다시 $p(x+h),\  p(x)$에 $f(x)g(x)$를 대입해보자.</p>

\[p'(x) = \lim_{h -&gt; 0} \frac{p(x+h) - p(x)}{h} = \lim_{h-&gt;0} \frac{f(x+h)g(x+h) - f(x)(g(x)}{h}\ \ \ (2)\]

<p>이 지점에서 우리가 뭘하려고 지금 이렇게 수식을 전개하고 있는지 상기할 필요가 있다. 우리는 곱의 형태를 갖는 함수의 <code class="language-plaintext highlighter-rouge">도함수</code>를 구하고 싶은 것이다. (1)번처럼 함수에 대입하는 입력값을 뺀 결과가 $h$가 되도록 말이다. (1)과 같은 꼴을 만들어주기 위해 약간의 트릭을 쓸 필요가 있다. 사용할 트릭은 대수학에서 정말 빈번하게 사용되므로 잘 기억하고 있는게 좋다. 바로 $A-A = 0$이라는 것을 이용하는 것이다. 이게 무슨 말인가는 아래 수식을 보면 알 수 있다.</p>

\[p'(x) = \lim_{h-&gt;0} \frac{f(x+h)g(x+h) - f(x)g(x+h) + f(x)g(x+h) - f(x)g(x)}{h}\ \ \ (3)\]

<p>(3)번 수식은 (2)번 수식의 분자에 $- f(x)g(x+h) + f(x)g(x+h)$만 추가된 형태다. 두항을 더하면 0이 되기 때문에 사실 (2)번과 (3)번은 같은 수식이라고 볼 수 있는 것이다. 그래서 두항을 추가해도 전혀 문제가 없다. 이제 우리가 익숙한 도함수 정의를 만족하는 항들이 직관적으로 보인다.</p>

\[p'(x) = f'(x)g(x) + f(x)g'(x)\ \ \ (4)\]

<p>따라서 수식을 정리하면 결국 곱의 형태를 갖는 함수의 도함수는 (4)번과 같은 공식을 갖게 되는 것이다. 한편, 곱의 미분법은 (0)번 수식을 직사각형의 넓이라고 간주하면 좀 더 직관적으로 이해할 수 있다. 따라서 직사각형 넓이 $p(x)$에 대한 도함수 $p’(x)$는 넓이의 순간 변화율로 해석할 수 있다.</p>

<p align="center">
<img src="/assets/images/optimization/product_rule_rectangle.jpeg" alt="곱의 미분의 직관적 해석" class="align-center image-caption" width="50%&quot;, height=&quot;30%" />
<strong><em><a href="https://blog.naver.com/sodong212/220924875183">곱의 미분의 직관적 해석</a></em></strong>
</p>

<p>우변의 왼쪽항이 직사각형의 아래 부분의 증가하는 영역이 되고, 우측항이 회색이 칠해진 영역이 되는 것이다. 그렇다면 그림의 우측 하단에 위치한 작은 사각형의 넓이는 어떻게 처리해줘야 할까?? 곱의 미분 공식에는 해당 영역을 반영하는 항이 전혀 없다. 그 이유는 영역의 넓이가 너무나 작아서 근사치로 간주하고 무시해도 될 정도라서 그렇다. 해당 사각형의 가로 길이는 $g’(h)$, 세로 길이는 $f’(h)$가 된다. 도함수의 정의를 다시 떠올려보면 $h$는 0의 극한으로 근사하기 때문에 두 항의 곱인 영역의 넓이 역시 0에 매우 근접하게 된다. 따라서 고려할 필요 없이 무시해도 된다.</p>

<h3 id="quotient-rule"><code class="language-plaintext highlighter-rouge">➗ Quotient Rule</code></h3>

<p>몫의 미분은 곱의 미분 공식을 이용하고 나서 남은 지저분한 수식만 잘 정리하면 된다. 다음과 같이 분수 꼴 형태의 함수 $q(x)$가 있다고 가정해보자.</p>

\[q(x) = \frac{f(x)}{g(x)} \ \ \ (0)\]

<p>곱의 미분 공식을 이용하기 위해 분수 꼴의 함수를 다시 곱의 형태로 바꿔보자.</p>

\[f(x) = q(x)g(x) \ \ \ (1)\]

<p>이제 공식을 이용해 좌변에 대한 도함수를 구해보자.</p>

\[f'(x) = q'(x)g(x) + q(x)g'(x)\ \ \  (2)\]

<p>우리가 구하고 싶은 것은 분수 꼴을 갖는 함수 $q(x)$의 도함수 $q’(x)$이다. 따라서 (2)번 수식을 $q’(x)$에 대해서 정리해야 한다.</p>

\[q'(x) = \frac{f'(x) - q(x)g'(x)}{g(x)}\ \ \ (3)\]

<p>(3)번 수식을 예쁘게 정리하기 위해 공통 분모 $\frac{1}{g(x)}$를 앞으로 빼주고, $q(x)$에 (0)번 수식을 대입해 정리하면 몫의 미분법 공식, (5)번이 도출된다.</p>

\[q'(x) = \frac{1}{g(x)}(\frac{f'(x)g(x)-f(x)g'(x)}{g(x)})\ \ \ (4) \\
q'(x) = \frac{f'(x)g(x)-f(x)g'(x)}{g(x)^2}\ \ \ (5)\]

<p>이렇게 곱의 미분법, 몫의 미분법의 공식이 유도되는 과정을 모두 살펴보았다.</p>]]></content><author><name>qcqced</name><email>qcqced123@gmail.com</email></author><category term="Optimization Theory" /><category term="Calculus" /><category term="Product Rule" /><category term="Quotient Rule" /><summary type="html"><![CDATA[곱의 미분, 몫의 미분 공식 유도]]></summary></entry><entry><title type="html">🗄️ SVD: Singular Value Decomposition</title><link href="http://localhost:4000/linear-algebra/svd" rel="alternate" type="text/html" title="🗄️ SVD: Singular Value Decomposition" /><published>2023-11-26T00:00:00+09:00</published><updated>2023-11-27T13:00:00+09:00</updated><id>http://localhost:4000/linear-algebra/svd</id><content type="html" xml:base="http://localhost:4000/linear-algebra/svd"><![CDATA[<p>특이값 분해는 고유값 분해를 일반적인 상황으로 확장시킨 개념으로 LSA(Latent Semantic Anaylsis), Collaborative Filtering과 같은 머신러닝 기법에 사용되기 때문에 자연어처리, 추천시스템에 관심이 있다면 반드시 이해하고 넘어가야 하는 중요한 방법론이다. <a href="https://www.youtube.com/watch?v=PP9VQXKvSCY&amp;t=108s&amp;ab_channel=%ED%98%81%ED%8E%9C%ED%95%98%EC%9E%84%7CAI%26%EB%94%A5%EB%9F%AC%EB%8B%9D%EA%B0%95%EC%9D%98"><strong><u>혁펜하임님의 선형대수학 강의</u></strong></a>와 <a href="https://www.youtube.com/watch?v=7dmV3p3Iy90&amp;ab_channel=%EA%B3%B5%EB%8F%8C%EC%9D%B4%EC%9D%98%EC%88%98%ED%95%99%EC%A0%95%EB%A6%AC%EB%85%B8%ED%8A%B8"><strong><u>공돌이의 수학정리님의 강의 및 포스트</u></strong></a> 그리고 <a href="https://product.kyobobook.co.kr/detail/S000001743773"><strong><u>딥러닝을 위한 선형대수학 교재</u></strong></a>을 참고하고 개인적인 해석을 더해 정리했다.</p>

<h3 id="concept-of-svd"><code class="language-plaintext highlighter-rouge">🌟 Concept of SVD</code></h3>

\[A = UΣV^T\]

<p>크기가 <code class="language-plaintext highlighter-rouge">mxn</code>인 임의의 행렬 $A$를 위 수식의 우변처럼 여러 다른 행렬로 분해하는 방법을 말한다. 행렬 분해 기법이라는 점에서 고유값 분해와 궤를 같이하지만 좀 더 실용적인 형태로 변화했다. 고유값 분해는 행렬 $A$가 정사각행렬 일 때만 적용할 수 있다는 한계를 가지고 있다. 실생활에서 다루는 행렬 모형의 테이블 데이터는 99.9999999%의 확률로 직사각행렬이다. 필자는 살면서 한 번도 정사각행렬 형태의 테이블 데이터를 본적이 없다. 즉 실생활의 데이터에 고유값 분해를 적용하기 위해 직사각행렬 형태로 확장한 것이 바로 특이값 분해다.</p>

<p>선형변환 $A$
가 다음과 같다고 가정하고 구체적인 예시와 함께 우변의 항들에 대해 자세히 살펴보자.</p>

\[A = \begin{pmatrix}
1 &amp; 2 \\
3 &amp; 4 \\
5 &amp; 6
\end{pmatrix}\]

<p>먼저 행렬 $U$는 크기가 <code class="language-plaintext highlighter-rouge">mxm</code>인 정사각행렬이다.</p>

\[U = \begin{pmatrix}-0.2298 &amp; 0.8835 &amp; 0.4082 \\-0.5247 &amp; 0.2408 &amp; -0.8165 \\-0.8196 &amp; -0.4019 &amp; 0.4082\end{pmatrix}\]

<p>고유값 분해에서 고유벡터를 쌓아 만든 행렬 $V$와 비슷한 역할을 하면서 특이값 행렬의 왼족에 있다는 의미에서 <code class="language-plaintext highlighter-rouge">Left Singular Vector</code>라고도 부른다. 행렬의 개별 열차원의 벡터는 고유한 특이벡터가 된다. 따라서 개별 고유벡터는 서로에게 독립이며 직교한다. 따라서 전체 행렬 $U$는 <code class="language-plaintext highlighter-rouge">Orthogonal Matrix</code>가 된다.</p>

<p>한편,  $Σ$는 개별 특이벡터에 해당되는 특이값들을 저장한 행렬로 크기는 <code class="language-plaintext highlighter-rouge">mxn</code>이다.</p>

\[\Sigma = \begin{pmatrix}
\sigma_1 &amp; 0 \\
0 &amp; \sigma_2 \\
0 &amp; 0
\end{pmatrix}\]

<p>현재 제시된 예시 행렬의 크기가 <code class="language-plaintext highlighter-rouge">m&gt;n</code>이기 때문에 $Σ$의 모양 역시 행 방향으로 더 길게 놓인 직사각행렬이 된다. 반대의 상황이라면 열방향으로 길게 누적된 직사각행렬이 될 것이다. 또한 $Σ$은 직사각행렬이지만 대각행렬이라서, 주대각성분 원소를 제외한 나머지 원소는 모두 반드시 0이 된다.</p>

<p>마지막으로 행렬 $V^T$는 크기가 <code class="language-plaintext highlighter-rouge">nxn</code>인 정사각행렬이다.</p>

\[V^T = \begin{pmatrix}-0.6196 &amp; -0.7849 \\-0.7849 &amp; 0.6196\end{pmatrix}\]

<p>행렬 $U$때와 같은 논리와 더불어 특이값 행렬의 오른족에 배치되어 있기 때문에 <code class="language-plaintext highlighter-rouge">Right singular Vector</code> 라고 부른다. 마찬가지로 개별 고유벡터는 서로에게 독립이며 직교한다. 따라서 전체 행렬 $V^T$역시 <code class="language-plaintext highlighter-rouge">Orthogonal Matrix</code>가 된다.</p>

<p>그렇다면 특이값 분해는 어떻게 고유값 분해의 제약조건을 탈피했는지 살펴보자. 바로 크기가 <code class="language-plaintext highlighter-rouge">mxn</code>인 행렬 $A$와 그것의 전치 행렬 $A^T$ 사이의 곱은 항상 대칭 행렬이 되고, 대칭행렬은 대각화-가능 행렬이라는 점을 이용한다.</p>

\[A•A^T = UΣV^T•VΣ^TU^T = UΣΣ^TU^T \\
A^T•A = VΣ^TU^T•UΣV^T = VΣ^TΣV^T\]

<p>새롭게 정의된 행렬 $A•A^T$은 크기가 mxm인 정사각•대칭행렬이 되고, 행렬 $A^T•A$은 크기가 nxn인 정사각•대칭행렬이 된다. 행렬 $U,V$ 모두 <code class="language-plaintext highlighter-rouge">Orthogonal Matrix</code> 라는 점을 이용하면 우리는 새롭게 정의된 두 행렬을 고유값분해 결과와 유사하게 나타낼 수 있다.</p>

\[A•A^T = Q_m \Lambda_m Q^T_m \\
A^T•A = Q_n \Lambda_n Q^T_n\]

<p>수식 우변에 놓인 $\Lambda_m, \Lambda_n$ 의 구체적인 예시는 다음과 같다.</p>

\[\Lambda_m = \Sigma\Sigma^T = \begin{pmatrix}
\sigma_1^2 &amp; 0 &amp; 0 \\
0 &amp; \sigma_2^2 &amp; 0 \\
0 &amp; 0 &amp; 0 \\
\end{pmatrix} \\

\Lambda_n = \Sigma^T\Sigma = \begin{pmatrix}
\sigma_1^2 &amp; 0 \\
0 &amp; \sigma_2^2 \\
\end{pmatrix}\]

<p>이제 두 행렬에 각각 양의 제곱근을 사용해 $\sigma_1, \sigma_2$(특이값)를 구하고 원래 수식($A = UΣV^T$)의 $Σ$에 대입한다. 이렇게 하면 선형변환 $A$에 대한 특이값 분해를 모두 끝마친 셈이다. 지금까지 살펴본 내용을 요약 정리하면 다음과 같다.</p>

<ul>
  <li>1) $AA^T, A^TA$ 계산</li>
  <li>2) $AA^T, A^TA$의 고유값, 고유벡터 계산</li>
  <li>3) $A^TA$로 Right Singular Vector $V$를,  $AA^T$로 Left Singular Vector $U$를 도출</li>
  <li>4) 고유값($\Sigma\Sigma^T$)의 제곱근에서 나온 값들을 $\Sigma$의 대각선에 배치합니다. 이 값들이 특이값에 해당</li>
</ul>

<h3 id="insight-of-svd"><code class="language-plaintext highlighter-rouge">💡 Insight of SVD</code></h3>

\[A\vec x = \sigma_1u_1v_1^T\vec x + \sigma_2u_2v_2^T\vec x + ... +\sigma_n u_nv_n^T\vec x\]

<p>고유값 분해의 활용과 사실상 다른게 없다. 역시 데이터 압축•복원 혹은 PCA 같은 차원 축소 기법에 사용된다. 고유값 분해와 다른게 있다면 선형변환 $A$가 반드시 정사각행렬일 필요가 없다는 것이다. 그리고 고유값 분해를 데이터 압축이나 차원 축소에 간편히 사용하려면, 선형변환 $A$가 대칭행렬이여야 한다는 조건도 붙고 상당히 사용하게 까다로웠는데 특이값 분해에서는 그런 번거로운 제약 조건이 모두 사라진다는 장점이 있다.</p>

<p>특히 특이값 분해는 분해되는 과정 자체보다 분해되는 행렬을 개별 특이벡터에 대한 특이값의 가중합 방식으로 조합하는 과정에서 그 빛을 발한다. 특이값에 $argmax$를 적용해 상위 p개의 특이벡터만 반영한 부분 행렬 $A$를 이용할 수도 있고, 필요에 따라서 다시 p개의 개수를 늘려 원본으로 복원하는 것도 가능해진다. 이러한 기능이 가장 빛을 발하는 분야가 이미지 해상도 압축•복원 분야다. 현재 내가 사용하려면 이미지의 해상도가 너무 높아서 용량이 커져 사용하기 어렵다면 SVD를 이용해 필수적인 특이벡터 몇개만 남겨놓는 방식으로 해상도 압축을 수행해 용량을 줄일 수 도 있다. <a href="https://angeloyeo.github.io/2019/08/01/SVD.html#%ED%8A%B9%EC%9D%B4%EA%B0%92-%EB%B6%84%ED%95%B4%EC%9D%98-%ED%99%9C%EC%9A%A9"><strong><u>공돌이의 수학정리님의 포스트 하단부에</u></strong></a> 정말 직관적으로 잘 만들어진 사례가 있으니 꼭 한 번씩 보고 오시길 권장한다.</p>]]></content><author><name>qcqced</name><email>qcqced123@gmail.com</email></author><category term="Linear Algebra" /><category term="Linear Algebra" /><category term="Singular Value Decomposition" /><category term="Singular Vector" /><category term="Singular Value" /><category term="SVD" /><category term="PCA" /><summary type="html"><![CDATA[💡 Concept of Singular Value Decomposition]]></summary></entry><entry><title type="html">🔢 Eigen Decomposition</title><link href="http://localhost:4000/linear-algebra/eigen-decomposition" rel="alternate" type="text/html" title="🔢 Eigen Decomposition" /><published>2023-11-25T00:00:00+09:00</published><updated>2023-11-26T13:00:00+09:00</updated><id>http://localhost:4000/linear-algebra/eigen-decomposition</id><content type="html" xml:base="http://localhost:4000/linear-algebra/eigen-decomposition"><![CDATA[<p>고유값, 고유벡터, 고유값 분해는 비단 선형대수학뿐만 아니라 해석기하학 나아가 데이터 사이언스 전반에서 가장 중요한 개념 중 하나라고 생각한다. 머신러닝에서 자주 사용하는 여러 행렬 분해(Matrix Factorization) 기법(ex: <code class="language-plaintext highlighter-rouge">SVD</code>)과 <code class="language-plaintext highlighter-rouge">PCA</code>의 이론적 토대가 되므로 반드시 완벽하게 숙지하고 넘어가야 하는 파트다. 이번 포스팅 역시 <a href="https://www.youtube.com/watch?v=PP9VQXKvSCY&amp;t=108s&amp;ab_channel=%ED%98%81%ED%8E%9C%ED%95%98%EC%9E%84%7CAI%26%EB%94%A5%EB%9F%AC%EB%8B%9D%EA%B0%95%EC%9D%98"><strong><u>혁펜하임님의 선형대수학 강의</u></strong></a>와 <a href="https://www.youtube.com/watch?v=7dmV3p3Iy90&amp;ab_channel=%EA%B3%B5%EB%8F%8C%EC%9D%B4%EC%9D%98%EC%88%98%ED%95%99%EC%A0%95%EB%A6%AC%EB%85%B8%ED%8A%B8"><strong><u>공돌이의 수학정리님의 강의 및 포스트</u></strong></a> 그리고 <a href="https://product.kyobobook.co.kr/detail/S000001743773"><strong><u>딥러닝을 위한 선형대수학 교재</u></strong></a>을 참고하고 개인적인 해석을 더해 정리했다.</p>

<h3 id="concept-of-eigen-value--vector"><code class="language-plaintext highlighter-rouge">🌟 Concept of Eigen Value &amp; Vector</code></h3>

\[Av = \lambda v\]

<p>등식을 만족시키는 벡터 $v$를 <code class="language-plaintext highlighter-rouge">고유 벡터(Eigen Vector)</code>, 람다 $\lambda$를 <code class="language-plaintext highlighter-rouge">고유값(Eigen Value)</code>이라고 정의한다. 좌변의 $A$는 <code class="language-plaintext highlighter-rouge">선형 변환(행렬)</code>을 의미한다. 이러한 정보를 활용해 위 등식의 의미를 살펴보자. 어떤 선형변환 $A$와 벡터 $v$를 곱했더니, 어떤 스칼라와 벡터를 곱한 결과와 같았다는 것인데, 벡터에 스칼라를 곱하면 그 크기만 변화할 뿐 방향은 이전과 동일하다. 따라서 선형변환 $A$를 가해도 그 크기만 스칼라 배(고유값 배)만큼 변할뿐 방향은 동일한 벡터를 찾고자 하는게 위 수식의 목적이며 이게 바로 고유벡터의 정의가 된다.</p>

<p>그렇다면 수식을 풀어서 고유값과 고유벡터를 직접 구해보자. 먼저 좌변으로 모든 항을 넘긴 뒤, 고유 벡터 $v$로 좌변의 모든 항을 묶어준다.</p>

\[(A - \lambda I) v = 0\]

<p>고유 벡터 $v$로 묶어준다고만 했는데 왜 갑자기 람다 뒤에 항등행렬이 붙게 되었을까?? 람다는 고유값, 다시 말해 스칼라다. <code class="language-plaintext highlighter-rouge">행렬 - 스칼라</code>는 불가능하기 때문에 선형변환 $A$와 크기를 맞춰주기 위해 곱한 것이다. 다시 등식을 전체적인 관점에서 살펴보자. 지금 <code class="language-plaintext highlighter-rouge">행렬•벡터 = 0</code> 의 형태를 취하고 있다. 어디서 많이 본 듯한 꼴이 아닌가?? 바로 행렬의 영공간을 구할 때 사용하던 수식이다. 따라서 우리는 고유벡터 $v$가 좌측 괄호 안의 행렬 $A-\lambda I$의 영공간이 span하는 공간 어딘가에 위치했다는 것을 알 수 있다.</p>

<p>한편, 우리가 이 등식을 풀어헤친 목적은 고유값 그리고 고유벡터를 구하기 위함이다. 등식을 만족시키려면 고유벡터가 0이기만 하면 되겠지만, $v=0$인 경우를 찾자고 우리가 이렇게 고생하는 것은 당연히 아닐 것이다. 따라서 $v=0$이 아니라 좌측 괄호 안의 항 $A-\lambda I=0$이 되어야 한다. 이 때 $det(A-\lambda I) =0$를 만족해야 한다. 그 이유는 만약 행렬식이 0이 아니라면 역행렬이 존재한다는 것이 되고, 전체 등식에서 좌측 항에 대한 역행렬을 양변에 곱해주면 다시 $v=0$이라는 결과를 얻게 된다. 따라서 반드시 $det(A-\lambda I) =0$을 충족해 역행렬이 없도록 만들어야 한다.</p>

<p>따라서 결론적으로 우리는 두 가지 수식을 풀어내면 고유값과 고유벡터를 구할 수 있다.</p>

\[N(A-\lambda I) = V \\
det(A-\lambda I) = 0\]

<p>이 때, 영공간에 <code class="language-plaintext highlighter-rouge">span</code>하는 벡터는 무수히 많기 때문에 일반적으로 <code class="language-plaintext highlighter-rouge">Basis</code>를 고유값으로 간주한다.</p>

<h3 id="-eigen-decomposition"><code class="language-plaintext highlighter-rouge">🔢 Eigen Decomposition</code></h3>

\[A = V\Lambda V^{-1} \\
\Lambda = V^{-1}AV\]

<p>위 수식과 같은 형태로 임의의 정사각행렬 $A$를 표현 가능하다면, 우리는 이러한 행렬 $A$를 <code class="language-plaintext highlighter-rouge">Diagonalizable Matrix</code>라고 부르며, <code class="language-plaintext highlighter-rouge">Diagonalizable Matrix</code>를 고유벡터와 고유값 행렬로 분해하는 것을 <code class="language-plaintext highlighter-rouge">고유값 분해(Eidgen Decomposition)</code>라고 한다.</p>

<p>여기서 <code class="language-plaintext highlighter-rouge">Diagonalizable Matrix</code> 이란, 고유값 행렬을 이용해 대각행렬로 변환이 가능한 정사각행렬을 말한다. 두번째 수식이 바로 <code class="language-plaintext highlighter-rouge">Diagonalizable Matrix</code> 를 표현한 것이다. 어떤 행렬이 <code class="language-plaintext highlighter-rouge">Diagonalizable Matrix</code> 하다는 것은 다시 말해, 행렬에 <code class="language-plaintext highlighter-rouge">Independent</code>한 고유벡터가 N개 있다는 것과 동치다. 방금 서술한 사실을 유도해보자.</p>

<p>3X3 크기의 행렬 $A$와 서로 독립인 고유 벡터 $v_1, v_2, v_3$과 이에 대응되는 고유값 $\lambda_1, \lambda_2, \lambda_3$이 있다고 가정해보자. 여러개의 고유 벡터와 고유값을 수식 하나로 표현하기 위해 벡터화를 이용하고자 한다.</p>

\[A[v_1, v_2, v_3] = [v_1, v_2, v_3]•   \begin{bmatrix} 
   \lambda_1 &amp; 0 &amp; 0 \\
   0 &amp; \lambda_2 &amp; 0 \\
   0 &amp; 0 &amp; \lambda_3 \\
   \end{bmatrix} \\\]

<p>우리는 지금 어떤 행렬이 <code class="language-plaintext highlighter-rouge">Diagonalizable Matrix</code> 일 때 벌어지는 현상에 대해 증명하는게 목표라서 좌변에 행렬 $A$만 남기려고 한다. $[v_1, v_2, v_3]$ 은 서로 독립인 고유 벡터다. 그리고 사이즈는 3x3으로 정사각행렬에 해당된다. 열벡터가 서로 독립이면서 정사각행렬에 해당되기 때문에 $[v_1, v_2, v_3]$ 은 가역행렬의 조건을 모두 충족한다. 따라서 양변에 $[v_1, v_2, v_3]$ 의 역행렬을 곱해주자. 이제부터 편의상 $[v_1, v_2, v_3]$ 은 $V$,  고유값-대각행렬(우변 오른쪽 항) $\Lambda$로 표기하겠다.</p>

\[A = V\Lambda V^{-1} \\\]

<p>$V$가 <code class="language-plaintext highlighter-rouge">Independent</code>한 고유벡터가 N개를 갖고 있기 때문에 $V$를 일부분으로 갖고 있는 행렬 $A$는 당연히 <code class="language-plaintext highlighter-rouge">Independent</code>한 고유벡터가 N개 있다고 말할 수 있다.</p>

<h3 id="️-property-of-eigen-decomposition"><code class="language-plaintext highlighter-rouge">⭐️ Property of Eigen Decomposition</code></h3>

<p>고유값 분해가 가능한 <code class="language-plaintext highlighter-rouge">Diagonalizable Matrix</code> $A$의 속성에 대해 알아보자. 이런 속성들은 이후 <code class="language-plaintext highlighter-rouge">PCA</code>, <code class="language-plaintext highlighter-rouge">SVD</code>에서 사용되니 숙지하고 있는게 좋다.</p>

<ul>
  <li><strong>1) $A^k = V \Lambda V^{-1}•V \Lambda V^{-1} … = V \Lambda^k V^{-1}$</strong></li>
  <li><strong>2) $A^{-1} = (V \Lambda V^{-1})^{-1}$= $(V \Lambda^{-1} V^{-1})$</strong>
    <ul>
      <li>$AA^{-1}=I$</li>
    </ul>
  </li>
  <li><strong>3) $det(A) = det(V \Lambda V^{-1}) = det(V)det(\Lambda)det(V ^{-1}) = \prod_{i=1}^{N} {\lambda_i}$</strong>
    <ul>
      <li><strong>행렬식은 곱으로 쪼개는게 성립</strong></li>
    </ul>
  </li>
  <li><strong>4) $tr(A)=tr(V \Lambda V^{-1})=tr( \Lambda V^{-1}V)=tr(\Lambda)=\sum_i^{N}\lambda_i$</strong>
    <ul>
      <li><strong><code class="language-plaintext highlighter-rouge">trace</code> 는 원소의 순서를 바꾸는거 허용</strong></li>
    </ul>
  </li>
  <li><strong>5) rank-difficient == $det(A)=0$ ⇒ 행렬 $A$에는 값이 0인 고유값이 적어도 하나 이상 존재</strong>
    <ul>
      <li><strong>3)번 속성 이용</strong></li>
    </ul>
  </li>
  <li><strong>6) Diagonalizable Matrix의 non-zero eidgen value 개수 == rank(A)</strong>
    <ul>
      <li>$rank(A) = rank(V \Lambda V^{-1}) = rank(\Lambda)$</li>
      <li>$V, V^{-1}$ <strong>은 서로 독립인 열벡터를 쌓아 만든 행렬이라서 반드시 Full Rank</strong></li>
      <li><strong>랭크의 성질에 의해 가장 작은 값이 행렬의 랭크가 된다</strong></li>
      <li><strong><code class="language-plaintext highlighter-rouge">non-zero eigen value 개수</code> ==</strong> $rank(\Lambda)$</li>
    </ul>
  </li>
</ul>

<h3 id="insight-of-eigen-decomposition"><code class="language-plaintext highlighter-rouge">💡 Insight of Eigen Decomposition</code></h3>

<p>이렇게 고유값, 고유벡터, 고유값 분해에 대해서 전반적으로 살펴보았다. 하지만 아직도 왜 고유값 분해가 그리도 중요하다는 것인지 아직 감이 오지 않을 것이다. 고유값 분해의 중요성에 대해 알아보기 위해 먼저 다음과 같은 명제에 대해서 증명해보자.</p>

<p><strong><em>“대칭행렬(Symmetric Matrix)은 대각화 가능한 행렬(Diagonalizable Matrix)이다”</em></strong></p>

\[V^T = V^{-1} = Q\]

<p>대칭행렬은 정사각행렬 중에서 원본과 전치행렬이 동일한($A=A^{T}$) 특수 행렬을 말한다. 따라서 어떤 행렬 $A$가 대칭행렬이라면, $V \Lambda V^{-1} = V^{-T} \Lambda V^{T}$가 된다. 각변의 가장 마지막 항에 주목해보자. 등식 조건에 의해 $V^{-1} = V^T$가 성립하기 때문에 행렬 $V$는 전치행렬과 역행렬이 같은 행렬이 된다. 다시 말해, $V$는 직교행렬 $Q$가 된다. 따라서 행렬 $A$에 대한 고유값 분해식을 아래처럼 직교행렬로 표현할 수 있다.</p>

\[A = Q \Lambda Q^{-1} \\
A = [q_1, q_2, q_3]•\begin{bmatrix} 
   \lambda_1 &amp; 0 &amp; 0 \\
   0 &amp; \lambda_2 &amp; 0 \\
   0 &amp; 0 &amp; \lambda_3 \\
   \end{bmatrix}•\begin{bmatrix} 
   q_1^T \\
   q_2^T \\
   q_3^T \\
   \end{bmatrix} \\\]

<p>이제 우변을 수식을 전개해서 그 의미를 알아보자. 전개하면 아래와 같다.</p>

\[A = \lambda_1q_1q_1^T + \lambda_2q_2q_2^T + \lambda_3q_3q_3^T\]

<p>우변의 항을 하나 하나 살펴보자. 세개의 항은 모두 개별 고유벡터에 대한 <code class="language-plaintext highlighter-rouge">고유값</code>, <code class="language-plaintext highlighter-rouge">고유벡터</code> 그리고 <code class="language-plaintext highlighter-rouge">고유벡터의 전치</code>에 대한 곱으로 구성되어 있다. 고유벡터와 그것의 전치벡터의 곱은 크기는 <code class="language-plaintext highlighter-rouge">nxn</code>이지만, 사실 같은 벡터를 두번 곱한 것과 같기에 랭크는 1이된다. 다시 말해, 3차원 공간에서 1차원 직선 공간으로 <code class="language-plaintext highlighter-rouge">span</code>하는 부분 공간이 3개가 만들어지며 3개의 부분 공간은 서로 독립이면서, 모두 근본이 직교 행렬의 열벡터라는 점 때문에 서로 직교한다. 따라서 우리는 대칭행렬 $A$를 크기는 <code class="language-plaintext highlighter-rouge">NxN</code>이면서 랭크는 <code class="language-plaintext highlighter-rouge">1</code>인 행렬 3개를 고유값을 이용해 <code class="language-plaintext highlighter-rouge">가중합 방식</code>으로 더한 것이라고 해석할 수 있다. 뒤집어 서술하면 대칭행렬 $A$를 크기는 <code class="language-plaintext highlighter-rouge">NxN</code>이면서 랭크는 <code class="language-plaintext highlighter-rouge">1</code>인 행렬 <code class="language-plaintext highlighter-rouge">3</code>개로 쪼개는 방식이 바로 <code class="language-plaintext highlighter-rouge">고유값 분해</code>이다.</p>

<p>고유값에 따라, 부분 공간의 크기를 조절할 수 있다는 점에서 차원 축소나 제거처럼 중요도가 높은 데이터•특징만 추출하는게 가능해진다. 이러한 고유값 분해를 정사각행렬(대칭행렬)이 아닌 일반적인 직사각행렬에도 적용할 수 있도록 개념을 확장한게 바로 <code class="language-plaintext highlighter-rouge">SVD(Singular Vector Decomposition)</code>이고, 중요도(고유값의 크기)에 따라서 중요한 특징•데이터만 남기는 방법론은 <code class="language-plaintext highlighter-rouge">PCA(Princlpal Component Analysis)</code>의 이론적 토대가 된다.</p>

<p>이렇게 대칭행렬은 대각화 가능한 행렬이라는 점을 통해 <code class="language-plaintext highlighter-rouge">고유값 분해</code>의 의미에 대해서 알아보았다. 이제 마지막으로 선형변환으로서 행렬 $A$가 갖는 의미를 살펴보자. 고유벡터가 아닌 임의의 벡터 $\vec x$를 선형변환 $A$에 통과시켜보자. 그럼 우리는 아래와 같은 식을 얻을 수 있다.</p>

\[A\vec x = \lambda_1q_1q_1^T•\vec x + \lambda_2q_2q_2^T•\vec x + \lambda_3q_3q_3^T•\vec x\]

<p>우변을 해석해보자. 아까 고유값 분해의 의미를 살펴보면서 $q_1q_1^T$는 전체 공간에서 1차원 직선 공간으로 span하는 부분 공간을 의미한다고 했었다. 따라서 우변에는 서로 다른 항이 3개 있기 때문에 부분 공간이 3개 있는 3차원 공간이 형성된다. 이제 부분 공간과 벡터 $\vec x$를 내적한 형태로 바라볼 수 있다. 내적은 정사영이다. 따라서 $q_nq_n^T•\vec x$은 벡터 $\vec x$를 부분 공간에 정사영 내려준 벡터가 된다. 그리고 고유값을 곱해 정사영 내린 벡터들의 크기를 조절해주는게 우변의 의미가 된다.</p>]]></content><author><name>qcqced</name><email>qcqced123@gmail.com</email></author><category term="Linear Algebra" /><category term="Linear Algebra" /><category term="Eigen Decomposition" /><category term="Eigen Vector" /><category term="Eigen Value" /><category term="SVD" /><category term="PCA" /><summary type="html"><![CDATA[💡 Concept of Eigen Decomposition]]></summary></entry><entry><title type="html">📈 Gradient: Directional Derivative</title><link href="http://localhost:4000/optimization-theory/gradient" rel="alternate" type="text/html" title="📈 Gradient: Directional Derivative" /><published>2023-11-25T00:00:00+09:00</published><updated>2023-11-25T23:00:00+09:00</updated><id>http://localhost:4000/optimization-theory/gradient</id><content type="html" xml:base="http://localhost:4000/optimization-theory/gradient"><![CDATA[<h3 id="concept-of-gradient"><code class="language-plaintext highlighter-rouge">🤔 Concept of Gradient</code></h3>

<p>그라디언트는 다변수 함수의 기울기를 나타내는 벡터를 말한다. 그라디언트의 원소는 함수에 존재하는 모든 변수를 대상으로 편미분한 결과로 구성되는데, 예를 들어 변수가 $x_1, x_2$ 2개인 다변수 함수 $f(x_1, x_2)$가 있다고 가정해보자. 다변수 함수 $f$의 그라디언트는 아래 수식처럼 표현할 수 있다.</p>

\[f'(x_1, x_2) = \begin{vmatrix}
  \frac{∂f}{∂x_1} \\
  \frac{∂f}{∂x_2}
\end{vmatrix}\]

<p>이러한 그라디언트는 머신 러닝, 수치 최적화 학문에서 매우 중요한 개념으로 꼽힌다. 그라디언트 벡터가 가리키는 방향이 바로 다변수 함수가 특정 지점에서 가장 가파르게 증가하는 방향을 가리키기 때문이다. 이처럼 그라디언트는 함수의 입력 공간을 따라 함수가 어떻게 변화하는지를 알려주는 길잡이 역할을 하기 때문에, 그라디언트 방향을 따라 변수값을 튜닝하다 보면 함수의 최대값•최소값에 도달하여 최적화 문제를 해결할 수 있게 된다. 그렇다면 왜 그라디언트 벡터의 방향이 특정 지점에서 함수가 가장 가파르게 증가하는 방향을 나타내는 것일까?? 편미분, 도함수 정의 그리고 내적을 활용해 증명할 수 있다.</p>

<h3 id="-proof-of-gradient"><code class="language-plaintext highlighter-rouge">🪪 Proof of Gradient</code></h3>

<p align="center">
<img src="/assets/images/gradient/gradient.jpg" alt="Example of multivariate function" class="align-center image-caption" width="60%&quot;, height=&quot;25%" />
<strong><em><a href="https://m.blog.naver.com/PostView.naver?isHttpsRedirect=true&amp;blogId=galaxyenergy&amp;logNo=221431325545">Example of multivariate function</a></em></strong>
</p>

<p>그라디언트 벡터의 방향이 함수가 가장 가파르게 증가하는 방향과 일치한다는 명제를 증명하기 위해 최단 경로로 산 정상에 오르는 과정을 떠올려보려 한다. 우리는 현재 이변수 함수로 정의되는 산 중턱 어딘가, 점 $(x_1^0, x_2^0)$를 지나고 있다. 산 정상을 최단 경로로 오르려면 어떻게 해야할까?? 가장 경사가 가파른 급경사 지대를 향해 나아가면 될 것이다. 하지만 산 중턱에 있는 우리가 어느 방향이 가장 가파른 급경사 지대인지 직관적으로 알 길이 없다. 그래서 방향 도함수를 도입해 급경사 지대로 향할 수 있는 방향을 구해 보기로 했다. 아래 수식을 보자.</p>

\[\lim_{\Delta{x}-&gt;0}\frac{f(x+\Delta{x}) - f(x)}{\Delta{x}} =    \frac{df}{dx}= f'(x) \\
df = f'(x)dx\]

<p>너무나도 익숙한 형태 아닌가?? 우리가 일반적으로 알고 있는 일변수 함수의 미분 정의 그리고 좌변의 $dx$를 우변으로 넘겨 살짝 변형한 식이다. 이것을 이제 다변수 함수에 적용하면 바로 방향 도함수가 된다. 다시 우리가 오르려는 산(이변수 함수)으로 돌아와 보자.</p>

\[f(x_1 + dx_1, x_2) = f(x_1, x_2) + f'(x_1, x_2)dx_1 \\
f(x_1, x_2 + dx_2) = f(x_1, x_2) + f'(x_1, x_2)dx_2 \\\]

<p>위에서 서술한 도함수 정의를 활용해 우리가 다음에 발걸음을 옮길 위치를 점  $A$를 $(x_1^0 + dx_1, x_2^0+dx_2)$ 이라고 표현할 수 있다. 이 표현을 활용해 다변수 함수의 미분을 정의해보자. 우리는 이미 다변수 함수의 개별 변수에 편미분을 취하고 행벡터로 쌓은 결과가 바로 전미분이라는 것을 알고 있다.</p>

\[f(x_1 + dx_1, x_2 + dx_2) - f(x_1, x_2) = f'(x_1)dx_1 + f'(x_2)dx_2\]

<p>다시 편미분의 정의를 활용해 수식을 정리하면 방향 벡터와 편미분 결과의 내적으로 표현할 수 있다.</p>

\[dL = \frac{∂L}{∂{x_1}}dx_1 + \frac{∂L}{∂{x_2}}dx_2 \\
dL = [dx_1, dx_2]\ •\ \begin{vmatrix}
  \frac{∂L}{∂x_1} \\
  \frac{∂L}{∂x_2}
\end{vmatrix}\]

<p>쏟아지는 수식 속에 우리의 본래 목적을 잊어서는 안된다. 우리는 지금 가장 빠르게 산 정상에 도달할 수 있는 방법을 찾기 위해 지금까지 달려왔다. 산 정상에 가장 빠르게 도달하기 위해 가장 가파른 급경사 지대만 찾아서 올라가는 전략을 세웠었다. 다시 말해, 다변수 함수 $f(x)$의 극소 변화량 $dL$이 최대가 되는 방향으로 발걸음을 옮기면 된다는 것이다. 그렇다면 극소 변화량 $dL$은 언제 최대가 될까??</p>

<p>이제 까먹고 있었던 내적의 개념을 다시 한 번 상기시켜보자. 내적은 다양하게 해석되지만, 본디 서로 다른 두 벡터의 <code class="language-plaintext highlighter-rouge">닮은 정도</code>를 나타낸다. 극소 변화량 $dL$이 최대가 되려면 우변의 내적 결과가 최대가 되어야 한다. 내적의 최대값은 서로 다른 두 벡터 사이의 끼인각도가 0˚일 때 즉, 두 벡터가 동일한 방향을 나타낼 때 정의된다. <strong><u>따라서 방향 벡터가 그라디언트(편미분의 행벡터) 방향일 때</u></strong> <code class="language-plaintext highlighter-rouge">내적 결과</code>(극소 변화량 $dL$)<strong><u>가 최대가 된다.</u></strong></p>

<p><strong><u>한편, 실제 기계학습에서는 손실함수의 최적화를 목적 함수로 사용하기 때문에 그라디언트(손실함수의 전미분) 방향에 음수를 취해준 값을 사용하게 된다.</u></strong></p>]]></content><author><name>qcqced</name><email>qcqced123@gmail.com</email></author><category term="Optimization Theory" /><category term="Optimization Theory" /><category term="Calculus" /><category term="Partial Derivative" /><category term="Total Derivative" /><category term="loss function" /><category term="Gradient" /><category term="Gradient Descent" /><category term="Machine Learning" /><summary type="html"><![CDATA[Proof of gradient direction with Total Derivative]]></summary></entry><entry><title type="html">🍎 Newton-Raphson Method for Optimization</title><link href="http://localhost:4000/optimization-theory/newton-raphson" rel="alternate" type="text/html" title="🍎 Newton-Raphson Method for Optimization" /><published>2023-11-15T00:00:00+09:00</published><updated>2023-11-16T02:00:00+09:00</updated><id>http://localhost:4000/optimization-theory/newton_raphson</id><content type="html" xml:base="http://localhost:4000/optimization-theory/newton-raphson"><![CDATA[<h3 id="zero-find-ver"><code class="language-plaintext highlighter-rouge">🤔 Zero-Find Ver</code></h3>

<p>비선형 방정식의 근사해를 찾거나 최적화 문제를 해결하는 방식으로, 같은 과정을 반복해 최적값에 수렴한다는 점에서 경사하강법이랑 근본이 같다. 반면, 경사하강에 비해 빠른 수렴 속도를 자랑하고 풀이 방식이 매우 간단하다는 장점이 있다. 하지만 여러 제약 조건과 더불어 해당 알고리즘이 잘 작동하는 상황이 비현실적인 부분이 많아 경사하강에 비해 자주 사용되지는 않고 있다. 뉴턴-랩슨 방식은 근사해를 찾거나, 최적화 문제를 푸는 두 가지 버젼이 있는데 먼저 해를 찾는 버전부터 살펴보자. 알고리즘의 수식은 다음과 같다.ㅂ</p>

\[x_{n+1}:= x_n - \frac{f(x_n)}{f'(x_n)}\]

<p>반복법을 사용하기 때문에 수식의 생김새가 상당히 경사하강법과 비슷하다. 왜 이런 수식이 등장하게 되었을까?? 일단 뉴턴-랩슨 방식의 풀이 과정을 살펴보자. 먼저 초기값을 설정한다. 그 다음 해당 점을 지나는 접선의 방정식을 세운다. 이제 접선의 방정식의 $x$절편을 구하고 이것을 다음 초기값으로 사용한다. 이제 $f(x_n) \approx 0$이 될 때까지 위 과정을 지속적으로 반복하면 된다. 아래 그래프와 함께 다시 살펴보자.</p>

<p align="center">
<img src="/assets/images/optimization/zero_find.png" alt="Newton-Raphson for Zero Find" class="align-center image-caption" width="60%&quot;, height=&quot;50%" />
<strong><em><a href="">Newton-Raphson for Zero Find</a></em></strong>
</p>

<p>초기값은 $x_0=3$이다. 시작점 $(x_0, f(x_0))$을 지나는 접선의 방정식을 세우고 해당 방정식의 $x$절편을 구하는 수식을 작성하면 아래와 같다.</p>

\[f'(x_0)(x-x_n) + f(x_0) = 0\]

<p>이제 이것을 예쁘게 잘 정리해서 다음 초기값 $x_1$을 구해보자.</p>

\[x = x_0 - \frac{f(x_0)}{f'(x_0)}\]

<p>이번 포스트 맨 처음에 봤던 뉴턴-랩슨 방법의 수식과 똑같다는 것을 알 수 있다. 다시 말해 뉴턴-랩슨의 Zero-Find 버전은 접선의 방정식의 $x$절편을 활용해 목적 함수의 해를 찾아가는 방식인 것이다.</p>

<p>지금까지 근사해를 찾아주는 뉴턴-랩슨 메서드를 살펴보았다. 하지만 머신러닝처럼 현실의 최적화 문제를 풀어야 하는 우리 입장에서는 단순 목적 함수의 근을 찾는 것만으로는 주어진 문제를 해결할 수 없다. 머신러닝의 최적화 대상인 비용 함수는 거의 모든 경우에 근이 없기(베이지안 오차까지 고려하면 사실상 불가능) 때문에 일단 알고리즘의 가정 자체가 성립하지 않는다. 이러한 한계점을 극복하고자 최적화 버전의 뉴턴-랩슨 메서드가 등장하게 된다.</p>

<h3 id="optimization-ver"><code class="language-plaintext highlighter-rouge">📉 Optimization Ver</code></h3>

\[x_{n+1}:= x_n - \frac{f'(x_n)}{f''(x_n)}\]

<p>최적화 버전의 뉴턴 랩슨 메서드는 이계도함수를 사용한다. 원함수(비용함수)가 근이 없을지라도, 함수의 극점이 존재하는한 도함수의 근은 항상 존재한다는 가정에서 출발한다. 근을 찾는 행위는 동일하게 하되, 이번에는 원함수의 근이 아니라 도함수의 근을 찾는다. 도함수의 근사해를 찾으면, 해당 위치는 국소/전역 최적값에 근접한 수치일 것이라고 기대해볼 수 있다.</p>

<p>하지만 최적화 버전의 뉴턴 랩슨 메서드 역시 여전히 많은 단점을 갖고 있다. 일단 먼저 계산량이 지나치게 많아진다. 예시를 모두 스칼라 형태로 들어서 간단해 보이지만, 다변수함수에 적용하면 과정이 매우 매우 복잡해진다. 모든 <code class="language-plaintext highlighter-rouge">iteration</code> 마다 자코비안, 헤시안 행렬을 구해줘야 한다. 도함수만 이용하는 경사 하강에 비해 연산 부담이 상당히 커질 수 밖에 없는 것이다. 그리고 결정적으로 헤시안 행렬이 <code class="language-plaintext highlighter-rouge">invertible</code> 해야한다. 이게 개인적으로 <code class="language-plaintext highlighter-rouge">뉴턴-랩슨</code> 방식의 가장 큰 단점이라고 생각한다. 헤시안 행렬의 역행렬이 존재하려면 반드시 원함수는 <code class="language-plaintext highlighter-rouge">Convex Function</code>이어야 하기 때문이다. 따라서 상당히 비현실적인 풀이 방식이라고 볼 수 있다.</p>

<p>한편, 위 모든 제약 조건을 만족한다면 최적화 버전의 뉴턴-랩슨 방식은 경사하강에 비해 상당히 빠른 수렴 속도를 갖는데 그 이유를 간단히 살펴보자. 결과부터 설명하면 뉴턴-랩슨 방식이 사실상 <code class="language-plaintext highlighter-rouge">Least Square Method(최소 자승법)</code> 와 동치라서 그렇다. 목적함수 $f(x)$를 <code class="language-plaintext highlighter-rouge">MSE</code> 로 두고 선형 회귀 문제를 푸는 상황을 가정해보자.</p>

\[Z = Ax+n \\
f(x) = (Z-Ax)^T(Z-Ax)\]

<p>목적함수를 정의했기 때문에 우리는 이제 목적함수의 도함수와 이계도함수를 구할 수 있다.</p>

\[f'(x) = -2A^T(Z-Ax)\\
f''(x) = 2A^tA\]

<p>도함수와 이계도 함수를 뉴턴—랩슨 수식에 대입하면 다음과 같다.</p>

\[x_{n+1} := x_n + \frac{A^T(Z-Ax)} {A^TA} = x_n + (A^TA)^{-1}A^T(Z-Ax)\]

<p>분모는 헤시안 행렬과 동치다. 행렬로 어떤 수를 나눌 수는 없기 때문에 나눗셈 표현 대신 역행렬로 표기했다. 그리고 수식이 상당히 더럽기 때문에 정리를 위해 전개를 해보려 한다. 전개 결과는 다음과 같다.</p>

\[x_{n+1} := x_n + (A^TA)^{-1}A^TZ - (A^TA)^{-1}A^TAx_n = (A^TA)^{-1}A^TZ\]

<p>헤시안 행렬이 <code class="language-plaintext highlighter-rouge">invertible</code> 해야한다라는 제약 조건이 여기서 등장한다. 만약 헤시안 행렬이 <code class="language-plaintext highlighter-rouge">invertible</code> 이라면, 다 날라가고 우변의 항만 남게 된다. 우변의 항을 자세히 살펴보면, <code class="language-plaintext highlighter-rouge">Least Square Method(최소 자승법)</code> 의 수식과 동일하다는 것을 알 수 있다. 경사하강과는 다르게 $x_n$과 관련된 항이 수식에 전혀 남아있지 않기 때문에, 최소 자승법 수식을 한 번 풀어내는 것만으로 극점에 도달하여 수렴속도가 훨씬 빠르게 되는 것이다.</p>

<p align="center">
<img src="/assets/images/optimization/gd_nr_1.png" alt="Newton-Raphson for Optimization" class="align-center image-caption" width="60%&quot;, height=&quot;50%" />
<strong><em><a href="">Newton-Raphson for Optimization</a></em></strong>
</p>

<p>두 방식이 최적화 문제를 풀어나가는 과정을 비교하기 위해 시각화를 시도해봤다. 필자의 시각화 실력이 매우 좋지 못해 그 차이가 직관적으로 잘 안보인다… 빨간 직선은 뉴턴-랩슨 방식이고 파란 직선은 경사 하강 방법이다. 전자는 위에서 살펴본 것처럼 한번에 극소점으로 이동하는 것을 볼 수 있다. 한편 후자는 수많은 <code class="language-plaintext highlighter-rouge">Iteration</code> 을 거쳐 극소점에 도달한다. 필자의 시각화 자료가 상당히 좋지 못하다고 생각해 하단에 <a href="https://www.youtube.com/watch?v=MlZoafOnMS0&amp;list=PL_iJu012NOxeMJ5TPPW1JZKec7rhjKXUy&amp;index=6&amp;ab_channel=%ED%98%81%ED%8E%9C%ED%95%98%EC%9E%84%7CAI%26%EB%94%A5%EB%9F%AC%EB%8B%9D%EA%B0%95%EC%9D%98">혁펜하임</a>님의 자료도 함께 첨부했으니 참고하자. 훨씬 직관적으로 잘 보인다.</p>

<p align="center">
<img src="/assets/images/optimization/gradient_vs_newton.png" alt="Newton-Raphson vs Gradient-Descent" class="align-center image-caption" width="60%&quot;, height=&quot;50%" />
<strong><em><a href="https://ibb.co/VjvkYL7">Newton-Raphson vs Gradient-Descent</a></em></strong>
</p>]]></content><author><name>qcqced</name><email>qcqced123@gmail.com</email></author><category term="Optimization Theory" /><category term="Optimization Theory" /><category term="Newton-Raphson" /><summary type="html"><![CDATA[최적화 문제를 위한 뉴턴-랩슨 메서드 설명]]></summary></entry><entry><title type="html">🗂️ Convex Optimization Problem</title><link href="http://localhost:4000/optimization-theory/convex" rel="alternate" type="text/html" title="🗂️ Convex Optimization Problem" /><published>2023-11-13T00:00:00+09:00</published><updated>2023-11-14T02:00:00+09:00</updated><id>http://localhost:4000/optimization-theory/convex_problem</id><content type="html" xml:base="http://localhost:4000/optimization-theory/convex"><![CDATA[<h3 id="convex-optimization-problem"><code class="language-plaintext highlighter-rouge">❓ Convex Optimization Problem</code></h3>

\[f(wx_1 + (1-w)x_2)≤ wf(x_1) + (1-w)f(x_2),\ \ w \in [0,1] \\
f''(x) ≥ 0\]

<p><code class="language-plaintext highlighter-rouge">Convex Problem</code> 이란, 목적 함수 $f(x)$가 <code class="language-plaintext highlighter-rouge">Convex Function</code> 이면서 <code class="language-plaintext highlighter-rouge">Feasible Set</code> 역시 <code class="language-plaintext highlighter-rouge">Convex Set</code> 이 되는 문제 상황을 일컫는다. <code class="language-plaintext highlighter-rouge">Convex Problem</code> 는 수학적 최적화에서 매우 중요한 개념인데, 그 이유는 해당 조건을 만족하면 <code class="language-plaintext highlighter-rouge">국소 최적해</code>가 <code class="language-plaintext highlighter-rouge">전역 최적해</code>와 동치가 되어 최적화 난이도가 급격히 낮아지기 때문이다. 또한 <code class="language-plaintext highlighter-rouge">Convex Problem</code>을 해결해 국소 최적해를 구하는 알고리즘은 이미 많이 개발 되어 있기 때문에 주어진 최적화 문제를 <code class="language-plaintext highlighter-rouge">Convex Problem</code>으로 치환해 해결하는게 가장 효율적이다. 한편, 여기서 <code class="language-plaintext highlighter-rouge">Feasible Set</code> 이란, 함수의 <code class="language-plaintext highlighter-rouge">실행 가능 영역•정의역</code>이라고 생각하면 된다. 아래 그림, 빨간 직선의 영역에 해당한다. 세트라는 명칭은 무한한 직선이 아닌 유한한 선분을 표현하는 용어라고 생각하면 된다.</p>

<p>위에 서술한 두개의 수식은 어떤 문제 상황이 <code class="language-plaintext highlighter-rouge">Convex Problem</code> 인지 아닌지 구분해주는 판별식의 역할을 한다. 왜 두 수식을 만족하면 <code class="language-plaintext highlighter-rouge">Convex Problem</code>이 되는지 살펴보고 마지막에는 <code class="language-plaintext highlighter-rouge">Convex Problem</code> 에서 왜 <code class="language-plaintext highlighter-rouge">국소 최적해</code>가 <code class="language-plaintext highlighter-rouge">전역 최적해</code>와 동치가 되는지 그 증명을 해보려 한다.</p>

<p align="center">
<img src="/assets/images/optimization/convex_function.png" alt="Convex Function" class="align-center image-caption" width="60%&quot;, height=&quot;50%" />
<strong><em><a href="">Convex Function</a></em></strong>
</p>

<h3 id="-jensens-inequality"><code class="language-plaintext highlighter-rouge">﹤ Jensen’s Inequality</code></h3>

<p>첫번째 수식을 보자. 우리는 이것을 <code class="language-plaintext highlighter-rouge">얀센 부등식</code>이라고 부른다. <code class="language-plaintext highlighter-rouge">얀센 부등식</code> 은 어떤 함수 $f(x)$의 <code class="language-plaintext highlighter-rouge">Convex Function</code> 여부를 판정 하는데 사용된다. 좌변은 <code class="language-plaintext highlighter-rouge">Feasible Set</code> 에 해당되는 데이터 포인트의 함수값을 의미하며 그림 상에서 초록색 곡선으로 표현된다. 한편, 우변은 <code class="language-plaintext highlighter-rouge">Feasible Set</code>의 평균변화율을 기울기로 하면서 구간 양쪽 끝을 지나는 선분을 일컫는다. 그림에서 파란색 직선이 바로 부등식의 우변이다.</p>

<p>그렇다면 위 부등식을 항상 만족하려면 함수 $f(x)$는 어떤 형태를 가져야 할까?? 먼저 오목 함수인 <code class="language-plaintext highlighter-rouge">Concave Function</code>부터 생각해보자. 위 그림을 뒤집어서 생각해보면 되는데, 함수가 정의되는 전체 정의역에서 위 부등식을 만족하는 구간(파란색 직선이 초록색 곡선보다 위에 있는)을 전혀 찾아볼 수 없다. 따라서 얀센 부등식을 만족하려면 <code class="language-plaintext highlighter-rouge">Feasible Set</code> 의 구간이 반드시 <code class="language-plaintext highlighter-rouge">Convex Set</code> 이어야 하고, 해당 구간에서 목적함수는 반드시 <code class="language-plaintext highlighter-rouge">Convex Function</code>의 형태를 띄고 있어야 한다.</p>

<p>그렇다면 목적함수가 <code class="language-plaintext highlighter-rouge">Convex</code>인지는 어떻게 판별할 수 있을까?? 주어진 모든 상황에서 위 그림처럼 쉽게 함수의 그래프를 그릴 수는 없을 것이다. 그래서 수식으로 어떤 목적함수가 <code class="language-plaintext highlighter-rouge">Convex</code>인지 판별할 수 있어야 한다. 드디어 제시된 두번째 수식을 활용할 차례다.</p>

<h3 id="second-derivative"><code class="language-plaintext highlighter-rouge">📈 Second Derivative</code></h3>

<p>두번째 수식을 흔히 이계도함수라고 부른다. 아마 수능 수학에서 21, 30번과 같은 킬러 문제를 풀 때 가끔씩 사용하던 기억이 날 것이다. 이계도함수는 도함수를 한 번 더 미분한 것으로 원함수의 곡선이 얼마나 빠르게 변하는지 혹은 곡선의 곡률에 대한 정보를 알려준다. 이를 통해 원함수의 극대, 극소는 물론 변곡점의 위치를 알아낼 수 있다. 그래서 어떤 이차함수를 예시로 들어보자. 2차항의 부호가 양수라면 이계도함수의 값은 항상 양수가 될 것이고, 음수라면 항상 음수가 될 것이다. 그런데 우리는 이미 직관적으로 2차함수에서 최고차항의 부호가 양수면 아래로 볼록한 함수, 반대의 경우 위로 오목한 오목함수가 된다는 것을 알고 있다. 따라서 이계도 함수의 값이 항상 양수라면 해당 함수는 <code class="language-plaintext highlighter-rouge">Convex Function</code>이 된다.</p>

<p>지금까지는 단변수 함수에 대한 케이스만 살펴보았다. 그렇다면 이것을 다변수로 확장할 수는 없을까?? 물론 가능하다. 어떤 다변수 함수가 <code class="language-plaintext highlighter-rouge">Convex</code>인지 판정하는 것도 위와 동일한 조건을 통해 판별한다. 이 때 등장하는게 바로 헤시안 행렬이다. 헤시안 행렬이란, 어떤 다변수 함수의 이계도함수값을 행렬로 나타낸 것이다. 단변수 함수의 이계도함수 역할과 동일하다. 그래서 어떤 다변수 함수가 컨백스 함수이려면, 헤시안 행렬이 <code class="language-plaintext highlighter-rouge">Positive Semi-Define</code> 조건을 만족해야 한다. 헤시안 행렬과 <code class="language-plaintext highlighter-rouge">Positive Semi-Define</code>에 대해서는 다른 포스트에서 자세히 다루도록 하겠다.</p>

<h3 id="-proof-local-minimum-same-as-global-minimum-in-convex"><code class="language-plaintext highlighter-rouge">🪪 Proof: local minimum same as global minimum in Convex</code></h3>

<p>이제 드디어 목적함수가 컨백스 함수일 때, 국소 최적해가 전역 최적해와 동치가 되는지를 증명해보려 한다. 먼저 증명은 귀류법을 사용한다. 귀류법이란, 수학, 논리학, 철학 등에서 주장이 부정됨을 보이기 위해 모순 또는 부정된 가정을 유도하는 논증 기법으로, 특정 주장이 참임을 증명하기보다는 그 반대인 부정된 주장이 거짓임을 보이는 데 사용한다. 그래서 귀류법을 활용해 다음과 같은 명제가 거짓임을 증명해보려 한다.</p>

<p>어떤 함수 $f(x)$는 Convex면서 Feasible Set이 Convex Set임을 만족하는 동시에 아래 수식을 만족한다고 한다.</p>

\[f(x^@) &lt; f(x^!)\]

<p>부등식의 우변은 국소 최적해를 의미하고, 좌변은 국소 최적해보다 작은 함수값이 존재한다고 가정한 것이다. 이제 이 명제가 틀림을 증명하면 우리는 자연스럽게 국소 최적해가 전역 최적해와 동치가 된다는 것을 확인할 수 있게 된다.</p>

<p>우리는 함수 $f(x)$가 <code class="language-plaintext highlighter-rouge">Convex</code>면서 <code class="language-plaintext highlighter-rouge">Feasible Set</code>이 <code class="language-plaintext highlighter-rouge">Convex Set</code>이라고 가정했기 때문에, $\alpha x^! + (1-\alpha)x^@$ 역시 <code class="language-plaintext highlighter-rouge">Feasible Set</code>이 될 것이다. 이러한 사실을 활용해 두 점($x^!, x^@$)을 얀센 부등식에 넣어보자.</p>

\[f(wx^! + (1-w)x^@)≤ wf(x^!) + f(x^!) - f(x^!) + (1-w)f(x^@) \\
f(wx^! + (1-w)x^@)≤ f(x^!) +(1-w)(f(x^@) -f(x^!))\]

<p>우변부터 함께 살펴보자. $(1-w)$는 반드시 양수가 된다. 얀센 부등식 정의상 $w \in [0,1]$을 만족하기 때문이다. 한편, $f(x^@) -f(x^!)$는 음수가 된다. 증명을 시작하면서 $f(x^@)$가 $f(x^!)$보다 작다고 가정했기 때문이다. 따라서 우변은 $f(x^!)$보다 아주 조금 작은 값(w→1)이 될 것이다. 이제 좌변을 살펴보자. 마찬가지로 w→1을 해주면 좌변은 $f(x^!)$의 매우 근접한 위치의 함수값을 보자는 의미가 된다. 좌변을 $\alpha$, 우변을 $\beta$로 치환해 지금까지 증명 과정을 다시 부등식으로 표현하면 다음과 같다.</p>

\[\alpha ≤ \beta &lt; f(x^!)\]

<p>우리는 $f(x^!)$가 지역 최소값이라고 정의한 바 있다. 지역 최소값이라는 것은 그 주변에서 가장 작은값이라는 의미를 갖는다. 그런데 $f(x^!)$의 주변값인 $\alpha$가 지역 최소값보다 작다고 부등식은 말하고 있기 때문에, 지역 최소값 정의에 위배되어 위 명제는 거짓이 된다.</p>

<p>정리하자면, 어떤 함수 $f(x)$가 컨백스 성질을 만족하면서, Feasible Set 역시 Convex Set인 경우에 국소 최소값이 전역 최소값이 아닐 경우, 국소 최적이 국소 최적이 아닌 경우가 발생하기 떄문에 반드시 위 조건에서 국소 최적값은 전역 최적값이 되어야 한다.</p>]]></content><author><name>qcqced</name><email>qcqced123@gmail.com</email></author><category term="Optimization Theory" /><category term="Optimization Theory" /><category term="Convex Optimization" /><summary type="html"><![CDATA[컨백스 최적화 문제 증명]]></summary></entry></feed>