<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2023-07-19T10:59:07+09:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">AI/Business Study Log</title><subtitle>NLP, Marketing</subtitle><author><name>qcqced</name><email>qcqced123@gmail.com</email></author><entry><title type="html">ğŸ² RuntimeError: CUDA error: device-side assert triggered</title><link href="http://localhost:4000/framework-library/mismatch-dimension" rel="alternate" type="text/html" title="ğŸ² RuntimeError: CUDA error: device-side assert triggered" /><published>2023-07-17T00:00:00+09:00</published><updated>2023-07-18T07:00:00+09:00</updated><id>http://localhost:4000/framework-library/dim_mismatch</id><content type="html" xml:base="http://localhost:4000/framework-library/mismatch-dimension"><![CDATA[<h3 id="-ì‚¬ì „ì—-ì •ì˜-ì…ì¶œë ¥-ì°¨ì›--ì‹¤ì œ-ì…ì¶œë ¥-ì°¨ì›"><code class="language-plaintext highlighter-rouge">ğŸ˜µ ì‚¬ì „ì— ì •ì˜ ì…ì¶œë ¥ ì°¨ì› â‰  ì‹¤ì œ ì…ì¶œë ¥ ì°¨ì›</code></h3>

<p>ë‹¤ì–‘í•œ ì›ì¸ì´ ìˆë‹¤ê³  ì•Œë ¤ì ¸ ìˆëŠ” ì—ëŸ¬ì§€ë§Œ, í•„ìì˜ ê²½ìš° ìœ„ ì—ëŸ¬ëŠ” ì‚¬ì „ì— ì •ì˜í•œ ë°ì´í„°ì˜ ì…ì¶œë ¥ ì°¨ì›ê³¼ ì‹¤ì œ ì…ì¶œë ¥ ë°ì´í„° ì°¨ì›ì´ ì„œë¡œ ìƒì´í•  ë•Œ ë°œìƒí–ˆë‹¤. í•˜ì§€ë§Œ ì›ì¸ì„ í™•ì‹¤íˆ íŠ¹ì •í•˜ê³  ì‹¶ë‹¤ë©´ ì•„ë˜ ì˜ˆì‹œ ì½”ë“œë¥¼ ë¨¼ì € ì¶”ê°€í•œ ë’¤, ë‹¤ì‹œ í•œ ë²ˆ ì—ëŸ¬ ë¡œê·¸ë¥¼ í™•ì¸í•´ë³´ê¸¸ ê¶Œì¥í•œë‹¤.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">os</span>
<span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="s">'CUDA_LAUNCH_BLOCKING'</span><span class="p">]</span> <span class="o">=</span> <span class="s">"1"</span>
<span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="s">"CUDA_VISIBLE_DEVICES"</span><span class="p">]</span> <span class="o">=</span> <span class="s">"0"</span>
</code></pre></div></div>
<p>ì˜ˆì‹œ ì½”ë“œì²˜ëŸ¼ í™˜ê²½ë³€ìˆ˜ë¥¼ ì¶”ê°€í•˜ë©´ ì—ëŸ¬ê°€ ì–´ëŠ ë¶€ë¶„ì—ì„œ ë°œìƒí–ˆëŠ”ì§€ ë¡œê·¸ê°€ ì¢€ ë” êµ¬ì²´ì ìœ¼ë¡œ ë‚˜ì˜¨ë‹¤. ê±°ì˜ ëŒ€ë¶€ë¶„ì´ ì…ì¶œë ¥ ì°¨ì› ë¬¸ì œì¼í…Œë‹ˆ ê·€ì°®ìœ¼ë©´ ë°”ë¡œ ì°¨ì›ì„ ìˆ˜ì •í•˜ë„ë¡ í•˜ì.</p>]]></content><author><name>qcqced</name><email>qcqced123@gmail.com</email></author><category term="Framework &amp; Library" /><category term="Pytorch" /><category term="Dimension Mismatch" /><category term="CUDA" /><category term="Error Handling" /><summary type="html"><![CDATA[Pytorch Error: Mis-match between pre-defined dimension and input dimension]]></summary></entry><entry><title type="html">ğŸ² RuntimeError: CUDA error: CUBLAS_STATUS_ALLOC_FAILED when callingÂ cublasCreate(handâ‰¤)</title><link href="http://localhost:4000/framework-library/mismatch-embedding" rel="alternate" type="text/html" title="ğŸ² RuntimeError: CUDA error: CUBLAS_STATUS_ALLOC_FAILED when callingÂ cublasCreate(handâ‰¤)" /><published>2023-07-17T00:00:00+09:00</published><updated>2023-07-18T02:00:00+09:00</updated><id>http://localhost:4000/framework-library/embedding_mismatch</id><content type="html" xml:base="http://localhost:4000/framework-library/mismatch-embedding"><![CDATA[<h3 id="-nnembedding-ì°¨ì›--ì‹¤ì œ-ë°ì´í„°-ì…ë ¥-ì°¨ì›"><code class="language-plaintext highlighter-rouge">ğŸ˜µ nn.Embedding ì°¨ì› â‰  ì‹¤ì œ ë°ì´í„° ì…ë ¥ ì°¨ì›</code></h3>
<p><code class="language-plaintext highlighter-rouge">torch.nn.Embedding</code>ì—ì„œ ì •ì˜í•œ ì…ì¶œë ¥ ì°¨ì›ê³¼ ì‹¤ì œ ë°ì´í„°ì˜ ì°¨ì›ì´ ë‹¤ë¥¸ ê²½ìš°ì— ë°œìƒí•˜ëŠ” ì—ëŸ¬ë‹¤. ë‹¤ì–‘í•œ ìƒí™©ì—ì„œ ë§ˆì£¼í•  ìˆ˜ ìˆëŠ” ì—ëŸ¬ì§€ë§Œ, í•„ìì˜ ê²½ìš° <code class="language-plaintext highlighter-rouge">Huggingface</code>ì—ì„œ ë¶ˆëŸ¬ì˜¨<code class="language-plaintext highlighter-rouge">pretrained tokenizer</code>ì— <code class="language-plaintext highlighter-rouge">special token</code> ì„ ì¶”ê°€í•´ ì‚¬ìš©í•  ë•Œ, í† í°ì„ ì¶”ê°€í–ˆë‹¤ëŠ” ì‚¬ì‹¤ì„ ìŠê³  <code class="language-plaintext highlighter-rouge">nn.Embedding</code> ì— ì •ì˜í•œ ì…ì¶œë ¥ ì°¨ì›ì„ ë³€ê²½í•˜ì§€ ì•Šì•„ì„œ ë°œìƒí•˜ëŠ” ê²½ìš°ê°€ ë§ì•˜ë‹¤.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoConfig</span><span class="p">,</span> <span class="n">AutoModel</span>

<span class="k">class</span> <span class="nc">CFG</span><span class="p">:</span>
    <span class="n">model_name</span> <span class="o">=</span> <span class="s">'microsoft/deberta-v3-large'</span>
    <span class="n">config</span> <span class="o">=</span> <span class="n">AutoConfig</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>
    <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">AutoModel</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">add_markdown_token</span><span class="p">(</span><span class="n">cfg</span><span class="p">:</span> <span class="n">sCFG</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
    <span class="s">"""
    Add MarkDown token to pretrained tokenizer ('[MD]')
    Args:
        cfg: CFG, needed to load tokenizer from Huggingface AutoTokenizer
    """</span>
    <span class="n">markdown_token</span> <span class="o">=</span> <span class="s">'[MD]'</span>
    <span class="n">special_tokens_dict</span> <span class="o">=</span> <span class="p">{</span><span class="s">'additional_special_tokens'</span><span class="p">:</span> <span class="p">[</span><span class="sa">f</span><span class="s">'</span><span class="si">{</span><span class="n">markdown_token</span><span class="si">}</span><span class="s">'</span><span class="p">]}</span>
    <span class="n">cfg</span><span class="p">.</span><span class="n">tokenizer</span><span class="p">.</span><span class="n">add_special_tokens</span><span class="p">(</span><span class="n">special_tokens_dict</span><span class="p">)</span>
    <span class="n">markdown_token_id</span> <span class="o">=</span> <span class="n">cfg</span><span class="p">.</span><span class="n">tokenizer</span><span class="p">(</span><span class="sa">f</span><span class="s">'</span><span class="si">{</span><span class="n">markdown_token</span><span class="si">}</span><span class="s">'</span><span class="p">,</span> <span class="n">add_special_tokens</span><span class="o">=</span><span class="bp">False</span><span class="p">)[</span><span class="s">'input_ids'</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>

    <span class="nb">setattr</span><span class="p">(</span><span class="n">cfg</span><span class="p">.</span><span class="n">tokenizer</span><span class="p">,</span> <span class="s">'markdown_token'</span><span class="p">,</span> <span class="sa">f</span><span class="s">'</span><span class="si">{</span><span class="n">markdown_token</span><span class="si">}</span><span class="s">'</span><span class="p">)</span>
    <span class="nb">setattr</span><span class="p">(</span><span class="n">cfg</span><span class="p">.</span><span class="n">tokenizer</span><span class="p">,</span> <span class="s">'markdown_token_id'</span><span class="p">,</span> <span class="n">markdown_token_id</span><span class="p">)</span>
    <span class="n">cfg</span><span class="p">.</span><span class="n">tokenizer</span><span class="p">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="sa">f</span><span class="s">'</span><span class="si">{</span><span class="n">cfg</span><span class="p">.</span><span class="n">checkpoint_dir</span><span class="si">}</span><span class="s">/tokenizer/'</span><span class="p">)</span>


<span class="n">add_markdown_token</span><span class="p">(</span><span class="n">CFG</span><span class="p">)</span>
<span class="n">CFG</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">resize_token_embeddings</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">))</span>
</code></pre></div></div>
<p>êµ¬ê¸€ë§í•´ë³´ë‹ˆ í•´ê²°í•˜ëŠ” ë°©ë²•ì€ ë‹¤ì–‘í•œ ê²ƒ ê°™ì€ë°, <code class="language-plaintext highlighter-rouge">torch.nn.Embedding</code>ì— ì •ì˜ëœ ì…ì¶œë ¥ ì°¨ì›ì„ ì‹¤ì œ ë°ì´í„° ì°¨ì›ê³¼ ë§ì¶°ì£¼ë©´ ê°„ë‹¨í•˜ê²Œ í•´ê²°ëœë‹¤. í•„ìì²˜ëŸ¼ <code class="language-plaintext highlighter-rouge">special token</code> ì„ ì¶”ê°€í•´ ì‚¬ìš©í•˜ë‹¤ í•´ë‹¹ ì—ëŸ¬ê°€ ë°œìƒí•˜ëŠ” ìƒí™©ì´ë¼ë©´ ìƒˆë¡œìš´ í† í°ì´ ì¶”ê°€ëœ í† í¬ë‚˜ì´ì €ì˜ ê¸¸ì´ë¥¼ ë‹¤ì‹œ ì¸¡ì •í•œ ë’¤ ê°’ì„ <code class="language-plaintext highlighter-rouge">resize_token_embeddings</code> ë©”ì„œë“œì— ì „ë‹¬í•´ <code class="language-plaintext highlighter-rouge">nn.Embedding</code>ì„ ì—…ë°ì´íŠ¸ í•´ì£¼ë©´ ëœë‹¤.</p>]]></content><author><name>qcqced</name><email>qcqced123@gmail.com</email></author><category term="Framework &amp; Library" /><category term="Pytorch" /><category term="Dimension Mismatch" /><category term="nn.Embedding" /><category term="CUDA" /><category term="Error Handling" /><summary type="html"><![CDATA[Pytorch Error: Mis-match between pre-defined dimension and input dimension]]></summary></entry><entry><title type="html">ğŸª¢ assert len(optimizer_state[â€œfound_inf_per_deviceâ€]) &amp;gt; 0, â€œNo inf checks were recorded for this optimizer.â€ AssertionError: No inf checks were recorded for this optimizer.</title><link href="http://localhost:4000/framework-library/inf-per-device" rel="alternate" type="text/html" title="ğŸª¢ assert len(optimizer_state[â€œfound_inf_per_deviceâ€]) &amp;gt; 0, â€œNo inf checks were recorded for this optimizer.â€ AssertionError: No inf checks were recorded for this optimizer." /><published>2023-07-11T00:00:00+09:00</published><updated>2023-07-11T13:00:00+09:00</updated><id>http://localhost:4000/framework-library/found_inf_per_device</id><content type="html" xml:base="http://localhost:4000/framework-library/inf-per-device"><![CDATA[<h3 id="-optimizerê°€-ì†ì‹¤ê°’ì„-ì œëŒ€ë¡œ-backward-í• -ìˆ˜-ì—†ëŠ”-ë¬¸ì œ"><code class="language-plaintext highlighter-rouge">ğŸ¤” Optimizerê°€ ì†ì‹¤ê°’ì„ ì œëŒ€ë¡œ Backward í•  ìˆ˜ ì—†ëŠ” ë¬¸ì œ</code></h3>

<p>í…ì„œì˜ ê³„ì‚° ê·¸ë˜í”„ê°€ ì¤‘ê°„ì— ëŠì–´ì ¸ ì˜µí‹°ë§ˆì´ì €ê°€ ê·¸ë¼ë””ì–¸íŠ¸ë¥¼ ì œëŒ€ë¡œ <code class="language-plaintext highlighter-rouge">Backward</code> í•˜ì§€ ëª»í•´ ë°œìƒí•˜ëŠ” ì—ëŸ¬ë‹¤. ê³µë¶€ë¥¼ ì‹œì‘í•˜ê³  ì •ë§ ì²˜ìŒ ë§ˆì£¼í•˜ëŠ” ì—ëŸ¬ë¼ì„œ ì •ë§ ë§ì´ ë‹¹í™©í–ˆë‹¤. ë˜í¼ëŸ°ìŠ¤ ìë£Œ ì—­ì‹œ ê±°ì˜ ì—†ì–´ì„œ í•´ê²°í•˜ëŠ”ë° ì• ë¥¼ ë¨¹ì—ˆë˜  ì“°ë¼ë¦° ì‚¬ì—°ì´ ìˆëŠ” ì—ëŸ¬ë‹¤. ì´ ê¸€ì„ ì½ëŠ” ë…ìë¼ë©´ ëŒ€ë¶€ë¶„ í…ì„œì˜ ê³„ì‚° ê·¸ë˜í”„ê°€ ì¤‘ê°„ì— ëŠì–´ì§„ë‹¤ëŠ” ê²ƒì´ ë¬´ìŠ¨ ì˜ë¯¸ì¼ì§€ ì´í•´í•˜ì‹œì§€ ëª»í• ê±°ë¼ ìƒê°í•œë‹¤. ê·¸ê²Œ ì •ìƒì´ë‹¤. í•„ì ì—­ì‹œ ì•Œê³  ì‹¶ì§€ ì•Šì•˜ìœ¼ë‚˜ ìš•ì‹¬ë§Œ ë§ê³  ë©ì²­í•œ íƒ“ì—â€¦ ì•Œê²Œ ë˜ì—ˆë‹¤. ì•„ë˜ ì˜ˆì‹œ ì½”ë“œë¥¼ ë¨¼ì € ì‚´í´ë³´ì.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Before Append
</span><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">:</span> <span class="nb">dict</span><span class="p">,</span> <span class="n">position_list</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">feature</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
        <span class="n">feature</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">.</span><span class="n">last_hidden_state</span>
        <span class="n">pred</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">batch_size</span><span class="p">):</span>
            <span class="s">""" Apply Pooling &amp; Fully Connected Layer for each unique cell in batch (one notebook_id) """</span>
            <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">position_list</span><span class="p">[</span><span class="n">i</span><span class="p">])):</span>
                <span class="n">src</span><span class="p">,</span> <span class="n">end</span> <span class="o">=</span> <span class="n">position_list</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">idx</span><span class="p">]</span>
                <span class="n">embedding</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">pooling</span><span class="p">(</span><span class="n">feature</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">src</span><span class="p">:</span><span class="n">end</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="p">:].</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>  <span class="c1"># maybe don't need mask
</span>                <span class="n">logit</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">fc</span><span class="p">(</span><span class="n">embedding</span><span class="p">)</span>
                <span class="n">pred</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">logit</span><span class="p">)</span>  
            <span class="n">pred</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">pred</span>

<span class="c1"># After Append
</span><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">:</span> <span class="nb">dict</span><span class="p">,</span> <span class="n">position_list</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">feature</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
        <span class="n">feature</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">.</span><span class="n">last_hidden_state</span>
        <span class="n">pred</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([],</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">batch_size</span><span class="p">):</span>
            <span class="s">""" Apply Pooling &amp; Fully Connected Layer for each unique cell in batch (one notebook_id) """</span>
            <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">position_list</span><span class="p">[</span><span class="n">i</span><span class="p">])):</span>
                <span class="n">src</span><span class="p">,</span> <span class="n">end</span> <span class="o">=</span> <span class="n">position_list</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">idx</span><span class="p">]</span>
                <span class="n">embedding</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">pooling</span><span class="p">(</span><span class="n">feature</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">src</span><span class="p">:</span><span class="n">end</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="p">:].</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>  <span class="c1"># maybe don't need mask
</span>                <span class="n">logit</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">fc</span><span class="p">(</span><span class="n">embedding</span><span class="p">)</span>
                <span class="n">pred</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cat</span><span class="p">([</span><span class="n">pred</span><span class="p">,</span> <span class="n">logit</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">pred</span>
</code></pre></div></div>

<p>ë‹¤ìŒ ì½”ë“œëŠ” í•„ìê°€ ê³µë¶€ë¥¼ ìœ„í•´ ë§Œë“  ëª¨ë¸ í´ë˜ìŠ¤ì˜ <code class="language-plaintext highlighter-rouge">forward</code> ë©”ì„œë“œì´ë‹¤. ì „ìëŠ” ì´ë²ˆ í¬ìŠ¤íŒ…ì˜ ì£¼ì œì¸ ì—ëŸ¬ë¥¼ ì¼ìœ¼í‚¨ ì£¼ì¸ê³µì´ê³ , í›„ìëŠ” ì—ëŸ¬ë¥¼ ìˆ˜ì •í•œ ì´í›„ ì •ìƒì ìœ¼ë¡œ ì‘ë™í•˜ëŠ” ì½”ë“œë‹¤. ë…ì ì—¬ëŸ¬ë¶„ë“¤ë„ ë‘ ì½”ë“œì— ì–´ë–¤ ì°¨ì´ê°€ ìˆëŠ”ì§€ ìŠ¤ìŠ¤ë¡œ ì§ˆë¬¸ì„ ë˜ì§€ë©´ì„œ ì½ì–´ì£¼ì‹œê¸¸ ë°”ë€ë‹¤.</p>
<p align="center">
<img src="/assets/images/marginrankingloss.png" alt="Model Overview" class="align-center image-caption" width="100%&quot;, height=&quot;50%" />
<em>Modeling Overview</em>
</p>

<p>ìœ„ì˜ ì½”ë“œë“¤ì€ <code class="language-plaintext highlighter-rouge">DeBERTa-V3-Large</code> ì˜ ë§ˆì§€ë§‰ ì¸ì½”ë” ë ˆì´ì–´ê°€ ë°˜í™˜í•˜ëŠ” <code class="language-plaintext highlighter-rouge">last_hidden_state</code> ë¥¼ ë¯¸ë¦¬ ì„¤ì •í•œ ì„œë¸Œ êµ¬ê°„ë³„ë¡œ ë‚˜ëˆ„ê³  ê°œë³„ì ìœ¼ë¡œ <code class="language-plaintext highlighter-rouge">pooling &amp; fully connected layer</code> ì— í†µê³¼ì‹œì¼œ ë¡œì§“ê°’ìœ¼ë¡œ ë³€í™˜í•˜ê¸° ìœ„í•´ ë§Œë“¤ì—ˆë‹¤. ì‰½ê²Œ ë§í•´ ì…ë ¥ìœ¼ë¡œ í† í°(ë‹¨ì–´) 384ê°œ ì§œë¦¬ ë¬¸ì¥ì„ í•˜ë‚˜ ë„£ì—ˆê³ , ëª¨ë¸ì€ 384ê°œì˜ ê°œë³„ í† í°ì— ëŒ€í•œ ì„ë² ë”© ê°’ì„ ë°˜í™˜í–ˆëŠ”ë° ê·¸ê²ƒì„ ì „ë¶€ ì´ìš©í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ ì˜ˆë¥¼ ë“¤ì–´ <code class="language-plaintext highlighter-rouge">2ë²ˆ~4ë²ˆ</code> í† í°ì„ 1ë²ˆ êµ¬ê°„, <code class="language-plaintext highlighter-rouge">6ë²ˆ~20ë²ˆ</code> í† í°ì„ 2ë²ˆ êµ¬ê°„, <code class="language-plaintext highlighter-rouge">30ë²ˆ~50ë²ˆ</code> í† í°ì„ 3ë²ˆ êµ¬ê°„ â€¦ <code class="language-plaintext highlighter-rouge">370ë²ˆ~380ë²ˆ</code> í† í°ì„ 30ë²ˆ êµ¬ê°„ìœ¼ë¡œ ì„¤ì •í•˜ê³  êµ¬ê°„ ë³„ë¡œ ë”°ë¡œ <code class="language-plaintext highlighter-rouge">pooling &amp; fully connected layer</code> ì— í†µê³¼ì‹œì¼œ ë¡œì§“ì„ êµ¬í•˜ëŠ” ê²ƒì´ë‹¤. ì¼ë°˜ì ì´ë¼ë©´ 1ê°œì˜ ë¬¸ì¥ì—ì„œ 1ê°œì˜ ìµœì¢… ë¡œì§“ê°’ì´ ë„ì¶œë˜ëŠ” ê²ƒì´ë¼ë©´, ìœ„ ì½”ë“œëŠ” 30ê°œì˜ ë¡œì§“ê°’ì´ ë„ì¶œëœë‹¤.</p>

<h3 id="ï¸-ë‚´ê°€-í•´ê²°í•œ-ë°©ë²•"><code class="language-plaintext highlighter-rouge">ğŸ–ï¸ ë‚´ê°€ í•´ê²°í•œ ë°©ë²•</code></h3>

<p>ì½”ë“œ ì´í•´ë¥¼ ìœ„í•œ ì„¤ëª…ì€ ë§ˆì³¤ìœ¼ë‹ˆ ë³¸ê²©ì ìœ¼ë¡œ ë³¸ ì—ëŸ¬ì™€ ì–´ë–¤ ì—°ê´€ì´ ìˆëŠ”ì§€ ì‚´í´ë³´ì. <code class="language-plaintext highlighter-rouge">Before</code> ì½”ë“œëŠ” <code class="language-plaintext highlighter-rouge">pred</code> ë¼ëŠ” ë¦¬ìŠ¤íŠ¸ì— ê°œë³„ êµ¬ê°„ì— ëŒ€í•œ ë¡œì§“ê°’ì„ <code class="language-plaintext highlighter-rouge">append</code> í•˜ê³  ë§ˆì§€ë§‰ì— <code class="language-plaintext highlighter-rouge">torch.as_tensor</code>ë¥¼ í™œìš©í•´ í…ì„œë¡œ ë³€í™˜í•˜ê³  ìˆë‹¤. í•œí¸ í›„ìëŠ” <code class="language-plaintext highlighter-rouge">pred</code> ë¥¼ ê¹¡í†µ í…ì„œë¡œ ì„ ì–¸í•œ ë’¤, <code class="language-plaintext highlighter-rouge">torch.cat</code>ìœ¼ë¡œ ëª¨ë“  êµ¬ê°„ì— ëŒ€í•œ ë¡œì§“ê°’ì„ í•˜ë‚˜ì˜ í…ì„œ êµ¬ì¡°ì²´ì— ë‹´ê³  ìˆë‹¤.</p>

<p>ì–¼í•ë³´ë©´ í¬ê²Œ ë‹¤ë¥¸ì ì´ ì—†ì–´ ë³´ì¸ë‹¤. í•˜ì§€ë§Œ ì „ìëŠ” í…ì„œ êµ¬ì¡°ì²´ë¥¼ ìƒˆë¡œ ì •ì˜ í•˜ë©´ì„œ <code class="language-plaintext highlighter-rouge">torch.Tensor[[logit1], [logit2], â€¦.]</code> í˜•íƒœë¥¼ ê°–ê³  í›„ìëŠ” <code class="language-plaintext highlighter-rouge">torch.Tensor[logit1, logit2, â€¦]</code> í˜•íƒœë¥¼ ê°–ëŠ”ë‹¤. ì„œë¡œ ë‹¤ë¥¸ í…ì„œ êµ¬ì¡°ì²´ë¥¼ ê·¸ëŒ€ë¡œ ëª¨ë¸ ê°ì²´ì˜ <code class="language-plaintext highlighter-rouge">forward</code> ë©”ì„œë“œ ë° <code class="language-plaintext highlighter-rouge">loss function</code>ì— í†µê³¼ì‹œí‚¤ê³  ì˜¤ì°¨ ì—­ì „ì„ í•˜ë©´ ì–´ë–¤ ì¼ì´ ìƒê¸°ëŠ”ì§€ ì§€ê¸ˆë¶€í„° ì•Œì•„ë³´ì.</p>

<p>ì „ìì˜ ê²½ìš°ëŠ” ë„ì¶œëœ ì†ì‹¤í•¨ìˆ˜ì˜ ë¯¸ë¶„ê°’ì´ ì •ì˜ëœ ê³„ì‚° ê·¸ë˜í”„ë¥¼ íƒ€ê³  ì—­ì „ë  ìˆ˜ ì—†ë‹¤. ì´ìœ ëŠ” ì „ìì˜ <code class="language-plaintext highlighter-rouge">pred</code> ê°€ forward ë©”ì„œë“œ ë‚´ë¶€ì—ì„œ ìƒˆë¡œì´ ì •ì˜ ë˜ì—ˆê¸° ë•Œë¬¸ì´ë‹¤. í›„ì ì—­ì‹œ ë§ˆì°¬ê°€ì§€ ì•„ë‹Œê°€ ì‹¶ì„ ê²ƒì´ë‹¤. í›„ìì˜ <code class="language-plaintext highlighter-rouge">pred</code> ì—­ì‹œ <code class="language-plaintext highlighter-rouge">forward</code> ë©”ì„œë“œ ë‚´ë¶€ì—ì„œ ì •ì˜ëœ ê²ƒì€ ë§ì§€ë§Œ <code class="language-plaintext highlighter-rouge">torch.cat</code>ì„ ì‚¬ìš©í•˜ë©´ì„œ êµ¬ê°„ì˜ ë¡œì§“ê°’ë“¤ ìœ„ì— ìƒˆë¡œì´ ì°¨ì›ì„ ë®ì–´ì“°ëŠ”ê²ƒì´ ì•„ë‹ˆê²Œ ëœë‹¤. ì´ê²ƒì´ ë§¤ìš° ì¤‘ìš”í•œ ì°¨ì´ê°€ ë˜ëŠ”ë°, í›„ìì™€ ê°™ì€ í˜•íƒœê°€ ë˜ëŠ” ê²½ìš°, ì†ì‹¤ê°’ìœ¼ë¡œ ë¶€í„° <code class="language-plaintext highlighter-rouge">Backward</code> ë˜ëŠ” ë¯¸ë¶„ê°’ë“¤ì´ ê³§ë°”ë¡œ <code class="language-plaintext highlighter-rouge">forward</code> ê³¼ì •ì—ì„œ ê¸°ë¡ëœ ìì‹ ì˜ ê³„ì‚° ê·¸ë˜í”„ë¡œ ì°¾ì•„ ê°ˆ ìˆ˜ ìˆë‹¤. í•œí¸ ì „ìì˜ ê²½ìš° ìƒˆë¡­ê²Œ ë®ì–´ ì“°ì—¬ì§„ ì°¨ì› ë•Œë¬¸ì— ë¯¸ë¶„ê°’ë“¤ì´ ìì‹ ì˜ ê³„ì‚° ê·¸ë˜í”„ë¡œ ì°¾ì•„ê°ˆ ìˆ˜ ì—†ê²Œ ëœë‹¤. ë”°ë¼ì„œ ì˜µí‹°ë§ˆì´ì €ê°€ ë” ì´ìƒ <code class="language-plaintext highlighter-rouge">Backward</code> ë¥¼ ìˆ˜í–‰í•  ìˆ˜ ì—†ì–´ ì œëª©ê³¼ ê°™ì€ ì—ëŸ¬ë¥¼ ë°˜í™˜í•˜ê²Œ ë˜ëŠ” ê²ƒì´ë‹¤.</p>

<p>ì²˜ìŒ ì´ ì—ëŸ¬ë¥¼ ë§ˆì£¼í–ˆì„ ë•ŒëŠ”  <code class="language-plaintext highlighter-rouge">found_inf_per_device</code>, <code class="language-plaintext highlighter-rouge">No inf checks</code> ë¼ëŠ” í‚¤ì›Œë“œì— ê½‚í˜€ (íŠ¹íˆ <code class="language-plaintext highlighter-rouge">inf</code>)  <code class="language-plaintext highlighter-rouge">&lt;RuntimeError: Function 'LogSoftmaxBackward0' returned nan values in its 0th output&gt;</code> ì´ê²ƒê³¼ ìœ ì‚¬í•œ ì¢…ë¥˜ì˜ ì—ëŸ¬ë¼ ìƒê°í•˜ê³  ì—´ì‹¬íˆ ì—°ì‚° ê³¼ì •ì— ë¬¸ì œê°€ ì—†ëŠ”ì§€, ì–´ë””ì„œ NaNì´ ë°œìƒí•˜ëŠ”ì§€, í•™ìŠµë¥ ì„ ë„ˆë¬´ í¬ê²Œ ì„¤ì •í–ˆëŠ”ì§€ ë“±ì„ ê²€í† í•˜ë©° í•˜ë£¨ë¥¼ ë‚ ë ¸ì—ˆë˜ ê¸°ì–µì´ ìˆë‹¤.</p>]]></content><author><name>qcqced</name><email>qcqced123@gmail.com</email></author><category term="Framework &amp; Library" /><category term="Pytorch" /><category term="CUDA" /><category term="Error Handling" /><summary type="html"><![CDATA[Pytorch Error: Optimizer can't backward loss]]></summary></entry><entry><title type="html">ğŸ–¥ï¸ RuntimeError: Attempting to deserialize object on CUDA device 2 but torch.cuda.device_count() is 1. Please use torch.load with map_location to map your storages to an existing device</title><link href="http://localhost:4000/framework-library/cuda-num/" rel="alternate" type="text/html" title="ğŸ–¥ï¸ RuntimeError: Attempting to deserialize object on CUDA device 2 but torch.cuda.device_count() is 1. Please use torch.load with map_location to map your storages to an existing device" /><published>2023-07-11T00:00:00+09:00</published><updated>2023-07-11T13:00:00+09:00</updated><id>http://localhost:4000/framework-library/cuda-device-num</id><content type="html" xml:base="http://localhost:4000/framework-library/cuda-num/"><![CDATA[<h3 id="-pytorch-ì˜ëª»ëœ-cuda-ì¥ì¹˜-ë²ˆí˜¸-ì‚¬ìš©-ë¬¸ì œ"><code class="language-plaintext highlighter-rouge">ğŸ”¢ Pytorch ì˜ëª»ëœ CUDA ì¥ì¹˜ ë²ˆí˜¸ ì‚¬ìš© ë¬¸ì œ</code></h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span><span class="p">.</span><span class="n">load_state_dict</span><span class="p">(</span>
    <span class="n">torch</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">map_location</span><span class="o">=</span><span class="s">'cuda:0'</span><span class="p">)</span> 
<span class="p">)</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">pretrained model</code>, <code class="language-plaintext highlighter-rouge">weight</code>ë¥¼ <code class="language-plaintext highlighter-rouge">load</code>í•˜ê±°ë‚˜ í˜¹ì€ í›ˆë ¨ ë£¨í”„ë¥¼ <code class="language-plaintext highlighter-rouge">resume</code> ì„ ìœ„í•´ <code class="language-plaintext highlighter-rouge">torch.load()</code> ë¥¼ ì‚¬ìš©í•  ë•Œ ë§ˆì£¼í•  ìˆ˜ ìˆëŠ” ì—ëŸ¬ ë¡œê·¸ë‹¤. ë°œìƒí•˜ëŠ” ì´ìœ ëŠ” í˜„ì¬ <code class="language-plaintext highlighter-rouge">GPU</code> ì— í• ë‹¹í•˜ë ¤ëŠ” ëª¨ë¸ì´ ì‚¬ì „ í›ˆë ¨ë•Œ í• ë‹¹ ë˜ì—ˆë˜ <code class="language-plaintext highlighter-rouge">GPU</code> ë²ˆí˜¸ì™€ í˜„ì¬ í• ë‹¹í•˜ë ¤ëŠ” <code class="language-plaintext highlighter-rouge">GPU</code> ë²ˆí˜¸ê°€ ì„œë¡œ ìƒì´í•˜ê¸° ë•Œë¬¸ì´ë‹¤. ë”°ë¼ì„œ <code class="language-plaintext highlighter-rouge">torch.load</code>ì˜ <code class="language-plaintext highlighter-rouge">map_location</code>ì¸ìì— í˜„ì¬ ìì‹ ì´ ì‚¬ìš©í•˜ë ¤ëŠ” <code class="language-plaintext highlighter-rouge">GPU</code> ë²ˆí˜¸ë¥¼ ì…ë ¥í•´ì£¼ì.</p>]]></content><author><name>qcqced</name><email>qcqced123@gmail.com</email></author><category term="Framework &amp; Library" /><category term="Pytorch" /><category term="CUDA" /><summary type="html"><![CDATA[Pytorch Error: Wrong CUDA Device Number]]></summary></entry><entry><title type="html">ğŸ¤” RuntimeError: Function â€˜LogSoftmaxBackward0â€™ returned nan values in its 0th output</title><link href="http://localhost:4000/framework-library/backward-nan/" rel="alternate" type="text/html" title="ğŸ¤” RuntimeError: Function â€˜LogSoftmaxBackward0â€™ returned nan values in its 0th output" /><published>2023-07-11T00:00:00+09:00</published><updated>2023-07-11T13:00:00+09:00</updated><id>http://localhost:4000/framework-library/backward-nan</id><content type="html" xml:base="http://localhost:4000/framework-library/backward-nan/"><![CDATA[<h3 id="-pytorch-backward-ê³¼ì •ì—ì„œ-nan-ë°œìƒí•˜ëŠ”-ë¬¸ì œ"><code class="language-plaintext highlighter-rouge">ğŸ”¥ Pytorch Backward ê³¼ì •ì—ì„œ NaN ë°œìƒí•˜ëŠ” ë¬¸ì œ</code></h3>

<p>ì»¤ìŠ¤í…€ìœ¼ë¡œ ëª¨ë¸, ì—¬ëŸ¬ í’€ë§, ë§¤íŠ¸ë¦­, ì†ì‹¤ í•¨ìˆ˜ë“¤ì„ ì •ì˜í•˜ë©´ì„œë¶€í„° ì œì¼ ë§ì´ ë§ˆì£¼í•˜ê²Œ ë˜ëŠ” ì—ëŸ¬ë‹¤. ì§„ì‹¬ìœ¼ë¡œ ìš”ì¦˜ <code class="language-plaintext highlighter-rouge">CUDA OOM</code> ë³´ë‹¤ í›¨ì”¬ ìì£¼ ë³´ëŠ” ê²ƒ ê°™ë‹¤. í•´ë‹¹ ì—ëŸ¬ëŠ” <code class="language-plaintext highlighter-rouge">LogSoftmax</code> ë ˆì´ì–´ì— ì „ë‹¬ëœ ì…ë ¥ê°’ ì¤‘ì—ì„œ <code class="language-plaintext highlighter-rouge">nan</code>, <code class="language-plaintext highlighter-rouge">inf</code> ê°€ í¬í•¨ë˜ì–´ ì—°ì‚°ì„ ì§„í–‰í•  ìˆ˜ ì—†ë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•œë‹¤. ë”¥ëŸ¬ë‹ ì‹¤í—˜ì„ ì§„í–‰í•˜ë©´ì„œ ê°€ì¥ í•´ê²°í•˜ê¸° ê¹Œë‹¤ë¡œìš´ ë…€ì„ìœ¼ë¡œ ì›ì¸ì„ íŠ¹ì •í•˜ê¸° í˜ë“¤ê¸° ë•Œë¬¸ì´ë‹¤. ì›ì¸ì„ ì¡ê¸° ì–´ë ¤ìš´ ì´ìœ ëŠ” ë°”ë¡œ ìš°ë¦¬ê°€ ì§€ê¸ˆ í•˜ê³  ìˆëŠ”ê²Œ <code class="language-plaintext highlighter-rouge">â€˜ë”¥ëŸ¬ë‹â€™</code> ì´ë¼ì„œ ê·¸ë ‡ë‹¤. ìœ„ ì—ëŸ¬ëŠ” ëŒ€ë¶€ë¶„ ì—°ì‚°ìê°€ ìš°ë¦¬ê°€ ì˜ë„í•˜ì§€ ì•Šì€ ë™ì‘ì„ í•˜ëŠ” ì¼€ì´ìŠ¤ ë•Œë¬¸ì¸ë°, í•˜ë‚˜ í•˜ë‚˜ ë””ë²„ê¹…í•˜ê¸°ì—ëŠ” ë„ˆë¬´ë‚˜ë„ ì—°ì‚°ìê°€ ë§ë‹¤. ë˜í•œ ë”¥ëŸ¬ë‹ì€ ì…ì¶œë ¥ìœ¼ë¡œ ì—„ì²­ë‚˜ê²Œ í° ì‚¬ì´ì¦ˆì˜ í–‰ë ¬ì„ ì‚¬ìš©í•œë‹¤. ìš°ë¦¬ê°€ <code class="language-plaintext highlighter-rouge">nan</code>, <code class="language-plaintext highlighter-rouge">inf</code> ê°’ ì¡´ì¬ì— ëŒ€í•´ì„œ ì¸ì§€í•˜ê¸° ì‰½ì§€ ì•Šë‹¤.</p>

<p><strong><u>ìœ„ ì—ëŸ¬ëŠ” í•„ìì˜ ê²½í—˜ìƒ ëŒ€ë¶€ë¶„ ì»¤ìŠ¤í…€ìœ¼ë¡œ ì •ì˜í•œ ë ˆì´ì–´ì—ì„œ ë°œìƒí•˜ëŠ” ê²½ìš°ê°€ ë§ì•˜ìœ¼ë©° íŠ¹íˆ</u></strong> <code class="language-plaintext highlighter-rouge">ë¶„ìˆ˜</code>, <code class="language-plaintext highlighter-rouge">ê°ë„</code>, <code class="language-plaintext highlighter-rouge">ì œê³±ê·¼</code>, <code class="language-plaintext highlighter-rouge">ì§€ìˆ˜</code> <strong><u>ê°œë…ì„ ì‚¬ìš©í•˜ëŠ” ì—°ì‚°ìê°€ ëŒ€ë¶€ë¶„ ì›ì¸ì´ì—ˆë‹¤.</u></strong> ì˜ˆë¥¼ ë“¤ì–´ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ êµ¬í•˜ëŠ” ê³¼ì •ì—ì„œ ì—°ì‚° ëŒ€ìƒ ë²¡í„°ê°’ì—  <code class="language-plaintext highlighter-rouge">zero-value</code> ê°€ í¬í•¨ëœ ê²½ìš° ë¶„ëª¨ê°€ 0ì´ ë˜ê¸° ë•Œë¬¸ì— ì—°ì‚° ì •ì˜ê°€ ë˜ì§€ ì•Šì•„ <code class="language-plaintext highlighter-rouge">nan</code> ì„ ë°˜í™˜í•´ ìœ„ì™€ ê°™ì€ ì—ëŸ¬ê°€ ë°œìƒí•˜ëŠ” ê²½ìš°ê°€ ìˆë‹¤.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">check_nan</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
    <span class="s">""" Check if there is NaN in tensor """</span>
    <span class="n">checker</span> <span class="o">=</span> <span class="bp">False</span>
    <span class="k">if</span> <span class="bp">True</span> <span class="ow">in</span> <span class="n">torch</span><span class="p">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
        <span class="n">checker</span> <span class="o">=</span> <span class="bp">True</span>
    <span class="k">return</span> <span class="n">checker</span>

<span class="k">def</span> <span class="nf">zero_filtering</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="s">"""
    Add eps value for zero embedding, because competition metric is cosine similarity
    Cosine Similarity will be returned NaN, when input value has zero, like as torch.clamp()
    """</span>
    <span class="n">eps</span> <span class="o">=</span> <span class="mf">1e-4</span>
    <span class="n">x</span><span class="p">[</span><span class="n">x</span> <span class="o">&lt;=</span> <span class="n">eps</span><span class="p">]</span> <span class="o">=</span> <span class="n">eps</span>
    <span class="k">return</span> <span class="n">x</span>

<span class="k">def</span> <span class="nf">nan_filtering</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">eps</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-4</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="s">"""
    Change eps value for NaN Embedding, because competition metric is cosine similarity
    Cosine Similarity will be returned NaN
    """</span>
    <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">nan</span><span class="o">=</span><span class="n">eps</span><span class="p">)</span>

<span class="k">class</span> <span class="nc">CLIPGEMPooling</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="s">"""
    Generalized Mean Pooling for Natural Language Processing
    This class version of GEMPooling for CLIP, Transfer from NLP Task Code
    ViT don't use attention mask, because input image shape will be same

    Mean Pooling &lt;= GEMPooling &lt;= Max Pooling
    Because of doing exponent to each token embeddings, GEMPooling is like as weight to more activation token

    In original paper, they use p=3, but in this class, we use p=4 because torch doesn't support pow calculation
    for negative value tensor, only for non-negative value in odd number exponent
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">auto_cfg</span><span class="p">:</span> <span class="n">AutoConfig</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">CLIPGEMPooling</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>

    <span class="o">@</span><span class="nb">staticmethod</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">last_hidden_state</span><span class="p">,</span> <span class="n">p</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="s">"""
        last_hidden_state.size: [batch_size, patches_sequence, hidden_size]
        1) Pow last_hidden_state with p and then take a averaging
        2) pow sum_embeddings with 1/p
        """</span>
        <span class="n">p_embeddings</span> <span class="o">=</span> <span class="n">zero_filtering</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nb">pow</span><span class="p">(</span><span class="n">last_hidden_state</span><span class="p">,</span> <span class="n">p</span><span class="p">))</span>
        <span class="c1"># Check NaN value in Embedding after applying torch.pow
</span>        <span class="k">if</span> <span class="n">check_nan</span><span class="p">(</span><span class="n">p_embeddings</span><span class="p">):</span>
            <span class="n">p_embeddings</span> <span class="o">=</span> <span class="n">nan_filtering</span><span class="p">(</span><span class="n">p_embeddings</span><span class="p">)</span>
        <span class="n">sum_embeddings</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">p_embeddings</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

        <span class="n">gem_embeddings</span> <span class="o">=</span> <span class="n">zero_filtering</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nb">pow</span><span class="p">(</span><span class="n">sum_embeddings</span><span class="p">,</span> <span class="mf">1.</span> <span class="o">/</span> <span class="n">p</span><span class="p">))</span>
        <span class="c1"># Check NaN value in Embedding after applying torch.pow
</span>        <span class="k">if</span> <span class="n">check_nan</span><span class="p">(</span><span class="n">gem_embeddings</span><span class="p">):</span>
            <span class="n">gem_embeddings</span> <span class="o">=</span> <span class="n">nan_filtering</span><span class="p">(</span><span class="n">gem_embeddings</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">gem_embeddings</span>

<span class="k">class</span> <span class="nc">CLIPMultipleNegativeRankingLoss</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="s">"""
    Multiple Negative Ranking Loss for CLIP Model
    main concept is same as original one, but append suitable for other type of model (Not Sentence-Transformers)
    if you set more batch size, you can get more negative pairs for each anchor &amp; positive pair
    Args:
        scale: output of similarity function is multiplied by this value =&gt; I don't know why this is needed
        similarity_fct: standard of distance metrics, default cosine similarity
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">reduction</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">scale</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">20.0</span><span class="p">,</span> <span class="n">similarity_fct</span><span class="o">=</span><span class="n">cos_sim</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">reduction</span> <span class="o">=</span> <span class="n">reduction</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">scale</span> <span class="o">=</span> <span class="n">scale</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">similarity_fct</span> <span class="o">=</span> <span class="n">similarity_fct</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">reduction</span> <span class="o">=</span> <span class="n">reduction</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">cross_entropy_loss</span> <span class="o">=</span> <span class="n">CrossEntropyLoss</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">reduction</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">embeddings_a</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">embeddings_b</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="n">similarity_scores</span> <span class="o">=</span> <span class="n">zero_filtering</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">similarity_fct</span><span class="p">(</span><span class="n">embeddings_a</span><span class="p">,</span> <span class="n">embeddings_b</span><span class="p">))</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">scale</span>
        <span class="k">if</span> <span class="n">check_nan</span><span class="p">(</span><span class="n">similarity_scores</span><span class="p">):</span>
            <span class="s">""" Check NaN Value in similarity_scores """</span>
            <span class="n">similarity_scores</span> <span class="o">=</span> <span class="n">nan_filtering</span><span class="p">(</span><span class="n">similarity_scores</span><span class="p">)</span>

        <span class="n">labels</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">(</span>
            <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">similarity_scores</span><span class="p">)),</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">long</span><span class="p">,</span>
            <span class="n">device</span><span class="o">=</span><span class="n">similarity_scores</span><span class="p">.</span><span class="n">device</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">cross_entropy_loss</span><span class="p">(</span><span class="n">similarity_scores</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
</code></pre></div></div>

<p>í•„ìì˜ ê²½ìš°, ë‘ ê°œì˜ ì…ë ¥ í–‰ë ¬ì— ê°ê°  <code class="language-plaintext highlighter-rouge">sqrt()</code> ë¥¼ ì ìš©í•˜ê³  ë‘ í–‰ë ¬ì˜ ê°œë³„ ì›ì†Œ ì‚¬ì´ì˜ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ êµ¬í•´ì•¼ í–ˆë˜ ì ì´ ìˆë‹¤. <code class="language-plaintext highlighter-rouge">sqrt</code> <strong><u>ê³¼ì •ì—ì„œ ë„ˆë¬´ ì‘ì€ ê°’ë“¤ì´ ì…ë ¥ìœ¼ë¡œ ë“¤ì–´ê°€</u></strong> <code class="language-plaintext highlighter-rouge">underflow</code> <strong><u>ê°€ ë°œìƒí•´ í–‰ë ¬ì—</u></strong> <code class="language-plaintext highlighter-rouge">zero-value</code> <strong><u>ê°€ ìƒê²¼ê³ , ì´ë¥¼ ëª¨ë¥¸ì±„ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ êµ¬í•˜ë‹¤ê°€ í•œì°¸ì„ ìœ„ ì—ëŸ¬ì™€ ì‹¸ì› ë˜ ì ì´ ìˆë‹¤.</u></strong> ì‹¬ì§€ì–´ ì—°ì‚°ì†ë„ í–¥ìƒì„ ìœ„í•´ì„œ <strong><code class="language-plaintext highlighter-rouge">torch.autocast</code></strong> í´ë˜ìŠ¤ì˜ <code class="language-plaintext highlighter-rouge">grad_scaler(float32 to float16)</code> ê¹Œì§€ ì ìš©í•˜ê³  ìˆì—ˆë‹¤.</p>

<h3 id="ï¸-ë‚´ê°€-í•´ê²°í•œ-ë°©ë²•"><code class="language-plaintext highlighter-rouge">ğŸ–ï¸ ë‚´ê°€ í•´ê²°í•œ ë°©ë²•</code></h3>
<p>ì´ ê¸€ì„ ì½ëŠ” ë‹¹ì‹ ì´ ë§Œì•½ <code class="language-plaintext highlighter-rouge">sqrt</code> í˜¹ì€ <code class="language-plaintext highlighter-rouge">pow</code>ë¥¼ í™œìš©í•˜ëŠ” ê²½ìš°, <code class="language-plaintext highlighter-rouge">underflow</code> ë°©ì§€ë¥¼ ìœ„í•´ì„œ <del>ìœ„ ì˜ˆì‹œ ì½”ë“œì²˜ëŸ¼ ê¼­ ì ë‹¹í•œ ì…ì‹¤ë¡  ê°’ì„ ì—°ì‚° ì „í›„ì— í•„ìš”ì— ë”°ë¼ ë”í•´ì¤„ ê²ƒì„ ê¶Œì¥í•œë‹¤.</del> ì…ì‹¤ë¡  ê°’ì˜ ì„¤ì •ì€ í˜„ì¬ ìì‹ ì´ ì‚¬ìš©í•˜ê³  ìˆëŠ” ë¶€ë™ ì†Œìˆ˜ì  ì •í™•ë„ì— ë§ê²Œ ì„¤ì •í•´ì£¼ë©´ ë  ê²ƒ ê°™ë‹¤. <code class="language-plaintext highlighter-rouge">float32</code> ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²½ìš°ì—ëŠ” ëŒ€ë¶€ë¶„ <code class="language-plaintext highlighter-rouge">1e-6</code> ì„ ë§ì´ ì‚¬ìš©í•˜ëŠ” ê²ƒ ê°™ë‹¤. í•„ìë„ ì •í™•íˆ ì–´ë–¤ ê°’ì´ ì ë‹¹í•œì§€ ì•„ì§ ì˜ ëª¨ë¥´ê² ë‹¤â€¦ ê·¸ë¦¬ê³  ë”¥ëŸ¬ë‹ ì‹¤í—˜í•˜ë©´ì„œ <code class="language-plaintext highlighter-rouge">overflow</code> ë•Œë¬¸ì— <code class="language-plaintext highlighter-rouge">inf</code> ì´ ë°œìƒí–ˆë˜ ì ì€ ì—†ì—ˆë‹¤.</p>

<p>ì…ì‹¤ë¡  ê°’ì„ ë¬¸ì œê°€ ë˜ëŠ” ì—°ì‚° ì „ì— ì¼ê´„ì ìœ¼ë¡œ ë”í•  ê²½ìš°, ì•„ë¬´ë¦¬ ì‘ì€ ê°’ì´ë¼ë„ ì—°ì‚° ì¢…ë¥˜ì— ë”°ë¼ì„œ ê²°ê³¼ê°€ í¬ê²Œ ì™œê³¡ë˜ëŠ” ê²½ìš°ê°€ ë°œìƒí•œë‹¤. ë”°ë¼ì„œ ì—°ì‚°ì„ ë¨¼ì € ì ìš©í•œ ë’¤ ê²°ê³¼ì— <code class="language-plaintext highlighter-rouge">NaN</code>, <code class="language-plaintext highlighter-rouge">Inf</code>, <code class="language-plaintext highlighter-rouge">Zero</code>ê°€ ë°œìƒí•˜ëŠ”ì§€ ì²´í¬í•˜ê³ , ë°œìƒí•œ ë¶€ë¶„ì— í•œí•´ì„œ ì…ì‹¤ë¡  ê°’ì„ ë”í•´ì£¼ëŠ” ì»¤ìŠ¤í…€ <code class="language-plaintext highlighter-rouge">function</code>ìš¸ ì •ì˜í•´ ë¬¸ì œë¥¼ í•´ê²°í–ˆë‹¤.<br />
(ìœ„ì˜ ì½”ë“œ ì˜ˆì œ <code class="language-plaintext highlighter-rouge">check_nan</code>, <code class="language-plaintext highlighter-rouge">zero_filtering</code>, <code class="language-plaintext highlighter-rouge">nan_filtering</code>)</p>

<p>í•œí¸ <code class="language-plaintext highlighter-rouge">torch.autograd.set_detect_anomaly(True)</code> ë¥¼ í›ˆë ¨ ë£¨í”„ ì´ˆë°˜ì— ì •ì˜í•´ì£¼ë©´, <code class="language-plaintext highlighter-rouge">NaN</code>ì´ ë°œìƒí•˜ëŠ” ì¦‰ì‹œ ì‹¤í–‰ì´ ë©ˆì¶”ê³  <code class="language-plaintext highlighter-rouge">NaN</code>ì„ ìœ ë°œí•œ ë¼ì¸ì„ ì¶œë ¥í•´ì¤€ë‹¤. ê¼­ í™œìš©í•´ë³´ì.</p>]]></content><author><name>qcqced</name><email>qcqced123@gmail.com</email></author><category term="Framework &amp; Library" /><category term="Pytorch" /><category term="Logsoftmax" /><category term="NaN" /><category term="Error Handling" /><summary type="html"><![CDATA[Pytorch Error: Backward NaN values]]></summary></entry><entry><title type="html">ğŸšš RuntimeError: stack expects each tensor to be equal size, but got [32] at entry 0 and [24] at entry 1</title><link href="http://localhost:4000/framework-library/dataloader-collatefn" rel="alternate" type="text/html" title="ğŸšš RuntimeError: stack expects each tensor to be equal size, but got [32] at entry 0 and [24] at entry 1" /><published>2023-07-11T00:00:00+09:00</published><updated>2023-07-12T13:00:00+09:00</updated><id>http://localhost:4000/framework-library/dataloader-collate</id><content type="html" xml:base="http://localhost:4000/framework-library/dataloader-collatefn"><![CDATA[<h3 id="-ê°€ë³€-ê¸¸ì´ì˜-í…ì„œë¥¼-ë°ì´í„°ë¡œë”ì—-ì „ë‹¬í•˜ëŠ”-ê²½ìš°-"><code class="language-plaintext highlighter-rouge">ğŸ“ ê°€ë³€ ê¸¸ì´ì˜ í…ì„œë¥¼ ë°ì´í„°ë¡œë”ì— ì „ë‹¬í•˜ëŠ” ê²½ìš° </code></h3>

<p>ì»¤ìŠ¤í…€ ë°ì´í„° í´ë˜ìŠ¤ì™€ ë°ì´í„°ë¡œë”ë¥¼ í†µí•´ ë°˜í™˜ë˜ëŠ” ë°ì´í„° ì¸ìŠ¤í„´ìŠ¤ì˜ í…ì„œ í¬ê¸°ê°€ ì¼ì •í•˜ì§€ ì•Šì•„ ë°œìƒí•˜ëŠ” ì—ëŸ¬ë‹¤. íŠ¹íˆ ìì—°ì–´ ì²˜ë¦¬ì—ì„œ ìì£¼ ì°¾ì•„ ë³¼ ìˆ˜ ìˆëŠ”ë° ë°ì´í„°ë¡œë” ê°ì²´ ì„ ì–¸ ì‹œ, ë§¤ê°œë³€ìˆ˜ ì˜µì…˜ ì¤‘ì— <code class="language-plaintext highlighter-rouge">collate_fn=collate</code> ë¥¼ ì¶”ê°€í•´ì£¼ë©´ í•´ê²° ê°€ëŠ¥í•œ ì—ëŸ¬ë‹¤. ì´ ë•Œ ë§¤ê°œë³€ìˆ˜ <code class="language-plaintext highlighter-rouge">collate_fn</code> ì— ì „ë‹¬í•˜ëŠ” ê°’(ë©”ì„œë“œ)ì€ ì‚¬ìš©ìê°€ ì§ì ‘ ì •ì˜í•´ì¤˜ì•¼ í•œë‹¤. í—ˆê¹…í˜ì´ìŠ¤ ë¼ì´ë¸Œë¦¬ëŸ¬ì— ìƒí™©ì— ë§ê²Œ ë¯¸ë¦¬ ì œì‘ëœ <code class="language-plaintext highlighter-rouge">collate</code> ë©”ì„œë“œë¥¼ ì§€ì›í•´ì£¼ê³  ìˆê¸° ë•Œë¬¸ì— ì˜ ì´ìš©í•˜ë©´ ëœë‹¤. í•„ìì˜ ê²½ìš°ì—ëŠ” ì»¤ìŠ¤í…€ìœ¼ë¡œ ì§ì ‘ ì •ì˜í•œ ë©”ì„œë“œ, ê°ì²´ë¥¼ ì‚¬ìš©í•˜ê³  ìˆë‹¤.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># ë°ì´í„° ë¡œë” ì˜ˆì‹œ
</span><span class="n">loader_train</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span>
            <span class="n">train_dataset</span><span class="p">,</span>
            <span class="n">batch_size</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">batch_size</span><span class="p">,</span>
            <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
            <span class="n">worker_init_fn</span><span class="o">=</span><span class="n">seed_worker</span><span class="p">,</span>
            <span class="n">collate_fn</span><span class="o">=</span><span class="n">MiniBatchCollate</span><span class="p">,</span>  <span class="c1"># ì—¬ê¸°ì— ì‚¬ìš©í•˜ë ¤ëŠ” collate function í˜¹ì€ ê°ì²´ë¥¼ ì „ë‹¬í•˜ì!!
</span>            <span class="n">generator</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">generator</span><span class="p">,</span>
            <span class="n">num_workers</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">num_workers</span><span class="p">,</span>
            <span class="n">pin_memory</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
            <span class="n">drop_last</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
        <span class="p">)</span>

<span class="c1"># collate ë©”ì„œë“œ ì˜ˆì‹œ: 
</span><span class="k">class</span> <span class="nc">MiniBatchCollate</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="s">"""
    Collate class for torch.utils.data.DataLoader  
    This class object to use variable data such as NLP text sequence
    If you use static padding with AutoTokenizer, you don't need this class 
    But if you use dynamic padding with AutoTokenizer, you must use this class object &amp; call
    Args:
        batch: data instance from torch.utils.data.DataSet
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">DataLoader</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">batch</span> <span class="o">=</span> <span class="n">batch</span>

    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">dict</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">],</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
        <span class="n">inputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">position_list</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">batch</span>
        <span class="n">labels</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="n">rnn</span><span class="p">.</span><span class="n">pad_sequence</span><span class="p">(</span>
            <span class="n">labels</span><span class="p">,</span>
            <span class="n">batch_first</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
            <span class="n">padding_value</span><span class="o">=-</span><span class="mi">1</span>
        <span class="p">)</span>
        <span class="n">position_list</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="n">rnn</span><span class="p">.</span><span class="n">pad_sequence</span><span class="p">(</span>
            <span class="n">position_list</span><span class="p">,</span>
            <span class="n">batch_first</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
            <span class="n">padding_value</span><span class="o">=-</span><span class="mi">1</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">position_list</span>

<span class="k">def</span> <span class="nf">collate</span><span class="p">(</span><span class="n">inputs</span><span class="p">):</span>
    <span class="s">"""
    slice input sequence by maximum length sequence in mini-batch, used for speed up training
    if you want slice other variable such as label feature, you can add param on them
    This Function should be used after DataLoader return mini-batch instance
    Args:
        inputs: list of dict, dict has keys of "input_ids", "attention_mask", "token_type_ids"    
    """</span>
    <span class="n">mask_len</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">inputs</span><span class="p">[</span><span class="s">"attention_mask"</span><span class="p">].</span><span class="nb">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">).</span><span class="nb">max</span><span class="p">())</span>
    <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">inputs</span><span class="p">.</span><span class="n">items</span><span class="p">():</span>
        <span class="n">inputs</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="n">k</span><span class="p">][:,</span> <span class="p">:</span><span class="n">mask_len</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">inputs</span>
</code></pre></div></div>

<p>ì¼ë°˜ì ìœ¼ë¡œ <code class="language-plaintext highlighter-rouge">collate</code> ëŠ” ë©”ì„œë“œë¡œ êµ¬í˜„í•´ì„œ ì‚¬ìš©í•˜ì§€ë§Œ, ìœ„ ì½”ë“œì²˜ëŸ¼ ê°ì²´ë¡œ êµ¬í˜„í•˜ê³  ë‚´ë¶€ì— <code class="language-plaintext highlighter-rouge">__call__</code> ë¥¼ ì •ì˜í•´ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ë„ ìˆë‹¤. í•„ì ì—­ì‹œ ë‹¨ì¼ ë©”ì„œë“œ í˜•íƒœë¥¼ ê³„ì†í•´ì„œ ì‚¬ìš©í•˜ë‹¤ê°€ ìµœê·¼ ë“¤ì–´ ì—í­ í•œ ë²ˆì— ì„œë¡œ ë‹¤ë¥¸ ë°ì´í„° ì„¸íŠ¸ ë° ëª¨ë¸ì„ í›ˆë ¨ ì‹œì¼œì•¼ í•˜ëŠ” ìƒí™©ì„ ë§ˆì£¼í•œ ì´í›„ ê°ì²´ í˜•íƒœë¡œ ë‹¤ì‹œ êµ¬í˜„í•´ ì‚¬ìš©í•˜ê³  ìˆë‹¤.</p>

<p>í•œí¸ ì˜ˆì‹œ ì½”ë“œ ê°€ì¥ ë§ˆì§€ë§‰ <code class="language-plaintext highlighter-rouge">collate</code> ë©”ì„œë“œëŠ” ì…ë ¥ ì‹œí€€ìŠ¤ê°€ huggingfaceì˜ <code class="language-plaintext highlighter-rouge">AutoTokenizer.encode_plus</code> ë¥¼ ì´ìš©í•´ ì‚¬ìš©ì ì§€ì • <code class="language-plaintext highlighter-rouge">max_len</code>ê¹Œì§€ íŒ¨ë”©ì„ ë§ˆì¹œ ìƒíƒœë¼ëŠ” ê°€ì •í•˜ì— êµ¬í˜„ ë˜ì—ˆë‹¤. í•´ë‹¹ ë©”ì„œë“œëŠ” ìœ„ì— ë°œìƒí•œ ì—ëŸ¬ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•¨ë³´ë‹¤, ë¯¸ë‹ˆ ë°°ì¹˜ì— ì†í•œ ì „ì²´ ë°ì´í„° ì¤‘ì—ì„œ ìµœëŒ€ ê¸¸ì´ê°€ ì‚¬ìš©ì ì§€ì • <code class="language-plaintext highlighter-rouge">max_len</code>ê¹Œì§€ ë¯¸ì¹˜ì§€ ëª»í•˜ëŠ”ë° íŒ¨ë”©ì´ ëœ ê²½ìš°ì— ì‚¬ìš©í•˜ê¸° ìœ„í•´ ë§Œë“¤ì—ˆë‹¤. ë¶ˆí•„ìš”í•œ íŒ¨ë”©ì„ <code class="language-plaintext highlighter-rouge">trucation</code> í•˜ì—¬ ë‰´ëŸ´ ë„¤íŠ¸ì›Œí¬ì˜ í•™ìŠµ ì†ë„ë¥¼ ë†’ì´ê¸° ìœ„í•¨ì´ë‹¤. í•´ë‹¹ ë©”ì„œë“œëŠ” í¬ìŠ¤íŒ…ì˜ ì œëª©ì— ë‹¬ë¦° ì—ëŸ¬ë¥¼ í•´ê²°í•˜ëŠ”ë° ì‚¬ìš©í•  ìˆ˜ëŠ” ì—†ì§€ë§Œ  <code class="language-plaintext highlighter-rouge">collate</code> ê¸°ëŠ¥ì„ ì–¸ê¸‰í•˜ëŠ” ê¹€ì— ìƒê°ì´ë‚˜ ê°™ì´ ì •ë¦¬í•´ë´¤ë‹¤. ì´ ë©”ì„œë“œëŠ” <code class="language-plaintext highlighter-rouge">torch.utils.data.DataLoader</code> ì˜ ì¸ìê°€ ì•„ë‹ˆë¼, ë©”ì¸ í•™ìŠµ ë£¨í”„ ë‚´ë¶€ì— ì‚¬ìš©í•œë‹¤. ë‹¤ì‹œ ë§í•´, ë°ì´í„°ë¡œë”ê°€ ë°°ì¹˜ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ë°˜í™˜í•œ ë‹¤ìŒ ì‚¬ìš©í•˜ë©´ ëœë‹¤ëŠ” ê²ƒì´ë‹¤. íŒ¨ë”©ë°©ì‹ê³¼ <code class="language-plaintext highlighter-rouge">collate</code> ê¸°ëŠ¥ì— ëŒ€í•œ ìì„¸í•œ ì„¤ëª…ì€ ë‹¤ë¥¸ í¬ìŠ¤íŒ…ì—ì„œ ë‹¤ë£¨ë„ë¡ í•˜ê² ë‹¤.</p>

<p>ë°˜ë©´ <code class="language-plaintext highlighter-rouge">MiniBatchCollate</code> ê°ì²´ëŠ” <code class="language-plaintext highlighter-rouge">torch.utils.data.DataLoader</code> ì˜ <code class="language-plaintext highlighter-rouge">collate_fn</code> ì¸ìì— ì „ë‹¬í•˜ë©´ ëœë‹¤. í•„ìì˜ ê²½ìš°ëŠ” <code class="language-plaintext highlighter-rouge">Dynamic Padding</code> ê¸°ë²•ì„ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì— ë¯¸ë‹ˆ ë°°ì¹˜ ë‚´ë¶€ì˜ ì¸ìŠ¤í„´ìŠ¤ë“¤ì´ ì„œë¡œ ë‹¤ë¥¸ ì‹œí€€ìŠ¤ ê¸¸ì´ë¥¼ ê°–ëŠ” ê²½ìš°ê°€ ë°œìƒí•œë‹¤. ë°ì´í„°ë¡œë”ëŠ” ë¯¸ë‹ˆ ë°°ì¹˜ì— ì†í•˜ëŠ” ë°ì´í„°ì˜ ê¸¸ì´ê°€ í†µì¼ë˜ì§€ ì•Šìœ¼ë©´ ë°°ì¹˜ ë‹¨ìœ„ë¡œ ë°ì´í„°ë¥¼ ë¬¶ì„ ìˆ˜ ì—†ê²Œ ëœë‹¤. ë”°ë¼ì„œ ë¯¸ë‹ˆ ë°°ì¹˜ ë‹¨ìœ„ì˜ ê¸¸ì´ í†µì¼ì„ ìœ„í•´ <code class="language-plaintext highlighter-rouge">torch.nn.utils.rnn.pad_sequence</code> ë©”ì„œë“œë¥¼ ì‚¬ìš©í•œë‹¤. ì´ ë©”ì„œë“œëŠ” ì…ë ¥í•œ ë¯¸ë‹ˆ ë°°ì¹˜ ë°ì´í„° ì¤‘ì—ì„œ ê°€ì¥ ê¸´ ì‹œí€€ìŠ¤ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ëª¨ë“  ë°ì´í„° ê¸¸ì´ë¥¼ í†µì¼í•œë‹¤. <code class="language-plaintext highlighter-rouge">batch_first=True</code> ë¥¼ ì£¼ëª©í•˜ì. ì´ ì¸ìë¥¼ <code class="language-plaintext highlighter-rouge">False</code> ë¡œ ì„¤ì •í•  ê²½ìš°, ë°°ì¹˜ ì°¨ì›ì´ ë§¨ ì•ì´ ì•„ë‹ˆë¼ ì¤‘ê°„ì— ì •ì˜ëœë‹¤. ì¼ë°˜ì ìœ¼ë¡œëŠ” ë°°ì¹˜ ì°¨ì›ì„ ë§¨ ì•ì— ë‘ëŠ” ì›Œí¬í”Œë¡œìš°ë¥¼ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì— ê¼­ í•´ë‹¹ ì¸ìë¥¼ <code class="language-plaintext highlighter-rouge">True</code> ë¡œ ì„¤ì •í•˜ê³  ì‚¬ìš©í•˜ì.</p>]]></content><author><name>qcqced</name><email>qcqced123@gmail.com</email></author><category term="Framework &amp; Library" /><category term="Pytorch" /><category term="DataLoader" /><category term="collate_fn" /><category term="Dynamic Padding" /><category term="Padding" /><summary type="html"><![CDATA[Pytorch Error: Dataloader get non-equal size of tensor]]></summary></entry><entry><title type="html">ğŸ“ Inner Product: Projection Matrix, Least Sqaure Method</title><link href="http://localhost:4000/linear-algebra/inner-product" rel="alternate" type="text/html" title="ğŸ“ Inner Product: Projection Matrix, Least Sqaure Method" /><published>2023-07-10T00:00:00+09:00</published><updated>2023-07-11T13:00:00+09:00</updated><id>http://localhost:4000/linear-algebra/Inner_Product</id><content type="html" xml:base="http://localhost:4000/linear-algebra/inner-product"><![CDATA[<h3 id="concept-of-inner-product"><code class="language-plaintext highlighter-rouge">ğŸ’¡Â Concept of Inner Product</code></h3>

\[a^Tb = ||a||â€¢||b||cos\theta\]

<p>ë‚´ì ì€ <code class="language-plaintext highlighter-rouge">Inner Product</code>, <code class="language-plaintext highlighter-rouge">Dot Product</code>, <code class="language-plaintext highlighter-rouge">Scalar Product</code>ë¡œ ë¶ˆë¦¬ë©° ë‘ ë²¡í„°ì˜ ìœ ì‚¬ë„, ì¦‰ ë‹®ì€ ì •ë„ë¥¼ êµ¬í•˜ëŠ”ë° ì‚¬ìš©ë˜ëŠ” ë²¡í„°â€¢í–‰ë ¬ ì—°ì‚°ì˜ í•œ ì¢…ë¥˜ë‹¤. ë‘ ë²¡í„°ì˜ ì •ì‚¬ì˜ê³¼ë„ ë™ì¼í•œ ê°œë…ìœ¼ë¡œ ì‚¬ìš©ëœë‹¤. ìœ„ ìˆ˜ì‹ì˜ ìš°ë³€ì— ì£¼ëª©í•´ë³´ì. 
$||a||cos\theta$ ëŠ” ë²¡í„° $a$ë¥¼ ë²¡í„° $b$ì— ì •ì‚¬ì˜ ë‚´ë¦° í¬ê¸°ë¡œ í•´ì„í•  ìˆ˜ ìˆë‹¤. í•œí¸ $||b||$ ëŠ” ë²¡í„° $b$ì˜ ê¸¸ì´ì´ë¯€ë¡œ, ê²°êµ­ ë‚´ì ì´ë€ í•œ ë²¡í„°ë¥¼ ë‹¤ë¥¸ ë²¡í„°ì— ì •ì‚¬ì˜ í•´ì¤€ ê²°ê³¼ì™€ ë²¡í„° í¬ê¸°ì˜ ê³±ì´ ëœë‹¤.</p>

<p align="center">
<img src="/assets/images/inner_product.png" alt="Inner Product Image" class="align-center image-caption" width="50%&quot;, height=&quot;50%" />
<strong><em><a href="https://wikidocs.net/22384">Inner Product Image</a></em></strong>
</p>

<p>ë‚´ì ì„ ê¸°í•˜í•™ì ìœ¼ë¡œ ìƒê°í•´ë³´ì. $\theta$ëŠ” ë‘ ë²¡í„° ì‚¬ì´ì˜ ë¼ì¸ê°ì´ë‹¤. ê·¸ë ‡ë‹¤ë©´ ë¼ì¸ê°ê³¼ ë‚´ì ì˜ í¬ê¸° ì‚¬ì´ì˜ ìƒê´€ê´€ê³„ëŠ” ì–´ë–»ê²Œ ë ê¹Œ?? ë‚´ì ì˜ ì˜ë¯¸ëŠ” ì„œë¡œ ê°™ì€ ì •ë„ê°€ ì•„ë‹ˆë¼ <code class="language-plaintext highlighter-rouge">â€œì„œë¡œ ë‹®ì€ ì •ë„â€</code>ë¼ê³  í–ˆë‹¤. ì¤‘í•™êµ ë•Œ ë°°ì› ë˜ ë‹®ìŒ ê°œë…ì„ ë– ì˜¬ë ¤ë³´ì. ë‹®ìŒì´ë€ ìœ í´ë¦¬ë“œ ê³µê°„ì—ì„œ ëª¨ë“  ê°ì„ ë³´ì¡´í•˜ë©° ëª¨ë“  ê±°ë¦¬ë¥¼ ì¼ì •í•œ ë¹„ìœ¨ë¡œ í™•ëŒ€ ë˜ëŠ” ì¶•ì†Œì‹œí‚¤ëŠ” ì•„í•€ ë³€í™˜ì´ë‹¤. ë‹¤ì‹œ ë§í•´, ì–´ë–¤ ë‘ ë„í˜•ì„ ë‹®ì•˜ë‹¤ê³  ë§í•˜ëŠ”ë° ì ˆëŒ€ì ì¸ ê±°ë¦¬ í˜¹ì€ ê¸¸ì´ê°€ ê°™ì„ í•„ìš”ê°€ ì—†ë‹¤. ê·¸ë˜ì„œ ë‘ ë²¡í„°ì˜ ë‹®ì€ ì •ë„ë¥¼ íŒŒì•…í•˜ë ¤ë©´ ìš°ë¦¬ëŠ” ë²¡í„°ì˜ ê¸¸ì´ ëŒ€ì‹  ë°©í–¥ì´ë¼ëŠ” ë¬¼ë¦¬ëŸ‰ì— ì£¼ëª©í•´ì•¼ í•œë‹¤. ë²¡í„°ëŠ” ì§ì„ ìœ¼ë¡œ í‘œí˜„ë˜ê¸° ë•Œë¬¸ì— ë‘ ë²¡í„°ê°€ ì™„ì „íˆ ë‹®ì•˜ë‹¤ê³  ë§í•˜ë ¤ë©´, ë¼ì¸ê°ì˜ í¬ê¸°ê°€ 0ì´ ë˜ì–´ì•¼ í•œë‹¤. ë”°ë¼ì„œ ë¼ì¸ê°ì˜ í¬ê¸°ê°€ ì‘ì•„ì§ˆìˆ˜ë¡ ë‘ ë²¡í„°ì˜ ë‹®ì€ ì •ë„ëŠ” ì»¤ì§€ê²Œ ë˜ê³ , $\theta=0$ ì—ì„œ ë‚´ì ê°’ì€ ìµœëŒ€ê°€ ëœë‹¤. ë§Œì•½ $\theta=90$ ì´ë¼ë©´ ë‚´ì ê°’ì€ ì–´ë–»ê²Œ ë ê¹Œ?? ì‚¼ê°ë¹„ ì •ì˜ì— ì˜í•´ $cos \theta = 0$ ì´ ë  ê²ƒì´ë‹¤. ë”°ë¼ì„œ ë‚´ì ê°’ì€ 0ì´ ë˜ê³ , ë‘ ë²¡í„°ëŠ” ì„œë¡œ ì „í˜€ ë‹®ì§€ ì•Šì•˜ë‹¤ê³  íŒë‹¨í•  ìˆ˜ ìˆë‹¤. í•œí¸ $\theta=180$ ì¼ ë•Œ ë‚´ì ê°’ì€ ìµœì†Œê°€ ë˜ê³ , ë‘ ë²¡í„°ëŠ” ìŒì˜ ë°©í–¥ìœ¼ë¡œ ë‹®ì€ ìƒíƒœë¥¼ ê°–ëŠ”ë‹¤.</p>

\[N_a=\frac{a}{\sqrt{a^Ta}} = \frac{a}{||a||}\]

<p>í•œí¸ ë‚´ì ì„ ë²¡í„° ì •ê·œí™”ì—ë„ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ”ë°, ë°©ë²•ì€ ìœ„ ìˆ˜ì‹ê³¼ ê°™ë‹¤. ì¼ë°˜ì ìœ¼ë¡œ ë²¡í„°ë¥¼ ì •ê·œí™”í•˜ëŠ” ë°©ë²•ì€ ë²¡í„°ë¥¼ ë²¡í„°ì˜ í¬ê¸°ë¡œ ë‚˜ëˆ„ë©´ ëœë‹¤ê³ ë§Œ ì•Œê³  ìˆì„ ê²ƒì´ë‹¤. í•˜ì§€ë§Œ ë²¡í„°ì˜ ì „ì¹˜ì™€ ë²¡í„°ì™€ì˜ ë‚´ì ìœ¼ë¡œë„ ë²¡í„°ì˜ í¬ê¸°ë¥¼ êµ¬í•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì— (ë²¡í„°ì˜ ì „ì¹˜ì™€ ë²¡í„°ëŠ” $cos\theta=0$ì´ ë˜ê¸° ë•Œë¬¸) ìœ„ ë“±ì‹ì´ ì„±ë¦½í•œë‹¤. í•œí¸, ë²¡í„° ì •ê·œí™” ê²°ê³¼ëŠ” ë²¡í„°ì˜ ê¸¸ì´ê°€ 1ì´ ë˜ê¸° ë•Œë¬¸ì— <code class="language-plaintext highlighter-rouge">â€˜ë‹¨ìœ„ ë²¡í„°â€™</code> ë¼ê³  ì •ì˜í•œë‹¤. ë²¡í„°ì˜ ê¸¸ì´ê°€ 1ì´ë¼ëŠ” ì ì„ ì´ìš©í•˜ë©´, ë‹¨ìœ„ ë²¡í„°ì—ëŠ” ë°©í–¥ì— ëŒ€í•œ ë¬¼ë¦¬ëŸ‰ë§Œ ë‚¨ì•„ ìˆë‹¤ëŠ” ì‚¬ì‹¤ì„ ì•Œ ìˆ˜ ìˆë‹¤. ê·¸ë˜ì„œ ìš°ë¦¬ê°€ ì–´ë–¤ ë²¡í„°ì˜ ë°©í–¥ ì •ë³´ë¥¼ ì–»ê³  ì‹¶ì„ ë•Œ, ë²¡í„°ì˜ ì •ê·œí™”ë¥¼ ì‚¬ìš©í•˜ë©´ ê°„ë‹¨í•˜ê²Œ êµ¬í•  ìˆ˜ ìˆë‹¤.</p>

\[\frac{b^Ta}{||b||} * \frac{b}{||b||} = \frac{b^Ta}{\sqrt{b^Tb}} * \frac{b}{\sqrt{b^Tb}} = \frac{b^Ta}{b^Tb}*b\]

<p>ë‚´ì  ê³µì‹ê³¼ ë²¡í„° ì •ê·œí™” ê³µì‹ì„ í•¨ê»˜ ì‚¬ìš©í•˜ë©´ ë²¡í„° $a$ë¥¼ ë²¡í„° $b$ì— ì •ì‚¬ì˜ ë‚´ë ¸ì„ ë•Œ ë„ì¶œë˜ëŠ” ë²¡í„° ë˜í•œ ì§ì ‘ êµ¬í•  ìˆ˜ ìˆë‹¤. ìœ„ ìˆ˜ì‹ì„ ì‚´í´ë³´ì. ê° ë³€ì˜ ì¢Œì¸¡ í•­ì€ ë‚´ì  ê³µì‹ì— ì˜í•´ 
$||a||\cos\theta$ ê°€ ëœë‹¤. ì§€ê¸ˆ ìš°ë¦¬ê°€ êµ¬í•˜ê³  ì‹¶ì€ ê²ƒì€ ì •ì‚¬ì˜ ë‚´ë¦° ë²¡í„° ìì²´ì¸ë°, $||a||\cos\theta$ ì€ í¬ê¸°ì— ëŒ€í•œ ë¬¼ë¦¬ëŸ‰ë§Œ ë‹´ê³  ìˆì„ë¿ ë°©í–¥ì— ëŒ€í•œ ì •ë³´ê°€ ë‹´ê²¨ ìˆì§€ ì•Šë‹¤. ì•ì—ì„œ ì–¸ê¸‰í–ˆë“¯ì´, ë²¡í„° ì •ê·œí™” ê²°ê³¼ëŠ” í•´ë‹¹ ë²¡í„°ì˜ ë°©í–¥ì— ëŒ€í•œ ë¬¼ë¦¬ëŸ‰ì„ ì˜ë¯¸í•œë‹¤. ë”°ë¼ì„œ ìš°ë³€ì— ë²¡í„° $b$ ì˜ ë°©í–¥(ì •ê·œí™” ê²°ê³¼)ë¥¼ ê³±í•˜ê²Œ ë˜ëŠ” ê²ƒì´ë‹¤.</p>

\[(a - \hat{x}â€¢b)^Tâ€¢\ b\hat{x} = 0 \\
\hat{x} = \frac{b^Ta}{b^Tb} \\
\hat{x}â€¢b = \frac{b^Ta}{b^Tb}*b\]

<p>í•œí¸, ì •ì‚¬ì˜ ë‚´ë¦° ë²¡í„°(ë²¡í„° 
$\frac{a^Tb}{b^Tb}*b$)ë¥¼ ê¸°ì¤€ ë²¡í„°(ë²¡í„° $b$)ì˜ ìŠ¤ì¹¼ë¼ë°° í•´ì¤€ ë²¡í„°ë¡œ ìƒê°í•´ë„ ì‰½ê²Œ êµ¬í•  ìˆ˜ ìˆë‹¤. ìŠ¤ì¹¼ë¼ë¥¼ ë¯¸ì§€ìˆ˜ë¡œ ë‘ê³  ì •ì‚¬ì˜ ë‚´ë¦° ë²¡í„°ë¥¼ $bâ€¢\hat{x}$ë¼ê³  ì •ì˜í•˜ë©´ ìš°ë¦¬ëŠ” ë²¡í„° $a$ë¥¼ ë¹—ë³€, ì •ì‚¬ì˜ ë‚´ë¦° ë²¡í„°ë¥¼ ë°‘ë³€ìœ¼ë¡œ í•˜ëŠ” ì§ê°ì‚¼ê°í˜•ì˜ ë†’ì´ë¥¼ êµ¬í•  ìˆ˜ ìˆê²Œ ëœë‹¤. ì •ì‚¬ì˜ ë‚´ë¦° ë²¡í„° $bâ€¢\hat{x}$ì— ë§ˆì´ë„ˆìŠ¤ ë¶€í˜¸ë¥¼ ì·¨í•´ ë°˜ëŒ€ ë°©í–¥ìœ¼ë¡œ ë’¤ì§‘ì–´ì£¼ë©´ ìš°ë¦¬ëŠ” ì§ê°ì‚¼ê°í˜•ì˜ ë†’ì´ë¥¼ ë°‘ë³€ê³¼ ë¹—ë³€ì˜ í•©($a - bâ€¢\hat{x}$)ìœ¼ë¡œ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆê¸° ë•Œë¬¸ì´ë‹¤. ë”°ë¼ì„œ ë°‘ë³€ê³¼ ë†’ì´ì˜ ë¼ì¸ê°ì´ ìˆ˜ì§ì´ë¼ëŠ” ì ì„ ë‚´ì  ê³µì‹ì— ì ìš©í•´ ìš°ë¦¬ëŠ” ë‹¤ìŒ ìˆ˜ì‹ì„ í’€ì–´ë‚´ë©´ ì •ì‚¬ì˜ ë‚´ë¦° ë²¡í„°ë¥¼ ì–»ì„ ìˆ˜ ìˆë‹¤. ì •ì‚¬ì˜ ë‚´ë¦° ë²¡í„°ë¥¼ ì‹¤ì œë¡œëŠ” <code class="language-plaintext highlighter-rouge">íˆ¬ì˜ ë²¡í„°</code>ë¼ê³  ì •ì˜í•œë‹¤.</p>

<h3 id="-what-is-projection-matrix"><code class="language-plaintext highlighter-rouge">ğŸ”¢ What is Projection Matrix</code></h3>

<p>ë²¡í„° $a$ì™€ ë²¡í„° $b$ì˜ ë³€í™”ì— ë”°ë¥¸ íˆ¬ì˜ ë²¡í„° í¬ê¸°ì˜ ì¶”ì´ë¥¼ ì‚´í´ë³´ì. ë²¡í„° $a$ê°€ ë§Œì•½ 2ë°° ì»¤ì§„ë‹¤ë©´ $a$ê°€ ë¶„ìì—ë§Œ ìˆê¸° ë•Œë¬¸ì— íˆ¬ì˜ ë²¡í„° ì—­ì‹œ ê·¸ëŒ€ë¡œ 2ë°° ì¦ê°€í•  ê²ƒì´ë‹¤. ë°˜ë©´ ë²¡í„° $b$ëŠ” 2ë°°ê°€ ì»¤ì ¸ë„ ë¶„ìâ€¢ë¶„ëª¨ ëª¨ë‘ ë™ì¼í•˜ê²Œ 2ê°œì”©  $b$ê°€ ìˆê¸° ë•Œë¬¸ì— ì´ì „ê³¼ ë³€í™”ê°€ ì—†ë‹¤. ë”°ë¼ì„œ íˆ¬ì˜ ë²¡í„°ì˜ í¬ê¸°ëŠ” ì „ì ìœ¼ë¡œ ë²¡í„° $a$ì— ì˜ì¡´ì ì´ë‹¤. ë‹¤ì‹œ ë§í•´, ë²¡í„° $a$ëŠ” ë¬´ì–¸ê°€ ë§¤ê°œì²´ì— ì˜í•´ ë²¡í„° $b$ë¡œ ì •ì‚¬ì˜ ë˜ê³  ìˆìœ¼ë©° ê·¸ ë§¤ê°œì²´ë¥¼ ìš°ë¦¬ëŠ” <code class="language-plaintext highlighter-rouge">projection matrix</code> ë¼ê³  í•œë‹¤.</p>

\[P = \frac{bb^T}{b^Tb}\]

<p>ë¶„ëª¨ëŠ” í–‰ë²¡í„°ì™€ ì—´ë²¡í„°ì˜ ë‚´ì ì´ë¼ì„œ ìŠ¤ì¹¼ë¼(ìƒìˆ˜), ë¶„ìëŠ” ì—´ë²¡í„°ì™€ í–‰ë²¡í„°ì˜ ê³±ì´ë¼ì„œ í–‰ë ¬ í˜•íƒœê°€ ë  ê²ƒì´ë‹¤. ë”°ë¼ì„œ ëì— <code class="language-plaintext highlighter-rouge">Matrix</code> ë¼ëŠ” ë‹¨ì–´ë¥¼ ë¶™ì´ê²Œ ë˜ì—ˆë‹¤. ì´ëŸ¬í•œ <code class="language-plaintext highlighter-rouge">Projection Matrix</code>ëŠ” $n$ì°¨ì› ë²¡í„°ì—ë„ ë™ì¼í•˜ê²Œ ì ìš©í•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì— í›—ë‚  ì°¨ì›ì¶•ì†Œ í˜¹ì€ ì„ í˜•ë³€í™˜ ê°™ì€ í…Œí¬ë‹‰ìœ¼ë¡œ ë¨¸ì‹ ëŸ¬ë‹, ë”¥ëŸ¬ë‹ì—ì„œ ìœ ìš©í•˜ê²Œ ì‚¬ìš©ëœë‹¤.</p>

<h3 id="ï¸-least-square-method"><code class="language-plaintext highlighter-rouge">ğŸ–ï¸ Least Square Method</code></h3>

\[Measurement = Ax + n\]

<p>$(a - \hat{x}â€¢b)^T$
ë¥¼ ì—ëŸ¬ ë²¡í„° $e$ë¡œ ì¹˜í™˜í•˜ë©´ ë‚´ì â€¢ì •ì‚¬ì˜ì„ <code class="language-plaintext highlighter-rouge">Least Square Method</code> (ìµœì†Œ ììŠ¹ë²•)ìœ¼ë¡œ í•´ì„í•  ìˆ˜ë„ ìˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ 5ì°¨ì›ì˜ ê³µê°„ì—ì„œ 2ì°¨ì›ì˜ ê³µê°„ìœ¼ë¡œ <code class="language-plaintext highlighter-rouge">span</code>í•˜ëŠ” í–‰ë ¬ $A$(<code class="language-plaintext highlighter-rouge">[5x2]</code>, <code class="language-plaintext highlighter-rouge">full column rank</code>) ê·¸ë¦¬ê³  5ì°¨ì› ê³µê°„ì—ì„œ ì •ì˜ë˜ëŠ” ë²¡í„° $b$ê°€ ìˆë‹¤ê³  ê°€ì •í•´ë³´ì. í˜„ì¬ ìƒí™©ì€ $C(A)$ì™€ ë²¡í„° $b$ ì‚¬ì´ì˜ í•´ê°€ ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ìƒíƒœ, ì¦‰ ëª¨ë¸ì˜ ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œ ì •ë‹µì´ ì¼ì¹˜í•˜ì§€ ì•ŠëŠ”ë‹¤ëŠ” ê²ƒì´ë‹¤. ë¹„ë¡ í•´ê°€ ì—†ì§€ë§Œ ì˜ˆì¸¡ê°’ì„ ê·¸ë˜ë„ ìµœëŒ€í•œ ì •ë‹µì— ê°€ê¹ê²Œ ìœ„ì¹˜ ì‹œì¼œë³´ìëŠ” ì·¨ì§€ì—ì„œ ë‚˜ì˜¨ ê²ƒì´ ë°”ë¡œ <code class="language-plaintext highlighter-rouge">ìµœì†Œ ììŠ¹ë²•</code>ì´ë‹¤. ìµœì†ŒììŠ¹ë²•ì€ $||e||$ë¥¼ ëª¨ë¸ì˜ ì˜ˆì¸¡ê³¼ ì •ë‹µ ì‚¬ì´ ì˜¤ì°¨ë¡œ ì •ì˜í•˜ëŠ”ë°, ì´ ë•Œ $L_2$ì˜ ì œê³±ê·¼ ì—°ì‚°ì„ í”¼í•˜ê¸° ìœ„í•´ì„œ $||e||^2$ì„ ìµœì í™” í•œë‹¤. ì˜¤ì°¨ì˜ ì œê³±ì„ ìµœì†Œí™”í•œë‹¤ëŠ” ë™ì‘ ë°©ì‹ì— ë§ê²Œ <code class="language-plaintext highlighter-rouge">Least Square</code> ë¼ëŠ” ì´ë¦„ì„ ê°–ê²Œ ë˜ì—ˆë‹¤.<br />
í•œí¸, ì´ ì˜¤ì°¨ë¥¼ ìµœëŒ€í•œ ì¤„ì´ëŠ” ë°©ë²•ì€ ë¬´ì—‡ì¼ê¹Œ?? $C(A)$ ìƒì—ì„œ ì •ì˜ë˜ëŠ” ë²¡í„° $b\hat{x}$ì™€ ì—ëŸ¬ ë²¡í„° $e$ ì‚¬ì´ì˜ ë‚´ì (ë‘ ë²¡í„°ê°€ ìˆ˜ì§)ê°’ì´ 0ì¼ ë•Œ ë²¡í„° $e$ì˜ ê¸¸ì´ê°€ ìµœì†Œ ëœë‹¤ëŠ” ì ì„ ì´ìš©í•´ ë¯¸ì§€ìˆ˜ $\hat{x}$ë¥¼ ì°¾ê³  ìµœì í™” ì‹ì— ë„£ì–´ ì˜¤ì°¨ ì œê³±ì´ ìµœì†Œê°€ ë˜ëŠ” ê³„ìˆ˜ë¥¼ êµ¬í•´ì£¼ë©´ ëœë‹¤.</p>

<p>ë§ˆì§€ë§‰ìœ¼ë¡œ <code class="language-plaintext highlighter-rouge">Dot Product</code>, <code class="language-plaintext highlighter-rouge">Scalar Product</code> , <code class="language-plaintext highlighter-rouge">Inner Product</code>ëŠ” ë„“ì€ ì˜ë¯¸ì—ì„œ ëª¨ë‘ ë‚´ì ì— í¬í•¨ëœë‹¤. í•˜ì§€ë§Œ ë¯¸ì„¸í•œ ì˜ë¯¸ ì°¨ì´ëŠ” ìˆë‹¤. í•˜ë‚˜ í•˜ë‚˜ ì‚´í´ë³´ì. ë¨¼ì € <code class="language-plaintext highlighter-rouge">Dot Product</code>ë€, ë‚´ì ì„ ìœ í´ë¦¬ë“œ ì¢Œí‘œê³„ì—ì„œ ì •ì˜í•  ë•Œ ì‚¬ìš©ë˜ëŠ” ëª…ì¹­ì´ë©°, ì—°ì‚° ê¸°í˜¸ê°€ ì ê³±ì´ë¼ëŠ” ì ì„ ê°•ì¡°í•˜ê¸° ìœ„í•´ <code class="language-plaintext highlighter-rouge">Dot</code> ì´ë¼ëŠ” ë‹¨ì–´ë¥¼ ì‚¬ìš©í•˜ê²Œ ë˜ì—ˆë‹¤. í•œí¸, <code class="language-plaintext highlighter-rouge">Scalar Product</code> ì—­ì‹œ ë‚´ì ì„ ìœ í´ë¦¬ë“œ ì¢Œí‘œê³„ì—ì„œ ì •ì˜í•  ë•Œ ì‚¬ìš©í•˜ëŠ” ëª…ì¹­ì´ì§€ë§Œ, ê·¸ ê²°ê³¼ê°€ ìŠ¤ì¹¼ë¼ ê°’ì´ë¼ëŠ” ì ì„ ê°•ì¡°í•˜ê¸° ìœ„í•´ <code class="language-plaintext highlighter-rouge">Scalar Product</code>ë¼ê³  ëª…ëª…í–ˆë‹¤. ë§ˆì§€ë§‰ìœ¼ë¡œ <code class="language-plaintext highlighter-rouge">Inner Product</code>ëŠ” ë²¡í„° ê³µê°„ì—ì„œ ì •ì˜ ë˜ì–´ í–‰ë ¬ ê°™ì€ ë‹¤ë¥¸ ê°œì²´ë“¤ì— ëŒ€í•´ì„œ í™•ì¥ ì ìš©ì´ ê°€ëŠ¥í•˜ë‹¤ëŠ” ì ì—ì„œ <code class="language-plaintext highlighter-rouge">Dot Product</code>  , <code class="language-plaintext highlighter-rouge">Scalar Product</code> ë³´ë‹¤ ë” ì¼ë°˜ì ì¸ ê°œë…ìœ¼ë¡œ ë³¼ ìˆ˜ ìˆê² ë‹¤.</p>]]></content><author><name>qcqced</name><email>qcqced123@gmail.com</email></author><category term="Linear Algebra" /><category term="Linear Algebra" /><category term="Inner Product" /><category term="Projection Matrix" /><category term="ë‚´ì " /><category term="ì •ì‚¬ì˜" /><summary type="html"><![CDATA[ğŸ’¡ Concept & Insight of Inner Product]]></summary></entry><entry><title type="html">ğŸ“ Lp-Norm: Concept &amp;amp; Insight</title><link href="http://localhost:4000/linear-algebra/lp-norm" rel="alternate" type="text/html" title="ğŸ“ Lp-Norm: Concept &amp;amp; Insight" /><published>2023-07-04T00:00:00+09:00</published><updated>2023-07-05T13:00:00+09:00</updated><id>http://localhost:4000/linear-algebra/Lp_Norm</id><content type="html" xml:base="http://localhost:4000/linear-algebra/lp-norm"><![CDATA[\[||x||_p = (âˆ‘_{i=1}^n |x_i|^p)^{1/p}\]

<p><strong><code class="language-plaintext highlighter-rouge">Lp-Norm</code></strong>ì€ <code class="language-plaintext highlighter-rouge">Lebesgue</code>ë¼ëŠ” í”„ë‘ìŠ¤ ìˆ˜í•™ìì— ì˜í•´ ê³ ì•ˆëœ ê°œë…ìœ¼ë¡œ<strong>,</strong> ê¸°ê³„í•™ìŠµì„ ê³µë¶€í•˜ëŠ” ì‚¬ëŒì´ë¼ë©´ ì§€ê²¹ë„ë¡ ë“£ëŠ” <code class="language-plaintext highlighter-rouge">L2-Norm</code>, <code class="language-plaintext highlighter-rouge">L1-Norm</code>ì„ ì¼ë°˜í™” ë²„ì „ì´ë¼ê³  ìƒê°í•˜ë©´ ëœë‹¤. <strong><u>ë‹¤ì‹œ ë§í•´, ë²¡í„°ì˜ í¬ê¸°ë¥¼ ë‚˜íƒ€ë‚´ëŠ” í‘œí˜„ì‹ì„ ì¼ë°˜í™”í•œ</u></strong> ê²ƒì´ ë°”ë¡œ <code class="language-plaintext highlighter-rouge">Lp-Norm</code> ì´ë©° ìˆ˜ì‹ì€ ìœ„ì™€ ê°™ë‹¤.</p>

<p><code class="language-plaintext highlighter-rouge">p=1</code>ì´ë¼ê³  ê°€ì •í•˜ê³  ìˆ˜ì‹ì„ ì „ê°œí•´ë³´ì. 
$||x||_1 = (|x_1|^1 + |x_2|^1+ â€¦ + |x_n|^1)^{1/1}$ì´ ëœë‹¤. ìš°ë¦¬ê°€ ì•„ëŠ” <code class="language-plaintext highlighter-rouge">L1-Norm</code> ì˜ ìˆ˜ì‹ê³¼ ë™ì¼í•˜ë‹¤.</p>

<p>ê·¸ë ‡ë‹¤ë©´ <code class="language-plaintext highlighter-rouge">p=2</code>ì¼ ë•Œ ìˆ˜ì‹ì„ ì‚´í´ë³´ì. 
$||x||_2 = (|x_1|^2 + |x_2|^2+ â€¦ + |x_n|^2)^{1/2}$ìœ¼ë¡œ ì „ê°œ ëœë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤. ì—­ì‹œ ìš°ë¦¬ê°€ ë§¨ë‚  ë³´ëŠ” <code class="language-plaintext highlighter-rouge">L2-Norm</code> ê³¼ ë™ì¼í•˜ë‹¤.</p>

<p><code class="language-plaintext highlighter-rouge">L1-Norm</code>ì€ ë§¨í—ˆíŠ¼ ê±°ë¦¬, <code class="language-plaintext highlighter-rouge">L2-Norm</code> ì€ ìœ í´ë¦¬ë“œ ê±°ë¦¬ë¥¼ ì˜ë¯¸í•œë‹¤ëŠ” ê²ƒì€ ìµíˆ ë“¤ì–´ ë´¤ì„ ê²ƒì´ë‹¤. ë§Œì•½ $p=âˆ$ë¼ë©´, ìˆ˜ì‹ì€ ì–´ë–»ê²Œ ë ê¹Œ, ê³¼ì—° ì–´ë–¤ ì˜ë¯¸ë¥¼ ê°–ê³  ìˆì„ê¹Œ??</p>

<p>ì´ì „ê³¼ ë˜‘ê°™ì´ ì „ê°œí•´ë³´ë©´
\(||x||_âˆ = (|x_1|^âˆ + |x_2|^âˆ+ ... + |x_n|^âˆ)^{1/âˆ}\), ì´ë ‡ê²Œ ì‹ì´ ë„ì¶œë  ê²ƒì´ë‹¤. ì´ì œ ê´„í˜¸ ë‚´ë¶€ ì›ì†Œë“¤ì˜ ì§€ìˆ˜ê°€ <code class="language-plaintext highlighter-rouge">ë¬´í•œëŒ€</code>ë¼ëŠ” ì ì— ì£¼ëª©í•´ë³´ì. ì§ê´€ì ìœ¼ë¡œ ë¬´í•œëŒ€ ê°’ë“¤ ì‚¬ì´ì˜ ë§ì…ˆ, ê³±ì…ˆì˜ ê²°ê³¼ëŠ” <code class="language-plaintext highlighter-rouge">ë¬´í•œëŒ€</code> ë¼ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤. ê·¸ë ‡ë‹¤ë©´ ìš°ë¦¬ëŠ” ìœ„ ìˆ˜ì‹ì—ì„œ ì ˆëŒ€ê°’ì´ ê°€ì¥ í° $|x_i|$ë§Œ ë‚¨ê²¨ë„ ì—­ì‹œ ë¬´í•œëŒ€ ê°’ì„ ì–»ì„ ìˆ˜ ìˆë‹¤. <strong><u>ë¬´í•œëŒ€ëŠ” ë¯¸ì§€ìˆ˜ ê°œë…ì— ê°€ê¹ì§€ ì‹¤ì œ ì‹¤ìˆ˜ ê°œë…ì€ ì•„ë‹ˆê¸° ë•Œë¬¸ì´ë‹¤.</u></strong> ë”°ë¼ì„œ ê´„í˜¸ ë‚´ë¶€ì—ëŠ” $|x_i|^p$ ê°’ë§Œ ë‚¨ê²Œ ë˜ê³  ê´„í˜¸ ë°–ì˜ $1/p$ì™€ ë‚¨ì€ ì—°ì‚°ì„ í•´ì£¼ë©´ ê²°êµ­ $|x_i|$ë§Œ ë‚¨ê²Œ ëœë‹¤. ë”°ë¼ì„œ $||x||_âˆ = max(|x_1|, \ |x_2|, \ â€¦ \ , |x_n|)$ê°€ ëœë‹¤.</p>

<p>ì´ì™€ ê°™ì€ ì„±ì§ˆ ë•Œë¬¸ì— <code class="language-plaintext highlighter-rouge">Lp-Norm</code> ì€ <code class="language-plaintext highlighter-rouge">Lp-Pooling</code> ìœ¼ë¡œë„ í•´ì„í•  ìˆ˜ ìˆìœ¼ë©°, ìˆ˜ì‹ì˜ ìš°ë³€ì—  $1/n$ì„ ê³±í•´ì£¼ë©´ <code class="language-plaintext highlighter-rouge">Generalized Mean Pooling</code> ì´ ëœë‹¤ëŠ” ì‚¬ì‹¤ì„ ì•Œ ìˆ˜ ìˆë‹¤. ê²°êµ­ <code class="language-plaintext highlighter-rouge">Norm</code>ê³¼ <code class="language-plaintext highlighter-rouge">Pooling</code> ì€ ê°™ì€ ê°œë…ì´ì—ˆë˜ ê²ƒì´ë‹¤. 
<strong>ê·¸ë˜ì„œ ìœ„ì—ì„œ ì‚´í´ë³¸ $ L_âˆ $ ì—­ì‹œ <code class="language-plaintext highlighter-rouge">Max Pooling</code> ì´ë¼ í•´ì„ì´ ê°€ëŠ¥í•´ì§„ë‹¤.</strong></p>

<p>ì—¬ë‹´ìœ¼ë¡œ ë§¨ì•ì˜ ëŒ€ë¬¸ì Lì€ <code class="language-plaintext highlighter-rouge">Lebesgue</code> ì˜ ì´ë¦„ì—ì„œ ë³¸ë”°ì™”ë‹¤ê³  ì•Œë ¤ì ¸ ìˆë‹¤. ê·¸ë¦¬ê³  ì˜ˆì „ë¶€í„° $L_2$ê°’ì„ ìˆ˜ì‹ìœ¼ë¡œ í‘œí˜„í•  ë•Œ ì™œ ì§ëŒ€ê¸° ë‘ê°œë¥¼ ì‚¬ìš©í• ê¹Œ í•­ìƒ ê¶ê¸ˆí–ˆëŠ”ë°  $L_p$ì™€ <code class="language-plaintext highlighter-rouge">ì¼ë°˜ ì ˆëŒ€ê°’</code>ì„ êµ¬ë¶„í•˜ê¸° ìœ„í•´ ì§ëŒ€ê¸°ë¥¼ ë‘ ê°œ ì‚¬ìš©í•˜ê²Œ ë˜ì—ˆë‹¤ê³  í•œë‹¤.</p>
<p align="center">
<img src="/assets/images/220px-Vector-p-Norms_qtl1.svg.png" alt="Lp-Norm Image" class="align-center image-caption" width="50%&quot;, height=&quot;50%" />
<em>Lp-Norm</em>
</p>

<p>ìœ„ ìë£ŒëŠ” 
$L_p$
normì„ pê°’ ë³€í™” ì¶”ì´ì— ë”°ë¼ ê¸°í•˜í•™ì ìœ¼ë¡œ í‘œí˜„í•œ ê·¸ë¦¼ì´ë‹¤. <code class="language-plaintext highlighter-rouge">p=1</code> ì¼ ë•ŒëŠ” $L_1: |x| + |y| =1$ê°€ ë˜ê¸° ë•Œë¬¸ì— ë§ˆë¦„ëª¨ í˜•íƒœì˜ ì˜ì—­ì„ ê°–ëŠ”ë‹¤. í•œí¸ <code class="language-plaintext highlighter-rouge">p=2</code> ì¼ ë•ŒëŠ” $L_2: x^2 + y^2 =1^2$ê°€ ë˜ê¸° ë•Œë¬¸ì— ì›ì˜ ì˜ì—­ì„ ê°–ëŠ”ë‹¤. $p=âˆ$ ì¼ ë•ŒëŠ” $L_âˆ: max(|x_1|â€¦|x_n|) = 1$ ì´ ë˜ê¸° ë•Œë¬¸ì— ì •ì‚¬ê°í˜• í˜•íƒœì˜ ì˜ì—­ì„ ê°–ê²Œ ë  ê²ƒì´ë‹¤.</p>]]></content><author><name>qcqced</name><email>qcqced123@gmail.com</email></author><category term="Linear Algebra" /><category term="Linear Algebra" /><category term="Norm" /><category term="Pooling" /><summary type="html"><![CDATA[Concept of Lp-Norm & GeM Pool]]></summary></entry></feed>