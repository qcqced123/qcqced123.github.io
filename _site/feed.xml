<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2024-03-16T14:34:55+09:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">AI/Business Study Log</title><subtitle>NLP, Marketing</subtitle><author><name>qcqced</name><email>qcqced123@gmail.com</email></author><entry><title type="html">ğŸŒ† [Linear Attention] Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention</title><link href="http://localhost:4000/nlp/linear_attention" rel="alternate" type="text/html" title="ğŸŒ† [Linear Attention] Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention" /><published>2024-03-14T00:00:00+09:00</published><updated>2024-03-15T02:00:00+09:00</updated><id>http://localhost:4000/nlp/linear_attention</id><content type="html" xml:base="http://localhost:4000/nlp/linear_attention"><![CDATA[<h3 id="overview"><code class="language-plaintext highlighter-rouge">ğŸ”­Â Overview</code></h3>

<p><code class="language-plaintext highlighter-rouge">DistilBERT</code> ëŠ” í—ˆê¹… í˜ì´ìŠ¤ ì—°êµ¬ì§„ì´ 2019ë…„ ë°œí‘œí•œ BERTì˜ ë³€í˜•ìœ¼ë¡œì„œ, On-Device Ai ê°œë°œì„ ëª©í‘œë¡œ ê²½ëŸ‰í™”ì— ì´ˆì ì„ ë§ì¶˜ ëª¨ë¸ì´ë‹¤. GPT, BERTì˜ ë“±ì¥ ì´í›„, NLP ë¶„ì•¼ì—ì„œ ë¹„ì•½ì ì¸ ì„±ëŠ¥ í–¥ìƒì´ ì´ë¤„ì¡ŒìŒì—ë„ ë¶ˆêµ¬í•˜ê³ , í„°ë¬´ë‹ˆ ì—†ëŠ” ëª¨ë¸ ì‚¬ì´ì¦ˆì™€ ì»´í“¨íŒ… ë¦¬ì†ŒìŠ¤ ìš”êµ¬ë¡œ ì¸í•´ ì‹¤ìƒí™œ ì ìš© ê°™ì€ í™œìš©ì„±ì€ ì—¬ì „íˆ í•´ê²°í•´ì•¼í•  ë¬¸ì œë¡œ ë‚¨ì•„ ìˆì—ˆë‹¤. Googleì—ì„œ ë°œí‘œí•œ ì´ˆê¸° <code class="language-plaintext highlighter-rouge">BERT-base-uncased</code> ë§Œ í•´ë„ íŒŒë¼ë¯¸í„°ê°€ 1ì–µ 1ì²œë§Œê°œ ìˆ˜ì¤€ì— ë‹¬í•œë‹¤.</p>

<p>ì´ë¥¼ ë‹¤ì–‘í•œ ë¹„ì¦ˆë‹ˆìŠ¤ ìš”êµ¬ ìƒí™©ì— ì ìš©í•  ìˆ˜ ìˆìœ¼ë ¤ë©´ ìµœì†Œí•œ 8GB ì´ìƒì˜ ê°€ì†ê¸° ì „ìš© RAM ê³µê°„ì„ ìš”êµ¬ë¡œ í•œë‹¤. ì˜¤ëŠ˜ë‚  ê°œì¸ìš© PC í˜¹ì€ ì„œë²„ ì»´í“¨í„°ì˜ ê²½ìš°, 8GB ì´ìƒì˜ VRAMì´ ë‹¬ë¦° GPUê°€ ì¼ë°˜ì ìœ¼ë¡œ íƒ‘ì¬ë˜ê¸° ë•Œë¬¸ì— í¬ê²Œ ë¬¸ì œ ë  ê²ƒ ì—†ëŠ” ìš”êµ¬ì‚¬í•­ì´ì§€ë§Œ, On-Device í™˜ê²½ì—ì„œëŠ” ì´ì•¼ê¸°ê°€ ë‹¬ë¼ì§„ë‹¤. ìµœì‹  í•˜ì´ì—”ë“œ ìŠ¤ë§ˆíŠ¸í°ì¸ Galaxy S24 Ultra, iPhone 15 Proì˜ ê²½ìš° 12GB, 8GBì˜ ë¨ ìš©ëŸ‰ì„ ë³´ìœ í•˜ê³  ìˆë‹¤. ê·¸ë§ˆì €ë„ ëŒ€ë¶€ë¶„ì˜ ì˜¨ë””ë°”ì´ìŠ¤ í™˜ê²½ì€ SoC êµ¬ì¡°ë¥¼ ì±„íƒí•˜ê³  ìˆê¸° ë•Œë¬¸ì— ì „ìš© ê°€ì†ê¸°ê°€ ì˜¨ì „íˆ ì € ëª¨ë“  ë¨ ê³µê°„ì„ í™œìš©í•  ìˆ˜ ì—†ë‹¤.</p>

<p>ë”°ë¼ì„œ ì˜¨ë””ë°”ì´ìŠ¤ì— Aië¥¼ ì ìš©í•˜ê¸° ìœ„í•´ì„œëŠ” íšê¸°ì ì¸ ëª¨ë¸ ê²½ëŸ‰í™”ê°€ í•„ìš”í•œ ìƒí™©ì´ê³  ê·¸ ì¶œë°œì ì´ ëœ ì—°êµ¬ê°€ ë°”ë¡œ <code class="language-plaintext highlighter-rouge">DistilBERT</code>ë‹¤. ë¡œì»¬ ë””ë°”ì´ìŠ¤ í™˜ê²½ì—ì„œë„ ì–¸ì–´ ëª¨ë¸ì„ í™œìš©í•˜ê¸° ìœ„í•´ í—ˆê¹… í˜ì´ìŠ¤ ì—°êµ¬ì§„ì€ ì§€ì‹ ì¦ë¥˜ ê¸°ë²•ì„ í™œìš©í•´ ì¸ì½”ë” ê¸°ë°˜ ì–¸ì–´ ëª¨ë¸ì˜ íŒŒë¼ë¯¸í„°ë¥¼ íšê¸°ì ìœ¼ë¡œ ì¤„ì´ëŠ”ë° ì„±ê³µí•œë‹¤.</p>

<p>ì •ë¦¬í•˜ìë©´, <code class="language-plaintext highlighter-rouge">DistilBERT</code> ëª¨ë¸ì€ ê¸°ì¡´ BERTì˜ êµ¬ì¡°ì  ì¸¡ë©´ ê°œì„ ì´ ì•„ë‹Œ, ì‚¬ì „í•™ìŠµ ë°©ë²• íŠ¹íˆ ê²½ëŸ‰í™”ì— ì´ˆì ì„ ë§ì¶˜ ì‹œë„ë¼ê³  ë³¼ ìˆ˜ ìˆë‹¤. ë”°ë¼ì„œ ì–´ë–¤ ëª¨ë¸ì´ë”ë¼ë„, ì¸ì½”ë” ì–¸ì–´ ëª¨ë¸ì´ë¼ë©´ ëª¨ë‘ <code class="language-plaintext highlighter-rouge">DistilBERT</code> êµ¬ì¡°ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìœ¼ë©°, ê¸°ì¡´ ë…¼ë¬¸ì—ì„œëŠ” ì›ë³¸ BERT êµ¬ì¡°ë¥¼ ì‚¬ìš©í–ˆë‹¤. ì´ë²ˆ í¬ìŠ¤íŒ…ì—ì„œë„ BERT êµ¬ì¡°ì— ëŒ€í•œ ì„¤ëª… ëŒ€ì‹ , <code class="language-plaintext highlighter-rouge">DistilBERT</code>ì˜ ì‚¬ì „ í•™ìŠµ ë°©ë²•ë¡ ì¸ <code class="language-plaintext highlighter-rouge">Knowledge Distillation</code>ì— ëŒ€í•´ì„œë§Œ ë‹¤ë£¨ë ¤ê³  í•œë‹¤.</p>

<h3 id="knowledge-distillations"><code class="language-plaintext highlighter-rouge">ğŸŒ†Â Knowledge Distillations</code></h3>

\[\min_{\theta}\sum_{x \in X} \alpha \mathcal{L}_{\text{KL}}(x, \theta) + \beta \mathcal{L}_{\text{MLM}}(x, \theta) + \gamma \mathcal{L}_{\text{Cos}}(x, \theta)\]

<p><code class="language-plaintext highlighter-rouge">DistilBERT</code>ëŠ” Teacher-Student Architectureë¥¼ ì°¨ìš©í•´ ìƒëŒ€ì ìœ¼ë¡œ ì‘ì€ íŒŒë¼ë¯¸í„° ì‚¬ì´ì¦ˆë¥¼ ê°–ëŠ” <code class="language-plaintext highlighter-rouge">Student</code> ëª¨ë¸ì—ê²Œ <code class="language-plaintext highlighter-rouge">Teacher</code>ì˜ ì§€ì‹ì„ ì „ìˆ˜í•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•œë‹¤. ë”°ë¼ì„œ <code class="language-plaintext highlighter-rouge">Teacher</code> ëª¨ë¸ì€ ì´ë¯¸ ì‚¬ì „ í•™ìŠµì„ ë§ˆì¹˜ê³  ìˆ˜ë ´ëœ ìƒíƒœì˜ ê°€ì¤‘ì¹˜ë¥¼ ê°–ê³  ìˆëŠ” ëª¨ë¸ì„ ì‚¬ìš©í•´ì•¼ í•œë‹¤. ë”ë¶ˆì–´ Teacher ëª¨ë¸ì€ êµ¬ì¡°ë§Œ ê¸°ì¡´ BERTë¥¼ ë”°ë¥´ë˜, ì‚¬ì „ í•™ìŠµ ë°©ì‹ì€ RoBERTaì˜ ë°©ì‹ê³¼ ë™ì¼(NSP ì œê±°, Dynamic Masking ì ìš©)í•˜ê²Œ í›ˆë ¨ë˜ì–´ì•¼ í•œë‹¤.</p>

<p>í•œí¸, <code class="language-plaintext highlighter-rouge">Student</code> ëª¨ë¸ì€ <code class="language-plaintext highlighter-rouge">Teacher</code>ì˜ 60%ì •ë„ íŒŒë¼ë¯¸í„° ì‚¬ì´ì¦ˆë¥¼ ê°–ë„ë¡ ì¶•ì†Œí•˜ì—¬ ì‚¬ìš©í•œë‹¤. ì´ ë•Œ ì¶•ì†ŒëŠ” ëª¨ë¸ì˜ <code class="language-plaintext highlighter-rouge">depth</code>(ë ˆì´ì–´ ê°œìˆ˜)ì—ë§Œ ì ìš©í•˜ëŠ”ë°, ì—°êµ¬ì§„ì— ë”°ë¥´ë©´ <code class="language-plaintext highlighter-rouge">width</code>(ì€ë‹‰ì¸µ í¬ê¸°)ëŠ” ì¶•ì†Œë¥¼ ì ìš©í•´ë„ ì—°ì‚° íš¨ìœ¨ì´ ì¦ê°€í•˜ì§€ ì•ŠëŠ”ë‹¤ê³  í•œë‹¤. ì •ë¦¬í•˜ë©´ <code class="language-plaintext highlighter-rouge">Teacher</code> ëª¨ë¸ì˜ <code class="language-plaintext highlighter-rouge">ë ˆì´ì–´ ê°œìˆ˜*0.6</code>ì˜ ê°œìˆ˜ë§Œí¼ ì¸ì½”ë”ë¥¼ ìŒ“ìœ¼ë©´ ëœë‹¤ëŠ” ê²ƒì´ë‹¤.</p>

<p>ê·¸ë¦¬ê³  ìµœëŒ€í•œ <code class="language-plaintext highlighter-rouge">Teacher</code>ì˜ ì§€ì‹ì„ ì „ìˆ˜í•´ì•¼ í•˜ê¸° ë•Œë¬¸ì—, ë°ì´í„°ëŠ” <code class="language-plaintext highlighter-rouge">Teacher</code> ë¥¼ ìˆ˜ë ´ì‹œí‚¨ ê²ƒê³¼ ë™ì¼í•œ ì„¸íŠ¸ë¥¼ ì´ìš©í•´ì•¼ í•œë‹¤. ì´ ë•Œ, Teacher ëª¨ë¸ì€ ì´ë¯¸ MLE ë°©ì‹ìœ¼ë¡œ í›ˆë ¨ì´ ëœ ìƒíƒœë¼ì„œ ë¡œì§“ì´ ë‹¨ì¼ í† í° í•˜ë‚˜ ìª½ìœ¼ë¡œ ì ë ¤ ìˆì„ ê°€ëŠ¥ì„±ì´ ë§¤ìš° ë†’ë‹¤. ì´ëŠ” <code class="language-plaintext highlighter-rouge">Student</code> ëª¨ë¸ì˜ ì¼ë°˜í™” ëŠ¥ë ¥ì— ì•…ì˜í–¥ì„ ë¯¸ì¹  ê°€ëŠ¥ì„±ì´ ë†’ë‹¤. ë”°ë¼ì„œ Temperature ë³€ìˆ˜ $T$ ë„ì…í•´ ì†Œí”„íŠ¸ ë§¥ìŠ¤(ë¡œì§“)ì˜ ë¶„í¬ë¥¼ í‰íƒ„í™” í•œë‹¤. ì´ë ‡ê²Œ í•˜ë©´, <code class="language-plaintext highlighter-rouge">argmax()</code> ê°€ ì•„ë‹Œ ë‹¤ë¥¸ í† í° í‘œí˜„ì— ëŒ€í•´ì„œë„ <code class="language-plaintext highlighter-rouge">Student</code> ëª¨ë¸ì´ ì§€ì‹ì„ ìŠµë“í•  ìˆ˜ ìˆì–´ì„œ í’ë¶€í•œ ë¬¸ë§¥ì„ í•™ìŠµí•˜ê³  ì¼ë°˜í™” ëŠ¥ë ¥ì„ ë†’ì´ëŠ”ë° ë„ì›€ì´ ëœë‹¤. ì´ë¥¼ <code class="language-plaintext highlighter-rouge">ì•”í‘ ì§€ì‹(Dark Knowledge)</code> ì„ í™œìš©í•œë‹¤ê³  í‘œí˜„í•œë‹¤. Temperature ë³€ìˆ˜ $T$ ë„ì…í•œ ì†Œí”„íŠ¸ë§¥ìŠ¤ í•¨ìˆ˜ ìˆ˜ì‹ì€ ì•„ë˜ì™€ ê°™ë‹¤.</p>

\[\text{softmax}(x_i) = \frac{e^{\frac{x_i}{\tau}}}{\sum_{j} e^{\frac{x_j}{\tau}}}\]

<p>ìˆ˜ì‹ìƒ ë³€ìˆ˜ $T$ì˜ ê°’ì„ 1ì´ìƒìœ¼ë¡œ ì„¸íŒ…í•´ì•¼ í‰íƒ„í™”ë¥¼ í•  ìˆ˜ ìˆë‹¤. ë”°ë¼ì„œ ì—°êµ¬ì§„ì€ $T =2$ ë¡œ ë‘ê³  ì‚¬ì „ í•™ìŠµì„ ì§„í–‰í–ˆë‹¤(ë…¼ë¬¸ì— ê³µê°œì•ˆë¨, GitHubì— ìˆìŒ). ì´ë²ˆ íŒŒíŠ¸ ë§¨ ì²˜ìŒì— ë“±ì¥í•œ ìˆ˜ì‹ì„ ë‹¤ì‹œ ë³´ì. ê²°êµ­ <code class="language-plaintext highlighter-rouge">DisilBERT</code>ì˜ ëª©ì í•¨ìˆ˜ëŠ” 3ê°€ì§€ ì†ì‹¤ì˜ ê°€ì¤‘í•©ìœ¼ë¡œ êµ¬ì„±ëœë‹¤. ì´ì œë¶€í„°ëŠ” ê°œë³„ ì†ì‹¤ì— ëŒ€í•´ì„œ ìì„¸íˆ ì‚´í´ë³´ì.</p>

<h4 id="distillation-loss-kl-divergence-loss"><code class="language-plaintext highlighter-rouge">ğŸŒ†Â Distillation Loss: KL-Divergence Loss</code></h4>

\[\text{KL-Divergence}(P || Q) = \sum_{i} P(i) \log \frac{P(i)}{Q(i)}\]

<p>ì¦ë¥˜ ì†ì‹¤ë¡œ ì‚¬ìš©ë˜ëŠ” <code class="language-plaintext highlighter-rouge">KL-Divergence Loss</code>ëŠ” ë‘ í™•ë¥  ë¶„í¬ ê°„ì˜ ì°¨ì´ë¥¼ ì¸¡ì •í•˜ëŠ” ì§€í‘œ ì¤‘ í•˜ë‚˜ë‹¤. ì£¼ë¡œ í™•ë¥  ë¶„í¬ Pì™€ Q ì‚¬ì´ì˜ ì°¨ì´ë¥¼ ë‚˜íƒ€ë‚´ëŠ”ë°, ê°œë³„ ìš”ì†Œì˜ í™•ë¥ ê°’ ì°¨ì´ê°€ í´ìˆ˜ë¡ í•©ì‚°ê°’ì€ ì»¤ì ¸ ì†ì‹¤ì´ ì»¤ì§€ê²Œ ëœë‹¤. ë°˜ëŒ€ë¡œ ë‘ ë¶„í¬ì˜ ê°œë³„ ìš”ì†Œ í™•ë¥ ê°’ ì°¨ì´ê°€ ì‘ë‹¤ë©´ ë‹¹ì—°íˆ, ë‘ ë¶„í¬ê°€ ìœ ì‚¬í•˜ë‹¤ëŠ” ì˜ë¯¸ì´ë¯€ë¡œ ì†ì‹¤ ì—­ì‹œ ì‘ì•„ì§€ê²Œ ëœë‹¤. ì¼ë°˜ì ìœ¼ë¡œ <code class="language-plaintext highlighter-rouge">KL-Divergence Loss</code> ì—ì„œ í™•ë¥ ë¶„í¬ $P$ ê°€ ì´ìƒì ì¸ í™•ë¥  ë¶„í¬ë¥¼, $Q$ ê°€ ëª¨ë¸ì´ ì˜ˆì¸¡í•œ í™•ë¥ ë¶„í¬ë¥¼ ì˜ë¯¸í•œë‹¤. ë”°ë¼ì„œ <code class="language-plaintext highlighter-rouge">DistilBERT</code>ì˜ ê²½ìš° í™•ë¥ ë¶„í¬ $P$ ìë¦¬ì—ëŠ” <code class="language-plaintext highlighter-rouge">Teacher</code> ëª¨ë¸ì˜ ì†Œí”„íŠ¸ë§¥ìŠ¤ ë¶„í¬ê°€, $Q$ ì—ëŠ” <code class="language-plaintext highlighter-rouge">Student</code> ëª¨ë¸ì˜ ì†Œí”„íŠ¸ë§¥ìŠ¤ ë¶„í¬ê°€ ëŒ€ì…ë˜ë©´ ëœë‹¤. ì´ ë•Œ ë‘ í™•ë¥ ë¶„í¬ ëª¨ë‘, ì•”í‘ ì§€ì‹ íšë“ì„ ìœ„í•´ ì†Œí”„íŠ¸ë§¥ìŠ¤ í‰íƒ„í™”ë¥¼ ì ìš©í•œ ê²°ê³¼ë¥¼ ì‚¬ìš©í•œë‹¤. ë…¼ë¬¸ì—ì„œ, ì„ ìƒ ëª¨ë¸ ì˜ˆì¸¡ì— í‰íƒ„í™”ë¥¼ ì ìš©í•œ ê²ƒì„ <code class="language-plaintext highlighter-rouge">ì†Œí”„íŠ¸ ë¼ë²¨</code>, í•™ìƒ ëª¨ë¸ì˜ ê²ƒì— ì ìš©í•œ ê²°ê³¼ëŠ” <code class="language-plaintext highlighter-rouge">ì†Œí”„íŠ¸ ì˜ˆì¸¡</code>ì´ë¼ê³  ë¶€ë¥¸ë‹¤.</p>

<h4 id="student-loss-mlm-loss"><code class="language-plaintext highlighter-rouge">ğŸŒ†Â Student Loss: MLM Loss</code></h4>

\[\mathcal{L}_{\text{MLM}} = - \sum_{i=1}^{N} \sum_{j=1}^{L} \mathbb{1}_{m_{ij}} \log \text{softmax}(x_{ij})\]

<p>í•™ìƒ ì†ì‹¤ì€ ë§ê·¸ëŒ€ë¡œ ê¸°ë³¸ì ì¸ MLM ì†ì‹¤ì„ ë§í•œë‹¤. ì •í™•í•œ ì†ì‹¤ê°’ ê³„ì‚°ì„ ìœ„í•´ì„œ í•™ìƒì˜ ì†Œí”„íŠ¸ë§¥ìŠ¤ ë¶„í¬ì— í‰íƒ„í™”ë¥¼ ì ìš©í•˜ì§€ ì•ŠëŠ”ë‹¤. ì´ë¥¼ ë…¼ë¬¸ì—ì„œëŠ” <code class="language-plaintext highlighter-rouge">í•˜ë“œ ì˜ˆì¸¡</code>ì´ë¼ê³  ë¶€ë¥¸ë‹¤. ë¼ë²¨ ì—­ì‹œ <code class="language-plaintext highlighter-rouge">Teacher</code>ë¡œë¶€í„° ë‚˜ì˜¨ ê²ƒì´ ì•„ë‹Œ ì›ë˜ MLM ìˆ˜í–‰ì— ì‚¬ìš©ë˜ëŠ” ë§ˆìŠ¤í‚¹ ë¼ë²¨ì„ ì‚¬ìš©í•œë‹¤.</p>

<h4 id="cosine-embedding-loss-contrastive-loss-by-cosine-similarity"><code class="language-plaintext highlighter-rouge">ğŸŒ†Â Cosine Embedding Loss: Contrastive Loss by cosine similarity</code></h4>

\[\mathcal{L}_{\text{COS}}(x,y) = \begin{cases} 1 - \cos(x_1, x_2), &amp; \text{if } y = 1 \\ \max(0, \cos(x_1, x_2) - \text{margin}), &amp; \text{if } y = -1 \end{cases}\]

<p><code class="language-plaintext highlighter-rouge">Teacher</code> ëª¨ë¸ê³¼ <code class="language-plaintext highlighter-rouge">Student</code> ëª¨ë¸ì˜ ë§ˆì§€ë§‰ ì¸ì½”ë” ëª¨ë¸ì´ ì¶œë ¥í•˜ëŠ” ì€ë‹‰ê°’ì— ëŒ€í•œ <code class="language-plaintext highlighter-rouge">Contrastive Loss</code>ë¥¼ ì˜ë¯¸í•œë‹¤. ì´ ë•Œ <code class="language-plaintext highlighter-rouge">Distance Metric</code>ì€ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ ì‚¬ìš©í•œë‹¤. ê·¸ë˜ì„œ ì½”ì‚¬ì¸ ì„ë² ë”© ì†ì‹¤ì´ë¼ê³  ë…¼ë¬¸ì—ì„œ ì •ì˜í•˜ëŠ” ê²ƒìœ¼ë¡œ ì¶”ì •ëœë‹¤. ìœ„ ìˆ˜ì‹ì„ ìµœì í™”í•˜ëŠ” ê²ƒì„ ëª©ì ìœ¼ë¡œ í•œë‹¤. ì´ ë•Œ ë¼ë²¨ì€ <code class="language-plaintext highlighter-rouge">[BS, Seq_len]</code>ì˜ í¬ê¸°ë¥¼ ê°–ë˜, ëª¨ë“  ì›ì†ŒëŠ” 1ì´ ë˜ë„ë¡ ë§Œë“ ë‹¤. ì´ìœ ëŠ” ê°„ë‹¨í•˜ë‹¤. <code class="language-plaintext highlighter-rouge">Student</code> ëª¨ë¸ì˜ ì€ë‹‰ê°’ì´ <code class="language-plaintext highlighter-rouge">Teacher</code> ëª¨ë¸ì˜ ê²ƒê³¼ ìµœëŒ€í•œ ë¹„ìŠ·í•´ì§€ë„ë¡ ë§Œë“œëŠ”ê²Œ ìš°ë¦¬ ëª©ì ì´ê¸° ë•Œë¬¸ì´ë‹¤.</p>

<h3 id="implementation-by-pytorch"><code class="language-plaintext highlighter-rouge">ğŸ‘©â€ğŸ’»Â Implementation by Pytorch</code></h3>
<p>ë…¼ë¬¸ì˜ ë‚´ìš©ê³¼ ì˜¤í”¼ì…œë¡œ ê³µê°œëœ ì½”ë“œë¥¼ ì¢…í•©í•˜ì—¬ íŒŒì´í† ì¹˜ë¡œ <code class="language-plaintext highlighter-rouge">DistilBERT</code>ë¥¼ êµ¬í˜„í•´ë´¤ë‹¤. ë…¼ë¬¸ì— í¬í•¨ëœ ì•„ì´ë””ì–´ë¥¼ ì´í•´í•˜ëŠ”ë°ëŠ” ì—­ì‹œ ì–´ë µì§€ ì•Šì•˜ì§€ë§Œ, í˜ì´í¼ì— hyper-param í…Œì´ë¸”ì´ ë”°ë¡œ ì œì‹œë˜ì–´ ìˆì§€ ì•Šì•„ ê³µê°œëœ ì½”ë“œë¥¼ ì•ˆ ë³¼ìˆ˜ê°€ ì—†ì—ˆë‹¤.</p>

<p>ì „ì²´ ëª¨ë¸ êµ¬ì¡° ëŒ€í•œ ì½”ë“œëŠ” <strong><a href="https://github.com/qcqced123/model_study">ì—¬ê¸° ë§í¬</a></strong>ë¥¼ í†µí•´ ì°¸ê³ ë°”ë€ë‹¤.</p>

<h4 id="knowledge-distillation-pipeline"><code class="language-plaintext highlighter-rouge">ğŸ‘©â€ğŸ’»Â Knowledge Distillation Pipeline</code></h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">train_val_fn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">loader_train</span><span class="p">,</span> <span class="n">model</span><span class="p">:</span> <span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">criterion</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">],</span> <span class="n">optimizer</span><span class="p">,</span><span class="n">scheduler</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]:</span>
    <span class="s">""" Function for train loop with validation for each batch*N Steps
    DistillBERT has three loss:

        1) distillation loss, calculated by soft targets &amp; soft predictions
            (nn.KLDIVLoss(reduction='batchmean'))

        2) student loss, calculated by hard targets &amp; hard predictions
            (nn.CrossEntropyLoss(reduction='mean')), same as pure MLM Loss

        3) cosine similarity loss, calculated by student &amp; teacher logit similarity
            (nn.CosineEmbeddingLoss(reduction='mean')), similar as contrastive loss

    Those 3 losses are summed jointly and then backward to student model
    """</span>
    <span class="n">scaler</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">amp</span><span class="p">.</span><span class="n">GradScaler</span><span class="p">(</span><span class="n">enabled</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">amp_scaler</span><span class="p">)</span>
    <span class="n">model</span><span class="p">.</span><span class="n">train</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">step</span><span class="p">,</span> <span class="n">batch</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">tqdm</span><span class="p">(</span><span class="n">loader_train</span><span class="p">)):</span>
        <span class="n">optimizer</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">(</span><span class="n">set_to_none</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="n">inputs</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="s">'input_ids'</span><span class="p">].</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">device</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="n">labels</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="s">'labels'</span><span class="p">].</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">device</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="n">padding_mask</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="s">'padding_mask'</span><span class="p">].</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">device</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

        <span class="n">mask</span> <span class="o">=</span> <span class="n">padding_mask</span><span class="p">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">).</span><span class="n">expand</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">dim_model</span><span class="p">)</span>  <span class="c1"># for hidden states dim
</span>        <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">t_hidden_state</span><span class="p">,</span> <span class="n">soft_target</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">teacher_fw</span><span class="p">(</span>
                <span class="n">inputs</span><span class="o">=</span><span class="n">inputs</span><span class="p">,</span>
                <span class="n">padding_mask</span><span class="o">=</span><span class="n">padding_mask</span><span class="p">,</span>
                <span class="n">mask</span><span class="o">=</span><span class="n">mask</span>
            <span class="p">)</span>  <span class="c1"># teacher model's pred =&gt; hard logit
</span>
        <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">amp</span><span class="p">.</span><span class="n">autocast</span><span class="p">(</span><span class="n">enabled</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">amp_scaler</span><span class="p">):</span>
            <span class="n">s_hidden_state</span><span class="p">,</span> <span class="n">s_logit</span><span class="p">,</span> <span class="n">soft_pred</span><span class="p">,</span> <span class="n">c_labels</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">student_fw</span><span class="p">(</span>
                <span class="n">inputs</span><span class="o">=</span><span class="n">inputs</span><span class="p">,</span>
                <span class="n">padding_mask</span><span class="o">=</span><span class="n">padding_mask</span><span class="p">,</span>
                <span class="n">mask</span><span class="o">=</span><span class="n">mask</span>
            <span class="p">)</span>
            <span class="n">d_loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">[</span><span class="s">"KLDivLoss"</span><span class="p">](</span><span class="n">soft_pred</span><span class="p">.</span><span class="n">log</span><span class="p">(),</span> <span class="n">soft_target</span><span class="p">)</span>  <span class="c1"># nn.KLDIVLoss
</span>            <span class="n">s_loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">[</span><span class="s">"CrossEntropyLoss"</span><span class="p">](</span><span class="n">s_logit</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">vocab_size</span><span class="p">),</span> <span class="n">labels</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>  <span class="c1"># nn.CrossEntropyLoss
</span>            <span class="n">c_loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">[</span><span class="s">"CosineEmbeddingLoss"</span><span class="p">](</span><span class="n">s_hidden_state</span><span class="p">,</span> <span class="n">t_hidden_state</span><span class="p">,</span> <span class="n">c_labels</span><span class="p">)</span>  <span class="c1"># nn.CosineEmbeddingLoss
</span>            <span class="n">loss</span> <span class="o">=</span> <span class="n">d_loss</span><span class="o">*</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">alpha_distillation</span> <span class="o">+</span> <span class="n">s_loss</span><span class="o">*</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">alpha_student</span> <span class="o">+</span> <span class="n">c_loss</span><span class="o">*</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">alpha_cosine</span>  <span class="c1"># linear combination loss
</span>
        <span class="n">scaler</span><span class="p">.</span><span class="n">scale</span><span class="p">(</span><span class="n">loss</span><span class="p">).</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">scaler</span><span class="p">.</span><span class="n">step</span><span class="p">(</span><span class="n">optimizer</span><span class="p">)</span>
        <span class="n">scaler</span><span class="p">.</span><span class="n">update</span><span class="p">()</span>
        <span class="n">scheduler</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>
</code></pre></div></div>

<h4 id="knowledge-distillation-model"><code class="language-plaintext highlighter-rouge">ğŸ‘©â€ğŸ’»Â Knowledge Distillation Model</code></h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">DistillationKnowledge</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">AbstractTask</span><span class="p">):</span>
    <span class="s">""" Custom Task Module for Knowledge Distillation by DistilBERT Style Architecture
    DistilBERT Style Architecture is Teacher-Student Framework for Knowledge Distillation,

    And then they have 3 objective functions:
        1) distillation loss, calculated by soft targets &amp; soft predictions
            (nn.KLDIVLoss(reduction='batchmean'))
        2) student loss, calculated by hard targets &amp; hard predictions
            (nn.CrossEntropyLoss(reduction='mean')), same as pure MLM Loss
        3) cosine similarity loss, calculated by student &amp; teacher logit similarity
            (nn.CosineEmbeddingLoss(reduction='mean')), similar as contrastive loss

    References:
        https://arxiv.org/pdf/1910.01108.pdf
        https://github.com/huggingface/transformers/blob/main/examples/research_projects/distillation/distiller.py
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">cfg</span><span class="p">:</span> <span class="n">CFG</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">DistillationKnowledge</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span> <span class="o">=</span> <span class="n">CFG</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">DistilBERT</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">,</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">select_model</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">_init_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">teacher_load_pretrained</span><span class="p">:</span>  <span class="c1"># for teacher model
</span>            <span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">teacher</span><span class="p">.</span><span class="n">load_state_dict</span><span class="p">(</span>
                <span class="n">torch</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="n">cfg</span><span class="p">.</span><span class="n">checkpoint_dir</span> <span class="o">+</span> <span class="n">cfg</span><span class="p">.</span><span class="n">teacher_state_dict</span><span class="p">),</span>
                <span class="n">strict</span><span class="o">=</span><span class="bp">False</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">student_load_pretrained</span><span class="p">:</span>  <span class="c1"># for student model
</span>            <span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">student</span><span class="p">.</span><span class="n">load_state_dict</span><span class="p">(</span>
                <span class="n">torch</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="n">cfg</span><span class="p">.</span><span class="n">checkpoint_dir</span> <span class="o">+</span> <span class="n">cfg</span><span class="p">.</span><span class="n">student_state_dict</span><span class="p">),</span>
                <span class="n">strict</span><span class="o">=</span><span class="bp">True</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">freeze</span><span class="p">:</span>
            <span class="n">freeze</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">teacher</span><span class="p">)</span>
            <span class="n">freeze</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">mlm_head</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">gradient_checkpoint</span><span class="p">:</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">gradient_checkpointing_enable</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">teacher_fw</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">inputs</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">padding_mask</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">mask</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
        <span class="n">is_valid</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="bp">False</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
        <span class="s">""" teacher forward pass to make soft target, last_hidden_state for distillation loss """</span>
        <span class="c1"># 1) make soft target
</span>        <span class="n">temperature</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="k">if</span> <span class="n">is_valid</span> <span class="k">else</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">temperature</span>
        <span class="n">last_hidden_state</span><span class="p">,</span> <span class="n">t_logit</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">teacher_fw</span><span class="p">(</span>
            <span class="n">inputs</span><span class="p">,</span>
            <span class="n">padding_mask</span><span class="p">,</span>
            <span class="n">attention_mask</span>
        <span class="p">)</span>
        <span class="n">last_hidden_state</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">masked_select</span><span class="p">(</span><span class="n">last_hidden_state</span><span class="p">,</span> <span class="o">~</span><span class="n">mask</span><span class="p">)</span>  <span class="c1"># for inverse select
</span>        <span class="n">last_hidden_state</span> <span class="o">=</span> <span class="n">last_hidden_state</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">dim_model</span><span class="p">)</span>  <span class="c1"># flatten last_hidden_state
</span>        <span class="n">soft_target</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="n">softmax</span><span class="p">(</span>
            <span class="n">t_logit</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">vocab_size</span><span class="p">)</span> <span class="o">/</span> <span class="n">temperature</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span>  <span class="c1"># flatten softmax distribution
</span>            <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span>
        <span class="p">)</span>  <span class="c1"># [bs* seq, vocab_size]
</span>        <span class="k">return</span> <span class="n">last_hidden_state</span><span class="p">,</span> <span class="n">soft_target</span>

    <span class="k">def</span> <span class="nf">student_fw</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">inputs</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">padding_mask</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">mask</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
        <span class="n">is_valid</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="bp">False</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
        <span class="s">""" student forward pass to make soft prediction, hard prediction for student loss """</span>
        <span class="n">temperature</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="k">if</span> <span class="n">is_valid</span> <span class="k">else</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">temperature</span>
        <span class="n">last_hidden_state</span><span class="p">,</span> <span class="n">s_logit</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">teacher_fw</span><span class="p">(</span>
            <span class="n">inputs</span><span class="p">,</span>
            <span class="n">padding_mask</span><span class="p">,</span>
            <span class="n">attention_mask</span>
        <span class="p">)</span>
        <span class="n">last_hidden_state</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">masked_select</span><span class="p">(</span><span class="n">last_hidden_state</span><span class="p">,</span> <span class="o">~</span><span class="n">mask</span><span class="p">)</span>  <span class="c1"># for inverse select
</span>        <span class="n">last_hidden_state</span> <span class="o">=</span> <span class="n">last_hidden_state</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">dim_model</span><span class="p">)</span>  <span class="c1"># flatten last_hidden_state
</span>        <span class="n">c_labels</span> <span class="o">=</span> <span class="n">last_hidden_state</span><span class="p">.</span><span class="n">new</span><span class="p">(</span><span class="n">last_hidden_state</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)).</span><span class="n">fill_</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">soft_pred</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="n">softmax</span><span class="p">(</span>
            <span class="n">s_logit</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">vocab_size</span><span class="p">)</span> <span class="o">/</span> <span class="n">temperature</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span>  <span class="c1"># flatten softmax distribution
</span>            <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">last_hidden_state</span><span class="p">,</span> <span class="n">s_logit</span><span class="p">,</span> <span class="n">soft_pred</span><span class="p">,</span> <span class="n">c_labels</span>
</code></pre></div></div>

<h4 id="distilbert-model"><code class="language-plaintext highlighter-rouge">ğŸ‘©â€ğŸ’»Â DistilBERT Model</code></h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">DistilBERT</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">AbstractModel</span><span class="p">):</span>
    <span class="s">""" Main class for DistilBERT Style Model, Teacher-Student Framework
    for Knowledge Distillation aim to lighter Large Scale LLM model. This model have 3 objective functions:

        1) distillation loss, calculated by soft targets &amp; soft predictions
            (nn.KLDIVLoss(reduction='batchmean'))

        2) student loss, calculated by hard targets &amp; hard predictions
            (nn.CrossEntropyLoss(reduction='mean')), same as pure MLM Loss

        3) cosine similarity loss, calculated by student &amp; teacher logit similarity
            (nn.CosineEmbeddingLoss(reduction='mean')), similar as contrastive loss

    soft targets &amp; soft predictions are meaning that logit are passed through softmax function applied with temperature T
    temperature T aim to flatten softmax layer distribution for making "Dark Knowledge" from teacher model

    hard targets &amp; hard predictions are meaning that logit are passed through softmax function without temperature T
    hard targets are same as just simple labels from MLM Collator returns for calculating cross entropy loss

    cosine similarity loss is calculated by cosine similarity between student &amp; teacher
    in official repo, they mask padding tokens for calculating cosine similarity, target for this task is 1
    cosine similarity is calculated by nn.CosineSimilarity() function, values are range to [-1, 1]

    you can select any other backbone model architecture for Teacher &amp; Student Model for knowledge distillation
    but, in original paper, BERT is used for Teacher Model &amp; Student
    and you must select pretrained model for Teacher Model, because Teacher Model is used for knowledge distillation,
    which is containing pretrained mlm head

    Do not pass gradient backward to teacher model!!
    (teacher model must be frozen or register_buffer to model or use no_grad() context manager)

    Args:
        cfg: configuration.CFG
        model_func: make model instance in runtime from config.json

    References:
        https://arxiv.org/pdf/1910.01108.pdf
        https://github.com/huggingface/transformers/blob/main/examples/research_projects/distillation/distiller.py
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">cfg</span><span class="p">:</span> <span class="n">CFG</span><span class="p">,</span> <span class="n">model_func</span><span class="p">:</span> <span class="n">Callable</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">DistilBERT</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span> <span class="o">=</span> <span class="n">cfg</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">teacher</span> <span class="o">=</span> <span class="n">model_func</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">teacher_num_layers</span><span class="p">)</span>  <span class="c1"># must be loading pretrained model containing mlm head
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">mlm_head</span> <span class="o">=</span> <span class="n">MLMHead</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">)</span>  <span class="c1"># must be loading pretrained model's mlm head
</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">student</span> <span class="o">=</span> <span class="n">model_func</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">student_num_layers</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">s_mlm_head</span> <span class="o">=</span> <span class="n">MLMHead</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">teacher_fw</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">inputs</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">padding_mask</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
        <span class="s">""" forward pass for teacher model
        """</span>
        <span class="n">last_hidden_state</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">teacher</span><span class="p">(</span>
            <span class="n">inputs</span><span class="p">,</span>
            <span class="n">padding_mask</span><span class="p">,</span>
            <span class="n">attention_mask</span>
        <span class="p">)</span>
        <span class="n">t_logit</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">mlm_head</span><span class="p">(</span><span class="n">last_hidden_state</span><span class="p">)</span>  <span class="c1"># hard logit =&gt; to make soft logit
</span>        <span class="k">return</span> <span class="n">last_hidden_state</span><span class="p">,</span> <span class="n">t_logit</span>

    <span class="k">def</span> <span class="nf">student_fw</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">inputs</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">padding_mask</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="bp">None</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
        <span class="s">""" forward pass for student model
        """</span>
        <span class="n">last_hidden_state</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">student</span><span class="p">(</span>
            <span class="n">inputs</span><span class="p">,</span>
            <span class="n">padding_mask</span><span class="p">,</span>
            <span class="n">attention_mask</span>
        <span class="p">)</span>
        <span class="n">s_logit</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">s_mlm_head</span><span class="p">(</span><span class="n">last_hidden_state</span><span class="p">)</span>  <span class="c1"># hard logit =&gt; to make soft logit
</span>        <span class="k">return</span> <span class="n">last_hidden_state</span><span class="p">,</span> <span class="n">s_logit</span>
</code></pre></div></div>]]></content><author><name>qcqced</name><email>qcqced123@gmail.com</email></author><category term="NLP" /><category term="Natural Language Process" /><category term="Linear-Attention" /><category term="Transformer" /><category term="BERT" /><category term="Kernel Trick" /><category term="Self-Attention" /><category term="Pytorch" /><summary type="html"><![CDATA[Linear Attention Official Paper Review with Pytorch Implementation]]></summary></entry><entry><title type="html">ğŸ‘® [ELECTRA] Pre-training Text Encoders as Discriminators Rather Than Generators</title><link href="http://localhost:4000/nlp/electra" rel="alternate" type="text/html" title="ğŸ‘® [ELECTRA] Pre-training Text Encoders as Discriminators Rather Than Generators" /><published>2024-03-11T00:00:00+09:00</published><updated>2024-03-12T02:00:00+09:00</updated><id>http://localhost:4000/nlp/electra</id><content type="html" xml:base="http://localhost:4000/nlp/electra"><![CDATA[<h3 id="overview"><code class="language-plaintext highlighter-rouge">ğŸ”­Â Overview</code></h3>

<p><code class="language-plaintext highlighter-rouge">ELECTRA</code>ëŠ” 2020ë…„ Googleì—ì„œ ì²˜ìŒ ë°œí‘œí•œ ëª¨ë¸ë¡œ, GAN(Generative Adversarial Networks) Style ì•„í‚¤í…ì²˜ë¥¼ NLPì— ì ìš©í•œ ê²ƒì´ íŠ¹ì§•ì´ë‹¤. ìƒˆë¡œìš´ êµ¬ì¡° ì°¨ìš©ì— ë§ì¶°ì„œ <code class="language-plaintext highlighter-rouge">RTD(Replace Token Dection)</code> Taskë¥¼ ê³ ì•ˆì— ì‚¬ì „ í•™ìŠµìœ¼ë¡œ ì‚¬ìš©í–ˆë‹¤. ëª¨ë“  ì•„ì´ë””ì–´ëŠ” ê¸°ì¡´ MLM(Masked Language Model)ì„ ì‚¬ì „í•™ìŠµ ë°©ë²•ë¡ ìœ¼ë¡œ ì‚¬ìš©í•˜ëŠ” ì¸ì½”ë” ì–¸ì–´ ëª¨ë¸(BERT ê³„ì—´)ì˜ ë‹¨ì ìœ¼ë¡œë¶€í„° ì¶œë°œí•œë‹¤.</p>

<p><strong>[MLM ë‹¨ì ]</strong></p>
<ul>
  <li>1) ì‚¬ì „í•™ìŠµê³¼ íŒŒì¸íŠœë‹ ì‚¬ì´ ë¶ˆì¼ì¹˜
    <ul>
      <li>íŒŒì¸íŠœë‹ ë•Œ Masking Taskê°€ ì—†ìŒ</li>
    </ul>
  </li>
  <li>2) ì—°ì‚°ëŸ‰ ëŒ€ë¹„ í•™ìŠµëŸ‰ì€ ì ì€í¸
    <ul>
      <li>ì „ì²´ ì‹œí€€ìŠ¤ì˜ 15%ë§Œ ë§ˆìŠ¤í‚¹ í™œìš©(15%ë§Œ í•™ìŠµ)</li>
      <li>ì „ì—­ ì–´í…ì…˜ì˜ ì‹œê³µê°„ ë³µì¡ë„ ê³ ë ¤í•˜ë©´ ìƒë‹¹íˆ ë¹„íš¨ìœ¨ì ì¸ ìˆ˜ì¹˜
        <ul>
          <li>ì‹œí€€ìŠ¤ê¸¸ì´ ** 2ì˜ ë³µì¡ë„</li>
          <li>Vocab Sizeë§Œí¼ì˜ ì°¨ì›ì„ ê°–ëŠ” ì†Œí”„íŠ¸ë§¥ìŠ¤ ê³„ì‚° ë°˜ë³µ</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p>ê·¸ë˜ì„œ MLMì€ í™œìš©í•˜ë˜, íŒŒì¸íŠœë‹ê³¼ ê´´ë¦¬ëŠ” í¬ì§€ ì•Šì€ ëª©ì í•¨ìˆ˜ë¥¼ ì„¤ê³„í•¨ìœ¼ë¡œì„œ ì…ë ¥ëœ ì „ì²´ ì‹œí€€ìŠ¤ì— ëŒ€í•´ì„œ ëª¨ë¸ì´ í•™ìŠµí•˜ì—¬ ì—°ì‚°ëŸ‰ ëŒ€ë¹„ í•™ìŠµëŸ‰ì„ ëŠ˜ë¦¬ê³ ì í–ˆë˜ê²Œ ë°”ë¡œ ELECTRA ëª¨ë¸ì´ë‹¤.</p>

<p>ì •ë¦¬í•˜ìë©´, ELECTRA ëª¨ë¸ì€ ê¸°ì¡´ BERTì˜ êµ¬ì¡°ì  ì¸¡ë©´ ê°œì„ ì´ ì•„ë‹Œ, ì‚¬ì „í•™ìŠµ ë°©ë²•ì— ëŒ€í•œ ê°œì„  ì‹œë„ë¼ê³  ë³¼ ìˆ˜ ìˆë‹¤. ë”°ë¼ì„œ ì–´ë–¤ ëª¨ë¸ì´ë”ë¼ë„, ì¸ì½”ë” ì–¸ì–´ ëª¨ë¸ì´ë¼ë©´ ëª¨ë‘ ELECTRA êµ¬ì¡°ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìœ¼ë©°, ê¸°ì¡´ ë…¼ë¬¸ì—ì„œëŠ” ì›ë³¸ BERT êµ¬ì¡°ë¥¼ ì‚¬ìš©í–ˆë‹¤. ê·¸ë˜ì„œ ë³¸ í¬ìŠ¤íŒ…ì—ì„œë„ BERTì— ëŒ€í•œ ì„¤ëª… ì—†ì´ RTDì— ëŒ€í•´ì„œë§Œ ë‹¤ë£¨ë ¤ê³  í•œë‹¤.</p>

<h3 id="rtd-new-pre-train-task"><code class="language-plaintext highlighter-rouge">ğŸ‘®Â RTD: New Pre-train Task</code></h3>

<p align="center">
<img src="/assets/images/electra/electra.png" alt="RTD Task" class="align-center image-caption" width="100%&quot;, height=&quot;100%" />
<strong><em><a href="https://arxiv.org/abs/2003.10555">RTD Task</a></em></strong>
</p>

<p>RTDì˜ ì•„ì´ë””ì–´ëŠ” ê°„ë‹¨í•˜ë‹¤. ìƒì„±ì(Generator)ê°€ ì¶œë ¥ìœ¼ë¡œ ë‚´ë†“ì€ í† í° ì‹œí€€ìŠ¤ì— ëŒ€í•´ì„œ íŒë³„ì(Discriminator)ê°€ ê°œë³„ í† í°ë“¤ì´ ì›ë³¸ì¸ì§€ ì•„ë‹Œì§€ë¥¼ íŒì •(ì´ì§„ ë¶„ë¥˜)í•˜ë„ë¡ ë§Œë“ ë‹¤. ìƒì„±ìëŠ” ê¸°ì¡´ì˜ MLMì„ ê·¸ëŒ€ë¡œ ìˆ˜í–‰í•˜ê³ , íŒë³„ìëŠ” ìƒì„±ìì˜ ì˜ˆì¸¡ì— ëŒ€í•´ ì§„ì§œì¸ì§€ ê°€ì§œì¸ì§€ ë¶„ë¥˜í•˜ëŠ” ì‹ì´ë‹¤.</p>

<p>ìœ„ ê·¸ë¦¼ì„ ì˜ˆì‹œë¡œ ì‚´í´ë³´ì. ëª¨ë¸ì— ì…ë ¥ìœ¼ë¡œ <code class="language-plaintext highlighter-rouge">the chef cooked the meal</code>ë¼ëŠ” ì‹œí€€ìŠ¤ ì¤€ë‹¤. ê·¸ëŸ¬ë©´ MLM ê·œì¹™ì— ë”°ë¼ì„œ 15%ì˜ í† í°ì´ ë¬´ì‘ìœ„ë¡œ ì„ íƒëœë‹¤. ê·¸ë˜ì„œ <code class="language-plaintext highlighter-rouge">the</code>, <code class="language-plaintext highlighter-rouge">cooked</code>ê°€ ë§ˆìŠ¤í‚¹ ë˜ì—ˆë‹¤. ì´ì œ ìƒì„±ìëŠ” ë§ˆìŠ¤í‚¹ í† í°ì— ëŒ€í•´ <code class="language-plaintext highlighter-rouge">the</code>, <code class="language-plaintext highlighter-rouge">ate</code>ë¼ëŠ” ê²°ê³¼ë¥¼ ë‚´ë†“ëŠ”ë‹¤. ê·¸ë˜ì„œ ìµœì¢…ì ìœ¼ë¡œ ìƒì„±ìê°€ ë°˜í™˜í•˜ëŠ” ì‹œí€€ìŠ¤ëŠ” <code class="language-plaintext highlighter-rouge">the chef ate the meal</code>ì´ ëœë‹¤. ì´ì œ ìƒì„±ìê°€ ë°˜í™˜í•œ ì‹œí€€ìŠ¤ë¥¼ íŒë³„ìì— ì…ë ¥ìœ¼ë¡œ ëŒ€ì…í•œë‹¤. íŒë³„ìëŠ” ê°œë³„ í† í°ë“¤ì´ ì›ë³¸ì¸ì§€ ì•„ë‹Œì§€ë¥¼ íŒì •í•´ ê²°ê³¼ë¥¼ ì¶œë ¥í•œë‹¤.</p>

<p>ì´ëŸ¬í•œ êµ¬ì¡° ë° ì‚¬ì „í•™ìŠµ ë°©ì‹ì˜ ì¥ì ì€ íŒë³„ìê°€ MLM í•™ìŠµì— ë”°ë¥¸ ì§€ì‹ì„ ìƒì„±ìë¡œë¶€í„° ì „ìˆ˜ ë°›ëŠ” ë™ì‹œì— ì „ì²´ ì‹œí€€ìŠ¤ì— ëŒ€í•´ì„œ í•™ìŠµí•  ê¸°íšŒê°€ ìƒê¸´ë‹¤ëŠ” ê²ƒì´ë‹¤. ì‹œí€€ìŠ¤ ë‚´ë¶€ ëª¨ë“  í† í°ì— ëŒ€í•´ì„œ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•˜ê³  ì†ì‹¤ì„ ê³„ì‚°í•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì— ê°™ì€ í¬ê¸°ì˜ ì‹œí€€ìŠ¤ë¥¼ ì‚¬ìš©í•´ë„ ê¸°ì¡´ MLM ëŒ€ë¹„ ë” í’ë¶€í•œ ë¬¸ë§¥ ì •ë³´ë¥¼ ëª¨ë¸ì´ í¬ì°©í•  ìˆ˜ ìˆê²Œ ëœë‹¤. ë˜í•œ íŒë³„ìë¥¼ íŒŒì¸íŠœë‹ì˜ BackBoneìœ¼ë¡œ ì‚¬ìš©í•˜ë©´, íŒë³„ìì˜ ì‚¬ì „í•™ìŠµì€ ê²°êµ­ ë§ˆìŠ¤í‚¹ ì—†ì´ ëª¨ë“  ì‹œí€€ìŠ¤ë¥¼ í™œìš©í•œ ì´ì§„ ë¶„ë¥˜ë¼ê³  ë³¼ ìˆ˜ ìˆê¸° ë•Œë¬¸ì—, ì‚¬ì „í•™ìŠµê³¼ íŒŒì¸íŠœë‹ ì‚¬ì´ì˜ ê´´ë¦¬ë„ ìƒë‹¹íˆ ë§ì´ ì¤„ì–´ë“¤ê²Œ ëœë‹¤.</p>

<h3 id="architecture"><code class="language-plaintext highlighter-rouge">ğŸŒŸÂ Architecture</code></h3>

<p align="center">
<img src="/assets/images/electra/electra_experiment.png" alt="Model Architecture" class="align-center image-caption" width="100%&quot;, height=&quot;100%" />
<strong><em><a href="https://arxiv.org/abs/2003.10555">Model Architecture</a></em></strong>
</p>

<p>ì €ìëŠ” ìœ„ì™€ ê°™ì€ ì‹¤í—˜ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ìƒì„±ìì˜ width (ì€ë‹‰ì¸µ) í¬ê¸°ê°€ íŒë³„ìë³´ë‹¤ ì‘ë„ë¡ ëª¨ë¸ í¬ê¸°ë¥¼ ì„¸íŒ…í•˜ëŠ”ê²Œ ê°€ì¥ íš¨ìœ¨ì ì´ë¼ê³  ì£¼ì¥í•œë‹¤. ì œì‹œëœ ê·¸ë˜í”„ëŠ” ìƒì„±ìì™€ íŒë³„ìì˜ í¬ê¸° ë³€í™” ëŒ€ë¹„ íŒŒì¸íŠœë‹ ì„±ëŠ¥ì˜ ì¶”ì´ë¥¼ ë‚˜íƒ€ë‚¸ë‹¤. ìƒì„±ìì˜ width í¬ê¸°ê°€ 256, íŒë³„ìì˜ width í¬ê¸°ê°€ 768ì¼ ë•Œ ê°€ì¥ ì ìˆ˜ê°€ ë†’ë‹¤. depth(ë ˆì´ì–´ ê°œìˆ˜)ì— ëŒ€í•œ ì–¸ê¸‰ì€ ë”°ë¡œ ì—†ì§€ë§Œ, ì €ìì— ì˜í•´ ê³µê°œëœ Hyper-Param í…Œì´ë¸”ì„ ë³´ë©´ ì€ë‹‰ì¸µì˜ í¬ê¸°ë§Œ ì¤„ì´ê³ , ë ˆì´ì–´ ê°œìˆ˜ëŠ” ìƒì„±ìì™€ íŒë³„ìê°€ ê°™ì€ ê²ƒìœ¼ë¡œ ì¶”ì •ëœë‹¤.</p>

<p>ì¶”ê°€ë¡œ, ìƒì„±ìì™€ íŒë³„ìê°€ ì„ë² ë”© ì¸µì„ ì„œë¡œ ê³µìœ í•˜ëŠ”ê²Œ ê°€ì¥ ë†’ì€ ì„±ëŠ¥ì„ ë‚¸ë‹¤ê³  ì£¼ì¥í•œë‹¤. ì˜¤ë¥¸ìª½ ê·¸ë˜í”„ ì¶”ì´ë¥¼ ë³´ë©´ ê°™ì€ ì—°ì‚°ëŸ‰ì´ë¼ë©´, ì„ë² ë”© ê³µìœ (íŒŒë€ìƒ‰ ì‹¤ì„ ) ë°©ì‹ì´ ê°€ì¥ ë†’ì€ íŒŒì¸íŠœë‹ ì„±ëŠ¥ì„ ë³´ì—¬ì¤€ë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤. ë”°ë¼ì„œ ë‹¨ì–´ ì„ë² ë”©, ì ˆëŒ€ ìœ„ì¹˜ ì„ë² ë”©ì„ ì„œë¡œ ê³µìœ í•˜ë„ë¡ ì„¤ê³„í•œë‹¤. ëŒ€ì‹  ìƒì„±ì ì€ë‹‰ì¸µì˜ í¬ê¸°ê°€ ë” ì‘ì€ê²Œ ìœ ë¦¬í•˜ë‹¤ê³  ì–¸ê¸‰í–ˆê¸° ë•Œë¬¸ì—, ì´ê²ƒì„ ì‹¤ì œë¡œ êµ¬í˜„í•˜ë ¤ë©´ ì„ë² ë”© ì¸µìœ¼ë¡œë¶€í„° ë‚˜ì˜¨ ê²°ê³¼ê°’ì„ ìƒì„±ìì˜ ì€ë‹‰ì¸µ ì°¨ì›ìœ¼ë¡œ ì„ í˜• íˆ¬ì˜í•´ì¤˜ì•¼ í•œë‹¤. ê·¸ë˜ì„œ ìƒì„±ìì˜ ì„ë² ë”© ì¸µê³¼ ì¸ì½”ë” ì‚¬ì´ì— linear layerê°€ ì¶”ê°€ë˜ì–´ì•¼ í•œë‹¤.</p>

\[\min_{\theta_G, \theta_D}\sum_{x \in X} \mathcal{L}_{\text{MLM}}(x, \theta_G) + \lambda \mathcal{L}_{\text{Disc}}(x, \theta_D)\]

<p>ë”°ë¼ì„œ, ì§€ê¸ˆê¹Œì§€ ì‚´í´ë³¸ ëª¨ë“  ë‚´ìš©ì„ ì¢…í•©í•´ë³´ë©´ ELECTRAì˜ ëª©ì í•¨ìˆ˜ëŠ” ë‹¤ìŒ ìˆ˜ì‹ê³¼ ê°™ë‹¤. ìƒì„±ìì˜ MLM ì†ì‹¤ê³¼ íŒë³„ìì˜ ì´ì§„ ë¶„ë¥˜ ì†ì‹¤ì„ ë”í•´ì„œ ëª¨ë¸ì— ì˜¤ì°¨ ì—­ì „í•´ì£¼ë©´ ë˜ëŠ”ë°, íŠ¹ì´í•œ ì ì€ íŒë³„ìì˜ ì†ì‹¤ì— ìƒìˆ˜ê°’ì¸ ëŒë‹¤ê°€ ê³±í•´ì§„ë‹¤ëŠ” ê²ƒì´ë‹¤. ì‹¤ì œ ëª¨ë¸ì„ êµ¬í˜„í•˜ê³  ì‚¬ì „í•™ìŠµì„ í•´ë³´ë©´, ë°ì´í„°ì˜ ì–‘ì´ë‚˜ ëª¨ë¸ í¬ê¸° í˜¹ì€ ì¢…ë¥˜ì— ë”°ë¼ ë‹¬ë¼ì§€ê² ì§€ë§Œ ë‘ ì†ì‹¤ ì‚¬ì´ì˜ ìŠ¤ì¼€ì¼ì˜ ì°¨ì´ê°€ 10ë°°ì •ë„ ì°¨ì´ ë‚˜ê²Œ ëœë‹¤. ë‘ ì†ì‹¤ì˜ ìŠ¤ì¼€ì¼ì„ ë§ì¶°ì£¼ëŠ” ë™ì‹œì—, ì„ë² ë”©ì¸µì˜ í•™ìŠµì´ íŒë³„ìì˜ ì†ì‹¤ì„ ì¤„ì´ëŠ”ë° ë” ì§‘ì¤‘í•˜ë„ë¡ ë§Œë“¤ê¸° ìœ„í•´ ë„ì…í•œ ê²ƒìœ¼ë¡œ ì¶”ì •ëœë‹¤. ë…¼ë¬¸ê³¼ ì½”ë“œë¥¼ ë³´ë©´ ì €ìëŠ” $\lambda=50$ ìœ¼ë¡œ ë‘ê³  í•™ìŠµí•˜ê³  ìˆë‹¤.</p>

<h3 id="implementation-by-pytorch"><code class="language-plaintext highlighter-rouge">ğŸ‘©â€ğŸ’»Â Implementation by Pytorch</code></h3>

<p>ë…¼ë¬¸ì˜ ë‚´ìš©ê³¼ ì €ìê°€ ì§ì ‘ ê³µê°œí•œ ì½”ë“œë¥¼ ì¢…í•©í•˜ì—¬ íŒŒì´í† ì¹˜ë¡œ ELECTRAë¥¼ êµ¬í˜„í•´ë´¤ë‹¤. ë‘ ê°œì˜ ì„œë¡œ ë‹¤ë¥¸ ëª¨ë¸ì„ ê°™ì€ ìŠ¤íƒ­ì—ì„œ í•™ìŠµì‹œì¼œì•¼ í•˜ê¸° ë•Œë¬¸ì—, ì œì‹œëœ ë‚´ìš©ì— ë¹„í•´ ì‹¤ì œ êµ¬í˜„ì€ ë§¤ìš° ê¹Œë‹¤ë¡œìš´ í¸ì´ì—ˆë‹¤. ë³¸ í¬ìŠ¤íŒ…ì—ì„œëŠ” ELECTRA ëª¨ë¸ êµ¬ì¡°ë¥¼ ë¹„ë¡¯í•´ RTD í•™ìŠµ íŒŒì´í”„ë¼ì¸ êµ¬ì„±ì— í•„ìˆ˜ì ì¸ ìš”ì†Œ ëª‡ ê°€ì§€ì— ëŒ€í•´ì„œë§Œ ì„¤ëª…í•˜ë ¤ í•œë‹¤. ì „ì²´ êµ¬ì¡°ì— ëŒ€í•œ ì½”ë“œëŠ” <strong><a href="https://github.com/qcqced123/model_study">ì—¬ê¸° ë§í¬</a></strong>ë¥¼ í†µí•´ ì°¸ê³  ë¶€íƒë“œë¦°ë‹¤.</p>

<p>ELECTRAì˜ ì‚¬ì „ í•™ìŠµì¸ RTDì˜ í•™ìŠµ íŒŒì´í”„ë¼ì¸ì„ êµ¬í˜„í•œ ì½”ë“œë¥¼ ë³¸ ë’¤, ì„¸ë¶€ êµ¬ì„± ìš”ì†Œë“¤ì— ëŒ€í•´ì„œ ì‚´í´ë³´ì.</p>

<h4 id="-rtd-trainer-method"><strong><code class="language-plaintext highlighter-rouge">ğŸŒ† RTD trainer method</code></strong></h4>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">train_val_fn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">loader_train</span><span class="p">,</span> <span class="n">model</span><span class="p">:</span> <span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">criterion</span><span class="p">:</span> <span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">scheduler</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]:</span>
  <span class="n">scaler</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">amp</span><span class="p">.</span><span class="n">GradScaler</span><span class="p">(</span><span class="n">enabled</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">amp_scaler</span><span class="p">)</span>
  <span class="n">model</span><span class="p">.</span><span class="n">train</span><span class="p">()</span>
  <span class="k">for</span> <span class="n">step</span><span class="p">,</span> <span class="n">batch</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">tqdm</span><span class="p">(</span><span class="n">loader_train</span><span class="p">)):</span>
      <span class="n">optimizer</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">(</span><span class="n">set_to_none</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
      <span class="n">inputs</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="s">'input_ids'</span><span class="p">].</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">device</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
      <span class="n">labels</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="s">'labels'</span><span class="p">].</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">device</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>  
      <span class="n">padding_mask</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="s">'padding_mask'</span><span class="p">].</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">device</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>  

      <span class="n">mask_labels</span> <span class="o">=</span> <span class="bp">None</span>
      <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">rtd_masking</span> <span class="o">==</span> <span class="s">'SpanBoundaryObjective'</span><span class="p">:</span>
          <span class="n">mask_labels</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="s">'mask_labels'</span><span class="p">].</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">device</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

      <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">amp</span><span class="p">.</span><span class="n">autocast</span><span class="p">(</span><span class="n">enabled</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">amp_scaler</span><span class="p">):</span>
          <span class="n">g_logit</span><span class="p">,</span> <span class="n">d_inputs</span><span class="p">,</span> <span class="n">d_labels</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">generator_fw</span><span class="p">(</span>
              <span class="n">inputs</span><span class="p">,</span>
              <span class="n">labels</span><span class="p">,</span>
              <span class="n">padding_mask</span><span class="p">,</span>
              <span class="n">mask_labels</span>
          <span class="p">)</span>
          <span class="n">d_logit</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">discriminator_fw</span><span class="p">(</span>
              <span class="n">d_inputs</span><span class="p">,</span>
              <span class="n">padding_mask</span>
          <span class="p">)</span>
          <span class="n">g_loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">g_logit</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">vocab_size</span><span class="p">),</span> <span class="n">labels</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>
          <span class="n">d_loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">d_logit</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">d_labels</span><span class="p">)</span>
          <span class="n">loss</span> <span class="o">=</span> <span class="n">g_loss</span> <span class="o">+</span> <span class="n">d_loss</span><span class="o">*</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">discriminator_lambda</span>

      <span class="n">scaler</span><span class="p">.</span><span class="n">scale</span><span class="p">(</span><span class="n">loss</span><span class="p">).</span><span class="n">backward</span><span class="p">()</span>
      <span class="n">scaler</span><span class="p">.</span><span class="n">step</span><span class="p">(</span><span class="n">optimizer</span><span class="p">)</span>
      <span class="n">scaler</span><span class="p">.</span><span class="n">update</span><span class="p">()</span>
      <span class="n">scheduler</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>
</code></pre></div></div>
<p>ë°ì´í„°ë¡œë”ë¡œë¶€í„° ë°›ì€ ì…ë ¥ë“¤ì„ ìƒì„±ìì— ë„£ê³  MLM ì˜ˆì¸¡ ê²°ê³¼, RTD ìˆ˜í–‰ì„ ìœ„í•´ í•„ìš”í•œ ìƒˆë¡œìš´ ë¼ë²¨ê°’ì„ ë°˜í™˜ ë°›ëŠ”ë‹¤. ê·¸ë¦¬ê³  ì´ê²ƒì„ ë‹¤ì‹œ íŒë³„ìì˜ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©í•˜ê³ , íŒë³„ìì˜ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ë°˜í™˜ë°›ì•„ ì„œë¡œ ë‹¤ë¥¸ ë‘ ëª¨ë¸ì— ëŒ€í•œ ê°€ì¤‘ ì†ì‹¤í•©ì‚°ì„ êµ¬í•œ ë’¤, ì˜µí‹°ë§ˆì´ì €ì— ë³´ë‚´ê³  ìµœì í™”ë¥¼ ìˆ˜í–‰í•œë‹¤. ì´ ë•Œ, ì²˜ìŒì— ë°ì´í„°ë¡œë”ê°€ ë°˜í™˜í•˜ëŠ” ì…ë ¥ ì‹œí€€ìŠ¤ì™€ ë¼ë²¨ì€ MLMì˜ ê·¸ê²ƒê³¼ ë™ì¼í•˜ë‹¤,</p>

<p>êµ¬í˜„í•˜ë©´ì„œ ê°€ì¥ ì–´ë ¤ì› ë˜ê²Œ, ì˜µí‹°ë§ˆì´ì € ë° ìŠ¤ì¼€ì¤„ëŸ¬ì˜ êµ¬ì„±ì´ì—ˆë‹¤. ë‘ ê°œì˜ ëª¨ë¸ì„ ê°™ì€ ìŠ¤íƒ­ì—ì„œ í•™ìŠµì‹œí‚¤ëŠ” ê²½í—˜ì´ ì²˜ìŒì´ë¼ì„œ ì²˜ìŒì— ëª¨ë¸ ê°œìˆ˜ë§Œí¼ ì˜µí‹°ë§ˆì´ì €ì™€ ìŠ¤ì¼€ì¤„ëŸ¬ ê°ì²´ë¥¼ ë§Œë“¤ì–´ì¤˜ì•¼ í•œë‹¤ê³  ìƒê°í–ˆë‹¤. íŠ¹íˆ ë‘ ëª¨ë¸ì˜ ìŠ¤ì¼€ì¼ì´ ë‹¤ë¥´ê¸° ë•Œë¬¸ì— ì„œë¡œ ë‹¤ë¥¸ ì˜µí‹°ë§ˆì´ì €, ìŠ¤ì¼€ì¤„ëŸ¬ ë„ì…ìœ¼ë¡œ ê°ê¸° ë‹¤ë¥¸ í•™ìŠµë¥ ì„ ì ìš©í•˜ëŠ”ê²Œ ì •í™•í•  ê²ƒì´ë¼ ìƒê°í–ˆë‹¤.</p>

<p>í•˜ì§€ë§Œ, ì˜µí‹°ë§ˆì´ì €ë¥¼ ë‘ ê°œ ì‚¬ìš©í•˜ëŠ” ê²ƒì€ ë§¤ìš° ë§ì€ ë©”ëª¨ë¦¬ë¥¼ ì°¨ì§€í•  ë¿ë”ëŸ¬ ë…¼ë¬¸ì—ì„œ ê³µê°œí•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° í…Œì´ë¸”ì„ ë³´ë©´ ë‘ ëª¨ë¸ì— ê°™ì€ í•™ìŠµë¥ ì„ ì ìš©í•˜ê³  ìˆëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆì—ˆë‹¤. ë”°ë¼ì„œ ê·¸ì— ë§ê²Œ ê°™ì€ ì˜µí‹°ë§ˆì´ì €, ìŠ¤ì¼€ì¤„ëŸ¬ë¥¼ ì‚¬ìš©í•´ ë™ì‹œì— ë‘ ëª¨ë¸ì´ í•™ìŠµë˜ë„ë¡ íŒŒì´í”„ë¼ì¸ì„ ë§Œë“¤ê²Œ ë˜ì—ˆë‹¤.</p>

<p>ì¶”ê°€ë¡œ, ê³µê°œëœ ì˜¤í”¼ì…œ ì½”ë“œ ì—­ì‹œ ë‹¨ì¼ ì˜µí‹°ë§ˆì´ì € ë° ìŠ¤ì¼€ì¤„ëŸ¬ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì„ í™•ì¸í–ˆë‹¤.</p>

<h4 id="-electra-module"><strong><code class="language-plaintext highlighter-rouge">ğŸŒ† ELECTRA Module</code></strong></h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="nn">experiment.models.abstract_model</span> <span class="kn">import</span> <span class="n">AbstractModel</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">Tensor</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">Callable</span>
<span class="kn">from</span> <span class="nn">einops.layers.torch</span> <span class="kn">import</span> <span class="n">Rearrange</span>
<span class="kn">from</span> <span class="nn">experiment.tuner.mlm</span> <span class="kn">import</span> <span class="n">MLMHead</span>
<span class="kn">from</span> <span class="nn">experiment.tuner.sbo</span> <span class="kn">import</span> <span class="n">SBOHead</span>
<span class="kn">from</span> <span class="nn">experiment.tuner.rtd</span> <span class="kn">import</span> <span class="n">get_discriminator_input</span><span class="p">,</span> <span class="n">RTDHead</span>
<span class="kn">from</span> <span class="nn">configuration</span> <span class="kn">import</span> <span class="n">CFG</span>

<span class="k">class</span> <span class="nc">ELECTRA</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">AbstractModel</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">cfg</span><span class="p">:</span> <span class="n">CFG</span><span class="p">,</span> <span class="n">model_func</span><span class="p">:</span> <span class="n">Callable</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">ELECTRA</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span> <span class="o">=</span> <span class="n">cfg</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">generator</span> <span class="o">=</span> <span class="n">model_func</span><span class="p">(</span><span class="n">cfg</span><span class="p">.</span><span class="n">generator_num_layers</span><span class="p">)</span>  <span class="c1"># init generator
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">mlm_head</span> <span class="o">=</span> <span class="n">MLMHead</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">rtd_masking</span> <span class="o">==</span> <span class="s">'SpanBoundaryObjective'</span><span class="p">:</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">mlm_head</span> <span class="o">=</span> <span class="n">SBOHead</span><span class="p">(</span>
                <span class="n">cfg</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">,</span>
                <span class="n">is_concatenate</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">is_concatenate</span><span class="p">,</span>
                <span class="n">max_span_length</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">max_span_length</span>
            <span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">discriminator</span> <span class="o">=</span> <span class="n">model_func</span><span class="p">(</span><span class="n">cfg</span><span class="p">.</span><span class="n">discriminator_num_layers</span><span class="p">)</span>  <span class="c1"># init generator
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">rtd_head</span> <span class="o">=</span> <span class="n">RTDHead</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">share_embed_method</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">share_embed_method</span>  <span class="c1"># instance, es, gdes
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">share_embedding</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">share_embedding</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="k">def</span> <span class="nf">discriminator_hook</span><span class="p">(</span><span class="n">module</span><span class="p">:</span> <span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">,</span> <span class="o">*</span><span class="n">inputs</span><span class="p">):</span>
            <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">share_embed_method</span> <span class="o">==</span> <span class="s">'instance'</span><span class="p">:</span>  <span class="c1"># Instance Sharing
</span>                <span class="bp">self</span><span class="p">.</span><span class="n">discriminator</span><span class="p">.</span><span class="n">embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">generator</span><span class="p">.</span><span class="n">embeddings</span>

            <span class="k">elif</span> <span class="bp">self</span><span class="p">.</span><span class="n">share_embed_method</span> <span class="o">==</span> <span class="s">'ES'</span><span class="p">:</span>  <span class="c1"># ES (Embedding Sharing)
</span>                <span class="bp">self</span><span class="p">.</span><span class="n">discriminator</span><span class="p">.</span><span class="n">embeddings</span><span class="p">.</span><span class="n">word_embedding</span><span class="p">.</span><span class="n">weight</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">generator</span><span class="p">.</span><span class="n">embeddings</span><span class="p">.</span><span class="n">word_embedding</span><span class="p">.</span><span class="n">weight</span>
                <span class="bp">self</span><span class="p">.</span><span class="n">discriminator</span><span class="p">.</span><span class="n">embeddings</span><span class="p">.</span><span class="n">abs_pos_emb</span><span class="p">.</span><span class="n">weight</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">generator</span><span class="p">.</span><span class="n">embeddings</span><span class="p">.</span><span class="n">abs_pos_emb</span><span class="p">.</span><span class="n">weight</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">discriminator</span><span class="p">.</span><span class="n">register_forward_pre_hook</span><span class="p">(</span><span class="n">discriminator_hook</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">generator_fw</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span>
            <span class="n">inputs</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
            <span class="n">labels</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
            <span class="n">padding_mask</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
            <span class="n">mask_labels</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
            <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="bp">None</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
        <span class="n">g_last_hidden_states</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">generator</span><span class="p">(</span>
            <span class="n">inputs</span><span class="p">,</span>
            <span class="n">padding_mask</span><span class="p">,</span>
            <span class="n">attention_mask</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">rtd_masking</span> <span class="o">==</span> <span class="s">'MaskedLanguageModel'</span><span class="p">:</span>
            <span class="n">g_logit</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">mlm_head</span><span class="p">(</span>
                <span class="n">g_last_hidden_states</span>
            <span class="p">)</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">rtd_masking</span> <span class="o">==</span> <span class="s">'SpanBoundaryObjective'</span><span class="p">:</span>
            <span class="n">g_logit</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">mlm_head</span><span class="p">(</span>
                <span class="n">g_last_hidden_states</span><span class="p">,</span>
                <span class="n">mask_labels</span>
            <span class="p">)</span>
        <span class="n">pred</span> <span class="o">=</span> <span class="n">g_logit</span><span class="p">.</span><span class="n">clone</span><span class="p">().</span><span class="n">detach</span><span class="p">()</span>
        <span class="n">d_inputs</span><span class="p">,</span> <span class="n">d_labels</span> <span class="o">=</span> <span class="n">get_discriminator_input</span><span class="p">(</span>
            <span class="n">inputs</span><span class="p">,</span>
            <span class="n">labels</span><span class="p">,</span>
            <span class="n">pred</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">g_logit</span><span class="p">,</span> <span class="n">d_inputs</span><span class="p">,</span> <span class="n">d_labels</span>

    <span class="k">def</span> <span class="nf">discriminator_fw</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span>
            <span class="n">inputs</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
            <span class="n">padding_mask</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
            <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="bp">None</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="n">d_last_hidden_states</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">discriminator</span><span class="p">(</span>
            <span class="n">inputs</span><span class="p">,</span>
            <span class="n">padding_mask</span><span class="p">,</span>
            <span class="n">attention_mask</span>
        <span class="p">)</span>
        <span class="n">d_logit</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">rtd_head</span><span class="p">(</span>
            <span class="n">d_last_hidden_states</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">d_logit</span>

</code></pre></div></div>
<p>ELECTRA ëª¨ë¸ ê°ì²´ëŠ” í¬ê²Œ ì„ë°°ë”© ë ˆì´ì–´ ê³µìœ , ìƒì„±ì í¬ì›Œë“œ, íŒë³„ì í¬ì›Œë“œ íŒŒíŠ¸ë¡œ ë‚˜ë‰œë‹¤. ë¨¼ì € ì„ë² ë”© ë ˆì´ì–´ ê³µìœ ëŠ” í¬ê²Œ ë‘ ê°€ì§€ ë°©ì‹ìœ¼ë¡œ êµ¬í˜„ ê°€ëŠ¥í•˜ë‹¤. í•˜ë‚˜ëŠ” ì„ë² ë”© ë ˆì´ì–´ ì¸ìŠ¤í„´ìŠ¤ ìì²´ë¥¼ ê³µìœ í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ, ìƒì„±ìì™€ íŒë³„ìì˜ ìŠ¤ì¼€ì¼ì´ ë™ì¼í•  ë•Œ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤. ë‚˜ë¨¸ì§€ëŠ” ë‹¨ì–´ ì„ë² ë”©, í¬ì§€ì…˜ ì„ë² ë”©ì˜ ê°€ì¤‘ì¹˜ í–‰ë ¬ë§Œ ê³µìœ í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ, ì„œë¡œ ìŠ¤ì¼€ì¼ì´ ë‹¬ë¼ë„ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤. ë…¼ë¬¸ì—ì„œ ì œì‹œí•˜ëŠ” ê°€ì¥ íš¨ìœ¨ì ì¸ ë°©ë²•ì€ í›„ìì´ë©°, íŒë³„ìì˜ ì„ë² ë”© í–‰ë ¬ì´ ìƒì„±ìì˜ ì„ë² ë”© í–‰ë ¬ì˜ ì£¼ì†Œë¥¼ ê°€ë¦¬í‚¤ë„ë¡ í•¨ìœ¼ë¡œì„œ êµ¬í˜„ ê°€ëŠ¥í•˜ë‹¤.</p>

<h4 id="-rtd-input-making"><strong><code class="language-plaintext highlighter-rouge">ğŸŒ† RTD Input Making</code></strong></h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">Tensor</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Tuple</span>
<span class="kn">from</span> <span class="nn">configuration</span> <span class="kn">import</span> <span class="n">CFG</span>

<span class="k">def</span> <span class="nf">get_discriminator_input</span><span class="p">(</span><span class="n">inputs</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">labels</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">pred</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
    <span class="s">""" Post Processing for Replaced Token Detection Task
    1) get index of the highest probability of [MASK] token in pred tensor
    2) convert [MASK] token to prediction token
    3) make label for Discriminator

    Args:
        inputs: pure inputs from tokenizing by tokenizer
        labels: labels for masked language modeling
        pred: prediction tensor from Generator

    returns:
        d_inputs: torch.Tensor, shape of [Batch, Sequence], for Discriminator inputs
        d_labels: torch.Tensor, shape of [Sequence], for Discriminator labels
    """</span>
    <span class="c1"># 1) flatten pred to 2D Tensor
</span>    <span class="n">d_inputs</span><span class="p">,</span> <span class="n">d_labels</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">.</span><span class="n">clone</span><span class="p">().</span><span class="n">detach</span><span class="p">().</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="bp">None</span>  <span class="c1"># detach to prevent back-propagation
</span>    <span class="n">flat_pred</span><span class="p">,</span> <span class="n">flat_label</span> <span class="o">=</span> <span class="n">pred</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">pred</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)),</span> <span class="n">labels</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># (batch * sequence, vocab_size)
</span>
    <span class="c1"># 2) get index of the highest probability of [MASK] token
</span>    <span class="n">pred_token_idx</span><span class="p">,</span> <span class="n">mlm_mask_idx</span> <span class="o">=</span> <span class="n">flat_pred</span><span class="p">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">),</span> <span class="n">torch</span><span class="p">.</span><span class="n">where</span><span class="p">(</span><span class="n">flat_label</span> <span class="o">!=</span> <span class="o">-</span><span class="mi">100</span><span class="p">)</span>
    <span class="n">pred_tokens</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">index_select</span><span class="p">(</span><span class="n">pred_token_idx</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">mlm_mask_idx</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

    <span class="c1"># 3) convert [MASK] token to prediction token
</span>    <span class="n">d_inputs</span><span class="p">[</span><span class="n">mlm_mask_idx</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span> <span class="o">=</span> <span class="n">pred_tokens</span>

    <span class="c1"># 4) make label for Discriminator
</span>    <span class="n">original_tokens</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">.</span><span class="n">clone</span><span class="p">().</span><span class="n">detach</span><span class="p">().</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">original_tokens</span><span class="p">[</span><span class="n">mlm_mask_idx</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span> <span class="o">=</span> <span class="n">flat_label</span><span class="p">[</span><span class="n">mlm_mask_idx</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span>
    <span class="n">d_labels</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">eq</span><span class="p">(</span><span class="n">original_tokens</span><span class="p">,</span> <span class="n">d_inputs</span><span class="p">).</span><span class="nb">long</span><span class="p">()</span>
    <span class="n">d_inputs</span> <span class="o">=</span> <span class="n">d_inputs</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="n">pred</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># covert to [batch, sequence]
</span>    <span class="k">return</span> <span class="n">d_inputs</span><span class="p">,</span> <span class="n">d_labels</span>
</code></pre></div></div>
<p>ì´ì œ ë§ˆì§€ë§‰ìœ¼ë¡œ íŒë³„ìì˜ ì…ë ¥ì„ ë§Œë“œëŠ” ì•Œê³ ë¦¬ì¦˜ì— ëŒ€í•œ ì½”ë“œë¥¼ ë³´ì. ì•Œê³ ë¦¬ì¦˜ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.</p>
<ul>
  <li>1) ê°œë³„ ë§ˆìŠ¤í‚¹ í† í°ì— ëŒ€í•œ ì˜ˆì¸¡ í† í° êµ¬í•˜ê¸°
    <ul>
      <li>ë¡œì§“ì„ ì‹¤ì œ í† í° ì¸ë±ìŠ¤ë¡œ ë³€í™˜</li>
    </ul>
  </li>
  <li>2) ëª¨ë“  ë§ˆìŠ¤í‚¹ ë¶€ë¶„ì— ì˜ˆì¸¡ í† í°ë“¤ë¡œ ëŒ€ì²´</li>
  <li>3) ê¸°ì¡´ ì…ë ¥ê³¼ 2ë²ˆìœ¼ë¡œ ë§Œë“¤ì–´ì§„ ì‹œí€€ìŠ¤ ë¹„êµí•´ ë¼ë²¨ ìƒì„±
    <ul>
      <li>ì„œë¡œ ê°™ìœ¼ë©´ 0</li>
      <li>ì„œë¡œ ë‹¤ë¥´ë©´ 1
ì´ë ‡ê²Œ ë§Œë“¤ì–´ì§„ ìƒˆë¡œìš´ ì…ë ¥ ì‹œí€€ìŠ¤ì™€ ë¼ë²¨ì„ ELECTRA ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ì˜ íŒë³„ì í¬ì›Œë“œ ë©”ì„œë“œì— ì¸ìë¡œ ì „ë‹¬í•˜ë©´ ëœë‹¤.</li>
    </ul>
  </li>
</ul>

<h3 id="-future-work-ì½ê³ -êµ¬í˜„í•˜ë©´ì„œ-ëŠë‚€ì --ê°œì„ ë°©í–¥"><code class="language-plaintext highlighter-rouge">ğŸŒŸ Future Work (ì½ê³  êµ¬í˜„í•˜ë©´ì„œ ëŠë‚€ì  &amp; ê°œì„ ë°©í–¥)</code></h3>

<p>ì´ë ‡ê²Œ ELECTRA ëª¨ë¸ì— ëŒ€í•œ êµ¬í˜„ê¹Œì§€ ì‚´í´ë´¤ë‹¤. ë…¼ë¬¸ì„ ì½ê³  êµ¬í˜„í•˜ë©´ì„œ ê°€ì¥ ì˜ë¬¸ìŠ¤ëŸ¬ì› ë˜ ë¶€ë¶„ì€ ì„ë² ë”© ê³µìœ  ë°©ë²•ì´ì—ˆë‹¤. ìˆ˜í•™ì ìœ¼ë¡œ ì—„ë°€í•˜ê²Œ ê³„ì‚°í•˜ê³  ë”°ì ¸ë³´ì§€ ëª»í–ˆì§€ë§Œ, ì§ê´€ì ìœ¼ë¡œë„ ìƒì„±ìì˜ MLMê³¼ íŒë³„ìì˜ RTDëŠ” ì„œë¡œ ì„±ê²©ì´ ìƒë‹¹íˆ ë‹¤ë¥¸ ì‚¬ì „ í•™ìŠµ ë°©ë²•ë¡ ì´ë¼ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤. ê·¸ë ‡ë‹¤ë©´ ë‹¨ìˆœíˆ ë‹¨ì–´, í¬ì§€ì…˜ ì„ë² ë”©ì„ ê³µìœ í•˜ëŠ” ê²½ìš° í•™ìŠµ ë°©í–¥ì„±ì´ ë‹¬ë¼ì„œ ê°„ì„­ì´ ë°œìƒí•˜ê³  ëª¨ë¸ì´ ìˆ˜ë ´í•˜ì§€ ëª»í•  ì—¬ì§€ê°€ ìƒê¸´ë‹¤. ì´ëŸ¬í•œ <code class="language-plaintext highlighter-rouge">ì¤„ë‹¤ë¦¬ê¸° í˜„ìƒ(tag-of-war)</code>ì„ ì–´ë–»ê²Œ í•´ê²°í•  ìˆ˜ ìˆì„ê¹Œì— ëŒ€í•œ ê³ ë¯¼ì´ ë” í•„ìš”í•˜ë‹¤ê³  ìƒê°í•œë‹¤.</p>

<p>ê·¸ë˜ì„œ ë‹¤ìŒ í¬ìŠ¤íŒ…ì—ì„œëŠ” ì´ëŸ¬í•œ ì¤„ë‹¤ë¦¬ê¸° í˜„ìƒì„ í•´ê²°í•˜ê³ ìí•œ ë…¼ë¬¸ì¸ <strong><a href="https://arxiv.org/abs/2111.09543">&lt;DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing&gt;</a></strong>ì„ ë¦¬ë·°í•´ë³´ê³ ì í•œë‹¤.</p>]]></content><author><name>qcqced</name><email>qcqced123@gmail.com</email></author><category term="NLP" /><category term="Natural Language Process" /><category term="ELECTRA" /><category term="BERT" /><category term="GAN" /><category term="Transformer" /><category term="Self-Attention" /><category term="Pytorch" /><summary type="html"><![CDATA[ELECTRA Official Paper Review with Pytorch Implementation]]></summary></entry><entry><title type="html">ğŸ§‘â€ğŸ« [DistilBERT] DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter</title><link href="http://localhost:4000/nlp/distilbert" rel="alternate" type="text/html" title="ğŸ§‘â€ğŸ« [DistilBERT] DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter" /><published>2024-03-11T00:00:00+09:00</published><updated>2024-03-12T02:00:00+09:00</updated><id>http://localhost:4000/nlp/distilbert</id><content type="html" xml:base="http://localhost:4000/nlp/distilbert"><![CDATA[<h3 id="overview"><code class="language-plaintext highlighter-rouge">ğŸ”­Â Overview</code></h3>

<p><code class="language-plaintext highlighter-rouge">DistilBERT</code> ëŠ” í—ˆê¹… í˜ì´ìŠ¤ ì—°êµ¬ì§„ì´ 2019ë…„ ë°œí‘œí•œ BERTì˜ ë³€í˜•ìœ¼ë¡œì„œ, On-Device Ai ê°œë°œì„ ëª©í‘œë¡œ ê²½ëŸ‰í™”ì— ì´ˆì ì„ ë§ì¶˜ ëª¨ë¸ì´ë‹¤. GPT, BERTì˜ ë“±ì¥ ì´í›„, NLP ë¶„ì•¼ì—ì„œ ë¹„ì•½ì ì¸ ì„±ëŠ¥ í–¥ìƒì´ ì´ë¤„ì¡ŒìŒì—ë„ ë¶ˆêµ¬í•˜ê³ , í„°ë¬´ë‹ˆ ì—†ëŠ” ëª¨ë¸ ì‚¬ì´ì¦ˆì™€ ì»´í“¨íŒ… ë¦¬ì†ŒìŠ¤ ìš”êµ¬ë¡œ ì¸í•´ ì‹¤ìƒí™œ ì ìš© ê°™ì€ í™œìš©ì„±ì€ ì—¬ì „íˆ í•´ê²°í•´ì•¼í•  ë¬¸ì œë¡œ ë‚¨ì•„ ìˆì—ˆë‹¤. Googleì—ì„œ ë°œí‘œí•œ ì´ˆê¸° <code class="language-plaintext highlighter-rouge">BERT-base-uncased</code> ë§Œ í•´ë„ íŒŒë¼ë¯¸í„°ê°€ 1ì–µ 1ì²œë§Œê°œ ìˆ˜ì¤€ì— ë‹¬í•œë‹¤.</p>

<p>ì´ë¥¼ ë‹¤ì–‘í•œ ë¹„ì¦ˆë‹ˆìŠ¤ ìš”êµ¬ ìƒí™©ì— ì ìš©í•  ìˆ˜ ìˆìœ¼ë ¤ë©´ ìµœì†Œí•œ 8GB ì´ìƒì˜ ê°€ì†ê¸° ì „ìš© RAM ê³µê°„ì„ ìš”êµ¬ë¡œ í•œë‹¤. ì˜¤ëŠ˜ë‚  ê°œì¸ìš© PC í˜¹ì€ ì„œë²„ ì»´í“¨í„°ì˜ ê²½ìš°, 8GB ì´ìƒì˜ VRAMì´ ë‹¬ë¦° GPUê°€ ì¼ë°˜ì ìœ¼ë¡œ íƒ‘ì¬ë˜ê¸° ë•Œë¬¸ì— í¬ê²Œ ë¬¸ì œ ë  ê²ƒ ì—†ëŠ” ìš”êµ¬ì‚¬í•­ì´ì§€ë§Œ, On-Device í™˜ê²½ì—ì„œëŠ” ì´ì•¼ê¸°ê°€ ë‹¬ë¼ì§„ë‹¤. ìµœì‹  í•˜ì´ì—”ë“œ ìŠ¤ë§ˆíŠ¸í°ì¸ Galaxy S24 Ultra, iPhone 15 Proì˜ ê²½ìš° 12GB, 8GBì˜ ë¨ ìš©ëŸ‰ì„ ë³´ìœ í•˜ê³  ìˆë‹¤. ê·¸ë§ˆì €ë„ ëŒ€ë¶€ë¶„ì˜ ì˜¨ë””ë°”ì´ìŠ¤ í™˜ê²½ì€ SoC êµ¬ì¡°ë¥¼ ì±„íƒí•˜ê³  ìˆê¸° ë•Œë¬¸ì— ì „ìš© ê°€ì†ê¸°ê°€ ì˜¨ì „íˆ ì € ëª¨ë“  ë¨ ê³µê°„ì„ í™œìš©í•  ìˆ˜ ì—†ë‹¤.</p>

<p>ë”°ë¼ì„œ ì˜¨ë””ë°”ì´ìŠ¤ì— Aië¥¼ ì ìš©í•˜ê¸° ìœ„í•´ì„œëŠ” íšê¸°ì ì¸ ëª¨ë¸ ê²½ëŸ‰í™”ê°€ í•„ìš”í•œ ìƒí™©ì´ê³  ê·¸ ì¶œë°œì ì´ ëœ ì—°êµ¬ê°€ ë°”ë¡œ <code class="language-plaintext highlighter-rouge">DistilBERT</code>ë‹¤. ë¡œì»¬ ë””ë°”ì´ìŠ¤ í™˜ê²½ì—ì„œë„ ì–¸ì–´ ëª¨ë¸ì„ í™œìš©í•˜ê¸° ìœ„í•´ í—ˆê¹… í˜ì´ìŠ¤ ì—°êµ¬ì§„ì€ ì§€ì‹ ì¦ë¥˜ ê¸°ë²•ì„ í™œìš©í•´ ì¸ì½”ë” ê¸°ë°˜ ì–¸ì–´ ëª¨ë¸ì˜ íŒŒë¼ë¯¸í„°ë¥¼ íšê¸°ì ìœ¼ë¡œ ì¤„ì´ëŠ”ë° ì„±ê³µí•œë‹¤.</p>

<p>ì •ë¦¬í•˜ìë©´, <code class="language-plaintext highlighter-rouge">DistilBERT</code> ëª¨ë¸ì€ ê¸°ì¡´ BERTì˜ êµ¬ì¡°ì  ì¸¡ë©´ ê°œì„ ì´ ì•„ë‹Œ, ì‚¬ì „í•™ìŠµ ë°©ë²• íŠ¹íˆ ê²½ëŸ‰í™”ì— ì´ˆì ì„ ë§ì¶˜ ì‹œë„ë¼ê³  ë³¼ ìˆ˜ ìˆë‹¤. ë”°ë¼ì„œ ì–´ë–¤ ëª¨ë¸ì´ë”ë¼ë„, ì¸ì½”ë” ì–¸ì–´ ëª¨ë¸ì´ë¼ë©´ ëª¨ë‘ <code class="language-plaintext highlighter-rouge">DistilBERT</code> êµ¬ì¡°ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìœ¼ë©°, ê¸°ì¡´ ë…¼ë¬¸ì—ì„œëŠ” ì›ë³¸ BERT êµ¬ì¡°ë¥¼ ì‚¬ìš©í–ˆë‹¤. ì´ë²ˆ í¬ìŠ¤íŒ…ì—ì„œë„ BERT êµ¬ì¡°ì— ëŒ€í•œ ì„¤ëª… ëŒ€ì‹ , <code class="language-plaintext highlighter-rouge">DistilBERT</code>ì˜ ì‚¬ì „ í•™ìŠµ ë°©ë²•ë¡ ì¸ <code class="language-plaintext highlighter-rouge">Knowledge Distillation</code>ì— ëŒ€í•´ì„œë§Œ ë‹¤ë£¨ë ¤ê³  í•œë‹¤.</p>

<h3 id="knowledge-distillations"><code class="language-plaintext highlighter-rouge">ğŸ“²Â Knowledge Distillations</code></h3>

\[\min_{\theta}\sum_{x \in X} \alpha \mathcal{L}_{\text{KL}}(x, \theta) + \beta \mathcal{L}_{\text{MLM}}(x, \theta) + \gamma \mathcal{L}_{\text{Cos}}(x, \theta)\]

<p><code class="language-plaintext highlighter-rouge">DistilBERT</code>ëŠ” Teacher-Student Architectureë¥¼ ì°¨ìš©í•´ ìƒëŒ€ì ìœ¼ë¡œ ì‘ì€ íŒŒë¼ë¯¸í„° ì‚¬ì´ì¦ˆë¥¼ ê°–ëŠ” <code class="language-plaintext highlighter-rouge">Student</code> ëª¨ë¸ì—ê²Œ <code class="language-plaintext highlighter-rouge">Teacher</code>ì˜ ì§€ì‹ì„ ì „ìˆ˜í•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•œë‹¤. ë”°ë¼ì„œ <code class="language-plaintext highlighter-rouge">Teacher</code> ëª¨ë¸ì€ ì´ë¯¸ ì‚¬ì „ í•™ìŠµì„ ë§ˆì¹˜ê³  ìˆ˜ë ´ëœ ìƒíƒœì˜ ê°€ì¤‘ì¹˜ë¥¼ ê°–ê³  ìˆëŠ” ëª¨ë¸ì„ ì‚¬ìš©í•´ì•¼ í•œë‹¤. ë”ë¶ˆì–´ Teacher ëª¨ë¸ì€ êµ¬ì¡°ë§Œ ê¸°ì¡´ BERTë¥¼ ë”°ë¥´ë˜, ì‚¬ì „ í•™ìŠµ ë°©ì‹ì€ RoBERTaì˜ ë°©ì‹ê³¼ ë™ì¼(NSP ì œê±°, Dynamic Masking ì ìš©)í•˜ê²Œ í›ˆë ¨ë˜ì–´ì•¼ í•œë‹¤.</p>

<p>í•œí¸, <code class="language-plaintext highlighter-rouge">Student</code> ëª¨ë¸ì€ <code class="language-plaintext highlighter-rouge">Teacher</code>ì˜ 60%ì •ë„ íŒŒë¼ë¯¸í„° ì‚¬ì´ì¦ˆë¥¼ ê°–ë„ë¡ ì¶•ì†Œí•˜ì—¬ ì‚¬ìš©í•œë‹¤. ì´ ë•Œ ì¶•ì†ŒëŠ” ëª¨ë¸ì˜ <code class="language-plaintext highlighter-rouge">depth</code>(ë ˆì´ì–´ ê°œìˆ˜)ì—ë§Œ ì ìš©í•˜ëŠ”ë°, ì—°êµ¬ì§„ì— ë”°ë¥´ë©´ <code class="language-plaintext highlighter-rouge">width</code>(ì€ë‹‰ì¸µ í¬ê¸°)ëŠ” ì¶•ì†Œë¥¼ ì ìš©í•´ë„ ì—°ì‚° íš¨ìœ¨ì´ ì¦ê°€í•˜ì§€ ì•ŠëŠ”ë‹¤ê³  í•œë‹¤. ì •ë¦¬í•˜ë©´ <code class="language-plaintext highlighter-rouge">Teacher</code> ëª¨ë¸ì˜ <code class="language-plaintext highlighter-rouge">ë ˆì´ì–´ ê°œìˆ˜*0.6</code>ì˜ ê°œìˆ˜ë§Œí¼ ì¸ì½”ë”ë¥¼ ìŒ“ìœ¼ë©´ ëœë‹¤ëŠ” ê²ƒì´ë‹¤.</p>

<p>ê·¸ë¦¬ê³  ìµœëŒ€í•œ <code class="language-plaintext highlighter-rouge">Teacher</code>ì˜ ì§€ì‹ì„ ì „ìˆ˜í•´ì•¼ í•˜ê¸° ë•Œë¬¸ì—, ë°ì´í„°ëŠ” <code class="language-plaintext highlighter-rouge">Teacher</code> ë¥¼ ìˆ˜ë ´ì‹œí‚¨ ê²ƒê³¼ ë™ì¼í•œ ì„¸íŠ¸ë¥¼ ì´ìš©í•´ì•¼ í•œë‹¤. ì´ ë•Œ, Teacher ëª¨ë¸ì€ ì´ë¯¸ MLE ë°©ì‹ìœ¼ë¡œ í›ˆë ¨ì´ ëœ ìƒíƒœë¼ì„œ ë¡œì§“ì´ ë‹¨ì¼ í† í° í•˜ë‚˜ ìª½ìœ¼ë¡œ ì ë ¤ ìˆì„ ê°€ëŠ¥ì„±ì´ ë§¤ìš° ë†’ë‹¤. ì´ëŠ” <code class="language-plaintext highlighter-rouge">Student</code> ëª¨ë¸ì˜ ì¼ë°˜í™” ëŠ¥ë ¥ì— ì•…ì˜í–¥ì„ ë¯¸ì¹  ê°€ëŠ¥ì„±ì´ ë†’ë‹¤. ë”°ë¼ì„œ Temperature ë³€ìˆ˜ $T$ ë„ì…í•´ ì†Œí”„íŠ¸ ë§¥ìŠ¤(ë¡œì§“)ì˜ ë¶„í¬ë¥¼ í‰íƒ„í™” í•œë‹¤. ì´ë ‡ê²Œ í•˜ë©´, <code class="language-plaintext highlighter-rouge">argmax()</code> ê°€ ì•„ë‹Œ ë‹¤ë¥¸ í† í° í‘œí˜„ì— ëŒ€í•´ì„œë„ <code class="language-plaintext highlighter-rouge">Student</code> ëª¨ë¸ì´ ì§€ì‹ì„ ìŠµë“í•  ìˆ˜ ìˆì–´ì„œ í’ë¶€í•œ ë¬¸ë§¥ì„ í•™ìŠµí•˜ê³  ì¼ë°˜í™” ëŠ¥ë ¥ì„ ë†’ì´ëŠ”ë° ë„ì›€ì´ ëœë‹¤. ì´ë¥¼ <code class="language-plaintext highlighter-rouge">ì•”í‘ ì§€ì‹(Dark Knowledge)</code> ì„ í™œìš©í•œë‹¤ê³  í‘œí˜„í•œë‹¤. Temperature ë³€ìˆ˜ $T$ ë„ì…í•œ ì†Œí”„íŠ¸ë§¥ìŠ¤ í•¨ìˆ˜ ìˆ˜ì‹ì€ ì•„ë˜ì™€ ê°™ë‹¤.</p>

\[\text{softmax}(x_i) = \frac{e^{\frac{x_i}{\tau}}}{\sum_{j} e^{\frac{x_j}{\tau}}}\]

<p>ìˆ˜ì‹ìƒ ë³€ìˆ˜ $T$ì˜ ê°’ì„ 1ì´ìƒìœ¼ë¡œ ì„¸íŒ…í•´ì•¼ í‰íƒ„í™”ë¥¼ í•  ìˆ˜ ìˆë‹¤. ë”°ë¼ì„œ ì—°êµ¬ì§„ì€ $T =2$ ë¡œ ë‘ê³  ì‚¬ì „ í•™ìŠµì„ ì§„í–‰í–ˆë‹¤(ë…¼ë¬¸ì— ê³µê°œì•ˆë¨, GitHubì— ìˆìŒ). ì´ë²ˆ íŒŒíŠ¸ ë§¨ ì²˜ìŒì— ë“±ì¥í•œ ìˆ˜ì‹ì„ ë‹¤ì‹œ ë³´ì. ê²°êµ­ <code class="language-plaintext highlighter-rouge">DisilBERT</code>ì˜ ëª©ì í•¨ìˆ˜ëŠ” 3ê°€ì§€ ì†ì‹¤ì˜ ê°€ì¤‘í•©ìœ¼ë¡œ êµ¬ì„±ëœë‹¤. ì´ì œë¶€í„°ëŠ” ê°œë³„ ì†ì‹¤ì— ëŒ€í•´ì„œ ìì„¸íˆ ì‚´í´ë³´ì.</p>

<h4 id="distillation-loss-kl-divergence-loss"><code class="language-plaintext highlighter-rouge">ğŸª¢Â Distillation Loss: KL-Divergence Loss</code></h4>

\[\text{KL-Divergence}(P || Q) = \sum_{i} P(i) \log \frac{P(i)}{Q(i)}\]

<p>ì¦ë¥˜ ì†ì‹¤ë¡œ ì‚¬ìš©ë˜ëŠ” <code class="language-plaintext highlighter-rouge">KL-Divergence Loss</code>ëŠ” ë‘ í™•ë¥  ë¶„í¬ ê°„ì˜ ì°¨ì´ë¥¼ ì¸¡ì •í•˜ëŠ” ì§€í‘œ ì¤‘ í•˜ë‚˜ë‹¤. ì£¼ë¡œ í™•ë¥  ë¶„í¬ Pì™€ Q ì‚¬ì´ì˜ ì°¨ì´ë¥¼ ë‚˜íƒ€ë‚´ëŠ”ë°, ê°œë³„ ìš”ì†Œì˜ í™•ë¥ ê°’ ì°¨ì´ê°€ í´ìˆ˜ë¡ í•©ì‚°ê°’ì€ ì»¤ì ¸ ì†ì‹¤ì´ ì»¤ì§€ê²Œ ëœë‹¤. ë°˜ëŒ€ë¡œ ë‘ ë¶„í¬ì˜ ê°œë³„ ìš”ì†Œ í™•ë¥ ê°’ ì°¨ì´ê°€ ì‘ë‹¤ë©´ ë‹¹ì—°íˆ, ë‘ ë¶„í¬ê°€ ìœ ì‚¬í•˜ë‹¤ëŠ” ì˜ë¯¸ì´ë¯€ë¡œ ì†ì‹¤ ì—­ì‹œ ì‘ì•„ì§€ê²Œ ëœë‹¤. ì¼ë°˜ì ìœ¼ë¡œ <code class="language-plaintext highlighter-rouge">KL-Divergence Loss</code> ì—ì„œ í™•ë¥ ë¶„í¬ $P$ ê°€ ì´ìƒì ì¸ í™•ë¥  ë¶„í¬ë¥¼, $Q$ ê°€ ëª¨ë¸ì´ ì˜ˆì¸¡í•œ í™•ë¥ ë¶„í¬ë¥¼ ì˜ë¯¸í•œë‹¤. ë”°ë¼ì„œ <code class="language-plaintext highlighter-rouge">DistilBERT</code>ì˜ ê²½ìš° í™•ë¥ ë¶„í¬ $P$ ìë¦¬ì—ëŠ” <code class="language-plaintext highlighter-rouge">Teacher</code> ëª¨ë¸ì˜ ì†Œí”„íŠ¸ë§¥ìŠ¤ ë¶„í¬ê°€, $Q$ ì—ëŠ” <code class="language-plaintext highlighter-rouge">Student</code> ëª¨ë¸ì˜ ì†Œí”„íŠ¸ë§¥ìŠ¤ ë¶„í¬ê°€ ëŒ€ì…ë˜ë©´ ëœë‹¤. ì´ ë•Œ ë‘ í™•ë¥ ë¶„í¬ ëª¨ë‘, ì•”í‘ ì§€ì‹ íšë“ì„ ìœ„í•´ ì†Œí”„íŠ¸ë§¥ìŠ¤ í‰íƒ„í™”ë¥¼ ì ìš©í•œ ê²°ê³¼ë¥¼ ì‚¬ìš©í•œë‹¤. ë…¼ë¬¸ì—ì„œ, ì„ ìƒ ëª¨ë¸ ì˜ˆì¸¡ì— í‰íƒ„í™”ë¥¼ ì ìš©í•œ ê²ƒì„ <code class="language-plaintext highlighter-rouge">ì†Œí”„íŠ¸ ë¼ë²¨</code>, í•™ìƒ ëª¨ë¸ì˜ ê²ƒì— ì ìš©í•œ ê²°ê³¼ëŠ” <code class="language-plaintext highlighter-rouge">ì†Œí”„íŠ¸ ì˜ˆì¸¡</code>ì´ë¼ê³  ë¶€ë¥¸ë‹¤.</p>

<h4 id="student-loss-mlm-loss"><code class="language-plaintext highlighter-rouge">ğŸ§‘â€ğŸ“Â Student Loss: MLM Loss</code></h4>

\[\mathcal{L}_{\text{MLM}} = - \sum_{i=1}^{N} \sum_{j=1}^{L} \mathbb{1}_{m_{ij}} \log \text{softmax}(x_{ij})\]

<p>í•™ìƒ ì†ì‹¤ì€ ë§ê·¸ëŒ€ë¡œ ê¸°ë³¸ì ì¸ MLM ì†ì‹¤ì„ ë§í•œë‹¤. ì •í™•í•œ ì†ì‹¤ê°’ ê³„ì‚°ì„ ìœ„í•´ì„œ í•™ìƒì˜ ì†Œí”„íŠ¸ë§¥ìŠ¤ ë¶„í¬ì— í‰íƒ„í™”ë¥¼ ì ìš©í•˜ì§€ ì•ŠëŠ”ë‹¤. ì´ë¥¼ ë…¼ë¬¸ì—ì„œëŠ” <code class="language-plaintext highlighter-rouge">í•˜ë“œ ì˜ˆì¸¡</code>ì´ë¼ê³  ë¶€ë¥¸ë‹¤. ë¼ë²¨ ì—­ì‹œ <code class="language-plaintext highlighter-rouge">Teacher</code>ë¡œë¶€í„° ë‚˜ì˜¨ ê²ƒì´ ì•„ë‹Œ ì›ë˜ MLM ìˆ˜í–‰ì— ì‚¬ìš©ë˜ëŠ” ë§ˆìŠ¤í‚¹ ë¼ë²¨ì„ ì‚¬ìš©í•œë‹¤.</p>

<h4 id="cosine-embedding-loss-contrastive-loss-by-cosine-similarity"><code class="language-plaintext highlighter-rouge">ğŸŒ†Â Cosine Embedding Loss: Contrastive Loss by cosine similarity</code></h4>

\[\mathcal{L}_{\text{COS}}(x,y) = \begin{cases} 1 - \cos(x_1, x_2), &amp; \text{if } y = 1 \\ \max(0, \cos(x_1, x_2) - \text{margin}), &amp; \text{if } y = -1 \end{cases}\]

<p><code class="language-plaintext highlighter-rouge">Teacher</code> ëª¨ë¸ê³¼ <code class="language-plaintext highlighter-rouge">Student</code> ëª¨ë¸ì˜ ë§ˆì§€ë§‰ ì¸ì½”ë” ëª¨ë¸ì´ ì¶œë ¥í•˜ëŠ” ì€ë‹‰ê°’ì— ëŒ€í•œ <code class="language-plaintext highlighter-rouge">Contrastive Loss</code>ë¥¼ ì˜ë¯¸í•œë‹¤. ì´ ë•Œ <code class="language-plaintext highlighter-rouge">Distance Metric</code>ì€ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ ì‚¬ìš©í•œë‹¤. ê·¸ë˜ì„œ ì½”ì‚¬ì¸ ì„ë² ë”© ì†ì‹¤ì´ë¼ê³  ë…¼ë¬¸ì—ì„œ ì •ì˜í•˜ëŠ” ê²ƒìœ¼ë¡œ ì¶”ì •ëœë‹¤. ìœ„ ìˆ˜ì‹ì„ ìµœì í™”í•˜ëŠ” ê²ƒì„ ëª©ì ìœ¼ë¡œ í•œë‹¤. ì´ ë•Œ ë¼ë²¨ì€ <code class="language-plaintext highlighter-rouge">[BS, Seq_len]</code>ì˜ í¬ê¸°ë¥¼ ê°–ë˜, ëª¨ë“  ì›ì†ŒëŠ” 1ì´ ë˜ë„ë¡ ë§Œë“ ë‹¤. ì´ìœ ëŠ” ê°„ë‹¨í•˜ë‹¤. <code class="language-plaintext highlighter-rouge">Student</code> ëª¨ë¸ì˜ ì€ë‹‰ê°’ì´ <code class="language-plaintext highlighter-rouge">Teacher</code> ëª¨ë¸ì˜ ê²ƒê³¼ ìµœëŒ€í•œ ë¹„ìŠ·í•´ì§€ë„ë¡ ë§Œë“œëŠ”ê²Œ ìš°ë¦¬ ëª©ì ì´ê¸° ë•Œë¬¸ì´ë‹¤.</p>

<h3 id="implementation-by-pytorch"><code class="language-plaintext highlighter-rouge">ğŸ‘©â€ğŸ’»Â Implementation by Pytorch</code></h3>
<p>ë…¼ë¬¸ì˜ ë‚´ìš©ê³¼ ì˜¤í”¼ì…œë¡œ ê³µê°œëœ ì½”ë“œë¥¼ ì¢…í•©í•˜ì—¬ íŒŒì´í† ì¹˜ë¡œ <code class="language-plaintext highlighter-rouge">DistilBERT</code>ë¥¼ êµ¬í˜„í•´ë´¤ë‹¤. ë…¼ë¬¸ì— í¬í•¨ëœ ì•„ì´ë””ì–´ë¥¼ ì´í•´í•˜ëŠ”ë°ëŠ” ì—­ì‹œ ì–´ë µì§€ ì•Šì•˜ì§€ë§Œ, í˜ì´í¼ì— hyper-param í…Œì´ë¸”ì´ ë”°ë¡œ ì œì‹œë˜ì–´ ìˆì§€ ì•Šì•„ ê³µê°œëœ ì½”ë“œë¥¼ ì•ˆ ë³¼ìˆ˜ê°€ ì—†ì—ˆë‹¤.</p>

<p>ì „ì²´ ëª¨ë¸ êµ¬ì¡° ëŒ€í•œ ì½”ë“œëŠ” <strong><a href="https://github.com/qcqced123/model_study">ì—¬ê¸° ë§í¬</a></strong>ë¥¼ í†µí•´ ì°¸ê³ ë°”ë€ë‹¤.</p>

<h4 id="knowledge-distillation-pipeline"><code class="language-plaintext highlighter-rouge">ğŸ‘©â€ğŸ’»Â Knowledge Distillation Pipeline</code></h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">train_val_fn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">loader_train</span><span class="p">,</span> <span class="n">model</span><span class="p">:</span> <span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">criterion</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">],</span> <span class="n">optimizer</span><span class="p">,</span><span class="n">scheduler</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]:</span>
    <span class="s">""" Function for train loop with validation for each batch*N Steps
    DistillBERT has three loss:

        1) distillation loss, calculated by soft targets &amp; soft predictions
            (nn.KLDIVLoss(reduction='batchmean'))

        2) student loss, calculated by hard targets &amp; hard predictions
            (nn.CrossEntropyLoss(reduction='mean')), same as pure MLM Loss

        3) cosine similarity loss, calculated by student &amp; teacher logit similarity
            (nn.CosineEmbeddingLoss(reduction='mean')), similar as contrastive loss

    Those 3 losses are summed jointly and then backward to student model
    """</span>
    <span class="n">scaler</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">amp</span><span class="p">.</span><span class="n">GradScaler</span><span class="p">(</span><span class="n">enabled</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">amp_scaler</span><span class="p">)</span>
    <span class="n">model</span><span class="p">.</span><span class="n">train</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">step</span><span class="p">,</span> <span class="n">batch</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">tqdm</span><span class="p">(</span><span class="n">loader_train</span><span class="p">)):</span>
        <span class="n">optimizer</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">(</span><span class="n">set_to_none</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="n">inputs</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="s">'input_ids'</span><span class="p">].</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">device</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="n">labels</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="s">'labels'</span><span class="p">].</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">device</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="n">padding_mask</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="s">'padding_mask'</span><span class="p">].</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">device</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

        <span class="n">mask</span> <span class="o">=</span> <span class="n">padding_mask</span><span class="p">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">).</span><span class="n">expand</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">dim_model</span><span class="p">)</span>  <span class="c1"># for hidden states dim
</span>        <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">t_hidden_state</span><span class="p">,</span> <span class="n">soft_target</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">teacher_fw</span><span class="p">(</span>
                <span class="n">inputs</span><span class="o">=</span><span class="n">inputs</span><span class="p">,</span>
                <span class="n">padding_mask</span><span class="o">=</span><span class="n">padding_mask</span><span class="p">,</span>
                <span class="n">mask</span><span class="o">=</span><span class="n">mask</span>
            <span class="p">)</span>  <span class="c1"># teacher model's pred =&gt; hard logit
</span>
        <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">amp</span><span class="p">.</span><span class="n">autocast</span><span class="p">(</span><span class="n">enabled</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">amp_scaler</span><span class="p">):</span>
            <span class="n">s_hidden_state</span><span class="p">,</span> <span class="n">s_logit</span><span class="p">,</span> <span class="n">soft_pred</span><span class="p">,</span> <span class="n">c_labels</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">student_fw</span><span class="p">(</span>
                <span class="n">inputs</span><span class="o">=</span><span class="n">inputs</span><span class="p">,</span>
                <span class="n">padding_mask</span><span class="o">=</span><span class="n">padding_mask</span><span class="p">,</span>
                <span class="n">mask</span><span class="o">=</span><span class="n">mask</span>
            <span class="p">)</span>
            <span class="n">d_loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">[</span><span class="s">"KLDivLoss"</span><span class="p">](</span><span class="n">soft_pred</span><span class="p">.</span><span class="n">log</span><span class="p">(),</span> <span class="n">soft_target</span><span class="p">)</span>  <span class="c1"># nn.KLDIVLoss
</span>            <span class="n">s_loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">[</span><span class="s">"CrossEntropyLoss"</span><span class="p">](</span><span class="n">s_logit</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">vocab_size</span><span class="p">),</span> <span class="n">labels</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>  <span class="c1"># nn.CrossEntropyLoss
</span>            <span class="n">c_loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">[</span><span class="s">"CosineEmbeddingLoss"</span><span class="p">](</span><span class="n">s_hidden_state</span><span class="p">,</span> <span class="n">t_hidden_state</span><span class="p">,</span> <span class="n">c_labels</span><span class="p">)</span>  <span class="c1"># nn.CosineEmbeddingLoss
</span>            <span class="n">loss</span> <span class="o">=</span> <span class="n">d_loss</span><span class="o">*</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">alpha_distillation</span> <span class="o">+</span> <span class="n">s_loss</span><span class="o">*</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">alpha_student</span> <span class="o">+</span> <span class="n">c_loss</span><span class="o">*</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">alpha_cosine</span>  <span class="c1"># linear combination loss
</span>
        <span class="n">scaler</span><span class="p">.</span><span class="n">scale</span><span class="p">(</span><span class="n">loss</span><span class="p">).</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">scaler</span><span class="p">.</span><span class="n">step</span><span class="p">(</span><span class="n">optimizer</span><span class="p">)</span>
        <span class="n">scaler</span><span class="p">.</span><span class="n">update</span><span class="p">()</span>
        <span class="n">scheduler</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>
</code></pre></div></div>

<h4 id="knowledge-distillation-model"><code class="language-plaintext highlighter-rouge">ğŸ‘©â€ğŸ’»Â Knowledge Distillation Model</code></h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">DistillationKnowledge</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">AbstractTask</span><span class="p">):</span>
    <span class="s">""" Custom Task Module for Knowledge Distillation by DistilBERT Style Architecture
    DistilBERT Style Architecture is Teacher-Student Framework for Knowledge Distillation,

    And then they have 3 objective functions:
        1) distillation loss, calculated by soft targets &amp; soft predictions
            (nn.KLDIVLoss(reduction='batchmean'))
        2) student loss, calculated by hard targets &amp; hard predictions
            (nn.CrossEntropyLoss(reduction='mean')), same as pure MLM Loss
        3) cosine similarity loss, calculated by student &amp; teacher logit similarity
            (nn.CosineEmbeddingLoss(reduction='mean')), similar as contrastive loss

    References:
        https://arxiv.org/pdf/1910.01108.pdf
        https://github.com/huggingface/transformers/blob/main/examples/research_projects/distillation/distiller.py
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">cfg</span><span class="p">:</span> <span class="n">CFG</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">DistillationKnowledge</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span> <span class="o">=</span> <span class="n">CFG</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">DistilBERT</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">,</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">select_model</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">_init_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">teacher_load_pretrained</span><span class="p">:</span>  <span class="c1"># for teacher model
</span>            <span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">teacher</span><span class="p">.</span><span class="n">load_state_dict</span><span class="p">(</span>
                <span class="n">torch</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="n">cfg</span><span class="p">.</span><span class="n">checkpoint_dir</span> <span class="o">+</span> <span class="n">cfg</span><span class="p">.</span><span class="n">teacher_state_dict</span><span class="p">),</span>
                <span class="n">strict</span><span class="o">=</span><span class="bp">False</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">student_load_pretrained</span><span class="p">:</span>  <span class="c1"># for student model
</span>            <span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">student</span><span class="p">.</span><span class="n">load_state_dict</span><span class="p">(</span>
                <span class="n">torch</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="n">cfg</span><span class="p">.</span><span class="n">checkpoint_dir</span> <span class="o">+</span> <span class="n">cfg</span><span class="p">.</span><span class="n">student_state_dict</span><span class="p">),</span>
                <span class="n">strict</span><span class="o">=</span><span class="bp">True</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">freeze</span><span class="p">:</span>
            <span class="n">freeze</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">teacher</span><span class="p">)</span>
            <span class="n">freeze</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">mlm_head</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">gradient_checkpoint</span><span class="p">:</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">gradient_checkpointing_enable</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">teacher_fw</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">inputs</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">padding_mask</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">mask</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
        <span class="n">is_valid</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="bp">False</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
        <span class="s">""" teacher forward pass to make soft target, last_hidden_state for distillation loss """</span>
        <span class="c1"># 1) make soft target
</span>        <span class="n">temperature</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="k">if</span> <span class="n">is_valid</span> <span class="k">else</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">temperature</span>
        <span class="n">last_hidden_state</span><span class="p">,</span> <span class="n">t_logit</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">teacher_fw</span><span class="p">(</span>
            <span class="n">inputs</span><span class="p">,</span>
            <span class="n">padding_mask</span><span class="p">,</span>
            <span class="n">attention_mask</span>
        <span class="p">)</span>
        <span class="n">last_hidden_state</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">masked_select</span><span class="p">(</span><span class="n">last_hidden_state</span><span class="p">,</span> <span class="o">~</span><span class="n">mask</span><span class="p">)</span>  <span class="c1"># for inverse select
</span>        <span class="n">last_hidden_state</span> <span class="o">=</span> <span class="n">last_hidden_state</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">dim_model</span><span class="p">)</span>  <span class="c1"># flatten last_hidden_state
</span>        <span class="n">soft_target</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="n">softmax</span><span class="p">(</span>
            <span class="n">t_logit</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">vocab_size</span><span class="p">)</span> <span class="o">/</span> <span class="n">temperature</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span>  <span class="c1"># flatten softmax distribution
</span>            <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span>
        <span class="p">)</span>  <span class="c1"># [bs* seq, vocab_size]
</span>        <span class="k">return</span> <span class="n">last_hidden_state</span><span class="p">,</span> <span class="n">soft_target</span>

    <span class="k">def</span> <span class="nf">student_fw</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">inputs</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">padding_mask</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">mask</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
        <span class="n">is_valid</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="bp">False</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
        <span class="s">""" student forward pass to make soft prediction, hard prediction for student loss """</span>
        <span class="n">temperature</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="k">if</span> <span class="n">is_valid</span> <span class="k">else</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">temperature</span>
        <span class="n">last_hidden_state</span><span class="p">,</span> <span class="n">s_logit</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">teacher_fw</span><span class="p">(</span>
            <span class="n">inputs</span><span class="p">,</span>
            <span class="n">padding_mask</span><span class="p">,</span>
            <span class="n">attention_mask</span>
        <span class="p">)</span>
        <span class="n">last_hidden_state</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">masked_select</span><span class="p">(</span><span class="n">last_hidden_state</span><span class="p">,</span> <span class="o">~</span><span class="n">mask</span><span class="p">)</span>  <span class="c1"># for inverse select
</span>        <span class="n">last_hidden_state</span> <span class="o">=</span> <span class="n">last_hidden_state</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">dim_model</span><span class="p">)</span>  <span class="c1"># flatten last_hidden_state
</span>        <span class="n">c_labels</span> <span class="o">=</span> <span class="n">last_hidden_state</span><span class="p">.</span><span class="n">new</span><span class="p">(</span><span class="n">last_hidden_state</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)).</span><span class="n">fill_</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">soft_pred</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="n">softmax</span><span class="p">(</span>
            <span class="n">s_logit</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">vocab_size</span><span class="p">)</span> <span class="o">/</span> <span class="n">temperature</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span>  <span class="c1"># flatten softmax distribution
</span>            <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">last_hidden_state</span><span class="p">,</span> <span class="n">s_logit</span><span class="p">,</span> <span class="n">soft_pred</span><span class="p">,</span> <span class="n">c_labels</span>
</code></pre></div></div>

<h4 id="distilbert-model"><code class="language-plaintext highlighter-rouge">ğŸ‘©â€ğŸ’»Â DistilBERT Model</code></h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">DistilBERT</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">AbstractModel</span><span class="p">):</span>
    <span class="s">""" Main class for DistilBERT Style Model, Teacher-Student Framework
    for Knowledge Distillation aim to lighter Large Scale LLM model. This model have 3 objective functions:

        1) distillation loss, calculated by soft targets &amp; soft predictions
            (nn.KLDIVLoss(reduction='batchmean'))

        2) student loss, calculated by hard targets &amp; hard predictions
            (nn.CrossEntropyLoss(reduction='mean')), same as pure MLM Loss

        3) cosine similarity loss, calculated by student &amp; teacher logit similarity
            (nn.CosineEmbeddingLoss(reduction='mean')), similar as contrastive loss

    soft targets &amp; soft predictions are meaning that logit are passed through softmax function applied with temperature T
    temperature T aim to flatten softmax layer distribution for making "Dark Knowledge" from teacher model

    hard targets &amp; hard predictions are meaning that logit are passed through softmax function without temperature T
    hard targets are same as just simple labels from MLM Collator returns for calculating cross entropy loss

    cosine similarity loss is calculated by cosine similarity between student &amp; teacher
    in official repo, they mask padding tokens for calculating cosine similarity, target for this task is 1
    cosine similarity is calculated by nn.CosineSimilarity() function, values are range to [-1, 1]

    you can select any other backbone model architecture for Teacher &amp; Student Model for knowledge distillation
    but, in original paper, BERT is used for Teacher Model &amp; Student
    and you must select pretrained model for Teacher Model, because Teacher Model is used for knowledge distillation,
    which is containing pretrained mlm head

    Do not pass gradient backward to teacher model!!
    (teacher model must be frozen or register_buffer to model or use no_grad() context manager)

    Args:
        cfg: configuration.CFG
        model_func: make model instance in runtime from config.json

    References:
        https://arxiv.org/pdf/1910.01108.pdf
        https://github.com/huggingface/transformers/blob/main/examples/research_projects/distillation/distiller.py
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">cfg</span><span class="p">:</span> <span class="n">CFG</span><span class="p">,</span> <span class="n">model_func</span><span class="p">:</span> <span class="n">Callable</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">DistilBERT</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span> <span class="o">=</span> <span class="n">cfg</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">teacher</span> <span class="o">=</span> <span class="n">model_func</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">teacher_num_layers</span><span class="p">)</span>  <span class="c1"># must be loading pretrained model containing mlm head
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">mlm_head</span> <span class="o">=</span> <span class="n">MLMHead</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">)</span>  <span class="c1"># must be loading pretrained model's mlm head
</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">student</span> <span class="o">=</span> <span class="n">model_func</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">student_num_layers</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">s_mlm_head</span> <span class="o">=</span> <span class="n">MLMHead</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">teacher_fw</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">inputs</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">padding_mask</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
        <span class="s">""" forward pass for teacher model
        """</span>
        <span class="n">last_hidden_state</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">teacher</span><span class="p">(</span>
            <span class="n">inputs</span><span class="p">,</span>
            <span class="n">padding_mask</span><span class="p">,</span>
            <span class="n">attention_mask</span>
        <span class="p">)</span>
        <span class="n">t_logit</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">mlm_head</span><span class="p">(</span><span class="n">last_hidden_state</span><span class="p">)</span>  <span class="c1"># hard logit =&gt; to make soft logit
</span>        <span class="k">return</span> <span class="n">last_hidden_state</span><span class="p">,</span> <span class="n">t_logit</span>

    <span class="k">def</span> <span class="nf">student_fw</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">inputs</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">padding_mask</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="bp">None</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
        <span class="s">""" forward pass for student model
        """</span>
        <span class="n">last_hidden_state</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">student</span><span class="p">(</span>
            <span class="n">inputs</span><span class="p">,</span>
            <span class="n">padding_mask</span><span class="p">,</span>
            <span class="n">attention_mask</span>
        <span class="p">)</span>
        <span class="n">s_logit</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">s_mlm_head</span><span class="p">(</span><span class="n">last_hidden_state</span><span class="p">)</span>  <span class="c1"># hard logit =&gt; to make soft logit
</span>        <span class="k">return</span> <span class="n">last_hidden_state</span><span class="p">,</span> <span class="n">s_logit</span>
</code></pre></div></div>]]></content><author><name>qcqced</name><email>qcqced123@gmail.com</email></author><category term="NLP" /><category term="Natural Language Process" /><category term="DistilBERT" /><category term="BERT" /><category term="Self-Attention" /><category term="Pytorch" /><summary type="html"><![CDATA[DistilBERT Official Paper Review with Pytorch Implementation]]></summary></entry><entry><title type="html">ğŸª¢Â [DeBERTa-V3] DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing</title><link href="http://localhost:4000/nlp/deberta_v3" rel="alternate" type="text/html" title="ğŸª¢Â [DeBERTa-V3] DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing" /><published>2024-03-11T00:00:00+09:00</published><updated>2024-03-12T02:00:00+09:00</updated><id>http://localhost:4000/nlp/deberta-v3</id><content type="html" xml:base="http://localhost:4000/nlp/deberta_v3"><![CDATA[<h3 id="overview"><code class="language-plaintext highlighter-rouge">ğŸ”­Â Overview</code></h3>

<p>2021ë…„ Microsoftì—ì„œ ê³µê°œí•œ <code class="language-plaintext highlighter-rouge">DeBERTa-V3</code>ì€ ê¸°ì¡´ DeBERTaì˜ ëª¨ë¸ êµ¬ì¡°ëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€í•˜ë˜, ELECTRAì˜ Generator-Discriminator êµ¬ì¡°ë¥¼ ì°¨ìš©í•˜ì—¬ ì „ì‘ ëŒ€ë¹„ ì„±ëŠ¥ì„ í–¥ìƒ ì‹œí‚¨ ëª¨ë¸ì´ë‹¤. ELECTRAì—ì„œ BackBone ëª¨ë¸ë¡œ BERT ëŒ€ì‹  <code class="language-plaintext highlighter-rouge">DeBERTaì„</code> ì‚¬ìš©í–ˆë‹¤ê³  ìƒê°í•˜ë©´ ëœë‹¤. ê±°ê¸°ì— ë”í•´ ELECTRAì˜ <code class="language-plaintext highlighter-rouge">Tug-of-War</code> í˜„ìƒì„ ë°©ì§€í•˜ê¸° ìœ„í•´ ìƒˆë¡œìš´ ì„ë² ë”© ê³µìœ  ê¸°ë²•ì¸ <code class="language-plaintext highlighter-rouge">GDES(Gradient Disentagnled Embedding Sharing)</code>ë°©ë²•ì„ ì œì‹œí–ˆë‹¤.</p>

<p>ì´ë²ˆ í¬ìŠ¤íŒ…ì—ì„œëŠ” êµ¬í˜„ ì½”ë“œì™€ í•¨ê»˜ GDESì— ëŒ€í•´ì„œë§Œ ì‚´í´ë³´ë ¤ í•œë‹¤. ELECTRA, DeBERTaì— ëŒ€í•´ ê¶ê¸ˆí•˜ë‹¤ë©´ ì´ì „ í¬ìŠ¤íŒ…ì„, ì „ì²´ êµ¬ì¡°ì— ëŒ€í•œ ì½”ë“œëŠ” <strong><a href="https://github.com/qcqced123/model_study">ì—¬ê¸° ë§í¬</a></strong>ë¥¼ í†µí•´ í™•ì¸ ê°€ëŠ¥í•˜ë‹¤.</p>

<h3 id="gdes-gradient-disentangled-embedding-sharing"><code class="language-plaintext highlighter-rouge">ğŸª¢GDES: Gradient Disentangled Embedding Sharing</code></h3>

<p align="center">
<img src="/assets/images/deberta_v3/deberta_v3.png" alt="GDES" class="align-center image-caption" width="100%&quot;, height=&quot;100%" />
<strong><em><a href="https://arxiv.org/abs/2111.09543">GDES</a></em></strong>
</p>

<p>ê·¸ë¦¼ì˜ (a)ê°€ ê¸°ì¡´ ELECTRAì˜ ê°€ì¤‘ì¹˜ ê³µìœ  ë°©ì‹, (c)ê°€ GDESì— í•´ë‹¹ëœë‹¤. ê·¸ë¦¼ ì† ëª¨ì‹ë„ì™€ ì„¤ëª…ì´ ì¢€ ë³µì¡í•´ ë³´ì´ì§€ë§Œ ì•„ì´ë””ì–´ëŠ” ë§¤ìš° ê°„ë‹¨í•˜ë‹¤.</p>

<p>ìƒì„±ìì™€ íŒë³„ìê°€ ì„œë¡œ í¬ì›Œë“œ íŒ¨ìŠ¤ ì‹œì ì—ëŠ” ë‹¨ì–´, ìœ„ì¹˜ ì„ë² ë”©ì„ ê³µìœ í•˜ë˜, ë°±ì›Œë“œ íŒ¨ìŠ¤ ì‹œì ì—ì„œëŠ” ê³µìœ ë˜ì§€ ëª»í•˜ë„ë¡ í•˜ì—¬, íŒë³„ìì˜ í•™ìŠµ ê²°ê³¼ì— ì˜í•´ ìƒì„±ìì˜ ë‹¨ì–´ ì„ë² ë”©, ìœ„ì¹˜ ì„ë² ë”©ì´ ì—…ë°ì´íŠ¸ ë˜ì§€ ëª»í•˜ë„ë¡ í•˜ì§€ëŠ” ê²ƒì´ë‹¤. ì˜¤ì§ ìƒì„±ìì˜ MLM í•™ìŠµì— ì˜í•´ì„œë§Œ ë‹¨ì–´ ë° ìœ„ì¹˜ ì„ë² ë”©ì´ ì—…ë°ì´íŠ¸ ë˜ì–´ì•¼ í•œë‹¤.</p>

\[E_{D} = \text{sg}(E_{G}) + E_{\Delta}\]

<p>í•„ìê°€ ì¶”ì •í•˜ê¸°ë¡œëŠ” <code class="language-plaintext highlighter-rouge">Skip-Connection</code>ì—ì„œ ì˜ê°ì„ ë°›ì§€ ì•Šì•˜ë‚˜ ì‹¶ì€ ì´ ìˆ˜ì‹ì€, ìƒì„±ìì˜ ì„ë² ë”©ì— ì”ì°¨ê°’ë“¤ì„ ë”í•´ íŒë³„ìì˜ ì„ë² ë”© í–‰ë ¬ì´ RTDì— ìµœì í™” ë˜ë„ë¡ ì„¤ê³„ ë˜ì—ˆë‹¤. ì—¬ê¸°ì„œ <code class="language-plaintext highlighter-rouge">sg()</code> ëŠ” <code class="language-plaintext highlighter-rouge">stop gradient</code>ë¥¼ ì˜ë¯¸í•œë‹¤. ë‹¤ì‹œ ë§í•´, ìƒì„±ìì˜ ì„ë² ë”© ê°€ì¤‘ì¹˜ë¥¼ íŒë³„ì í•™ìŠµì— ì‚¬ìš©í•˜ë˜, í•´ë‹¹ ì‹œì ì—ì„œëŠ” ê³„ì‚° ê·¸ë˜í”„ ì‘ì„±ì„ ì¤‘ë‹¨ì‹œì¼œ íŒë³„ìì˜ í•™ìŠµ ê²°ê³¼(ì´ì§„ ë¶„ë¥˜ ì†ì‹¤)ê°€ ìƒì„±ìì˜ ì„ë² ë”© ê°€ì¤‘ì¹˜ì— ì˜í–¥ì„ ë¯¸ì¹˜ì§€ ëª»í•˜ë„ë¡ í•œ ê²ƒì´ë‹¤.</p>

<p>ì´ëŸ¬í•œ ì•„ì´ë””ì–´ëŠ” ì‹¤ì œë¡œ ì–´ë–»ê²Œ ì½”ë“œë¡œ êµ¬í˜„í•´ì•¼í• ê¹Œ, ì•„ë˜ ì½”ë“œì™€ í•¨ê»˜ ì‚´í´ë³´ì.</p>
<h3 id="implementation-by-pytorch"><code class="language-plaintext highlighter-rouge">ğŸ‘©â€ğŸ’»Â Implementation by Pytorch</code></h3>
<p>ELECTRA ëª¨ë“ˆ <strong><code class="language-plaintext highlighter-rouge">__init__</code></strong>ì˜ <strong><code class="language-plaintext highlighter-rouge">share_embed_method</code></strong>ì— ë”°ë¼ ë¸Œëœì¹˜ê°€ ë°œìƒí•˜ëŠ” êµ¬ê°„ê³¼, ì•„ë˜ <strong><code class="language-plaintext highlighter-rouge">share_embedding()</code></strong> ë©”ì„œë“œì— ì£¼ëª©í•´ë³´ì.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="nn">experiment.models.abstract_model</span> <span class="kn">import</span> <span class="n">AbstractModel</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">Tensor</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">Callable</span>
<span class="kn">from</span> <span class="nn">einops.layers.torch</span> <span class="kn">import</span> <span class="n">Rearrange</span>
<span class="kn">from</span> <span class="nn">experiment.tuner.mlm</span> <span class="kn">import</span> <span class="n">MLMHead</span>
<span class="kn">from</span> <span class="nn">experiment.tuner.sbo</span> <span class="kn">import</span> <span class="n">SBOHead</span>
<span class="kn">from</span> <span class="nn">experiment.tuner.rtd</span> <span class="kn">import</span> <span class="n">get_discriminator_input</span><span class="p">,</span> <span class="n">RTDHead</span>
<span class="kn">from</span> <span class="nn">configuration</span> <span class="kn">import</span> <span class="n">CFG</span>


<span class="k">class</span> <span class="nc">ELECTRA</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">AbstractModel</span><span class="p">):</span>
    <span class="s">""" If you want to use pure ELECTRA, you should set share_embedding = ES
    elif you want to use ELECTRA with GDES, you should set share_embedding = GDES
    GDES is new approach of embedding sharing method from DeBERTa-V3 paper

    Args:
        cfg: configuration.CFG
        model_func: make model instance in runtime from config.json

    Var:
        cfg: configuration.CFG
        generator: Generator, which is used for generating replaced tokens for RTD
                   should select backbone model ex) BERT, RoBERTa, DeBERTa, ...
        discriminator: Discriminator, which is used for detecting replaced tokens for RTD
                       should select backbone model ex) BERT, RoBERTa, DeBERTa, ...
        share_embedding: whether or not to share embedding layer (word &amp; pos) between Generator &amp; Discriminator
        self.word_bias: Delta_E in paper
        self.abs_pos_bias: Delta_E in paper
        self.rel_pos_bias: Delta_E in paper

    References:
        https://arxiv.org/pdf/2003.10555.pdf
        https://arxiv.org/pdf/2111.09543.pdf
        https://github.com/google-research/electra
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">cfg</span><span class="p">:</span> <span class="n">CFG</span><span class="p">,</span> <span class="n">model_func</span><span class="p">:</span> <span class="n">Callable</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">ELECTRA</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span> <span class="o">=</span> <span class="n">cfg</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">generator</span> <span class="o">=</span> <span class="n">model_func</span><span class="p">(</span><span class="n">cfg</span><span class="p">.</span><span class="n">generator_num_layers</span><span class="p">)</span>  <span class="c1"># init generator
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">mlm_head</span> <span class="o">=</span> <span class="n">MLMHead</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">rtd_masking</span> <span class="o">==</span> <span class="s">'SpanBoundaryObjective'</span><span class="p">:</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">mlm_head</span> <span class="o">=</span> <span class="n">SBOHead</span><span class="p">(</span>
                <span class="n">cfg</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">,</span>
                <span class="n">is_concatenate</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">is_concatenate</span><span class="p">,</span>
                <span class="n">max_span_length</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">max_span_length</span>
            <span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">discriminator</span> <span class="o">=</span> <span class="n">model_func</span><span class="p">(</span><span class="n">cfg</span><span class="p">.</span><span class="n">discriminator_num_layers</span><span class="p">)</span>  <span class="c1"># init generator
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">rtd_head</span> <span class="o">=</span> <span class="n">RTDHead</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">share_embed_method</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">share_embed_method</span>  <span class="c1"># instance, es, gdes
</span>        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">share_embed_method</span> <span class="o">==</span> <span class="s">'GDES'</span><span class="p">:</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">word_bias</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Parameter</span><span class="p">(</span>
                <span class="n">torch</span><span class="p">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">discriminator</span><span class="p">.</span><span class="n">embeddings</span><span class="p">.</span><span class="n">word_embedding</span><span class="p">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">abs_pos_bias</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Parameter</span><span class="p">(</span>
                <span class="n">torch</span><span class="p">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">discriminator</span><span class="p">.</span><span class="n">embeddings</span><span class="p">.</span><span class="n">abs_pos_emb</span><span class="p">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>
            <span class="p">)</span>
            <span class="nb">delattr</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">discriminator</span><span class="p">.</span><span class="n">embeddings</span><span class="p">.</span><span class="n">word_embedding</span><span class="p">,</span> <span class="s">'weight'</span><span class="p">)</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">discriminator</span><span class="p">.</span><span class="n">embeddings</span><span class="p">.</span><span class="n">word_embedding</span><span class="p">.</span><span class="n">register_parameter</span><span class="p">(</span><span class="s">'_weight'</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">word_bias</span><span class="p">)</span>

            <span class="nb">delattr</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">discriminator</span><span class="p">.</span><span class="n">embeddings</span><span class="p">.</span><span class="n">abs_pos_emb</span><span class="p">,</span> <span class="s">'weight'</span><span class="p">)</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">discriminator</span><span class="p">.</span><span class="n">embeddings</span><span class="p">.</span><span class="n">abs_pos_emb</span><span class="p">.</span><span class="n">register_parameter</span><span class="p">(</span><span class="s">'_weight'</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">abs_pos_bias</span><span class="p">)</span>

            <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">model_name</span> <span class="o">==</span> <span class="s">'DeBERTa'</span><span class="p">:</span>
                <span class="bp">self</span><span class="p">.</span><span class="n">rel_pos_bias</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Parameter</span><span class="p">(</span>
                    <span class="n">torch</span><span class="p">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">discriminator</span><span class="p">.</span><span class="n">embeddings</span><span class="p">.</span><span class="n">rel_pos_emb</span><span class="p">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>
                <span class="p">)</span>
                <span class="nb">delattr</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">discriminator</span><span class="p">.</span><span class="n">embeddings</span><span class="p">.</span><span class="n">rel_pos_emb</span><span class="p">,</span> <span class="s">'weight'</span><span class="p">)</span>
                <span class="bp">self</span><span class="p">.</span><span class="n">discriminator</span><span class="p">.</span><span class="n">embeddings</span><span class="p">.</span><span class="n">rel_pos_emb</span><span class="p">.</span><span class="n">register_parameter</span><span class="p">(</span><span class="s">'_weight'</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">rel_pos_emb</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">share_embedding</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">share_embedding</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="k">def</span> <span class="nf">discriminator_hook</span><span class="p">(</span><span class="n">module</span><span class="p">:</span> <span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">,</span> <span class="o">*</span><span class="n">inputs</span><span class="p">):</span>
            <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">share_embed_method</span> <span class="o">==</span> <span class="s">'instance'</span><span class="p">:</span>  <span class="c1"># Instance Sharing
</span>                <span class="bp">self</span><span class="p">.</span><span class="n">discriminator</span><span class="p">.</span><span class="n">embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">generator</span><span class="p">.</span><span class="n">embeddings</span>

            <span class="k">elif</span> <span class="bp">self</span><span class="p">.</span><span class="n">share_embed_method</span> <span class="o">==</span> <span class="s">'ES'</span><span class="p">:</span>  <span class="c1"># ES (Embedding Sharing)
</span>                <span class="bp">self</span><span class="p">.</span><span class="n">discriminator</span><span class="p">.</span><span class="n">embeddings</span><span class="p">.</span><span class="n">word_embedding</span><span class="p">.</span><span class="n">weight</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">generator</span><span class="p">.</span><span class="n">embeddings</span><span class="p">.</span><span class="n">word_embedding</span><span class="p">.</span><span class="n">weight</span>
                <span class="bp">self</span><span class="p">.</span><span class="n">discriminator</span><span class="p">.</span><span class="n">embeddings</span><span class="p">.</span><span class="n">abs_pos_emb</span><span class="p">.</span><span class="n">weight</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">generator</span><span class="p">.</span><span class="n">embeddings</span><span class="p">.</span><span class="n">abs_pos_emb</span><span class="p">.</span><span class="n">weight</span>
                <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">model_name</span> <span class="o">==</span> <span class="s">'DeBERTa'</span><span class="p">:</span>
                    <span class="bp">self</span><span class="p">.</span><span class="n">discriminator</span><span class="p">.</span><span class="n">embeddings</span><span class="p">.</span><span class="n">rel_pos_emb</span><span class="p">.</span><span class="n">weight</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">generator</span><span class="p">.</span><span class="n">embeddings</span><span class="p">.</span><span class="n">rel_pos_emb</span><span class="p">.</span><span class="n">weight</span>

            <span class="k">elif</span> <span class="bp">self</span><span class="p">.</span><span class="n">share_embed_method</span> <span class="o">==</span> <span class="s">'GDES'</span><span class="p">:</span>  <span class="c1"># GDES (Generator Discriminator Embedding Sharing)
</span>                <span class="n">g_w_emb</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">generator</span><span class="p">.</span><span class="n">embeddings</span><span class="p">.</span><span class="n">word_embedding</span>
                <span class="n">d_w_emb</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">discriminator</span><span class="p">.</span><span class="n">embeddings</span><span class="p">.</span><span class="n">word_embedding</span>
                <span class="bp">self</span><span class="p">.</span><span class="n">_set_param</span><span class="p">(</span><span class="n">d_w_emb</span><span class="p">,</span> <span class="s">'weight'</span><span class="p">,</span> <span class="n">g_w_emb</span><span class="p">.</span><span class="n">weight</span><span class="p">.</span><span class="n">detach</span><span class="p">()</span> <span class="o">+</span> <span class="n">d_w_emb</span><span class="p">.</span><span class="n">_weight</span><span class="p">)</span>
                <span class="n">g_p_emb</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">generator</span><span class="p">.</span><span class="n">embeddings</span><span class="p">.</span><span class="n">abs_pos_emb</span>
                <span class="n">d_p_emb</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">discriminator</span><span class="p">.</span><span class="n">embeddings</span><span class="p">.</span><span class="n">abs_pos_emb</span>
                <span class="bp">self</span><span class="p">.</span><span class="n">_set_param</span><span class="p">(</span><span class="n">d_p_emb</span><span class="p">,</span> <span class="s">'weight'</span><span class="p">,</span> <span class="n">g_p_emb</span><span class="p">.</span><span class="n">weight</span><span class="p">.</span><span class="n">detach</span><span class="p">()</span> <span class="o">+</span> <span class="n">d_p_emb</span><span class="p">.</span><span class="n">_weight</span><span class="p">)</span>

                <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">model_name</span> <span class="o">==</span> <span class="s">'DeBERTa'</span><span class="p">:</span>
                    <span class="n">g_rp_emb</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">generator</span><span class="p">.</span><span class="n">embeddings</span><span class="p">.</span><span class="n">rel_pos_emb</span>
                    <span class="n">d_rp_emb</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">discriminator</span><span class="p">.</span><span class="n">embeddings</span><span class="p">.</span><span class="n">rel_pos_emb</span>
                    <span class="bp">self</span><span class="p">.</span><span class="n">_set_param</span><span class="p">(</span><span class="n">d_rp_emb</span><span class="p">,</span> <span class="s">'weight'</span><span class="p">,</span> <span class="n">g_rp_emb</span><span class="p">.</span><span class="n">weight</span><span class="p">.</span><span class="n">detach</span><span class="p">()</span> <span class="o">+</span> <span class="n">d_rp_emb</span><span class="p">.</span><span class="n">_weight</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">discriminator</span><span class="p">.</span><span class="n">register_forward_pre_hook</span><span class="p">(</span><span class="n">discriminator_hook</span><span class="p">)</span>

    <span class="o">@</span><span class="nb">staticmethod</span>
    <span class="k">def</span> <span class="nf">_set_param</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">param_name</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="n">module</span><span class="p">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="n">param_name</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">generator_fw</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">labels</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">padding_mask</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">mask_labels</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="bp">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
        <span class="n">g_last_hidden_states</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">generator</span><span class="p">(</span>
            <span class="n">inputs</span><span class="p">,</span>
            <span class="n">padding_mask</span><span class="p">,</span>
            <span class="n">attention_mask</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">rtd_masking</span> <span class="o">==</span> <span class="s">'MaskedLanguageModel'</span><span class="p">:</span>
            <span class="n">g_logit</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">mlm_head</span><span class="p">(</span>
                <span class="n">g_last_hidden_states</span>
            <span class="p">)</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">rtd_masking</span> <span class="o">==</span> <span class="s">'SpanBoundaryObjective'</span><span class="p">:</span>
            <span class="n">g_logit</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">mlm_head</span><span class="p">(</span>
                <span class="n">g_last_hidden_states</span><span class="p">,</span>
                <span class="n">mask_labels</span>
            <span class="p">)</span>
        <span class="n">pred</span> <span class="o">=</span> <span class="n">g_logit</span><span class="p">.</span><span class="n">clone</span><span class="p">().</span><span class="n">detach</span><span class="p">()</span>
        <span class="n">d_inputs</span><span class="p">,</span> <span class="n">d_labels</span> <span class="o">=</span> <span class="n">get_discriminator_input</span><span class="p">(</span>
            <span class="n">inputs</span><span class="p">,</span>
            <span class="n">labels</span><span class="p">,</span>
            <span class="n">pred</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">g_logit</span><span class="p">,</span> <span class="n">d_inputs</span><span class="p">,</span> <span class="n">d_labels</span>

    <span class="k">def</span> <span class="nf">discriminator_fw</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">padding_mask</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span><span class="n">attention_mask</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="bp">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="n">d_last_hidden_states</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">discriminator</span><span class="p">(</span>
            <span class="n">inputs</span><span class="p">,</span>
            <span class="n">padding_mask</span><span class="p">,</span>
            <span class="n">attention_mask</span>
        <span class="p">)</span>
        <span class="n">d_logit</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">rtd_head</span><span class="p">(</span>
            <span class="n">d_last_hidden_states</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">d_logit</span>
</code></pre></div></div>
<p>ë¨¼ì € <strong><code class="language-plaintext highlighter-rouge">__init__</code></strong>ì˜ ë¸Œëœì¹˜ êµ¬ê°„ì„ ì‚´í´ë³´ì. <strong><code class="language-plaintext highlighter-rouge">word_bias</code></strong>, <strong><code class="language-plaintext highlighter-rouge">pos_bias</code></strong>ë¥¼ ë§Œë“¤ì–´ <code class="language-plaintext highlighter-rouge">register_parameter</code>í™”ë¥¼ í•˜ê³  ìˆë‹¤. ìƒˆë¡­ê²Œ ìƒì„±ë˜ì–´ <strong><code class="language-plaintext highlighter-rouge">_weight</code></strong>ì´ë€ ì´ë¦„ìœ¼ë¡œ ìƒì„±ìì˜ íŒŒë¼ë¯¸í„°ê°€ ëœ ë‘ ê°€ì¤‘ì¹˜ê°€ ë°”ë¡œ $E_{\Delta}$ ê°€ ëœë‹¤.</p>

<p>ë‹¤ìŒ <strong><code class="language-plaintext highlighter-rouge">share_embedding()</code></strong> ë©”ì„œë“œë¥¼ ë³´ì. $E_{G}$ ì— <code class="language-plaintext highlighter-rouge">torch.detach()</code>ë¥¼ ì‚¬ìš©í•´ ìˆ˜ì‹ì˜ <code class="language-plaintext highlighter-rouge">stop gradient</code> íš¨ê³¼ë¥¼ ì ìš©í•œë‹¤. ê·¸ë¦¬ê³  ë‘ ê°€ì¤‘ì¹˜ë¥¼ ë”í•˜ê³ , <code class="language-plaintext highlighter-rouge">torch.register_buffer</code>ë¥¼ í˜¸ì¶œí•´ í¬ì›Œë“œ íŒ¨ìŠ¤ì— í™œìš©ì€ ë˜ì§€ë§Œ ë°±ì›Œë“œ íŒ¨ìŠ¤ì— ê·¸ë¼ë””ì–¸íŠ¸ê°€ í•´ë‹¹ ê°€ì¤‘ì¹˜ë¥¼ ì—…ë°ì´íŠ¸ í•˜ì§€ ëª»í•˜ë„ë¡ ì„¤ì •í•œë‹¤. ê·¸ë¦¬ê³  ë§ˆì§€ë§‰ì— <code class="language-plaintext highlighter-rouge">torch.register_forward_pre_hook</code>ì„ í˜¸ì¶œí•˜ëŠ”ë°, ê·¸ ì´ìœ ëŠ” $E_{G}$ ì— <code class="language-plaintext highlighter-rouge">torch.detach()</code> ë¥¼ ì‚¬ìš©í–ˆê¸° ë•Œë¬¸ì— í˜„ì¬ íŒë³„ìì˜ ë²„í¼ì— ìˆëŠ” $E_{G}$ ëŠ” ì´ì „ ì‹œì ì˜ ìƒì„±ì MLM ì†ì‹¤ì— ì˜í•´ ìƒˆë¡­ê²Œ ì—…ë°ì´íŠ¸ $E_{G}$ ê°€ ì•„ë‹ˆë‹¤. ë”°ë¼ì„œ ë§¤ë²ˆ íŒë³„ìì˜ í¬ì›Œë“œ íŒ¨ìŠ¤ê°€ í˜¸ì¶œ(ì‹œì‘)ë˜ëŠ” ì‹œì ì— ì—…ë°ì´íŠ¸ ëœ $E_{G}$ ë¥¼ ë°˜ì˜í•´ RTDë¥¼ ìˆ˜í–‰í•  ìˆ˜ ìˆë„ë¡ í•˜ê¸° ìœ„í•´ <code class="language-plaintext highlighter-rouge">register_forward_pre_hook</code> ë¥¼ ì‚¬ìš©í–ˆë‹¤.</p>

<h3 id="-gdes-experiment"><strong><code class="language-plaintext highlighter-rouge">ğŸ¤” GDES Experiment</code></strong></h3>

<p>GDESê°€ ì œëŒ€ë¡œ êµ¬í˜„ë˜ì—ˆëŠ”ì§€, ë…¼ë¬¸ ì£¼ì¥ëŒ€ë¡œ íŒë³„ì í•™ìŠµ ê²°ê³¼ê°€ ê°„ì„­ì„ ë°œìƒì‹œí‚¤ì§€ ì•ŠëŠ”ì§€ í™•ì¸í•˜ê¸° ìœ„í•´ í•œê°€ì§€ ì‹¤í—˜ì„ ì§„í–‰í–ˆë‹¤. ì‹¤í—˜ ë‚´ìš©ì€ ì´ë ‡ë‹¤. ë§Œì•½ GDESê°€ ì˜ë„ëŒ€ë¡œ êµ¬í˜„ëœê²Œ ë§ë‹¤ë©´, ì¸ì½”ë” ëª¨ë¸ì˜ MLM í•™ìŠµ ê²°ê³¼ ì¶”ì´ì™€ ELECTRAì˜ ìƒì„±ì í•™ìŠµ ê²°ê³¼ ì¶”ì´ ì–‘ìƒì´ ìœ ì‚¬í•´ì•¼ í•œë‹¤. ë§Œì•½ ìµœì í™” ì¶”ì„¸ê°€ ë‹¤ë¥´ë‹¤ë©´, í•„ìê°€ ì˜ëª» êµ¬í˜„í–ˆê±°ë‚˜, ì €ìì˜ ì£¼ì¥ê³¼ ë‹¤ë¥´ê²Œ ê°„ì„­ì´ ë°œìƒí•˜ëŠ” ê²ƒì´ë¼ ë³¼ ìˆ˜ ìˆì„ ê²ƒì´ë‹¤. Backboneì„ DeBERTaë¡œ ë‘ê³  ê°ê° í•™ìŠµì„ ì§„í–‰í–ˆë‹¤. ëª¨ë“  í•˜ì´í¼ íŒŒë¼ë¯¸í„°ë¥¼ ê³ ì •í•œ ë’¤, í•™ìŠµ ì´ˆë°˜ 120ìŠ¤íƒ­ì— ëŒ€í•œ ê²°ê³¼ ì¶”ì´ë¥¼ ë¹„êµí•´ë´¤ë‹¤.</p>

<p align="center">
<img src="/assets/images/deberta_v3/deberta_test.png" alt="DeBERTa MLM Result" class="align-center image-caption" width="100%&quot;, height=&quot;100%" />
<strong><em><a href="https://arxiv.org/abs/2111.09543">DeBERTa MLM Result</a></em></strong>
</p>

<p align="center">
<img src="/assets/images/deberta_v3/gdes_test.png" alt="GDES Result" class="align-center image-caption" width="100%&quot;, height=&quot;100%" />
<strong><em><a href="https://arxiv.org/abs/2111.09543">GDES Result</a></em></strong>
</p>

<p>ë¯¸ì²˜ ê¹Œë¨¹ê³  <code class="language-plaintext highlighter-rouge">torch.backends.cudnn.deterministic = False</code>ë¡œ ë‘ê³  ì‹¤í—˜ì„ ì§„í–‰í•˜ì—¬, ìƒì„±ìì˜ ìˆ˜ë ´ì´ ì¢€ ë” ë¹¨ë¦¬ ì§„í–‰ë˜ëŠ” ì–‘ìƒì„ ë³´ì´ê³  ìˆë‹¤. ì•„ë§ˆë„ ìƒì„±ì í•™ìŠµì„ í•  ë•Œ <code class="language-plaintext highlighter-rouge">cudnn</code> ì´ ì—´ì‹¬íˆ ì¼ì„ í•œ ê²ƒ ê°™ëŒœ. ìˆ˜ë ´ ì†ë„ì—ëŠ” ì°¨ì´ê°€ ì¡°ê¸ˆ ë‚˜ì§€ë§Œ, ìµœì í™” ë˜ëŠ” ì¶”ì„¸ ìì²´ëŠ” ë™ì¼í•œ ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤.</p>

<p>ë”°ë¼ì„œ GDESë¥¼ ì‚¬ìš©í•˜ë©´ ê°„ì„­ì´ ë°œìƒí•˜ì§€ ì•Šì•„ <code class="language-plaintext highlighter-rouge">Tug-of-War</code> í˜„ìƒì„ ë°©ì§€í•  ìˆ˜ ìˆë‹¤. ë‹¤ë§Œ, ì‹¤í—˜ì´ ë‹¤ì†Œ ì—„ë°€í•˜ì§€ ëª»í•œ ì¸¡ë©´ì´ ìˆë‹¤. ì¶”í›„ì— ì¢€ ë” ì—„ë°€í•œ ì¦ëª…ì„ í•  ìˆ˜ ìˆëŠ” ì‹¤í—˜ ë°©ë²•ì„ ìƒê°í•´ë´ì•¼ê² ë‹¤.</p>]]></content><author><name>qcqced</name><email>qcqced123@gmail.com</email></author><category term="NLP" /><category term="Natural Language Process" /><category term="DeBERTa-V3" /><category term="DeBERTa" /><category term="ELECTRA" /><category term="Weight Sharing" /><category term="GDES" /><category term="Pytorch" /><summary type="html"><![CDATA[DeBERTa-V3 Official Paper Review with Pytorch Implementation]]></summary></entry><entry><title type="html">ğŸ—‚ï¸[SpanBERT] SpanBERT: Improving Pre-training by Representing and Predicting Spans</title><link href="http://localhost:4000/nlp/spanbert" rel="alternate" type="text/html" title="ğŸ—‚ï¸[SpanBERT] SpanBERT: Improving Pre-training by Representing and Predicting Spans" /><published>2024-03-11T00:00:00+09:00</published><updated>2024-03-12T02:00:00+09:00</updated><id>http://localhost:4000/nlp/spanbert</id><content type="html" xml:base="http://localhost:4000/nlp/spanbert"><![CDATA[<h3 id="overview"><code class="language-plaintext highlighter-rouge">ğŸ”­Â Overview</code></h3>

<p><code class="language-plaintext highlighter-rouge">SpanBERT</code>ëŠ” 2020ë…„ í˜ì´ìŠ¤ë¶ì—ì„œ ë°œí‘œí•œ BERT ê³„ì—´ ëª¨ë¸ë¡œ, ìƒˆë¡œìš´ ë°©ë²•ë¡ ì¸ <code class="language-plaintext highlighter-rouge">SBO(Span Boundary Objective)</code>ë¥¼ ê³ ì•ˆí•´ ì‚¬ì „í•™ìŠµì„ í•˜ì—¬ ê¸°ì¡´ ëŒ€ë¹„ ë†’ì€ ì„±ëŠ¥ì„ ê¸°ë¡í–ˆë‹¤. ê¸°ì¡´ <code class="language-plaintext highlighter-rouge">MLM</code>, <code class="language-plaintext highlighter-rouge">CLM</code>ì€ ë‹¨ì¼ í† í°ì„ ì˜ˆì¸¡í•˜ëŠ” ë°©ì‹ì„ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì— Word-Level Taskì— ì•„ì£¼ ì í•©í•˜ì§€ë§Œ ìƒëŒ€ì ìœ¼ë¡œ QA, Sentence-Similarity ê°™ì€ ë¬¸ì¥ ë‹¨ìœ„ í…ŒìŠ¤í¬ì— ê·¸ëŒ€ë¡œ í™œìš©í•˜ê¸°ì—ëŠ” ë¶€ì¡±í•œ ì ì´ ìˆì—ˆë‹¤. ì´ëŸ¬í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ê³ ì•ˆëœ ë°©ë²•ë¡ ì´ ë°”ë¡œ <code class="language-plaintext highlighter-rouge">SBO</code>ë‹¤. <code class="language-plaintext highlighter-rouge">SBO</code>ë€, MLMê³¼ ë¹„ìŠ·í•˜ì§€ë§Œ, Span(ì ˆâ€¢êµ¬ë¬¸) ë‹¨ìœ„ë¡œ ë§ˆìŠ¤í‚¹í•˜ê³  ë‹¤ì‹œ Denoisingì„ í•˜ê¸° ë•Œë¬¸ì—, Sentence-Level Taskì— ì†í•˜ëŠ” Down-Stream Taskë¥¼ ìœ„í•œ ëª¨ë¸ì˜ ì‚¬ì „ í›ˆë ¨ìœ¼ë¡œ ì í•©í•˜ë‹¤.</p>

<p>ì •ë¦¬í•˜ìë©´, <code class="language-plaintext highlighter-rouge">SpanBERT</code> ëª¨ë¸ì€ ê¸°ì¡´ BERTì˜ êµ¬ì¡°ì  ì¸¡ë©´ ê°œì„ ì´ ì•„ë‹Œ, ì‚¬ì „í•™ìŠµ ë°©ë²•ì— ëŒ€í•œ ê°œì„  ì‹œë„ë¼ê³  ë³¼ ìˆ˜ ìˆë‹¤. ë”°ë¼ì„œ ì–´ë–¤ ëª¨ë¸ì´ë”ë¼ë„, ì¸ì½”ë” ì–¸ì–´ ëª¨ë¸ì´ë¼ë©´ ëª¨ë‘ <code class="language-plaintext highlighter-rouge">SpanBERT</code> êµ¬ì¡°ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìœ¼ë©°, ê¸°ì¡´ ë…¼ë¬¸ì—ì„œëŠ” ì›ë³¸ BERT êµ¬ì¡°ë¥¼ ì‚¬ìš©í–ˆë‹¤. ê·¸ë˜ì„œ ë³¸ í¬ìŠ¤íŒ…ì—ì„œë„ BERTì— ëŒ€í•œ ì„¤ëª… ì—†ì´ SBOì— ëŒ€í•´ì„œë§Œ ë‹¤ë£¨ë ¤ê³  í•œë‹¤.</p>

<h3 id="sbo-span-boundary-objective"><code class="language-plaintext highlighter-rouge">ğŸ“šÂ SBO: Span Boundary Objective</code></h3>

<p align="center">
<img src="/assets/images/spanbert/sbo.png" alt="SBO Task" class="align-center image-caption" width="100%&quot;, height=&quot;100%" />
<strong><em><a href="https://arxiv.org/abs/1907.10529">SBO Task</a></em></strong>
</p>

<p><strong>[SBO Algorithm Summary]</strong></p>
<ul>
  <li><strong>1) ì—°ì†ëœ ë²”ìœ„ì˜ Span ìƒì„±</strong>
    <ul>
      <li><strong>ë¬´ì‘ìœ„ë¡œ Spanì˜ ì–‘ìª½ ë í† í° ì§€ì • ($x_{4}, x_{9}$)</strong>
        <ul>
          <li><strong>$x_{5}$ to $x_{8}$ ì€ ìŠ¤íŒ¬ ë‚´ë¶€ í† í°</strong></li>
        </ul>
      </li>
      <li><strong>ë§ˆìŠ¤í‚¹ ì˜ˆì‚° ê³„ì‚°</strong>
        <ul>
          <li><strong>ë¬¸ì¥ ë‹¹ ë§ˆìŠ¤í‚¹ ì˜ˆì‚°(í•©ì‚° Span ê¸¸ì´)ì€ ë¬¸ì¥ ê¸¸ì´ì˜ 15%</strong></li>
          <li><strong>ì˜ˆì‹œ ì‹œí€€ìŠ¤ ê¸¸ì´: 512</strong></li>
          <li><strong>ë§ˆìŠ¤í‚¹ ì˜ˆì‚°: ëŒ€ëµ 75 = 512*0.15</strong></li>
        </ul>
      </li>
      <li><strong>ê¸°í•˜ ë¶„í¬ ì‚¬ìš©í•´ì„œ ê°œë³„ ìŠ¤íŒ¬ ê¸¸ì´ ì§€ì •</strong>
        <ul>
          <li><strong>ê°œë³„ ìŠ¤íŒ¬ë‹¹ ìµœëŒ€ ê¸¸ì´ ì§€ì •, ìµœëŒ€ 10ì´ ë„˜ì§€ ì•Šë„ë¡ ì„¤ì •</strong></li>
          <li><strong>ìµœëŒ€ ìŠ¤íŒ¬ í•©ì‚° ê¸¸ì´ ë„ë‹¬ê¹Œì§€ ë§ˆìŠ¤í‚¹ ë°˜ë³µ</strong>
            <ul>
              <li><strong>ë‚¨ì€ ë§ˆìŠ¤í‚¹ ì˜ˆì‚° &lt; í˜„ì¬ ìŠ¤íŒ¬ ê¸¸ì´</strong>
                <ul>
                  <li><strong>ë‚¨ì€ ë§ˆìŠ¤í‚¹ ì˜ˆì‚°ì„ í˜„ì¬ ìŠ¤íŒ¬ ê¸¸ì´ë¡œ ì„¤ì •</strong></li>
                </ul>
              </li>
            </ul>
          </li>
        </ul>
      </li>
      <li><strong>ë”°ë¼ì„œ Subword Tokenizingì´ ì•„ë‹ˆë¼ Whole Word Masking ë‹¨ìœ„ ì‘ì—…ì´ í•„ìš”</strong></li>
    </ul>
  </li>
  <li><strong>2) ì‹œì‘ í† í° ê¸°ì¤€, ìƒëŒ€ ìœ„ì¹˜ ê³„ì‚°</strong>
    <ul>
      <li><strong>ìŠ¤íŒ¬ ë‚´ë¶€ í† í°ì˜ ìƒëŒ€ ìœ„ì¹˜ ì„ë² ë”© ìƒì„± ë° ê³„ì‚°</strong></li>
      <li><strong>ì‹œì‘í† í°, ë§ˆì§€ë§‰í† í°, ìŠ¤íŒ¬ ë‚´ë¶€ í† í°ì˜ ìƒëŒ€ ìœ„ì¹˜ ì„ë² ë”©ì„ concat, ì€ë‹‰ ë²¡í„° ìƒì„±</strong></li>
      <li><strong>SpanHeadì— ì€ë‹‰ ë²¡í„° í†µê³¼ì‹œí‚¤ê¸°</strong></li>
    </ul>
  </li>
  <li><strong>3) SpanHead ì¶œë ¥ê°’ì„ ë§ˆìŠ¤í‚¹ì— ëŒ€í•œ ì˜ˆì¸¡ í‘œí˜„ìœ¼ë¡œ ì‚¬ìš©</strong></li>
</ul>

<p>SBOì˜ ì•„ì´ë””ì–´ ìì²´ëŠ” ìƒë‹¹íˆ ê°„ë‹¨í•˜ë‹¤. ê¸°ì¡´ MLMì²˜ëŸ¼ ë¬´ì‘ìœ„ë¡œ ì‹œí€€ìŠ¤ì—ì„œ ì•„ë¬´ í† í°ì´ë‚˜ ì„ íƒí•˜ëŠ”ê²Œ ì•„ë‹ˆë¼, ì£¼ì–´ì§„ ë¬¸ì¥ì—ì„œ ì¼ì • ê¸¸ì´ì˜ ì—°ì†ëœ í† í°ë“¤ì„ í•œë²ˆì— ì„ íƒí•´ ë§ˆìŠ¤í‚¹ ì²˜ë¦¬í•˜ì—¬ í•™ìŠµí•˜ê² ë‹¤ëŠ” ê²ƒì´ë‹¤. ë…¼ë¬¸ì—ì„œ ì œì‹œí•œ SBO ì•Œê³ ë¦¬ì¦˜ì„ ì •ë¦¬í•˜ë©´ ì•„ë˜ì™€ ê°™ë‹¤.</p>

\[\begin{align*}
h_0 &amp;= [x_{s-1}; x_{e+1}; p_{i-s+1}] \\
h_1 &amp;= \text{LayerNorm}(\text{GeLU}(W_1 h_0)) \\
y_i &amp;= \text{LayerNorm}(\text{GeLU}(W_2 h_1))
\end{align*}\]

<p>ìœ„ ê·¸ë¦¼ì„ ì˜ˆì‹œë¡œ ì•Œê³ ë¦¬ì¦˜ì„ ì‚´í´ë³´ì. ë¨¼ì € ì£¼ì–´ì§„ ìŠ¤íŒ¬ ê¸¸ì´ì— ë§ê²Œ, ìŠ¤íŒ¬ì˜ ì‹œì‘ê³¼ ë ì§€ì ì´ ë˜ëŠ” í† í°ì„ ë¬´ì‘ìœ„ë¡œ ì„ íƒí•œë‹¤. ê·¸ë‹¤ìŒ ì‹œì‘ ìœ„ì¹˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ, ìŠ¤íŒ¬ ë‚´ë¶€ì— ì†í•˜ëŠ” í† í°ë“¤ì˜ ìƒëŒ€ ìœ„ì¹˜ ì¸ë±ìŠ¤ë¥¼ ê³„ì‚°í•´ì¤€ë‹¤. ê·¸ë¦¼ ì† $x_{7}$ í† í°ì˜ ìƒëŒ€ ìœ„ì¹˜ ë²ˆí˜¸ëŠ” 3ì´ ëœë‹¤. ë¯¸ë¦¬ ì •ì˜í•œ ìƒëŒ€ ìœ„ì¹˜ ì„ë² ë”©ì—ì„œ í–‰ ì¸ë±ìŠ¤ê°€ 3ì¸ í–‰ë²¡í„°ë¥¼ ê°€ì ¸ì˜¨ë‹¤. ê·¸ ë‹¤ìŒ ì–‘ìª½ ë ë²¡í„°ì™€ concatì„ ìˆ˜í–‰í•˜ì—¬ $h_{0}$ ì„ ë§Œë“ ë‹¤. ê·¸ë¦¬ê³  ë¯¸ë¦¬ ì •ì˜ëœ <code class="language-plaintext highlighter-rouge">SBOHead</code>ì— í†µê³¼ì‹œí‚¨ë‹¤. <code class="language-plaintext highlighter-rouge">SBOHead</code>ì—ê²Œ ë°˜í™˜ ë°›ì€ ì€ë‹‰ ë²¡í„°ê°’ì€ í•´ë‹¹ ìœ„ì¹˜ì˜ ë§ˆìŠ¤í‚¹ì— ëŒ€í•œ ì˜ˆì¸¡ê°’($y_{i}$)ìœ¼ë¡œ ì‚¬ìš©í•˜ê³  ì´ë¥¼ ì´ìš©í•´ SBO ì†ì‹¤ì„ êµ¬í•œë‹¤. ì§€ê¸ˆê¹Œì§€ ë‚´ìš©ì„ ì •ë¦¬í•´ ìˆ˜ì‹ìœ¼ë¡œ í‘œí˜„í•˜ë©´ ìœ„ì™€ ê°™ë‹¤.</p>

\[L(x_i) = L_{MLM}(x_i) + L_{SBO}(x_i)\]

<p><code class="language-plaintext highlighter-rouge">SpanBERT</code>ì˜ ëª©ì í•¨ìˆ˜ëŠ” SBO ì†ì‹¤ ë¿ë§Œ ì•„ë‹ˆë¼ ê¸°ì¡´ MLM ì†ì‹¤ë„ í•¨ê¼ í¬í•¨ë˜ì–´ ìˆë‹¤. ë‹¤ë§Œ MLM ì†ì‹¤ì„ êµ¬í•˜ê¸° ìœ„í•´ ì£¼ì–´ì§„ ì‹œí€€ìŠ¤ì— ëŒ€í•´ ë”°ë¡œ ë§ˆìŠ¤í‚¹ì„ í•˜ëŠ” ê²ƒì€ ì•„ë‹ˆê³ , SBOë¥¼ ìœ„í•´ ì ìš©í–ˆë˜ Span Maskingì„ ê·¸ëŒ€ë¡œ í™œìš©í•œë‹¤. ëŒ€ì‹  ìœ„ì˜ SBO ìˆ˜ì‹ì˜ $h_{0}$ ì´ ì•„ë‹ˆë¼, $x_{i-s+1}$ ($i-s+1$ ë²ˆì§¸ í† í°ì˜ ì¸ì½”ë” ì¶œë ¥ê°’)ì„ ê·¸ëŒ€ë¡œ MLM ì†ì‹¤ì„ êµ¬í•˜ëŠ”ë° ì‚¬ìš©í•œë‹¤. ì •ë¦¬í•˜ë©´, <code class="language-plaintext highlighter-rouge">SpanBERT</code>ì˜ ìµœì¢… ì†ì‹¤ì€ ìœ„ ìˆ˜ì‹ê³¼ ê°™ë‹¤. í•œí¸, <code class="language-plaintext highlighter-rouge">ELECTRA</code> ë•Œì™€ëŠ” ë‹¤ë¥´ê²Œ ë‘ ì†ì‹¤ì˜ ìŠ¤ì¼€ì¼ ì°¨ì´ê°€ ê±°ì˜ ì—†ì–´ ë”°ë¡œ ìŠ¤ì¼€ì¼ ìƒìˆ˜ë¥¼ ê³±í•´ì£¼ì§€ëŠ” ì•ŠëŠ” ê²ƒ ê°™ë‹¤.</p>

<h3 id="implementation-by-pytorch"><code class="language-plaintext highlighter-rouge">ğŸ‘©â€ğŸ’»Â Implementation by Pytorch</code></h3>

<p>ë…¼ë¬¸ì˜ ë‚´ìš© ì¢…í•©í•˜ì—¬ íŒŒì´í† ì¹˜ë¡œ <code class="language-plaintext highlighter-rouge">SpanBERT</code>ë¥¼ êµ¬í˜„í•´ë´¤ë‹¤. ë…¼ë¬¸ì— í¬í•¨ëœ ì•„ì´ë””ì–´ë¥¼ ì´í•´í•˜ëŠ”ë°ëŠ” ì–´ë µì§€ ì•Šì•˜ì§€ë§Œ, ì œí•œëœ ì¡°ê±´ì— ë§ëŠ” ìŠ¤íŒ¬ì„ ì°¾ê³ , ë§ˆìŠ¤í‚¹í•˜ëŠ” ê³¼ì •ì„ ì‹¤ì œ êµ¬í˜„í•˜ëŠ” ê²ƒì€ ë§¤ìš° ê¹Œë‹¤ë¡œìš´ í¸ì´ì—ˆë‹¤.
ë³¸ í¬ìŠ¤íŒ…ì—ì„œëŠ” <code class="language-plaintext highlighter-rouge">SpanBERT</code>ì˜ SBO í•™ìŠµì„ ìœ„í•œ ì…ë ¥ ë§Œë“¤ê¸°, SBOHeadì— ëŒ€í•´ì„œë§Œ ì„¤ëª…í•˜ë ¤ê³  í•œë‹¤. <code class="language-plaintext highlighter-rouge">BERT</code>, <code class="language-plaintext highlighter-rouge">Whole World Masking</code>ì— ëŒ€í•´ ê¶ê¸ˆí•˜ë‹¤ë©´ ì´ì „ í¬ìŠ¤íŒ…ì„, ì „ì²´ ëª¨ë¸ êµ¬ì¡° ëŒ€í•œ ì½”ë“œëŠ” <strong><a href="https://github.com/qcqced123/model_study">ì—¬ê¸° ë§í¬</a></strong>ë¥¼ í†µí•´ ì°¸ê³ ë°”ë€ë‹¤.</p>

<p>ê³µê°œí•  ì½”ë“œëŠ” ì•„ì§ ì™„ë²½í•˜ê²Œ ë²¡í„°í™”ë¥¼ ì ìš©í•˜ì§€ ëª»í•´, GPU ë³‘ë ¬ ì—°ì‚°ì— ìµœì í™” ë˜ì§€ ëª»í•œ ì  ì–‘í•´ ë¶€íƒí•œë‹¤. ë¹ ë¥¸ ì‹œì¼ ì´ë‚´ì— ë²¡í„°í™”ë¥¼ ì ìš©í•´ì„œ ë‹¤ì‹œ ìˆ˜ì •ëœ ì½”ë“œë¥¼ ì˜¬ë¦¬ê² ë‹¤.</p>

<h4 id="span-masking-algoritm"><code class="language-plaintext highlighter-rouge">ğŸ‘©â€ğŸ’»Â Span Masking Algoritm</code></h4>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">random</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="nn">torch.nn.utils.rnn</span> <span class="kn">import</span> <span class="n">pad_sequence</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">Tensor</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">List</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">Any</span>
<span class="kn">from</span> <span class="nn">..tuner.mlm</span> <span class="kn">import</span> <span class="n">WholeWordMaskingCollator</span>
<span class="kn">from</span> <span class="nn">configuration</span> <span class="kn">import</span> <span class="n">CFG</span>

<span class="n">BPE</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s">'RobertaTokenizerFast'</span><span class="p">,</span>
    <span class="s">'GPT2TokenizerFast'</span><span class="p">,</span>
<span class="p">]</span>

<span class="n">SPM</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s">'DebertaV2TokenizerFast'</span><span class="p">,</span>
    <span class="s">'DebertaTokenizerFast'</span><span class="p">,</span>
    <span class="s">'XLMRobertaTokenizerFast'</span><span class="p">,</span>
<span class="p">]</span>

<span class="n">WORDPIECE</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s">'BertTokenizerFast'</span><span class="p">,</span>
    <span class="s">'ElectraTokenizerFast'</span><span class="p">,</span>
<span class="p">]</span>

<span class="k">def</span> <span class="nf">random_non_negative_integer</span><span class="p">(</span><span class="n">max_value</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">random</span><span class="p">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">max_value</span><span class="p">)</span>

<span class="k">class</span> <span class="nc">SpanCollator</span><span class="p">(</span><span class="n">WholeWordMaskingCollator</span><span class="p">):</span>
    <span class="s">""" Custom Collator for Span Boundary Objective Task, which is used for span masking algorithm
    Span Masking is similar to Whole Word Masking, but it has some differences:
        1) Span Masking does not use 10% of selected token left &amp; 10% of selected token replaced other vocab token
            - just replace all selected token to [MASK] token
    Algorithm:
    1) Select 2 random tokens from input tokens for spanning
    2) Calculate relative position embedding for each token with 2 random tokens froms step 1.
    3) Calculate span boundary objective with 2 random tokens from step 1 &amp; pos embedding from step 2.
    Args:
        cfg: configuration.CFG
        masking_budget: masking budget for Span Masking
                        (default: 0.15 =&gt; Recommended by original paper)
        span_probability: probability of span length for Geometric Distribution
                         (default: 0.2 =&gt; Recommended by original paper)
        max_span_length: maximum span length of each span in one batch sequence
                         (default: 10 =&gt; Recommended by original paper)
    References:
        https://arxiv.org/pdf/1907.10529.pdf
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">cfg</span><span class="p">:</span> <span class="n">CFG</span><span class="p">,</span>
        <span class="n">masking_budget</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.15</span><span class="p">,</span>
        <span class="n">span_probability</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.2</span><span class="p">,</span>
        <span class="n">max_span_length</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">SpanCollator</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">(</span><span class="n">cfg</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span> <span class="o">=</span> <span class="n">cfg</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">tokenizer</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">tokenizer</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">masking_budget</span> <span class="o">=</span> <span class="n">masking_budget</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">span_probability</span> <span class="o">=</span> <span class="n">span_probability</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">max_span_length</span> <span class="o">=</span> <span class="n">max_span_length</span>

    <span class="k">def</span> <span class="nf">_whole_word_mask</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">input_tokens</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span>
        <span class="n">max_predictions</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">CFG</span><span class="p">.</span><span class="n">max_seq</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]:</span>
        <span class="s">"""
        0) apply Whole Word Masking Algorithm for make gathering original token index in natural language
        1) calculate number of convert into masking tokens with masking budget*len(input_tokens)
        2) define span length of this iteration
            - span length follow geometric distribution
            - span length is limited by max_span_length
        """</span>
        <span class="n">cand_indexes</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">token</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">input_tokens</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">token</span> <span class="o">==</span> <span class="s">"[CLS]"</span> <span class="ow">or</span> <span class="n">token</span> <span class="o">==</span> <span class="s">"[SEP]"</span><span class="p">:</span>
                <span class="k">continue</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">cand_indexes</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="mi">1</span> <span class="ow">and</span> <span class="bp">self</span><span class="p">.</span><span class="n">select_post_string</span><span class="p">(</span><span class="n">token</span><span class="p">):</span>  <span class="c1"># method from WholeWordMaskingCollator
</span>                <span class="n">cand_indexes</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">].</span><span class="n">append</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
            <span class="k">elif</span> <span class="bp">self</span><span class="p">.</span><span class="n">select_src_string</span><span class="p">(</span><span class="n">token</span><span class="p">):</span>  <span class="c1"># method from WholeWordMaskingCollator
</span>                <span class="n">cand_indexes</span><span class="p">.</span><span class="n">append</span><span class="p">([</span><span class="n">i</span><span class="p">])</span>

        <span class="n">l</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">input_tokens</span><span class="p">)</span>
        <span class="n">src_l</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">cand_indexes</span><span class="p">)</span>
        <span class="n">num_convert_tokens</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">masking_budget</span> <span class="o">*</span> <span class="n">l</span><span class="p">)</span>
        <span class="n">budget</span> <span class="o">=</span> <span class="n">num_convert_tokens</span>  <span class="c1"># int is immutable object, so do not copy manually
</span>        <span class="n">masked_lms</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">covered_indexes</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
        <span class="k">while</span> <span class="n">budget</span><span class="p">:</span>
            <span class="n">span_length</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">min</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">distributions</span><span class="p">.</span><span class="n">Geometric</span><span class="p">(</span><span class="n">probs</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">span_probability</span><span class="p">).</span><span class="n">sample</span><span class="p">())))</span>
            <span class="n">src_index</span> <span class="o">=</span> <span class="n">random_non_negative_integer</span><span class="p">(</span><span class="n">src_l</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">span_length</span> <span class="o">&gt;</span> <span class="n">budget</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">budget</span> <span class="o">&lt;</span> <span class="mi">5</span><span class="p">:</span>  <span class="c1"># Set the span length to budget to avoid a large number of iter if the remaining budget is too small
</span>                    <span class="n">span_length</span> <span class="o">=</span> <span class="n">budget</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">continue</span>
            <span class="k">if</span> <span class="n">cand_indexes</span><span class="p">[</span><span class="n">src_index</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">span_length</span> <span class="o">&gt;</span> <span class="n">l</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>  <span class="c1"># If the index of the last token in the span is outside the full sequence range
</span>                <span class="k">continue</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">cand_indexes</span><span class="p">[</span><span class="n">src_index</span><span class="p">])</span> <span class="o">&gt;</span> <span class="n">span_length</span><span class="p">:</span>  <span class="c1"># handling bad case: violating WWM algorithm at start
</span>                <span class="k">continue</span>
            <span class="n">span_token_index</span> <span class="o">=</span> <span class="n">cand_indexes</span><span class="p">[</span><span class="n">src_index</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>  <span class="c1"># init span token index: src token
</span>            <span class="k">while</span> <span class="n">span_length</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">span_length</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="k">break</span>
                <span class="k">if</span> <span class="n">span_token_index</span> <span class="ow">in</span> <span class="n">covered_indexes</span><span class="p">:</span> <span class="c1"># If it encounters an index that is already masked, it ends, and starts the next iteration
</span>                    <span class="k">break</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">covered_indexes</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">span_token_index</span><span class="p">)</span>
                    <span class="n">masked_lms</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">span_token_index</span><span class="p">)</span>
                    <span class="n">span_length</span> <span class="o">-=</span> <span class="mi">1</span>
                    <span class="n">budget</span> <span class="o">-=</span> <span class="mi">1</span>
                    <span class="n">span_token_index</span> <span class="o">+=</span> <span class="mi">1</span>
                    <span class="k">continue</span>

        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">covered_indexes</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">masked_lms</span><span class="p">):</span>
            <span class="k">raise</span> <span class="nb">ValueError</span><span class="p">(</span><span class="s">"Length of covered_indexes is not equal to length of masked_lms."</span><span class="p">)</span>
        <span class="n">mask_labels</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span> <span class="k">if</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">covered_indexes</span> <span class="k">else</span> <span class="mi">0</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">input_tokens</span><span class="p">))]</span>
        <span class="k">return</span> <span class="n">mask_labels</span>

    <span class="k">def</span> <span class="nf">get_mask_tokens</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">inputs</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">mask_labels</span><span class="p">:</span> <span class="n">Tensor</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
        <span class="s">""" Prepare masked tokens inputs/labels for Span Boundary Objective with MLM (15%),
        All of masked tokens (15%) are replaced by [MASK] token,
        Unlike BERT MLM which is replaced by random token or stay original token left
        """</span>
        <span class="n">labels</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">.</span><span class="n">clone</span><span class="p">()</span>
        <span class="n">probability_matrix</span> <span class="o">=</span> <span class="n">mask_labels</span>

        <span class="n">special_tokens_mask</span> <span class="o">=</span> <span class="p">[</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">tokenizer</span><span class="p">.</span><span class="n">get_special_tokens_mask</span><span class="p">(</span><span class="n">val</span><span class="p">,</span> <span class="n">already_has_special_tokens</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span> <span class="k">for</span> <span class="n">val</span> <span class="ow">in</span> <span class="n">labels</span><span class="p">.</span><span class="n">tolist</span><span class="p">()</span>
        <span class="p">]</span>
        <span class="n">probability_matrix</span><span class="p">.</span><span class="n">masked_fill_</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">special_tokens_mask</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">bool</span><span class="p">),</span> <span class="n">value</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">tokenizer</span><span class="p">.</span><span class="n">pad_token</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">padding_mask</span> <span class="o">=</span> <span class="n">labels</span><span class="p">.</span><span class="n">eq</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">tokenizer</span><span class="p">.</span><span class="n">pad_token_id</span><span class="p">)</span>
            <span class="n">probability_matrix</span><span class="p">.</span><span class="n">masked_fill_</span><span class="p">(</span><span class="n">padding_mask</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)</span>

        <span class="n">masked_indices</span> <span class="o">=</span> <span class="n">probability_matrix</span><span class="p">.</span><span class="nb">bool</span><span class="p">()</span>
        <span class="n">labels</span><span class="p">[</span><span class="o">~</span><span class="n">masked_indices</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mi">100</span>  <span class="c1"># We only compute loss on masked tokens
</span>        <span class="n">inputs</span><span class="p">[</span><span class="n">masked_indices</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">tokenizer</span><span class="p">.</span><span class="n">convert_tokens_to_ids</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">tokenizer</span><span class="p">.</span><span class="n">mask_token</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">labels</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batched</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]])</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">:</span>
        <span class="s">""" Abstract Method for Collator, you must implement this method in child class """</span>
        <span class="n">input_ids</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="s">"input_ids"</span><span class="p">])</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">batched</span><span class="p">]</span>
        <span class="n">padding_mask</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="p">.</span><span class="n">get_padding_mask</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">input_ids</span><span class="p">]</span>

        <span class="n">padding_mask</span> <span class="o">=</span> <span class="n">pad_sequence</span><span class="p">(</span><span class="n">padding_mask</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">padding_value</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="n">input_ids</span> <span class="o">=</span> <span class="n">pad_sequence</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">padding_value</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

        <span class="n">mask_labels</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">batched</span><span class="p">:</span>
            <span class="n">ref_tokens</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">input_id</span> <span class="ow">in</span> <span class="n">x</span><span class="p">[</span><span class="s">"input_ids"</span><span class="p">]:</span>
                <span class="n">token</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">tokenizer</span><span class="p">.</span><span class="n">_convert_id_to_token</span><span class="p">(</span><span class="n">input_id</span><span class="p">)</span>
                <span class="n">ref_tokens</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">token</span><span class="p">)</span>
            <span class="n">mask_labels</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">_whole_word_mask</span><span class="p">(</span><span class="n">ref_tokens</span><span class="p">))</span>

        <span class="n">mask_labels</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">mask_labels</span><span class="p">]</span>
        <span class="n">mask_labels</span> <span class="o">=</span> <span class="n">pad_sequence</span><span class="p">(</span><span class="n">mask_labels</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">padding_value</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">input_ids</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">get_mask_tokens</span><span class="p">(</span>
            <span class="n">input_ids</span><span class="p">,</span>
            <span class="n">mask_labels</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="p">{</span>
            <span class="s">"input_ids"</span><span class="p">:</span> <span class="n">input_ids</span><span class="p">,</span>
            <span class="s">"labels"</span><span class="p">:</span> <span class="n">labels</span><span class="p">,</span>
            <span class="s">"padding_mask"</span><span class="p">:</span> <span class="n">padding_mask</span><span class="p">,</span>
            <span class="s">"mask_labels"</span><span class="p">:</span> <span class="n">mask_labels</span>
        <span class="p">}</span>
</code></pre></div></div>

<h4 id="sbo-head"><code class="language-plaintext highlighter-rouge">ğŸ‘©â€ğŸ’»Â SBO Head</code></h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">SBOHead</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="s">""" Custom Head for Span Boundary Objective Task, this module return logit value for each token
    we use z for class logit, each Fully Connected Layer doesn't have bias term in original paper
    so we don't use bias term in this module =&gt; nn.Linear(bias=False)

    You must select option for matrix sum or concatenate with x_s-1, x_e+1, p_i-s+1
    If you select concatenate option, you must pass is_concatenate=True to cfg.is_concatenate, default is True
    
    Math:
        h_0 = [x_s-1;x_e+1;p_i-s+1]
        h_t = LayerNorm(GELU(W_0â€¢h_0))
        z = LayerNorm(GELU(W_1â€¢h_t))

    Args:
        cfg: configuration.CFG
        is_concatenate: option for matrix sum or concatenate with x_s-1, x_e+1, p_i-s+1, default True
        max_span_length: maximum span length of each span in one batch sequence
                         (default: 10 =&gt; Recommended by original paper)
    References:
        https://arxiv.org/pdf/1907.10529.pdf
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">cfg</span><span class="p">:</span> <span class="n">CFG</span><span class="p">,</span>
        <span class="n">is_concatenate</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="bp">True</span><span class="p">,</span>
        <span class="n">max_span_length</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">SBOHead</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span> <span class="o">=</span> <span class="n">cfg</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">is_concatenate</span> <span class="o">=</span> <span class="n">is_concatenate</span>  <span class="c1"># for matrix sum or concatenate with x_s-1, x_e+1, p_i-s+1
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">projector</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">dim_model</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">dim_model</span><span class="o">*</span><span class="mi">3</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>  <span class="c1"># for concatenate x_s-1, x_e+1, p_i-s+1
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">span_pos_emb</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">max_span_length</span><span class="o">*</span><span class="mi">3</span><span class="p">,</span> <span class="n">cfg</span><span class="p">.</span><span class="n">dim_model</span><span class="p">)</span>  <span class="c1"># size of dim_model is research topic
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">head</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">dim_model</span><span class="o">*</span><span class="mi">3</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">dim_ffn</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">GELU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">dim_ffn</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">dim_ffn</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">dim_model</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">GELU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">dim_model</span><span class="p">),</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">classifier</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">dim_model</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">cfg</span><span class="p">.</span><span class="n">vocab_size</span><span class="p">))</span>  <span class="c1"># for matching vocab size
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">classifier</span><span class="p">.</span><span class="n">bias</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">bias</span>

    <span class="o">@</span><span class="nb">staticmethod</span>
    <span class="k">def</span> <span class="nf">find_consecutive_groups</span><span class="p">(</span><span class="n">mask_labels</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">target_value</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">Dict</span><span class="p">]]:</span>
        <span class="s">""" Get the start and end positions of consecutive groups in tensor for the target value
        This method is used for SBO Objective Function, this version is not best performance to make span groups

        Args:
            mask_labels: masking tensor for span
            target_value: target value for finding consecutive groups
        """</span>
        <span class="n">all_consecutive_groups</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">mask_label</span> <span class="ow">in</span> <span class="n">mask_labels</span><span class="p">:</span>
            <span class="n">consecutive_groups</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="n">current_group</span> <span class="o">=</span> <span class="bp">None</span>
            <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">mask_label</span><span class="p">):</span>
                <span class="k">if</span> <span class="n">value</span> <span class="o">==</span> <span class="n">target_value</span><span class="p">:</span>
                    <span class="k">if</span> <span class="n">current_group</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
                        <span class="n">current_group</span> <span class="o">=</span> <span class="p">{</span><span class="s">"start"</span><span class="p">:</span> <span class="n">i</span><span class="p">,</span> <span class="s">"end"</span><span class="p">:</span> <span class="n">i</span><span class="p">}</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">current_group</span><span class="p">[</span><span class="s">"end"</span><span class="p">]</span> <span class="o">=</span> <span class="n">i</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">if</span> <span class="n">current_group</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
                        <span class="n">consecutive_groups</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">current_group</span><span class="p">)</span>
                        <span class="n">current_group</span> <span class="o">=</span> <span class="bp">None</span>
            <span class="k">if</span> <span class="n">current_group</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
                <span class="n">consecutive_groups</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">current_group</span><span class="p">)</span>
            <span class="n">all_consecutive_groups</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">consecutive_groups</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">all_consecutive_groups</span>

    <span class="k">def</span> <span class="nf">cal_span_emb</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">h</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">consecutive_groups</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">Dict</span><span class="p">]])</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="s">""" Calculate span embedding for each span in one batch sequence

        Args:
            h: hidden states, already passed through projection layer (dim*3)
            hidden_states: hidden states from encoder
            consecutive_groups: consecutive groups for each batch sequence
        """</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">batch</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">consecutive_groups</span><span class="p">):</span>  <span class="c1"># batch level
</span>            <span class="k">for</span> <span class="n">j</span><span class="p">,</span> <span class="n">span</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">batch</span><span class="p">):</span>  <span class="c1"># span level
</span>                <span class="n">start</span><span class="p">,</span> <span class="n">end</span> <span class="o">=</span> <span class="n">span</span><span class="p">[</span><span class="s">"start"</span><span class="p">],</span> <span class="n">span</span><span class="p">[</span><span class="s">"end"</span><span class="p">]</span>
                <span class="n">length</span> <span class="o">=</span> <span class="n">end</span> <span class="o">-</span> <span class="n">start</span> <span class="o">+</span> <span class="mi">1</span>
                <span class="n">idx</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="n">length</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>   <span class="c1"># .to(self.cfg.device)
</span>                <span class="n">context_s</span><span class="p">,</span> <span class="n">context_e</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">start</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="p">:],</span> <span class="n">hidden_states</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">end</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="p">:]</span>
                <span class="n">span_pos_emb</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">span_pos_emb</span><span class="p">(</span><span class="n">idx</span><span class="p">).</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">length</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                    <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">p_h</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">span_pos_emb</span><span class="p">):</span>  <span class="c1"># length of span_pos_emb == length of span of this iterations
</span>                        <span class="n">h</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">start</span><span class="o">+</span><span class="n">k</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cat</span><span class="p">([</span><span class="n">context_s</span><span class="p">,</span> <span class="n">p_h</span><span class="p">,</span> <span class="n">context_e</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">h</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">start</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cat</span><span class="p">([</span><span class="n">context_s</span><span class="p">,</span> <span class="n">span_pos_emb</span><span class="p">,</span> <span class="n">context_e</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">h</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">mask_labels</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="n">consecutive_groups</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">find_consecutive_groups</span><span class="p">(</span><span class="n">mask_labels</span><span class="p">)</span>  <span class="c1"># [batch, num_consecutive_groups]
</span>        <span class="n">h</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">projector</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>  <span class="c1"># [batch, seq, dim_model*3]
</span>        <span class="n">h_t</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">cal_span_emb</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">,</span> <span class="n">consecutive_groups</span><span class="p">)</span>
        <span class="n">z</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">head</span><span class="p">(</span><span class="n">h_t</span><span class="p">)</span>
        <span class="n">logit</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">classifier</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">logit</span>

</code></pre></div></div>]]></content><author><name>qcqced</name><email>qcqced123@gmail.com</email></author><category term="NLP" /><category term="Natural Language Process" /><category term="SpanBERT" /><category term="BERT" /><category term="Self-Attention" /><category term="Pytorch" /><summary type="html"><![CDATA[SpanBERT Official Paper Review with Pytorch Implementation]]></summary></entry><entry><title type="html">ğŸ¡ [Roformer] RoFormer: Enhanced Transformer with Rotary Position Embedding</title><link href="http://localhost:4000/nlp/roformer" rel="alternate" type="text/html" title="ğŸ¡ [Roformer] RoFormer: Enhanced Transformer with Rotary Position Embedding" /><published>2024-03-11T00:00:00+09:00</published><updated>2024-03-12T02:00:00+09:00</updated><id>http://localhost:4000/nlp/roformer</id><content type="html" xml:base="http://localhost:4000/nlp/roformer"><![CDATA[<h3 id="overview"><code class="language-plaintext highlighter-rouge">ğŸ”­Â Overview</code></h3>

<p><code class="language-plaintext highlighter-rouge">DistilBERT</code> ëŠ” í—ˆê¹… í˜ì´ìŠ¤ ì—°êµ¬ì§„ì´ 2019ë…„ ë°œí‘œí•œ BERTì˜ ë³€í˜•ìœ¼ë¡œì„œ, On-Device Ai ê°œë°œì„ ëª©í‘œë¡œ ê²½ëŸ‰í™”ì— ì´ˆì ì„ ë§ì¶˜ ëª¨ë¸ì´ë‹¤. GPT, BERTì˜ ë“±ì¥ ì´í›„, NLP ë¶„ì•¼ì—ì„œ ë¹„ì•½ì ì¸ ì„±ëŠ¥ í–¥ìƒì´ ì´ë¤„ì¡ŒìŒì—ë„ ë¶ˆêµ¬í•˜ê³ , í„°ë¬´ë‹ˆ ì—†ëŠ” ëª¨ë¸ ì‚¬ì´ì¦ˆì™€ ì»´í“¨íŒ… ë¦¬ì†ŒìŠ¤ ìš”êµ¬ë¡œ ì¸í•´ ì‹¤ìƒí™œ ì ìš© ê°™ì€ í™œìš©ì„±ì€ ì—¬ì „íˆ í•´ê²°í•´ì•¼í•  ë¬¸ì œë¡œ ë‚¨ì•„ ìˆì—ˆë‹¤. Googleì—ì„œ ë°œí‘œí•œ ì´ˆê¸° <code class="language-plaintext highlighter-rouge">BERT-base-uncased</code> ë§Œ í•´ë„ íŒŒë¼ë¯¸í„°ê°€ 1ì–µ 1ì²œë§Œê°œ ìˆ˜ì¤€ì— ë‹¬í•œë‹¤.</p>

<p>ì´ë¥¼ ë‹¤ì–‘í•œ ë¹„ì¦ˆë‹ˆìŠ¤ ìš”êµ¬ ìƒí™©ì— ì ìš©í•  ìˆ˜ ìˆìœ¼ë ¤ë©´ ìµœì†Œí•œ 8GB ì´ìƒì˜ ê°€ì†ê¸° ì „ìš© RAM ê³µê°„ì„ ìš”êµ¬ë¡œ í•œë‹¤. ì˜¤ëŠ˜ë‚  ê°œì¸ìš© PC í˜¹ì€ ì„œë²„ ì»´í“¨í„°ì˜ ê²½ìš°, 8GB ì´ìƒì˜ VRAMì´ ë‹¬ë¦° GPUê°€ ì¼ë°˜ì ìœ¼ë¡œ íƒ‘ì¬ë˜ê¸° ë•Œë¬¸ì— í¬ê²Œ ë¬¸ì œ ë  ê²ƒ ì—†ëŠ” ìš”êµ¬ì‚¬í•­ì´ì§€ë§Œ, On-Device í™˜ê²½ì—ì„œëŠ” ì´ì•¼ê¸°ê°€ ë‹¬ë¼ì§„ë‹¤. ìµœì‹  í•˜ì´ì—”ë“œ ìŠ¤ë§ˆíŠ¸í°ì¸ Galaxy S24 Ultra, iPhone 15 Proì˜ ê²½ìš° 12GB, 8GBì˜ ë¨ ìš©ëŸ‰ì„ ë³´ìœ í•˜ê³  ìˆë‹¤. ê·¸ë§ˆì €ë„ ëŒ€ë¶€ë¶„ì˜ ì˜¨ë””ë°”ì´ìŠ¤ í™˜ê²½ì€ SoC êµ¬ì¡°ë¥¼ ì±„íƒí•˜ê³  ìˆê¸° ë•Œë¬¸ì— ì „ìš© ê°€ì†ê¸°ê°€ ì˜¨ì „íˆ ì € ëª¨ë“  ë¨ ê³µê°„ì„ í™œìš©í•  ìˆ˜ ì—†ë‹¤.</p>

<p>ë”°ë¼ì„œ ì˜¨ë””ë°”ì´ìŠ¤ì— Aië¥¼ ì ìš©í•˜ê¸° ìœ„í•´ì„œëŠ” íšê¸°ì ì¸ ëª¨ë¸ ê²½ëŸ‰í™”ê°€ í•„ìš”í•œ ìƒí™©ì´ê³  ê·¸ ì¶œë°œì ì´ ëœ ì—°êµ¬ê°€ ë°”ë¡œ <code class="language-plaintext highlighter-rouge">DistilBERT</code>ë‹¤. ë¡œì»¬ ë””ë°”ì´ìŠ¤ í™˜ê²½ì—ì„œë„ ì–¸ì–´ ëª¨ë¸ì„ í™œìš©í•˜ê¸° ìœ„í•´ í—ˆê¹… í˜ì´ìŠ¤ ì—°êµ¬ì§„ì€ ì§€ì‹ ì¦ë¥˜ ê¸°ë²•ì„ í™œìš©í•´ ì¸ì½”ë” ê¸°ë°˜ ì–¸ì–´ ëª¨ë¸ì˜ íŒŒë¼ë¯¸í„°ë¥¼ íšê¸°ì ìœ¼ë¡œ ì¤„ì´ëŠ”ë° ì„±ê³µí•œë‹¤.</p>

<p>ì •ë¦¬í•˜ìë©´, <code class="language-plaintext highlighter-rouge">DistilBERT</code> ëª¨ë¸ì€ ê¸°ì¡´ BERTì˜ êµ¬ì¡°ì  ì¸¡ë©´ ê°œì„ ì´ ì•„ë‹Œ, ì‚¬ì „í•™ìŠµ ë°©ë²• íŠ¹íˆ ê²½ëŸ‰í™”ì— ì´ˆì ì„ ë§ì¶˜ ì‹œë„ë¼ê³  ë³¼ ìˆ˜ ìˆë‹¤. ë”°ë¼ì„œ ì–´ë–¤ ëª¨ë¸ì´ë”ë¼ë„, ì¸ì½”ë” ì–¸ì–´ ëª¨ë¸ì´ë¼ë©´ ëª¨ë‘ <code class="language-plaintext highlighter-rouge">DistilBERT</code> êµ¬ì¡°ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìœ¼ë©°, ê¸°ì¡´ ë…¼ë¬¸ì—ì„œëŠ” ì›ë³¸ BERT êµ¬ì¡°ë¥¼ ì‚¬ìš©í–ˆë‹¤. ì´ë²ˆ í¬ìŠ¤íŒ…ì—ì„œë„ BERT êµ¬ì¡°ì— ëŒ€í•œ ì„¤ëª… ëŒ€ì‹ , <code class="language-plaintext highlighter-rouge">DistilBERT</code>ì˜ ì‚¬ì „ í•™ìŠµ ë°©ë²•ë¡ ì¸ <code class="language-plaintext highlighter-rouge">Knowledge Distillation</code>ì— ëŒ€í•´ì„œë§Œ ë‹¤ë£¨ë ¤ê³  í•œë‹¤.</p>

<h3 id="knowledge-distillations"><code class="language-plaintext highlighter-rouge">ğŸŒ†Â Knowledge Distillations</code></h3>

\[\min_{\theta}\sum_{x \in X} \alpha \mathcal{L}_{\text{KL}}(x, \theta) + \beta \mathcal{L}_{\text{MLM}}(x, \theta) + \gamma \mathcal{L}_{\text{Cos}}(x, \theta)\]

<p><code class="language-plaintext highlighter-rouge">DistilBERT</code>ëŠ” Teacher-Student Architectureë¥¼ ì°¨ìš©í•´ ìƒëŒ€ì ìœ¼ë¡œ ì‘ì€ íŒŒë¼ë¯¸í„° ì‚¬ì´ì¦ˆë¥¼ ê°–ëŠ” <code class="language-plaintext highlighter-rouge">Student</code> ëª¨ë¸ì—ê²Œ <code class="language-plaintext highlighter-rouge">Teacher</code>ì˜ ì§€ì‹ì„ ì „ìˆ˜í•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•œë‹¤. ë”°ë¼ì„œ <code class="language-plaintext highlighter-rouge">Teacher</code> ëª¨ë¸ì€ ì´ë¯¸ ì‚¬ì „ í•™ìŠµì„ ë§ˆì¹˜ê³  ìˆ˜ë ´ëœ ìƒíƒœì˜ ê°€ì¤‘ì¹˜ë¥¼ ê°–ê³  ìˆëŠ” ëª¨ë¸ì„ ì‚¬ìš©í•´ì•¼ í•œë‹¤. ë”ë¶ˆì–´ Teacher ëª¨ë¸ì€ êµ¬ì¡°ë§Œ ê¸°ì¡´ BERTë¥¼ ë”°ë¥´ë˜, ì‚¬ì „ í•™ìŠµ ë°©ì‹ì€ RoBERTaì˜ ë°©ì‹ê³¼ ë™ì¼(NSP ì œê±°, Dynamic Masking ì ìš©)í•˜ê²Œ í›ˆë ¨ë˜ì–´ì•¼ í•œë‹¤.</p>

<p>í•œí¸, <code class="language-plaintext highlighter-rouge">Student</code> ëª¨ë¸ì€ <code class="language-plaintext highlighter-rouge">Teacher</code>ì˜ 60%ì •ë„ íŒŒë¼ë¯¸í„° ì‚¬ì´ì¦ˆë¥¼ ê°–ë„ë¡ ì¶•ì†Œí•˜ì—¬ ì‚¬ìš©í•œë‹¤. ì´ ë•Œ ì¶•ì†ŒëŠ” ëª¨ë¸ì˜ <code class="language-plaintext highlighter-rouge">depth</code>(ë ˆì´ì–´ ê°œìˆ˜)ì—ë§Œ ì ìš©í•˜ëŠ”ë°, ì—°êµ¬ì§„ì— ë”°ë¥´ë©´ <code class="language-plaintext highlighter-rouge">width</code>(ì€ë‹‰ì¸µ í¬ê¸°)ëŠ” ì¶•ì†Œë¥¼ ì ìš©í•´ë„ ì—°ì‚° íš¨ìœ¨ì´ ì¦ê°€í•˜ì§€ ì•ŠëŠ”ë‹¤ê³  í•œë‹¤. ì •ë¦¬í•˜ë©´ <code class="language-plaintext highlighter-rouge">Teacher</code> ëª¨ë¸ì˜ <code class="language-plaintext highlighter-rouge">ë ˆì´ì–´ ê°œìˆ˜*0.6</code>ì˜ ê°œìˆ˜ë§Œí¼ ì¸ì½”ë”ë¥¼ ìŒ“ìœ¼ë©´ ëœë‹¤ëŠ” ê²ƒì´ë‹¤.</p>

<p>ê·¸ë¦¬ê³  ìµœëŒ€í•œ <code class="language-plaintext highlighter-rouge">Teacher</code>ì˜ ì§€ì‹ì„ ì „ìˆ˜í•´ì•¼ í•˜ê¸° ë•Œë¬¸ì—, ë°ì´í„°ëŠ” <code class="language-plaintext highlighter-rouge">Teacher</code> ë¥¼ ìˆ˜ë ´ì‹œí‚¨ ê²ƒê³¼ ë™ì¼í•œ ì„¸íŠ¸ë¥¼ ì´ìš©í•´ì•¼ í•œë‹¤. ì´ ë•Œ, Teacher ëª¨ë¸ì€ ì´ë¯¸ MLE ë°©ì‹ìœ¼ë¡œ í›ˆë ¨ì´ ëœ ìƒíƒœë¼ì„œ ë¡œì§“ì´ ë‹¨ì¼ í† í° í•˜ë‚˜ ìª½ìœ¼ë¡œ ì ë ¤ ìˆì„ ê°€ëŠ¥ì„±ì´ ë§¤ìš° ë†’ë‹¤. ì´ëŠ” <code class="language-plaintext highlighter-rouge">Student</code> ëª¨ë¸ì˜ ì¼ë°˜í™” ëŠ¥ë ¥ì— ì•…ì˜í–¥ì„ ë¯¸ì¹  ê°€ëŠ¥ì„±ì´ ë†’ë‹¤. ë”°ë¼ì„œ Temperature ë³€ìˆ˜ $T$ ë„ì…í•´ ì†Œí”„íŠ¸ ë§¥ìŠ¤(ë¡œì§“)ì˜ ë¶„í¬ë¥¼ í‰íƒ„í™” í•œë‹¤. ì´ë ‡ê²Œ í•˜ë©´, <code class="language-plaintext highlighter-rouge">argmax()</code> ê°€ ì•„ë‹Œ ë‹¤ë¥¸ í† í° í‘œí˜„ì— ëŒ€í•´ì„œë„ <code class="language-plaintext highlighter-rouge">Student</code> ëª¨ë¸ì´ ì§€ì‹ì„ ìŠµë“í•  ìˆ˜ ìˆì–´ì„œ í’ë¶€í•œ ë¬¸ë§¥ì„ í•™ìŠµí•˜ê³  ì¼ë°˜í™” ëŠ¥ë ¥ì„ ë†’ì´ëŠ”ë° ë„ì›€ì´ ëœë‹¤. ì´ë¥¼ <code class="language-plaintext highlighter-rouge">ì•”í‘ ì§€ì‹(Dark Knowledge)</code> ì„ í™œìš©í•œë‹¤ê³  í‘œí˜„í•œë‹¤. Temperature ë³€ìˆ˜ $T$ ë„ì…í•œ ì†Œí”„íŠ¸ë§¥ìŠ¤ í•¨ìˆ˜ ìˆ˜ì‹ì€ ì•„ë˜ì™€ ê°™ë‹¤.</p>

\[\text{softmax}(x_i) = \frac{e^{\frac{x_i}{\tau}}}{\sum_{j} e^{\frac{x_j}{\tau}}}\]

<p>ìˆ˜ì‹ìƒ ë³€ìˆ˜ $T$ì˜ ê°’ì„ 1ì´ìƒìœ¼ë¡œ ì„¸íŒ…í•´ì•¼ í‰íƒ„í™”ë¥¼ í•  ìˆ˜ ìˆë‹¤. ë”°ë¼ì„œ ì—°êµ¬ì§„ì€ $T =2$ ë¡œ ë‘ê³  ì‚¬ì „ í•™ìŠµì„ ì§„í–‰í–ˆë‹¤(ë…¼ë¬¸ì— ê³µê°œì•ˆë¨, GitHubì— ìˆìŒ). ì´ë²ˆ íŒŒíŠ¸ ë§¨ ì²˜ìŒì— ë“±ì¥í•œ ìˆ˜ì‹ì„ ë‹¤ì‹œ ë³´ì. ê²°êµ­ <code class="language-plaintext highlighter-rouge">DisilBERT</code>ì˜ ëª©ì í•¨ìˆ˜ëŠ” 3ê°€ì§€ ì†ì‹¤ì˜ ê°€ì¤‘í•©ìœ¼ë¡œ êµ¬ì„±ëœë‹¤. ì´ì œë¶€í„°ëŠ” ê°œë³„ ì†ì‹¤ì— ëŒ€í•´ì„œ ìì„¸íˆ ì‚´í´ë³´ì.</p>

<h4 id="distillation-loss-kl-divergence-loss"><code class="language-plaintext highlighter-rouge">ğŸŒ†Â Distillation Loss: KL-Divergence Loss</code></h4>

\[\text{KL-Divergence}(P || Q) = \sum_{i} P(i) \log \frac{P(i)}{Q(i)}\]

<p>ì¦ë¥˜ ì†ì‹¤ë¡œ ì‚¬ìš©ë˜ëŠ” <code class="language-plaintext highlighter-rouge">KL-Divergence Loss</code>ëŠ” ë‘ í™•ë¥  ë¶„í¬ ê°„ì˜ ì°¨ì´ë¥¼ ì¸¡ì •í•˜ëŠ” ì§€í‘œ ì¤‘ í•˜ë‚˜ë‹¤. ì£¼ë¡œ í™•ë¥  ë¶„í¬ Pì™€ Q ì‚¬ì´ì˜ ì°¨ì´ë¥¼ ë‚˜íƒ€ë‚´ëŠ”ë°, ê°œë³„ ìš”ì†Œì˜ í™•ë¥ ê°’ ì°¨ì´ê°€ í´ìˆ˜ë¡ í•©ì‚°ê°’ì€ ì»¤ì ¸ ì†ì‹¤ì´ ì»¤ì§€ê²Œ ëœë‹¤. ë°˜ëŒ€ë¡œ ë‘ ë¶„í¬ì˜ ê°œë³„ ìš”ì†Œ í™•ë¥ ê°’ ì°¨ì´ê°€ ì‘ë‹¤ë©´ ë‹¹ì—°íˆ, ë‘ ë¶„í¬ê°€ ìœ ì‚¬í•˜ë‹¤ëŠ” ì˜ë¯¸ì´ë¯€ë¡œ ì†ì‹¤ ì—­ì‹œ ì‘ì•„ì§€ê²Œ ëœë‹¤. ì¼ë°˜ì ìœ¼ë¡œ <code class="language-plaintext highlighter-rouge">KL-Divergence Loss</code> ì—ì„œ í™•ë¥ ë¶„í¬ $P$ ê°€ ì´ìƒì ì¸ í™•ë¥  ë¶„í¬ë¥¼, $Q$ ê°€ ëª¨ë¸ì´ ì˜ˆì¸¡í•œ í™•ë¥ ë¶„í¬ë¥¼ ì˜ë¯¸í•œë‹¤. ë”°ë¼ì„œ <code class="language-plaintext highlighter-rouge">DistilBERT</code>ì˜ ê²½ìš° í™•ë¥ ë¶„í¬ $P$ ìë¦¬ì—ëŠ” <code class="language-plaintext highlighter-rouge">Teacher</code> ëª¨ë¸ì˜ ì†Œí”„íŠ¸ë§¥ìŠ¤ ë¶„í¬ê°€, $Q$ ì—ëŠ” <code class="language-plaintext highlighter-rouge">Student</code> ëª¨ë¸ì˜ ì†Œí”„íŠ¸ë§¥ìŠ¤ ë¶„í¬ê°€ ëŒ€ì…ë˜ë©´ ëœë‹¤. ì´ ë•Œ ë‘ í™•ë¥ ë¶„í¬ ëª¨ë‘, ì•”í‘ ì§€ì‹ íšë“ì„ ìœ„í•´ ì†Œí”„íŠ¸ë§¥ìŠ¤ í‰íƒ„í™”ë¥¼ ì ìš©í•œ ê²°ê³¼ë¥¼ ì‚¬ìš©í•œë‹¤. ë…¼ë¬¸ì—ì„œ, ì„ ìƒ ëª¨ë¸ ì˜ˆì¸¡ì— í‰íƒ„í™”ë¥¼ ì ìš©í•œ ê²ƒì„ <code class="language-plaintext highlighter-rouge">ì†Œí”„íŠ¸ ë¼ë²¨</code>, í•™ìƒ ëª¨ë¸ì˜ ê²ƒì— ì ìš©í•œ ê²°ê³¼ëŠ” <code class="language-plaintext highlighter-rouge">ì†Œí”„íŠ¸ ì˜ˆì¸¡</code>ì´ë¼ê³  ë¶€ë¥¸ë‹¤.</p>

<h4 id="student-loss-mlm-loss"><code class="language-plaintext highlighter-rouge">ğŸŒ†Â Student Loss: MLM Loss</code></h4>

\[\mathcal{L}_{\text{MLM}} = - \sum_{i=1}^{N} \sum_{j=1}^{L} \mathbb{1}_{m_{ij}} \log \text{softmax}(x_{ij})\]

<p>í•™ìƒ ì†ì‹¤ì€ ë§ê·¸ëŒ€ë¡œ ê¸°ë³¸ì ì¸ MLM ì†ì‹¤ì„ ë§í•œë‹¤. ì •í™•í•œ ì†ì‹¤ê°’ ê³„ì‚°ì„ ìœ„í•´ì„œ í•™ìƒì˜ ì†Œí”„íŠ¸ë§¥ìŠ¤ ë¶„í¬ì— í‰íƒ„í™”ë¥¼ ì ìš©í•˜ì§€ ì•ŠëŠ”ë‹¤. ì´ë¥¼ ë…¼ë¬¸ì—ì„œëŠ” <code class="language-plaintext highlighter-rouge">í•˜ë“œ ì˜ˆì¸¡</code>ì´ë¼ê³  ë¶€ë¥¸ë‹¤. ë¼ë²¨ ì—­ì‹œ <code class="language-plaintext highlighter-rouge">Teacher</code>ë¡œë¶€í„° ë‚˜ì˜¨ ê²ƒì´ ì•„ë‹Œ ì›ë˜ MLM ìˆ˜í–‰ì— ì‚¬ìš©ë˜ëŠ” ë§ˆìŠ¤í‚¹ ë¼ë²¨ì„ ì‚¬ìš©í•œë‹¤.</p>

<h4 id="cosine-embedding-loss-contrastive-loss-by-cosine-similarity"><code class="language-plaintext highlighter-rouge">ğŸŒ†Â Cosine Embedding Loss: Contrastive Loss by cosine similarity</code></h4>

\[\mathcal{L}_{\text{COS}}(x,y) = \begin{cases} 1 - \cos(x_1, x_2), &amp; \text{if } y = 1 \\ \max(0, \cos(x_1, x_2) - \text{margin}), &amp; \text{if } y = -1 \end{cases}\]

<p><code class="language-plaintext highlighter-rouge">Teacher</code> ëª¨ë¸ê³¼ <code class="language-plaintext highlighter-rouge">Student</code> ëª¨ë¸ì˜ ë§ˆì§€ë§‰ ì¸ì½”ë” ëª¨ë¸ì´ ì¶œë ¥í•˜ëŠ” ì€ë‹‰ê°’ì— ëŒ€í•œ <code class="language-plaintext highlighter-rouge">Contrastive Loss</code>ë¥¼ ì˜ë¯¸í•œë‹¤. ì´ ë•Œ <code class="language-plaintext highlighter-rouge">Distance Metric</code>ì€ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ ì‚¬ìš©í•œë‹¤. ê·¸ë˜ì„œ ì½”ì‚¬ì¸ ì„ë² ë”© ì†ì‹¤ì´ë¼ê³  ë…¼ë¬¸ì—ì„œ ì •ì˜í•˜ëŠ” ê²ƒìœ¼ë¡œ ì¶”ì •ëœë‹¤. ìœ„ ìˆ˜ì‹ì„ ìµœì í™”í•˜ëŠ” ê²ƒì„ ëª©ì ìœ¼ë¡œ í•œë‹¤. ì´ ë•Œ ë¼ë²¨ì€ <code class="language-plaintext highlighter-rouge">[BS, Seq_len]</code>ì˜ í¬ê¸°ë¥¼ ê°–ë˜, ëª¨ë“  ì›ì†ŒëŠ” 1ì´ ë˜ë„ë¡ ë§Œë“ ë‹¤. ì´ìœ ëŠ” ê°„ë‹¨í•˜ë‹¤. <code class="language-plaintext highlighter-rouge">Student</code> ëª¨ë¸ì˜ ì€ë‹‰ê°’ì´ <code class="language-plaintext highlighter-rouge">Teacher</code> ëª¨ë¸ì˜ ê²ƒê³¼ ìµœëŒ€í•œ ë¹„ìŠ·í•´ì§€ë„ë¡ ë§Œë“œëŠ”ê²Œ ìš°ë¦¬ ëª©ì ì´ê¸° ë•Œë¬¸ì´ë‹¤.</p>

<h3 id="implementation-by-pytorch"><code class="language-plaintext highlighter-rouge">ğŸ‘©â€ğŸ’»Â Implementation by Pytorch</code></h3>
<p>ë…¼ë¬¸ì˜ ë‚´ìš©ê³¼ ì˜¤í”¼ì…œë¡œ ê³µê°œëœ ì½”ë“œë¥¼ ì¢…í•©í•˜ì—¬ íŒŒì´í† ì¹˜ë¡œ <code class="language-plaintext highlighter-rouge">DistilBERT</code>ë¥¼ êµ¬í˜„í•´ë´¤ë‹¤. ë…¼ë¬¸ì— í¬í•¨ëœ ì•„ì´ë””ì–´ë¥¼ ì´í•´í•˜ëŠ”ë°ëŠ” ì—­ì‹œ ì–´ë µì§€ ì•Šì•˜ì§€ë§Œ, í˜ì´í¼ì— hyper-param í…Œì´ë¸”ì´ ë”°ë¡œ ì œì‹œë˜ì–´ ìˆì§€ ì•Šì•„ ê³µê°œëœ ì½”ë“œë¥¼ ì•ˆ ë³¼ìˆ˜ê°€ ì—†ì—ˆë‹¤.</p>

<p>ì „ì²´ ëª¨ë¸ êµ¬ì¡° ëŒ€í•œ ì½”ë“œëŠ” <strong><a href="https://github.com/qcqced123/model_study">ì—¬ê¸° ë§í¬</a></strong>ë¥¼ í†µí•´ ì°¸ê³ ë°”ë€ë‹¤.</p>

<h4 id="knowledge-distillation-pipeline"><code class="language-plaintext highlighter-rouge">ğŸ‘©â€ğŸ’»Â Knowledge Distillation Pipeline</code></h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">train_val_fn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">loader_train</span><span class="p">,</span> <span class="n">model</span><span class="p">:</span> <span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">criterion</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">],</span> <span class="n">optimizer</span><span class="p">,</span><span class="n">scheduler</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]:</span>
    <span class="s">""" Function for train loop with validation for each batch*N Steps
    DistillBERT has three loss:

        1) distillation loss, calculated by soft targets &amp; soft predictions
            (nn.KLDIVLoss(reduction='batchmean'))

        2) student loss, calculated by hard targets &amp; hard predictions
            (nn.CrossEntropyLoss(reduction='mean')), same as pure MLM Loss

        3) cosine similarity loss, calculated by student &amp; teacher logit similarity
            (nn.CosineEmbeddingLoss(reduction='mean')), similar as contrastive loss

    Those 3 losses are summed jointly and then backward to student model
    """</span>
    <span class="n">scaler</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">amp</span><span class="p">.</span><span class="n">GradScaler</span><span class="p">(</span><span class="n">enabled</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">amp_scaler</span><span class="p">)</span>
    <span class="n">model</span><span class="p">.</span><span class="n">train</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">step</span><span class="p">,</span> <span class="n">batch</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">tqdm</span><span class="p">(</span><span class="n">loader_train</span><span class="p">)):</span>
        <span class="n">optimizer</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">(</span><span class="n">set_to_none</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="n">inputs</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="s">'input_ids'</span><span class="p">].</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">device</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="n">labels</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="s">'labels'</span><span class="p">].</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">device</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="n">padding_mask</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="s">'padding_mask'</span><span class="p">].</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">device</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

        <span class="n">mask</span> <span class="o">=</span> <span class="n">padding_mask</span><span class="p">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">).</span><span class="n">expand</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">dim_model</span><span class="p">)</span>  <span class="c1"># for hidden states dim
</span>        <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">t_hidden_state</span><span class="p">,</span> <span class="n">soft_target</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">teacher_fw</span><span class="p">(</span>
                <span class="n">inputs</span><span class="o">=</span><span class="n">inputs</span><span class="p">,</span>
                <span class="n">padding_mask</span><span class="o">=</span><span class="n">padding_mask</span><span class="p">,</span>
                <span class="n">mask</span><span class="o">=</span><span class="n">mask</span>
            <span class="p">)</span>  <span class="c1"># teacher model's pred =&gt; hard logit
</span>
        <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">amp</span><span class="p">.</span><span class="n">autocast</span><span class="p">(</span><span class="n">enabled</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">amp_scaler</span><span class="p">):</span>
            <span class="n">s_hidden_state</span><span class="p">,</span> <span class="n">s_logit</span><span class="p">,</span> <span class="n">soft_pred</span><span class="p">,</span> <span class="n">c_labels</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">student_fw</span><span class="p">(</span>
                <span class="n">inputs</span><span class="o">=</span><span class="n">inputs</span><span class="p">,</span>
                <span class="n">padding_mask</span><span class="o">=</span><span class="n">padding_mask</span><span class="p">,</span>
                <span class="n">mask</span><span class="o">=</span><span class="n">mask</span>
            <span class="p">)</span>
            <span class="n">d_loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">[</span><span class="s">"KLDivLoss"</span><span class="p">](</span><span class="n">soft_pred</span><span class="p">.</span><span class="n">log</span><span class="p">(),</span> <span class="n">soft_target</span><span class="p">)</span>  <span class="c1"># nn.KLDIVLoss
</span>            <span class="n">s_loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">[</span><span class="s">"CrossEntropyLoss"</span><span class="p">](</span><span class="n">s_logit</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">vocab_size</span><span class="p">),</span> <span class="n">labels</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>  <span class="c1"># nn.CrossEntropyLoss
</span>            <span class="n">c_loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">[</span><span class="s">"CosineEmbeddingLoss"</span><span class="p">](</span><span class="n">s_hidden_state</span><span class="p">,</span> <span class="n">t_hidden_state</span><span class="p">,</span> <span class="n">c_labels</span><span class="p">)</span>  <span class="c1"># nn.CosineEmbeddingLoss
</span>            <span class="n">loss</span> <span class="o">=</span> <span class="n">d_loss</span><span class="o">*</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">alpha_distillation</span> <span class="o">+</span> <span class="n">s_loss</span><span class="o">*</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">alpha_student</span> <span class="o">+</span> <span class="n">c_loss</span><span class="o">*</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">alpha_cosine</span>  <span class="c1"># linear combination loss
</span>
        <span class="n">scaler</span><span class="p">.</span><span class="n">scale</span><span class="p">(</span><span class="n">loss</span><span class="p">).</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">scaler</span><span class="p">.</span><span class="n">step</span><span class="p">(</span><span class="n">optimizer</span><span class="p">)</span>
        <span class="n">scaler</span><span class="p">.</span><span class="n">update</span><span class="p">()</span>
        <span class="n">scheduler</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>
</code></pre></div></div>

<h4 id="knowledge-distillation-model"><code class="language-plaintext highlighter-rouge">ğŸ‘©â€ğŸ’»Â Knowledge Distillation Model</code></h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">DistillationKnowledge</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">AbstractTask</span><span class="p">):</span>
    <span class="s">""" Custom Task Module for Knowledge Distillation by DistilBERT Style Architecture
    DistilBERT Style Architecture is Teacher-Student Framework for Knowledge Distillation,

    And then they have 3 objective functions:
        1) distillation loss, calculated by soft targets &amp; soft predictions
            (nn.KLDIVLoss(reduction='batchmean'))
        2) student loss, calculated by hard targets &amp; hard predictions
            (nn.CrossEntropyLoss(reduction='mean')), same as pure MLM Loss
        3) cosine similarity loss, calculated by student &amp; teacher logit similarity
            (nn.CosineEmbeddingLoss(reduction='mean')), similar as contrastive loss

    References:
        https://arxiv.org/pdf/1910.01108.pdf
        https://github.com/huggingface/transformers/blob/main/examples/research_projects/distillation/distiller.py
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">cfg</span><span class="p">:</span> <span class="n">CFG</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">DistillationKnowledge</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span> <span class="o">=</span> <span class="n">CFG</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">DistilBERT</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">,</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">select_model</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">_init_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">teacher_load_pretrained</span><span class="p">:</span>  <span class="c1"># for teacher model
</span>            <span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">teacher</span><span class="p">.</span><span class="n">load_state_dict</span><span class="p">(</span>
                <span class="n">torch</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="n">cfg</span><span class="p">.</span><span class="n">checkpoint_dir</span> <span class="o">+</span> <span class="n">cfg</span><span class="p">.</span><span class="n">teacher_state_dict</span><span class="p">),</span>
                <span class="n">strict</span><span class="o">=</span><span class="bp">False</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">student_load_pretrained</span><span class="p">:</span>  <span class="c1"># for student model
</span>            <span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">student</span><span class="p">.</span><span class="n">load_state_dict</span><span class="p">(</span>
                <span class="n">torch</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="n">cfg</span><span class="p">.</span><span class="n">checkpoint_dir</span> <span class="o">+</span> <span class="n">cfg</span><span class="p">.</span><span class="n">student_state_dict</span><span class="p">),</span>
                <span class="n">strict</span><span class="o">=</span><span class="bp">True</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">freeze</span><span class="p">:</span>
            <span class="n">freeze</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">teacher</span><span class="p">)</span>
            <span class="n">freeze</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">mlm_head</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">gradient_checkpoint</span><span class="p">:</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">gradient_checkpointing_enable</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">teacher_fw</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">inputs</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">padding_mask</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">mask</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
        <span class="n">is_valid</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="bp">False</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
        <span class="s">""" teacher forward pass to make soft target, last_hidden_state for distillation loss """</span>
        <span class="c1"># 1) make soft target
</span>        <span class="n">temperature</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="k">if</span> <span class="n">is_valid</span> <span class="k">else</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">temperature</span>
        <span class="n">last_hidden_state</span><span class="p">,</span> <span class="n">t_logit</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">teacher_fw</span><span class="p">(</span>
            <span class="n">inputs</span><span class="p">,</span>
            <span class="n">padding_mask</span><span class="p">,</span>
            <span class="n">attention_mask</span>
        <span class="p">)</span>
        <span class="n">last_hidden_state</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">masked_select</span><span class="p">(</span><span class="n">last_hidden_state</span><span class="p">,</span> <span class="o">~</span><span class="n">mask</span><span class="p">)</span>  <span class="c1"># for inverse select
</span>        <span class="n">last_hidden_state</span> <span class="o">=</span> <span class="n">last_hidden_state</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">dim_model</span><span class="p">)</span>  <span class="c1"># flatten last_hidden_state
</span>        <span class="n">soft_target</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="n">softmax</span><span class="p">(</span>
            <span class="n">t_logit</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">vocab_size</span><span class="p">)</span> <span class="o">/</span> <span class="n">temperature</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span>  <span class="c1"># flatten softmax distribution
</span>            <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span>
        <span class="p">)</span>  <span class="c1"># [bs* seq, vocab_size]
</span>        <span class="k">return</span> <span class="n">last_hidden_state</span><span class="p">,</span> <span class="n">soft_target</span>

    <span class="k">def</span> <span class="nf">student_fw</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">inputs</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">padding_mask</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">mask</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
        <span class="n">is_valid</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="bp">False</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
        <span class="s">""" student forward pass to make soft prediction, hard prediction for student loss """</span>
        <span class="n">temperature</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="k">if</span> <span class="n">is_valid</span> <span class="k">else</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">temperature</span>
        <span class="n">last_hidden_state</span><span class="p">,</span> <span class="n">s_logit</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">teacher_fw</span><span class="p">(</span>
            <span class="n">inputs</span><span class="p">,</span>
            <span class="n">padding_mask</span><span class="p">,</span>
            <span class="n">attention_mask</span>
        <span class="p">)</span>
        <span class="n">last_hidden_state</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">masked_select</span><span class="p">(</span><span class="n">last_hidden_state</span><span class="p">,</span> <span class="o">~</span><span class="n">mask</span><span class="p">)</span>  <span class="c1"># for inverse select
</span>        <span class="n">last_hidden_state</span> <span class="o">=</span> <span class="n">last_hidden_state</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">dim_model</span><span class="p">)</span>  <span class="c1"># flatten last_hidden_state
</span>        <span class="n">c_labels</span> <span class="o">=</span> <span class="n">last_hidden_state</span><span class="p">.</span><span class="n">new</span><span class="p">(</span><span class="n">last_hidden_state</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)).</span><span class="n">fill_</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">soft_pred</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="n">softmax</span><span class="p">(</span>
            <span class="n">s_logit</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">vocab_size</span><span class="p">)</span> <span class="o">/</span> <span class="n">temperature</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span>  <span class="c1"># flatten softmax distribution
</span>            <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">last_hidden_state</span><span class="p">,</span> <span class="n">s_logit</span><span class="p">,</span> <span class="n">soft_pred</span><span class="p">,</span> <span class="n">c_labels</span>
</code></pre></div></div>

<h4 id="distilbert-model"><code class="language-plaintext highlighter-rouge">ğŸ‘©â€ğŸ’»Â DistilBERT Model</code></h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">DistilBERT</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">AbstractModel</span><span class="p">):</span>
    <span class="s">""" Main class for DistilBERT Style Model, Teacher-Student Framework
    for Knowledge Distillation aim to lighter Large Scale LLM model. This model have 3 objective functions:

        1) distillation loss, calculated by soft targets &amp; soft predictions
            (nn.KLDIVLoss(reduction='batchmean'))

        2) student loss, calculated by hard targets &amp; hard predictions
            (nn.CrossEntropyLoss(reduction='mean')), same as pure MLM Loss

        3) cosine similarity loss, calculated by student &amp; teacher logit similarity
            (nn.CosineEmbeddingLoss(reduction='mean')), similar as contrastive loss

    soft targets &amp; soft predictions are meaning that logit are passed through softmax function applied with temperature T
    temperature T aim to flatten softmax layer distribution for making "Dark Knowledge" from teacher model

    hard targets &amp; hard predictions are meaning that logit are passed through softmax function without temperature T
    hard targets are same as just simple labels from MLM Collator returns for calculating cross entropy loss

    cosine similarity loss is calculated by cosine similarity between student &amp; teacher
    in official repo, they mask padding tokens for calculating cosine similarity, target for this task is 1
    cosine similarity is calculated by nn.CosineSimilarity() function, values are range to [-1, 1]

    you can select any other backbone model architecture for Teacher &amp; Student Model for knowledge distillation
    but, in original paper, BERT is used for Teacher Model &amp; Student
    and you must select pretrained model for Teacher Model, because Teacher Model is used for knowledge distillation,
    which is containing pretrained mlm head

    Do not pass gradient backward to teacher model!!
    (teacher model must be frozen or register_buffer to model or use no_grad() context manager)

    Args:
        cfg: configuration.CFG
        model_func: make model instance in runtime from config.json

    References:
        https://arxiv.org/pdf/1910.01108.pdf
        https://github.com/huggingface/transformers/blob/main/examples/research_projects/distillation/distiller.py
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">cfg</span><span class="p">:</span> <span class="n">CFG</span><span class="p">,</span> <span class="n">model_func</span><span class="p">:</span> <span class="n">Callable</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">DistilBERT</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span> <span class="o">=</span> <span class="n">cfg</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">teacher</span> <span class="o">=</span> <span class="n">model_func</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">teacher_num_layers</span><span class="p">)</span>  <span class="c1"># must be loading pretrained model containing mlm head
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">mlm_head</span> <span class="o">=</span> <span class="n">MLMHead</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">)</span>  <span class="c1"># must be loading pretrained model's mlm head
</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">student</span> <span class="o">=</span> <span class="n">model_func</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">student_num_layers</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">s_mlm_head</span> <span class="o">=</span> <span class="n">MLMHead</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">teacher_fw</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">inputs</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">padding_mask</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
        <span class="s">""" forward pass for teacher model
        """</span>
        <span class="n">last_hidden_state</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">teacher</span><span class="p">(</span>
            <span class="n">inputs</span><span class="p">,</span>
            <span class="n">padding_mask</span><span class="p">,</span>
            <span class="n">attention_mask</span>
        <span class="p">)</span>
        <span class="n">t_logit</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">mlm_head</span><span class="p">(</span><span class="n">last_hidden_state</span><span class="p">)</span>  <span class="c1"># hard logit =&gt; to make soft logit
</span>        <span class="k">return</span> <span class="n">last_hidden_state</span><span class="p">,</span> <span class="n">t_logit</span>

    <span class="k">def</span> <span class="nf">student_fw</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">inputs</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">padding_mask</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="bp">None</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
        <span class="s">""" forward pass for student model
        """</span>
        <span class="n">last_hidden_state</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">student</span><span class="p">(</span>
            <span class="n">inputs</span><span class="p">,</span>
            <span class="n">padding_mask</span><span class="p">,</span>
            <span class="n">attention_mask</span>
        <span class="p">)</span>
        <span class="n">s_logit</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">s_mlm_head</span><span class="p">(</span><span class="n">last_hidden_state</span><span class="p">)</span>  <span class="c1"># hard logit =&gt; to make soft logit
</span>        <span class="k">return</span> <span class="n">last_hidden_state</span><span class="p">,</span> <span class="n">s_logit</span>
</code></pre></div></div>]]></content><author><name>qcqced</name><email>qcqced123@gmail.com</email></author><category term="NLP" /><category term="Natural Language Process" /><category term="Roformer" /><category term="Linear-Attention" /><category term="Self-Attention" /><category term="Pytorch" /><category term="Transformation Matrix" /><category term="Complex Space" /><summary type="html"><![CDATA[Roformer Official Paper Review with Pytorch Implementation]]></summary></entry><entry><title type="html">ğŸ‘©â€ğŸ’»ğŸ„ [baekjoon] 1987ë²ˆ: ì•ŒíŒŒë²³</title><link href="http://localhost:4000/ps/baekjoon-1987" rel="alternate" type="text/html" title="ğŸ‘©â€ğŸ’»ğŸ„ [baekjoon] 1987ë²ˆ: ì•ŒíŒŒë²³" /><published>2024-01-30T00:00:00+09:00</published><updated>2024-01-31T02:00:00+09:00</updated><id>http://localhost:4000/ps/baekjoon_1987</id><content type="html" xml:base="http://localhost:4000/ps/baekjoon-1987"><![CDATA[<h3 id="ï¸solution"><strong><code class="language-plaintext highlighter-rouge">ğŸ–ï¸Â solution</code></strong></h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">sys</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">List</span>

<span class="k">def</span> <span class="nf">backtracking</span><span class="p">(</span><span class="n">y</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">count</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">visit</span><span class="p">:</span> <span class="n">List</span><span class="p">,</span> <span class="n">graph</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">]):</span>
    <span class="k">global</span> <span class="n">result</span>
    <span class="n">visit</span><span class="p">[</span><span class="nb">ord</span><span class="p">(</span><span class="n">graph</span><span class="p">[</span><span class="n">y</span><span class="p">][</span><span class="n">x</span><span class="p">])</span> <span class="o">-</span> <span class="mi">65</span><span class="p">]</span> <span class="o">=</span> <span class="bp">True</span>
    <span class="n">result</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">count</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">):</span>
        <span class="n">ny</span><span class="p">,</span> <span class="n">nx</span> <span class="o">=</span> <span class="n">dy</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">y</span><span class="p">,</span> <span class="n">dx</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">x</span>
        <span class="k">if</span> <span class="o">-</span><span class="mi">1</span> <span class="o">&lt;</span> <span class="n">ny</span> <span class="o">&lt;</span> <span class="n">r</span> <span class="ow">and</span> <span class="o">-</span><span class="mi">1</span> <span class="o">&lt;</span> <span class="n">nx</span> <span class="o">&lt;</span> <span class="n">c</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">visit</span><span class="p">[</span><span class="nb">ord</span><span class="p">(</span><span class="n">graph</span><span class="p">[</span><span class="n">ny</span><span class="p">][</span><span class="n">nx</span><span class="p">])</span> <span class="o">-</span> <span class="mi">65</span><span class="p">]:</span>
            <span class="n">backtracking</span><span class="p">(</span><span class="n">ny</span><span class="p">,</span> <span class="n">nx</span><span class="p">,</span> <span class="n">count</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">visit</span><span class="p">,</span> <span class="n">graph</span><span class="p">)</span>
            <span class="n">visit</span><span class="p">[</span><span class="nb">ord</span><span class="p">(</span><span class="n">graph</span><span class="p">[</span><span class="n">ny</span><span class="p">][</span><span class="n">nx</span><span class="p">])</span> <span class="o">-</span> <span class="mi">65</span><span class="p">]</span> <span class="o">=</span> <span class="bp">False</span>

<span class="n">r</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="nb">map</span><span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="n">sys</span><span class="p">.</span><span class="n">stdin</span><span class="p">.</span><span class="n">readline</span><span class="p">().</span><span class="n">split</span><span class="p">())</span>

<span class="n">result</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
<span class="n">dy</span><span class="p">,</span> <span class="n">dx</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">grid</span><span class="p">,</span> <span class="n">visited</span> <span class="o">=</span> <span class="p">[</span><span class="nb">list</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="nb">str</span><span class="p">,</span> <span class="n">sys</span><span class="p">.</span><span class="n">stdin</span><span class="p">.</span><span class="n">readline</span><span class="p">().</span><span class="n">rstrip</span><span class="p">()))</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">r</span><span class="p">)],</span> <span class="p">[</span><span class="bp">False</span><span class="p">]</span> <span class="o">*</span> <span class="mi">26</span>
<span class="n">backtracking</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">visited</span><span class="p">,</span> <span class="n">grid</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="nb">max</span><span class="p">(</span><span class="n">result</span><span class="p">))</span>
</code></pre></div></div>

<h3 id="idea"><strong><code class="language-plaintext highlighter-rouge">ğŸ’¡Â idea</code></strong></h3>

<ul>
  <li><strong>Back Tracking</strong></li>
  <li><strong>1) ë°©ë¬¸ ê¸°ë¡ ë°°ì—´ ë³€ê²½</strong>
    <ul>
      <li><strong>ì¡°ê±´ ì¤‘ì—ì„œ ê²½ë¡œì— ì•ŒíŒŒë²³ ì¤‘ë³µì´ ë¶ˆê°€ëŠ¥í•˜ë‹¤ëŠ” ì  ì´ìš©</strong></li>
      <li><strong>ì „ì²´ ê²©ì ì‚¬ì´ì¦ˆì™€ ë™ì¼í•œ ë°°ì—´ ëŒ€ì‹  ì•ŒíŒŒë²³ ì‚¬ì´ì¦ˆ(26)ë§Œ ì„ ì–¸</strong></li>
    </ul>
  </li>
</ul>

<p>ì¼ë°˜ì ì¸ ë°±íŠ¸ë˜í‚¹ ë¬¸ì œë¼ê³  ë³¼ ìˆ˜ ìˆë‹¤. í•˜ì§€ë§Œ íŒŒì´ì¬ìœ¼ë¡œ í•´ê²°í•˜ë ¤ëŠ” ê²½ìš° ì‹œê°„, ë©”ëª¨ë¦¬ ì œí•œ ë•Œë¬¸ì— ë¹¡ì„¼ ì½”ë“œ ìµœì í™”ê°€ í•„ìš”í•˜ë‹¤. ê²©ì ë¬¸ì œë¼ì„œ <code class="language-plaintext highlighter-rouge">bfs</code> ì„ íƒë„ ê°€ëŠ¥í•œë° ê·¸ë ‡ë‹¤ë©´ <code class="language-plaintext highlighter-rouge">python3</code>ë¡œë„ í•´ê²°ê°€ëŠ¥í•˜ë‹¤. í•œí¸, ì¼ë°˜ì ì¸ <code class="language-plaintext highlighter-rouge">dfs</code>ë¼ë©´ ë¹¡ì„¼ ìµœì í™”ë¥¼ í†µí•´ <code class="language-plaintext highlighter-rouge">pypy3</code>ìœ¼ë¡œë§Œ í†µê³¼ ê°€ëŠ¥í•˜ë‹¤.</p>

<p>ë¬¸ì œë¥¼ ë¦¬ë·°í•˜ë˜ ë„ì¤‘ ì¼ë°˜ì ì¸ <code class="language-plaintext highlighter-rouge">dfs</code> ë°±íŠ¸ë˜í‚¹ ë°©ì‹ì˜ ë¹„íš¨ìœ¨ì„±ì— ëŒ€í•´ ê³ ì°°í•´ë´¤ë‹¤. ì•„ë˜ì™€ ê°™ì€ ì…ë ¥ì´ ìˆë‹¤.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">IEFCJ</span>
<span class="n">FHFKC</span>
<span class="n">FFALF</span>
<span class="n">HFGCF</span>
<span class="n">HMCHH</span>
</code></pre></div></div>

<p>ì¼ë°˜ì ì¸ ë°±íŠ¸ë˜í‚¹ ì•Œê³ ë¦¬ì¦˜ì´ íƒìƒ‰í•˜ëŠ” ê³¼ì •ì„ ìƒê°í•´ë³´ì. ë¹¨ê°„ìƒ‰ìœ¼ë¡œ ì¹ í•´ì§„ ê¸€ìë¥¼ <code class="language-plaintext highlighter-rouge">IFHE</code> ìˆœì„œë¡œ íƒìƒ‰í–ˆë‹¤ë©´, ë‹¤ìŒì€ <code class="language-plaintext highlighter-rouge">F</code>ë¥¼ íƒìƒ‰í•´ ë°©ë¬¸í•´ë„ ë˜ëŠ”ì§€ ì—¬ë¶€ë¥¼ íŒì •í•  ê²ƒì´ë‹¤. ì´ë¯¸ <code class="language-plaintext highlighter-rouge">F</code>ëŠ” ë°©ë¬¸í–ˆê¸° ë•Œë¬¸ì— ì•„ë§ˆë„ ìŠ¤íƒ í”„ë ˆì„ í• ë‹¹ì„ ì·¨ì†Œí•˜ë©´ì„œ, ê²°êµ­ì—ëŠ” <code class="language-plaintext highlighter-rouge">I</code>ê¹Œì§€ ë˜ëŒì•„ ê°ˆ ê²ƒì´ë‹¤.</p>

<p>ê·¸ë¦¬ê³  ë‹¤ì‹œ ì˜¤ë¥¸ìª½ì— ìˆëŠ” <code class="language-plaintext highlighter-rouge">E</code>ë¥¼  ë°©ë¬¸í•œ ë’¤, <code class="language-plaintext highlighter-rouge">FCK</code> ìˆœì„œë¡œ ë°©ë¬¸í•˜ê²Œ ë  ê²ƒì´ë‹¤. ì´ ë•Œ ë“¤ê²Œ ë˜ëŠ” ì˜ë¬¸ì€ ë°”ë¡œ ì´ë ‡ë‹¤. êµ³ì´ <code class="language-plaintext highlighter-rouge">I</code>ê¹Œì§€ ë˜ëŒì•„ê°”ë‹¤ê°€ íƒìƒ‰í•´ì•¼ í• ê¹Œ?? ì´ë¯¸ <code class="language-plaintext highlighter-rouge">IE</code> ëŠ” íƒìƒ‰ì´ ê°€ëŠ¥í•œ ê²½ë¡œë¼ëŠ” ê²ƒì„ ìš°ë¦¬ëŠ” ì¶©ë¶„íˆ ì•Œ ìˆ˜ ìˆë‹¤. ë”°ë¼ì„œ <code class="language-plaintext highlighter-rouge">DP Tabulation</code> ê°œë…ì„ ì°¨ìš©í•œë‹¤ë©´ í›¨ì”¬ ë¹ ë¥´ê²Œ í’€ì´ê°€ ê°€ëŠ¥í•  ê²ƒì´ë‹¤.</p>

<p>ê²½ë¡œì˜ ìœ ì¼ì„±ì„ ë³´ì¥í•˜ë©´ì„œ ìˆ˜ì • ê°€ëŠ¥í•œ ìë£Œêµ¬ì¡°ê°€ í•„ìš”í•˜ê¸° ë•Œë¬¸ì— ë°°ì—´ ëŒ€ì‹  ì„¸íŠ¸ë¥¼ ì‚¬ìš©í•´ë³´ì. ì„¸íŠ¸ì—ëŠ” í˜„ì¬ê¹Œì§€ì˜ ê²½ë¡œ ê·¸ë¦¬ê³  í•´ë‹¹ ê²½ë¡œì˜ ë§ˆì§€ë§‰ ì¸ë±ìŠ¤ë¥¼ ì €ì¥í•´ì¤˜ì•¼ í•œë‹¤. ê°™ì€ ê²½ë¡œë¼ê³  í•  ì§€ë¼ë„ ì„œë¡œ ë‹¤ë¥¸ ì¸ë±ìŠ¤ì— ì˜í•´ ë§Œë“¤ì–´ì¡Œì„ ê°€ëŠ¥ì„±ì´ ìˆê¸° ë•Œë¬¸ì´ë‹¤. ì´ë ‡ê²Œ ì„¸íŠ¸ë¥¼ êµ¬ì„±í•œ ë’¤, í•˜ë‚˜ì”© popí•´ì„œ ê²½ë¡œë¥¼ ì–»ì–´ë‚¸ë‹¤. ê·¸ ë‹¤ìŒ í•´ë‹¹ ê²½ë¡œë¡œë¶€í„° íŒŒìƒë˜ëŠ” ì—¬ëŸ¬ ì ì¬ì  ê²½ë¡œë“¤ì„ ëª¨ë‘ ê²€ì‚¬í•´ ê²½ë¡œê°€ ë§Œë“¤ì–´ì§ˆ ìˆ˜ ìˆëŠ”ì§€ ì—¬ë¶€ë¥¼ íŒì •í•˜ë©´ ëœë‹¤. ì½”ë“œëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">sys</span>

<span class="k">def</span> <span class="nf">dfs</span><span class="p">(</span><span class="n">y</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
    <span class="n">dp</span><span class="p">,</span> <span class="n">result</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(),</span> <span class="mi">0</span>
    <span class="n">dp</span><span class="p">.</span><span class="n">add</span><span class="p">((</span><span class="n">y</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">grid</span><span class="p">[</span><span class="n">y</span><span class="p">][</span><span class="n">x</span><span class="p">]))</span>
    <span class="k">while</span> <span class="n">dp</span><span class="p">:</span>
        <span class="n">vy</span><span class="p">,</span> <span class="n">vx</span><span class="p">,</span> <span class="n">path</span> <span class="o">=</span> <span class="n">dp</span><span class="p">.</span><span class="n">pop</span><span class="p">()</span>
        <span class="n">result</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">result</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">path</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">result</span> <span class="o">==</span> <span class="mi">26</span><span class="p">:</span>
            <span class="k">return</span> <span class="mi">26</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">):</span>
            <span class="n">ny</span><span class="p">,</span> <span class="n">nx</span> <span class="o">=</span> <span class="n">dy</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">vy</span><span class="p">,</span> <span class="n">dx</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">vx</span>
            <span class="k">if</span> <span class="o">-</span><span class="mi">1</span> <span class="o">&lt;</span> <span class="n">ny</span> <span class="o">&lt;</span> <span class="n">r</span> <span class="ow">and</span> <span class="o">-</span><span class="mi">1</span> <span class="o">&lt;</span> <span class="n">nx</span> <span class="o">&lt;</span> <span class="n">c</span> <span class="ow">and</span> <span class="n">grid</span><span class="p">[</span><span class="n">ny</span><span class="p">][</span><span class="n">nx</span><span class="p">]</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">path</span><span class="p">:</span>
                <span class="n">dp</span><span class="p">.</span><span class="n">add</span><span class="p">((</span><span class="n">ny</span><span class="p">,</span> <span class="n">nx</span><span class="p">,</span> <span class="n">grid</span><span class="p">[</span><span class="n">ny</span><span class="p">][</span><span class="n">nx</span><span class="p">]</span> <span class="o">+</span> <span class="n">path</span><span class="p">))</span>
                
    <span class="k">return</span> <span class="n">result</span>

<span class="n">r</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="nb">map</span><span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="n">sys</span><span class="p">.</span><span class="n">stdin</span><span class="p">.</span><span class="n">readline</span><span class="p">().</span><span class="n">split</span><span class="p">())</span>
<span class="n">dy</span><span class="p">,</span> <span class="n">dx</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">grid</span> <span class="o">=</span> <span class="p">[</span><span class="nb">list</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="nb">str</span><span class="p">,</span> <span class="n">sys</span><span class="p">.</span><span class="n">stdin</span><span class="p">.</span><span class="n">readline</span><span class="p">().</span><span class="n">rstrip</span><span class="p">()))</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">r</span><span class="p">)]</span>
<span class="k">print</span><span class="p">(</span><span class="n">dfs</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
</code></pre></div></div>

<p align="center">
<img src="/assets/images/ps/after.png" alt="Common BackTracking" class="align-center image-caption" width="100%&quot;, height=&quot;100%" />
<strong><em>Common BackTracking</em></strong>
</p>

<p align="center">
<img src="/assets/images/ps/before.png" alt="DP Tabulation BackTracking" class="align-center image-caption" width="100%&quot;, height=&quot;100%" />
<strong><em>DP Tabulation BackTracking</em></strong>
</p>

<p>ìœ„ì—ëŠ” ê°œì„ ì´ì „ ê²°ê³¼ê³  ì•„ë˜ëŠ” ê°œì„  ì´í›„ ê²°ê³¼ë‹¤. ë¹„ì•½ì ì¸ ì†ë„ ìƒìŠ¹í•˜ëŠ” ë™ì‹œì— ë©”ëª¨ë¦¬ ì—­ì‹œ 3ë°°ë‚˜ ëœ ì‚¬ìš©í•˜ëŠ” ëª¨ìŠµì´ë‹¤. ì„¸íŠ¸ì— ìˆëŠ” ìœ ë‹ˆí¬í•œ ê²½ë¡œë“¤ì„ í•˜ë‚˜ì”© êº¼ë‚´ëŠ” ë°©ì‹ì„ ì„ íƒí–ˆê¸° ë•Œë¬¸ì— ì•Œê³ ë¦¬ì¦˜ ì„±ëŠ¥ì´ ì‹œë“œì— ì˜í–¥(<code class="language-plaintext highlighter-rouge">set.pop()</code>ì€ ëœë¤ìœ¼ë¡œ ì›ì†Œ ì„ íƒ)ì„ ë°›ëŠ”ë‹¤ëŠ” ì ë§Œ ê°ì•ˆí•œë‹¤ë©´ ë§¤ìš° ì¢‹ì€ í’€ì´ë¼ê³  ìƒê°í•œë‹¤.</p>]]></content><author><name>qcqced</name><email>qcqced123@gmail.com</email></author><category term="Problem Solving" /><category term="Python" /><category term="Codeing Test" /><category term="Algorithm" /><category term="Baekjoon" /><category term="Graph" /><category term="DFS" /><category term="BackTracking" /><summary type="html"><![CDATA[ë°±ì¤€ 1987ë²ˆ: ì•ŒíŒŒë²³]]></summary></entry><entry><title type="html">ğŸ”¥Â Pytorch Tensor Indexing ìì£¼ ì‚¬ìš©í•˜ëŠ” ë©”ì„œë“œ ëª¨ìŒì§‘</title><link href="http://localhost:4000/framework-library/torch-indexing-function" rel="alternate" type="text/html" title="ğŸ”¥Â Pytorch Tensor Indexing ìì£¼ ì‚¬ìš©í•˜ëŠ” ë©”ì„œë“œ ëª¨ìŒì§‘" /><published>2024-01-09T00:00:00+09:00</published><updated>2024-01-10T02:00:00+09:00</updated><id>http://localhost:4000/framework-library/Pytorch-Tensor-Indexing-Function</id><content type="html" xml:base="http://localhost:4000/framework-library/torch-indexing-function"><![CDATA[<p>íŒŒì´í† ì¹˜ì—ì„œ í•„ìê°€ ìì£¼ ì‚¬ìš©í•˜ëŠ” í…ì„œ ì¸ë±ì‹± ê´€ë ¨ ë©”ì„œë“œì˜ ì‚¬ìš©ë²• ë° ì‚¬ìš© ì˜ˆì‹œë¥¼ í•œë°©ì— ì •ë¦¬í•œ í¬ìŠ¤íŠ¸ë‹¤. ë©”ì„œë“œ í•˜ë‚˜ë‹¹ í•˜ë‚˜ì˜ í¬ìŠ¤íŠ¸ë¡œ ë§Œë“¤ê¸°ì—ëŠ” ë„ˆë¬´ ê¸¸ì´ê°€ ì§§ë‹¤ ìƒê°í•´ í•œ í˜ì´ì§€ì— ëª¨ë‘ ë„£ê²Œ ë˜ì—ˆë‹¤. ì§€ì†ì ìœ¼ë¡œ ì—…ë°ì´íŠ¸ ë  ì˜ˆì •ì´ë‹¤. ë˜í•œ í…ì„œ ì¸ë±ì‹± ë§ê³ ë„ ë‹¤ë¥¸ ì£¼ì œë¡œë„ ê´€ë ¨ ë©”ì„œë“œë¥¼ ì •ë¦¬í•´ ì˜¬ë¦´ ì˜ˆì •ì´ë‹ˆ ë§ì€ ê´€ì‹¬ ë¶€íƒë“œë¦°ë‹¤.</p>

<h3 id="torchargmax"><code class="language-plaintext highlighter-rouge">ğŸ”Â torch.argmax</code></h3>

<p>ì…ë ¥ í…ì„œì—ì„œ ê°€ì¥ í° ê°’ì„ ê°–ê³  ìˆëŠ” ì›ì†Œì˜ ì¸ë±ìŠ¤ë¥¼ ë°˜í™˜í•œë‹¤. ìµœëŒ€ê°’ì„ ì°¾ì„ ì°¨ì›ì„ ì§€ì •í•´ì¤„ ìˆ˜ ìˆë‹¤. ì•„ë˜ ì˜ˆì‹œ ì½”ë“œë¥¼ í™•ì¸í•´ë³´ì.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># torch.argmax params
</span><span class="n">torch</span><span class="p">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

<span class="c1"># torch.argmax example 1
</span><span class="n">test</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">29</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">45</span><span class="p">,</span><span class="mi">22</span><span class="p">,</span><span class="mi">3</span><span class="p">])</span>
<span class="n">torch</span><span class="p">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">test</span><span class="p">)</span>
<span class="n">torch</span><span class="p">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">test2</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="o">&lt;</span><span class="n">Result</span><span class="o">&gt;</span>
<span class="n">tensor</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>

<span class="c1"># torch.argmax example 2
</span><span class="n">test</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
                     <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">]])</span>

<span class="n">torch</span><span class="p">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">test</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="o">&lt;</span><span class="n">Result</span><span class="o">&gt;</span>
<span class="n">tensor</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>

<span class="c1"># torch.argmax example 3
</span><span class="n">test</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
                     <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">]])</span>

<span class="n">torch</span><span class="p">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">test</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">tensor</span><span class="p">([[</span><span class="mi">0</span><span class="p">],</span>
        <span class="p">[</span><span class="mi">2</span><span class="p">]])</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">dim</code> ë§¤ê°œë³€ìˆ˜ì— ì›í•˜ëŠ” ì°¨ì›ì„ ì…ë ¥í•˜ë©´ í•´ë‹¹ ì°¨ì› ë·°ì—ì„œ ê°€ì¥ í° ì›ì†Œë¥¼ ì°¾ì•„ ì¸ë±ìŠ¤ ê°’ì„ ë°˜í™˜í•´ì¤„ ê²ƒì´ë‹¤. ì´ ë•Œ <code class="language-plaintext highlighter-rouge">keepdim=True</code> ë¡œ ì„¤ì •í•œë‹¤ë©´ ì…ë ¥ ì°¨ì›ì—ì„œ ê°€ì¥ í° ì›ì†Œì˜ ì¸ë±ìŠ¤ë¥¼ ë°˜í™˜í•˜ë˜ ì›ë³¸ í…ì„œì˜ ì°¨ì›ê³¼ ë™ì¼í•œ í˜•íƒœë¡œ ì¶œë ¥í•´ì¤€ë‹¤. <code class="language-plaintext highlighter-rouge">example 2</code> ì˜ ê²½ìš° <code class="language-plaintext highlighter-rouge">dim=0</code> ë¼ì„œ í–‰ì´ ëˆ„ì ëœ ë°©í–¥ìœ¼ë¡œ í…ì„œë¥¼ ë°”ë¼ë´ì•¼ í•œë‹¤. í–‰ì´ ëˆ„ì ëœ ë°©í–¥ìœ¼ë¡œ í…ì„œë¥¼ ë³´ê²Œ ë˜ë©´ <code class="language-plaintext highlighter-rouge">tensor([[0, 1, 1]])</code>ì´ ëœë‹¤.</p>

<h3 id="torchstack"><code class="language-plaintext highlighter-rouge">ğŸ“šÂ torch.stack</code></h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="s">"""
torch.stack
Args:
	tensors(sequence of Tensors): í…ì„œê°€ ë‹´ê¸´ íŒŒì´ì¬ ì‹œí€€ìŠ¤ ê°ì²´
	dim(int): ì¶”ê°€í•  ì°¨ì› ë°©í–¥ì„ ì„¸íŒ…, ê¸°ë³¸ê°’ì€ 0
"""</span>
<span class="n">torch</span><span class="p">.</span><span class="n">stack</span><span class="p">(</span><span class="n">tensors</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</code></pre></div></div>

<p>ë§¤ê°œë³€ìˆ˜ë¡œ ì£¼ì–´ì§„ íŒŒì´ì¬ ì‹œí€€ìŠ¤ ê°ì²´(ë¦¬ìŠ¤íŠ¸, íŠœí”Œ)ë¥¼ ì‚¬ìš©ìê°€ ì§€ì •í•œ ìƒˆë¡œìš´ ì°¨ì›ì— ìŒ“ëŠ” ê¸°ëŠ¥ì„ í•œë‹¤. ë§¤ê°œë³€ìˆ˜ <code class="language-plaintext highlighter-rouge">tensors</code> ëŠ” í…ì„œê°€ ë‹´ê¸´ íŒŒì´ì¬ì˜ ì‹œí€€ìŠ¤ ê°ì²´ë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ëŠ”ë‹¤. <code class="language-plaintext highlighter-rouge">dim</code> ì€ ì‚¬ìš©ìê°€ í…ì„œ ì ì¬ë¥¼ í•˜ê³  ì‹¶ì€ ìƒˆë¡œìš´ ì°¨ì›ì„ ì§€ì •í•´ì£¼ë©´ ëœë‹¤. ê¸°ë³¸ê°’ì€ 0ì°¨ì›ìœ¼ë¡œ ì§€ì • ë˜ì–´ìˆìœ¼ë©°, í…ì„œì˜ ë§¨ ì•ì°¨ì›ì´ ìƒˆë¡­ê²Œ ìƒê¸°ê²Œ ëœë‹¤. <code class="language-plaintext highlighter-rouge">torch.stack</code> ì€ ê¸°ê³„í•™ìŠµ, íŠ¹íˆ ë”¥ëŸ¬ë‹ì—ì„œ ì •ë§ ìì£¼ ì‚¬ìš©ë˜ê¸° ë•Œë¬¸ì— ì‚¬ìš©ë²• ë° ì‚¬ìš©ìƒí™©ì„ ìµí˜€ë‘ë©´ ë„ì›€ì´ ëœë‹¤. ì˜ˆì‹œë¥¼ í†µí•´ í•´ë‹¹ ë©”ì„œë“œë¥¼ ì–´ë–¤ ìƒí™©ì—ì„œ ì–´ë–»ê²Œ ì‚¬ìš©í•˜ëŠ”ì§€ ì•Œì•„ë³´ì.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="s">""" torch.stack example """</span>

<span class="k">class</span> <span class="nc">Projector</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="s">"""
    Making projection matrix(Q, K, V) for each attention head
    When you call this class, it returns projection matrix of each attention head
    For example, if you call this class with 8 heads, it returns 8 set of projection matrices (Q, K, V)
    Args:
        num_heads: number of heads in MHA, default 8
        dim_head: dimension of each attention head, default 64
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span> <span class="n">dim_head</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">64</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Projector</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span> <span class="o">=</span> <span class="n">num_heads</span> <span class="o">*</span> <span class="n">dim_head</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dim_head</span> <span class="o">=</span> <span class="n">dim_head</span>

    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">fc_q</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">dim_head</span><span class="p">)</span>
        <span class="n">fc_k</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">dim_head</span><span class="p">)</span>
        <span class="n">fc_v</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">dim_head</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">fc_q</span><span class="p">,</span> <span class="n">fc_k</span><span class="p">,</span> <span class="n">fc_v</span>

<span class="n">num_heads</span> <span class="o">=</span> <span class="mi">8</span>
<span class="n">dim_head</span> <span class="o">=</span> <span class="mi">64</span>
<span class="n">projector</span> <span class="o">=</span> <span class="n">Projector</span><span class="p">(</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">dim_head</span><span class="p">)</span>  <span class="c1"># init instance
</span><span class="n">projector_list</span> <span class="o">=</span> <span class="p">[</span><span class="nb">list</span><span class="p">(</span><span class="n">projector</span><span class="p">())</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_heads</span><span class="p">)]</span>  <span class="c1"># call instance
</span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">512</span><span class="p">)</span> <span class="c1"># x.shape: [Batch_Size, Sequence_Length, Dim_model]
</span><span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">V</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[],</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_heads</span><span class="p">):</span>
    <span class="n">Q</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">projector_list</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">0</span><span class="p">](</span><span class="n">x</span><span class="p">))</span> <span class="c1"># [10, 512, 64]
</span>    <span class="n">K</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">projector_list</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">1</span><span class="p">](</span><span class="n">x</span><span class="p">))</span> <span class="c1"># [10, 512, 64]
</span>	  <span class="n">V</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">projector_list</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">2</span><span class="p">](</span><span class="n">x</span><span class="p">))</span> <span class="c1"># [10, 512, 64]
</span> 
<span class="n">Q</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">stack</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># Q.shape: [10, 8, 512, 64]
</span><span class="n">K</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">stack</span><span class="p">(</span><span class="n">K</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># K.shape: [10, 8, 512, 64]
</span><span class="n">V</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">stack</span><span class="p">(</span><span class="n">V</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># V.shape: [10, 8, 512, 64]
</span></code></pre></div></div>

<p>ìœ„ ì½”ë“œëŠ” <code class="language-plaintext highlighter-rouge">Transformer</code> ì˜ <code class="language-plaintext highlighter-rouge">Multi-Head Attention</code> êµ¬í˜„ì²´ ì¼ë¶€ë¥¼ ë°œì·Œí•´ì˜¨ ê²ƒì´ë‹¤. <code class="language-plaintext highlighter-rouge">Multi-Head Attention</code> ì€ ê°œë³„ ì–´í…ì…˜ í•´ë“œë³„ë¡œ í–‰ë ¬ $Q, K, V$ë¥¼ ê°€ì ¸ì•¼ í•œë‹¤. ë”°ë¼ì„œ ì…ë ¥ ì„ë² ë”©ì„ ê°œë³„ ì–´í…ì…˜ í—¤ë“œì— <code class="language-plaintext highlighter-rouge">Linear Combination</code> í•´ì¤˜ì•¼ í•˜ëŠ”ë° í—¤ë“œ ê°œìˆ˜ê°€ 8ê°œë‚˜ ë˜ê¸° ë•Œë¬¸ì— ê°œë³„ì ìœ¼ë¡œ <code class="language-plaintext highlighter-rouge">Projection Matrix</code> ë¥¼ ì„ ì–¸í•´ì£¼ëŠ” ê²ƒì€ ë§¤ìš° ë¹„íš¨ìœ¨ì ì´ë‹¤. ë”°ë¼ì„œ ê°ì²´  <code class="language-plaintext highlighter-rouge">Projector</code> ì— í–‰ë ¬ $Q, K, V$ì— ëŒ€í•œ <code class="language-plaintext highlighter-rouge">Projection Matrix</code> ë¥¼ ì •ì˜í•´ì¤¬ë‹¤. ì´í›„ í—¤ë“œ ê°œìˆ˜ë§Œí¼ ê°ì²´  <code class="language-plaintext highlighter-rouge">Projector</code> ë¥¼ í˜¸ì¶œí•´ ë¦¬ìŠ¤íŠ¸ì— í•´ë“œë³„ <code class="language-plaintext highlighter-rouge">Projection Matrix</code> ë¥¼ ë‹´ì•„ì¤€ë‹¤. ê·¸ ë‹¤ìŒ <code class="language-plaintext highlighter-rouge">torch.stack</code>ì„ ì‚¬ìš©í•´ <code class="language-plaintext highlighter-rouge">Attention Head</code> ë°©í–¥ì˜ ì°¨ì›ìœ¼ë¡œ ë¦¬ìŠ¤íŠ¸ ë‚´ë¶€ í…ì„œë“¤ì„ ìŒ“ì•„ì£¼ë©´ ëœë‹¤.</p>

<h3 id="torcharange"><code class="language-plaintext highlighter-rouge">ğŸ”¢Â torch.arange</code></h3>

<p>ì‚¬ìš©ìê°€ ì§€ì •í•œ ì‹œì‘ì ë¶€í„° ëì ê¹Œì§€ ì¼ì •í•œ ê°„ê²©ìœ¼ë¡œ í…ì„œë¥¼ ë‚˜ì—´í•œë‹¤. Pythonì˜ ë‚´ì¥ ë©”ì„œë“œ <code class="language-plaintext highlighter-rouge">range</code>ì™€ ë™ì¼í•œ ì—­í• ì„ í•˜ëŠ”ë°, ëŒ€ì‹  í…ì„œ ê·¸ ê²°ê³¼ë¥¼ í…ì„œ êµ¬ì¡°ì²´ë¡œ ë°˜í™˜í•œë‹¤ê³  ìƒê°í•˜ë©´ ë˜ê² ë‹¤.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># torch.arange usage
</span><span class="n">torch</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="n">start</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">end</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
<span class="n">tensor</span><span class="p">([</span> <span class="mi">0</span><span class="p">,</span>  <span class="mi">1</span><span class="p">,</span>  <span class="mi">2</span><span class="p">,</span>  <span class="mi">3</span><span class="p">,</span>  <span class="mi">4</span><span class="p">])</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">tensor</span><span class="p">([</span> <span class="mi">1</span><span class="p">,</span>  <span class="mi">2</span><span class="p">,</span>  <span class="mi">3</span><span class="p">])</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">)</span>
<span class="n">tensor</span><span class="p">([</span> <span class="mf">1.0000</span><span class="p">,</span>  <span class="mf">1.5000</span><span class="p">,</span>  <span class="mf">2.0000</span><span class="p">])</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">step</code> ë§¤ê°œë³€ìˆ˜ë¡œ ì›ì†Œê°„ ê°„ê²© ì¡°ì •ì„ í•  ìˆ˜ ìˆëŠ”ë°, ê¸°ë³¸ì€ 1ë¡œ ì§€ì • ë˜ì–´ ìˆìœ¼ë‹ˆ ì°¸ê³ í•˜ì. í•„ìì˜ ê²½ìš°ì—ëŠ” <code class="language-plaintext highlighter-rouge">nn.Embedding</code>ì˜ ì…ë ¥ í…ì„œë¥¼ ë§Œë“¤ ë•Œ ê°€ì¥ ë§ì´ ì‚¬ìš©í–ˆë‹¤. <code class="language-plaintext highlighter-rouge">nn.Embedding</code> ì˜ ê²½ìš° Inputìœ¼ë¡œ <code class="language-plaintext highlighter-rouge">IntTensor</code>, <code class="language-plaintext highlighter-rouge">LongTensor</code>ë¥¼ ë°›ê²Œ ë˜ì–´ ìˆìœ¼ë‹ˆ ì•Œì•„ë‘ì.</p>

<h3 id="torchrepeat"><code class="language-plaintext highlighter-rouge">ğŸ”Â torch.repeat</code></h3>

<p>ì…ë ¥ê°’ìœ¼ë¡œ ì£¼ì–´ì§„ í…ì„œë¥¼ ì‚¬ìš©ìê°€ ì§€ì •í•œ ë°˜ë³µ íšŸìˆ˜ë§Œí¼ íŠ¹ì • ì°¨ì› ë°©í–¥ìœ¼ë¡œ ëŠ˜ë¦°ë‹¤. ì˜ˆë¥¼ ë“¤ë©´ <code class="language-plaintext highlighter-rouge">[1,2,3] * 3</code>ì˜ ê²°ê³¼ëŠ” <code class="language-plaintext highlighter-rouge">[1, 2, 3, 1, 2, 3, 1, 2, 3]</code> ì¸ë°, ì´ê²ƒì„ ì‚¬ìš©ìê°€ ì§€ì •í•œ ë°˜ë³µ íšŸìˆ˜ë§Œí¼ íŠ¹ì • ì°¨ì›ìœ¼ë¡œ ìˆ˜í–‰í•˜ê² ë‹¤ëŠ” ê²ƒì´ë‹¤. ì•„ë˜ ì‚¬ìš© ì˜ˆì œë¥¼ í™•ì¸í•´ë³´ì.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># torch.repeat example
</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">x</span><span class="p">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">tensor</span><span class="p">([[</span> <span class="mi">1</span><span class="p">,</span>  <span class="mi">2</span><span class="p">,</span>  <span class="mi">3</span><span class="p">,</span>  <span class="mi">1</span><span class="p">,</span>  <span class="mi">2</span><span class="p">,</span>  <span class="mi">3</span><span class="p">],</span>
        <span class="p">[</span> <span class="mi">1</span><span class="p">,</span>  <span class="mi">2</span><span class="p">,</span>  <span class="mi">3</span><span class="p">,</span>  <span class="mi">1</span><span class="p">,</span>  <span class="mi">2</span><span class="p">,</span>  <span class="mi">3</span><span class="p">],</span>
        <span class="p">[</span> <span class="mi">1</span><span class="p">,</span>  <span class="mi">2</span><span class="p">,</span>  <span class="mi">3</span><span class="p">,</span>  <span class="mi">1</span><span class="p">,</span>  <span class="mi">2</span><span class="p">,</span>  <span class="mi">3</span><span class="p">],</span>
        <span class="p">[</span> <span class="mi">1</span><span class="p">,</span>  <span class="mi">2</span><span class="p">,</span>  <span class="mi">3</span><span class="p">,</span>  <span class="mi">1</span><span class="p">,</span>  <span class="mi">2</span><span class="p">,</span>  <span class="mi">3</span><span class="p">]])</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">x</span><span class="p">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">tensor</span><span class="p">([[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
         <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]],</span>

        <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
         <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]],</span>

        <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
         <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]],</span>

        <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
         <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]]])</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">x</span><span class="p">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">).</span><span class="n">size</span>
<span class="n">torch</span><span class="p">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">x</span><span class="p">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">tensor</span><span class="p">([[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
         <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]],</span>

        <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
         <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]],</span>

        <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
         <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]],</span>

        <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
         <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]]])</span>
</code></pre></div></div>

<p>$t$ë¥¼ ì–´ë–¤ í…ì„œ êµ¬ì¡°ì²´ $x$ì˜ ìµœëŒ€ ì°¨ì›ì´ë¼ê³  í–ˆì„ , $x_t$ë¥¼ ê°€ì¥ ì™¼ìª½ì— ë„£ê³  ê°€ì¥ ë‚®ì€ ì°¨ì›ì¸ 0ì°¨ì›ì— ëŒ€í•œ ë°˜ë³µ íšŸìˆ˜ë¥¼ ì˜¤ë¥¸ìª½ ëì— ëŒ€ì…í•´ì„œ ì‚¬ìš©í•˜ë©´ ëœë‹¤. (<code class="language-plaintext highlighter-rouge">torch.repeat(</code>$x_t, x_{t-1}, â€¦ x_2, x_1, x_0$<code class="language-plaintext highlighter-rouge">))</code>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># torch.arange &amp; torch.repeate usage example
</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">pos_x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">num_patches</span> <span class="o">+</span> <span class="mi">1</span><span class="p">).</span><span class="n">repeat</span><span class="p">(</span><span class="n">inputs</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]).</span><span class="n">to</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">pos_x</span><span class="p">.</span><span class="n">shape</span>
<span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">16</span><span class="p">,</span> <span class="mi">1025</span><span class="p">])</span>
</code></pre></div></div>

<p>í•„ìì˜ ê²½ìš°, <code class="language-plaintext highlighter-rouge">position embedding</code>ì˜ ì…ë ¥ì„ ë§Œë“¤ê³  ì‹¶ì„ ë•Œ <code class="language-plaintext highlighter-rouge">torch.arange</code> ì™€ ì—°ê³„í•´ ìì£¼ ì‚¬ìš© í–ˆë˜ ê²ƒ ê°™ë‹¤. ìœ„ ì½”ë“œë¥¼ ì°¸ê³ í•˜ì.</p>

<h3 id="torchclamp"><code class="language-plaintext highlighter-rouge">ğŸ”¬Â torch.clamp</code></h3>

<p>ì…ë ¥ í…ì„œì˜ ì›ì†Œê°’ì„ ì‚¬ìš©ìê°€ ì§€ì •í•œ ìµœëŒ€â€¢ìµœì†Œê°’ ë²”ìœ„ ì´ë‚´ë¡œ ì œí•œí•˜ëŠ” ë©”ì„œë“œë‹¤.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># torch.clamp params
</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">clamp</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="nb">min</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span> <span class="err">â†’</span> <span class="n">Tensor</span>

<span class="c1"># torch.clamp usage example
</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">a</span>
<span class="n">tensor</span><span class="p">([</span><span class="o">-</span><span class="mf">1.7120</span><span class="p">,</span>  <span class="mf">0.1734</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0478</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0922</span><span class="p">])</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="nb">min</span><span class="o">=-</span><span class="mf">0.5</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">tensor</span><span class="p">([</span><span class="o">-</span><span class="mf">0.5000</span><span class="p">,</span>  <span class="mf">0.1734</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0478</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0922</span><span class="p">])</span>
</code></pre></div></div>

<p>ì…ë ¥ëœ í…ì„œì˜ ì›ì†Œë¥¼ ì§€ì • ìµœëŒ€â€¢ìµœì†Œ ì„¤ì •ê°’ê³¼ í•˜ë‚˜ í•˜ë‚˜ ëŒ€ì¡°í•´ì„œ í…ì„œ ë‚´ë¶€ì˜ ëª¨ë“  ì›ì†Œê°€ ì§€ì • ë²”ìœ„ ì•ˆì— ë“¤ë„ë¡ ë§Œë“¤ì–´ì¤€ë‹¤. <code class="language-plaintext highlighter-rouge">torch.clamp</code> ì—­ì‹œ ë‹¤ì–‘í•œ ìƒí™©ì—ì„œ ì‚¬ìš©ë˜ëŠ”ë°, í•„ìì˜ ê²½ìš° ëª¨ë¸ ë ˆì´ì–´ ì¤‘ê°„ì— ì œê³±ê·¼ì´ë‚˜ ì§€ìˆ˜, ë¶„ìˆ˜ í˜¹ì€ ê°ë„ ê´€ë ¨ ì—°ì‚°ì´ ë“¤ì–´ê°€ <code class="language-plaintext highlighter-rouge">Backward Pass</code>ì—ì„œ <code class="language-plaintext highlighter-rouge">NaN</code>ì´ ë°œìƒí•  ìˆ˜ ìˆëŠ” ê²½ìš°ì— ì•ˆì „ì¥ì¹˜ë¡œ ë§ì´ ì‚¬ìš©í•˜ê³  ìˆë‹¤. (<a href="https://qcqced123.github.io/framework-library/backward-nan/">ìì„¸íˆ ì•Œê³  ì‹¶ë‹¤ë©´ í´ë¦­</a>)</p>

<h3 id="torchgather"><code class="language-plaintext highlighter-rouge">ğŸ‘©â€ğŸ‘©â€ğŸ‘§â€ğŸ‘¦Â torch.gather</code></h3>

<p>í…ì„œ ê°ì²´ ë‚´ë¶€ì—ì„œ ì›í•˜ëŠ” ì¸ë±ìŠ¤ì— ìœ„ì¹˜í•œ ì›ì†Œë§Œ ì¶”ì¶œí•˜ê³  ì‹¶ì„ ë•Œ ì‚¬ìš©í•˜ë©´ ë§¤ìš° ìœ ìš©í•œ ë©”ì„œë“œë‹¤. í…ì„œ ì—­ì‹œ <code class="language-plaintext highlighter-rouge">iterable</code> ê°ì²´ë¼ì„œ <code class="language-plaintext highlighter-rouge">loop</code> ë¥¼ ì‚¬ìš©í•´ ì ‘ê·¼í•˜ëŠ” ê²ƒì´ ì§ê´€ì ìœ¼ë¡œ ë³´ì¼ ìˆ˜ ìˆìœ¼ë‚˜, í†µìƒì ìœ¼ë¡œ í…ì„œë¥¼ ì‚¬ìš©í•˜ëŠ” ìƒí™©ì´ë¼ë©´ ê°ì²´ì˜ ì°¨ì›ì´ ì–´ë§ˆë¬´ì‹œ í•˜ê¸° ë•Œë¬¸ì— ë£¨í”„ë¡œ ì ‘ê·¼í•´ ê´€ë¦¬í•˜ëŠ” ê²ƒì€ ë§¤ìš° ë¹„íš¨ìœ¨ì ì´ë‹¤. ë£¨í”„ë¥¼ í†µí•´ ì ‘ê·¼í•˜ë©´ íŒŒì´ì¬ì˜ ë‚´ì¥ ë¦¬ìŠ¤íŠ¸ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒê³¼ ë³„ë°˜ ë‹¤ë¥¼ê²Œ ì—†ì–´ì§€ê¸° ë•Œë¬¸ì—, í…ì„œë¥¼ ì‚¬ìš©í•˜ëŠ” ë©”ë¦¬íŠ¸ê°€ ì‚¬ë¼ì§„ë‹¤. ë¹„êµì  í¬ì§€ ì•Šì€ 2~3ì°¨ì›ì˜ í…ì„œ ì •ë„ë¼ë©´ ì‚¬ìš©í•´ë„ í¬ê²Œ ë¬¸ì œëŠ” ì—†ì„ê±°ë¼ ìƒê°í•˜ì§€ë§Œ ê·¸ë˜ë„ ì½”ë“œì˜ ì¼ê´€ì„±ì„ ìœ„í•´ <code class="language-plaintext highlighter-rouge">torch.gather</code> ì‚¬ìš©ì„ ê¶Œì¥í•œë‹¤. ì´ì œ <code class="language-plaintext highlighter-rouge">torch.gather</code>ì˜ ì‚¬ìš©ë²•ì— ëŒ€í•´ ì•Œì•„ë³´ì.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># torch.gather params
</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">gather</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">sparse_grad</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">dim</code>ê³¼ <code class="language-plaintext highlighter-rouge">index</code>ì— ì£¼ëª©í•´ë³´ì. ë¨¼ì € <code class="language-plaintext highlighter-rouge">dim</code>ì€ ì‚¬ìš©ìê°€ ì¸ë±ì‹±ì„ ì ìš©í•˜ê³  ì‹¶ì€ ì°¨ì›ì„ ì§€ì •í•´ì£¼ëŠ” ì—­í• ì„ í•œë‹¤. <code class="language-plaintext highlighter-rouge">index</code> ë§¤ê°œë³€ìˆ˜ë¡œ ì „ë‹¬í•˜ëŠ” í…ì„œ ì•ˆì—ëŠ” ì›ì†Œì˜ <code class="language-plaintext highlighter-rouge">â€˜ì¸ë±ìŠ¤â€™</code>ë¥¼ ì˜ë¯¸í•˜ëŠ” ìˆ«ìë“¤ì´ ë§ˆêµ¬ì¡ì´ë¡œ ë‹´ê²¨ìˆëŠ”ë°, í•´ë‹¹ ì¸ë±ìŠ¤ê°€ ëŒ€ìƒ í…ì„œì˜ ì–´ëŠ ì°¨ì›ì„ ê°€ë¦¬í‚¬ ê²ƒì¸ì§€ë¥¼ ì»´í“¨í„°ì—ê²Œ ì•Œë ¤ì¤€ë‹¤ê³  ìƒê°í•˜ë©´ ëœë‹¤. <code class="language-plaintext highlighter-rouge">index</code> ëŠ” ì•ì—ì„œ ì„¤ëª…í–ˆë“¯ì´ ì›ì†Œì˜ <code class="language-plaintext highlighter-rouge">â€˜ì¸ë±ìŠ¤â€™</code>ë¥¼ ì˜ë¯¸í•˜ëŠ” ìˆ«ìë“¤ì´ ë‹´ê¸´ í…ì„œë¥¼ ì…ë ¥ìœ¼ë¡œ í•˜ëŠ” ë§¤ê°œë³€ìˆ˜ë‹¤. ì´ ë•Œ ì£¼ì˜í•  ì ì€ ëŒ€ìƒ í…ì„œ(<code class="language-plaintext highlighter-rouge">input</code>)ì™€ ì¸ë±ìŠ¤ í…ì„œì˜ ì°¨ì› í˜•íƒœê°€ ë°˜ë“œì‹œ ë™ì¼í•´ì•¼ í•œë‹¤ëŠ” ê²ƒì´ë‹¤. ì—­ì‹œ ë§ë¡œë§Œ ë“¤ìœ¼ë©´ ì´í•´í•˜ê¸° í˜ë“œë‹ˆ ì‚¬ìš© ì˜ˆì‹œë¥¼ í•¨ê¼ ì‚´í´ë³´ì.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># torch.gather usage example
</span><span class="o">&gt;&gt;&gt;</span> <span class="n">q</span><span class="p">,</span> <span class="n">kr</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1024</span><span class="p">,</span> <span class="mi">64</span><span class="p">),</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1024</span><span class="p">,</span> <span class="mi">64</span><span class="p">)</span> <span class="c1"># [batch, sequence, dim_head], [batch, 2*sequence, dim_head]
</span><span class="o">&gt;&gt;&gt;</span> <span class="n">tmp_c2p</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">kr</span><span class="p">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">))</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">tmp_c2p</span><span class="p">,</span> <span class="n">tmp_c2p</span><span class="p">.</span><span class="n">shape</span>
<span class="p">(</span><span class="n">tensor</span><span class="p">([[</span><span class="o">-</span><span class="mf">2.6477</span><span class="p">,</span> <span class="o">-</span><span class="mf">4.7478</span><span class="p">,</span> <span class="o">-</span><span class="mf">5.3250</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mf">1.6062</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.9717</span><span class="p">,</span>  <span class="mf">3.8004</span><span class="p">],</span>
         <span class="p">[</span> <span class="mf">0.0662</span><span class="p">,</span>  <span class="mf">1.5240</span><span class="p">,</span>  <span class="mf">0.1182</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mf">0.1653</span><span class="p">,</span>  <span class="mf">2.8476</span><span class="p">,</span>  <span class="mf">1.6337</span><span class="p">],</span>
         <span class="p">[</span><span class="o">-</span><span class="mf">0.5010</span><span class="p">,</span> <span class="o">-</span><span class="mf">4.2267</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.1179</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mf">1.1447</span><span class="p">,</span>  <span class="mf">1.7845</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1493</span><span class="p">],</span>
         <span class="p">...,</span>
         <span class="p">[</span><span class="o">-</span><span class="mf">2.1073</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2149</span><span class="p">,</span> <span class="o">-</span><span class="mf">4.8630</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mf">0.8238</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5833</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2066</span><span class="p">],</span>
         <span class="p">[</span> <span class="mf">2.1747</span><span class="p">,</span>  <span class="mf">3.2924</span><span class="p">,</span>  <span class="mf">6.5808</span><span class="p">,</span>  <span class="p">...,</span> <span class="o">-</span><span class="mf">0.2926</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2511</span><span class="p">,</span>  <span class="mf">2.6996</span><span class="p">],</span>
         <span class="p">[</span><span class="o">-</span><span class="mf">2.8362</span><span class="p">,</span>  <span class="mf">2.8700</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.9729</span><span class="p">,</span>  <span class="p">...,</span> <span class="o">-</span><span class="mf">4.9913</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3616</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1708</span><span class="p">]],</span>
        <span class="n">grad_fn</span><span class="o">=&lt;</span><span class="n">MmBackward0</span><span class="o">&gt;</span><span class="p">)</span>
<span class="n">torch</span><span class="p">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">1024</span><span class="p">,</span> <span class="mi">1024</span><span class="p">]))</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">max_seq</span><span class="p">,</span> <span class="n">max_relative_position</span> <span class="o">=</span> <span class="mi">1024</span><span class="p">,</span> <span class="mi">512</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">q_index</span><span class="p">,</span> <span class="n">k_index</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="n">max_seq</span><span class="p">),</span> <span class="n">torch</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">max_relative_position</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">q_index</span><span class="p">,</span> <span class="n">k_index</span>
<span class="p">(</span><span class="n">tensor</span><span class="p">([</span>   <span class="mi">0</span><span class="p">,</span>    <span class="mi">1</span><span class="p">,</span>    <span class="mi">2</span><span class="p">,</span>  <span class="p">...,</span> <span class="mi">1021</span><span class="p">,</span> <span class="mi">1022</span><span class="p">,</span> <span class="mi">1023</span><span class="p">]),</span>
 <span class="n">tensor</span><span class="p">([</span>   <span class="mi">0</span><span class="p">,</span>    <span class="mi">1</span><span class="p">,</span>    <span class="mi">2</span><span class="p">,</span>  <span class="p">...,</span> <span class="mi">1021</span><span class="p">,</span> <span class="mi">1022</span><span class="p">,</span> <span class="mi">1023</span><span class="p">]))</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">tmp_pos</span> <span class="o">=</span> <span class="n">q_index</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="o">-</span> <span class="n">k_index</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">rel_pos_matrix</span> <span class="o">=</span> <span class="n">tmp_pos</span> <span class="o">+</span> <span class="n">max_relative_position</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">rel_pos_matrix</span>
<span class="n">tensor</span><span class="p">([[</span> <span class="mi">512</span><span class="p">,</span>  <span class="mi">511</span><span class="p">,</span>  <span class="mi">510</span><span class="p">,</span>  <span class="p">...,</span> <span class="o">-</span><span class="mi">509</span><span class="p">,</span> <span class="o">-</span><span class="mi">510</span><span class="p">,</span> <span class="o">-</span><span class="mi">511</span><span class="p">],</span>
        <span class="p">[</span> <span class="mi">513</span><span class="p">,</span>  <span class="mi">512</span><span class="p">,</span>  <span class="mi">511</span><span class="p">,</span>  <span class="p">...,</span> <span class="o">-</span><span class="mi">508</span><span class="p">,</span> <span class="o">-</span><span class="mi">509</span><span class="p">,</span> <span class="o">-</span><span class="mi">510</span><span class="p">],</span>
        <span class="p">[</span> <span class="mi">514</span><span class="p">,</span>  <span class="mi">513</span><span class="p">,</span>  <span class="mi">512</span><span class="p">,</span>  <span class="p">...,</span> <span class="o">-</span><span class="mi">507</span><span class="p">,</span> <span class="o">-</span><span class="mi">508</span><span class="p">,</span> <span class="o">-</span><span class="mi">509</span><span class="p">],</span>
        <span class="p">...,</span>
        <span class="p">[</span><span class="mi">1533</span><span class="p">,</span> <span class="mi">1532</span><span class="p">,</span> <span class="mi">1531</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mi">512</span><span class="p">,</span>  <span class="mi">511</span><span class="p">,</span>  <span class="mi">510</span><span class="p">],</span>
        <span class="p">[</span><span class="mi">1534</span><span class="p">,</span> <span class="mi">1533</span><span class="p">,</span> <span class="mi">1532</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mi">513</span><span class="p">,</span>  <span class="mi">512</span><span class="p">,</span>  <span class="mi">511</span><span class="p">],</span>
        <span class="p">[</span><span class="mi">1535</span><span class="p">,</span> <span class="mi">1534</span><span class="p">,</span> <span class="mi">1533</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mi">514</span><span class="p">,</span>  <span class="mi">513</span><span class="p">,</span>  <span class="mi">512</span><span class="p">]])</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">rel_pos_matrix</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">rel_pos_matrix</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="o">*</span><span class="n">max_relative_position</span> <span class="o">-</span> <span class="mi">1</span><span class="p">).</span><span class="n">repeat</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">tmp_c2p</span> <span class="o">=</span> <span class="n">tmp_c2p</span><span class="p">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">rel_pos_matrix</span><span class="p">,</span> <span class="n">rel_pos_matrix</span><span class="p">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">tmp_c2p</span><span class="p">.</span><span class="n">shape</span> 
<span class="p">(</span><span class="n">tensor</span><span class="p">([[[</span> <span class="mi">512</span><span class="p">,</span>  <span class="mi">511</span><span class="p">,</span>  <span class="mi">510</span><span class="p">,</span>  <span class="p">...,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">],</span>
          <span class="p">[</span> <span class="mi">513</span><span class="p">,</span>  <span class="mi">512</span><span class="p">,</span>  <span class="mi">511</span><span class="p">,</span>  <span class="p">...,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">],</span>
          <span class="p">[</span> <span class="mi">514</span><span class="p">,</span>  <span class="mi">513</span><span class="p">,</span>  <span class="mi">512</span><span class="p">,</span>  <span class="p">...,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">],</span>
          <span class="p">...,</span>
          <span class="p">[</span><span class="mi">1023</span><span class="p">,</span> <span class="mi">1023</span><span class="p">,</span> <span class="mi">1023</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mi">512</span><span class="p">,</span>  <span class="mi">511</span><span class="p">,</span>  <span class="mi">510</span><span class="p">],</span>
          <span class="p">[</span><span class="mi">1023</span><span class="p">,</span> <span class="mi">1023</span><span class="p">,</span> <span class="mi">1023</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mi">513</span><span class="p">,</span>  <span class="mi">512</span><span class="p">,</span>  <span class="mi">511</span><span class="p">],</span>
          <span class="p">[</span><span class="mi">1023</span><span class="p">,</span> <span class="mi">1023</span><span class="p">,</span> <span class="mi">1023</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mi">514</span><span class="p">,</span>  <span class="mi">513</span><span class="p">,</span>  <span class="mi">512</span><span class="p">]],</span>
 
         <span class="p">[[</span> <span class="mi">512</span><span class="p">,</span>  <span class="mi">511</span><span class="p">,</span>  <span class="mi">510</span><span class="p">,</span>  <span class="p">...,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">],</span>
          <span class="p">[</span> <span class="mi">513</span><span class="p">,</span>  <span class="mi">512</span><span class="p">,</span>  <span class="mi">511</span><span class="p">,</span>  <span class="p">...,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">],</span>
          <span class="p">[</span> <span class="mi">514</span><span class="p">,</span>  <span class="mi">513</span><span class="p">,</span>  <span class="mi">512</span><span class="p">,</span>  <span class="p">...,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">],</span>
          <span class="p">...,</span>
          <span class="p">[</span><span class="mi">1023</span><span class="p">,</span> <span class="mi">1023</span><span class="p">,</span> <span class="mi">1023</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mi">512</span><span class="p">,</span>  <span class="mi">511</span><span class="p">,</span>  <span class="mi">510</span><span class="p">],</span>
          <span class="p">[</span><span class="mi">1023</span><span class="p">,</span> <span class="mi">1023</span><span class="p">,</span> <span class="mi">1023</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mi">513</span><span class="p">,</span>  <span class="mi">512</span><span class="p">,</span>  <span class="mi">511</span><span class="p">],</span>
          <span class="p">[</span><span class="mi">1023</span><span class="p">,</span> <span class="mi">1023</span><span class="p">,</span> <span class="mi">1023</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mi">514</span><span class="p">,</span>  <span class="mi">513</span><span class="p">,</span>  <span class="mi">512</span><span class="p">]],</span>
<span class="n">torch</span><span class="p">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1024</span><span class="p">,</span> <span class="mi">1024</span><span class="p">]),</span>
<span class="n">torch</span><span class="p">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1024</span><span class="p">,</span> <span class="mi">1024</span><span class="p">]))</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">gather</span><span class="p">(</span><span class="n">tmp_c2p</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">rel_pos_matrix</span><span class="p">)</span>
<span class="n">tensor</span><span class="p">([[[</span><span class="o">-</span><span class="mf">0.8579</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2178</span><span class="p">,</span>  <span class="mf">1.6323</span><span class="p">,</span>  <span class="p">...,</span> <span class="o">-</span><span class="mf">2.6477</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.6477</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.6477</span><span class="p">],</span>
         <span class="p">[</span> <span class="mf">1.1601</span><span class="p">,</span>  <span class="mf">2.1752</span><span class="p">,</span>  <span class="mf">0.7187</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mf">0.0662</span><span class="p">,</span>  <span class="mf">0.0662</span><span class="p">,</span>  <span class="mf">0.0662</span><span class="p">],</span>
         <span class="p">[</span> <span class="mf">3.4379</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2573</span><span class="p">,</span>  <span class="mf">0.1375</span><span class="p">,</span>  <span class="p">...,</span> <span class="o">-</span><span class="mf">0.5010</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5010</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5010</span><span class="p">],</span>
         <span class="p">...,</span>
         <span class="p">[</span><span class="o">-</span><span class="mf">1.2066</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2066</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2066</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mf">0.5943</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5169</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.0820</span><span class="p">],</span>
         <span class="p">[</span> <span class="mf">2.6996</span><span class="p">,</span>  <span class="mf">2.6996</span><span class="p">,</span>  <span class="mf">2.6996</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mf">0.2014</span><span class="p">,</span>  <span class="mf">1.1458</span><span class="p">,</span>  <span class="mf">3.2626</span><span class="p">],</span>
         <span class="p">[</span><span class="o">-</span><span class="mf">0.1708</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1708</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1708</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mf">1.9955</span><span class="p">,</span>  <span class="mf">4.1549</span><span class="p">,</span>  <span class="mf">2.6356</span><span class="p">]],</span>
</code></pre></div></div>

<p>ìœ„ ì½”ë“œëŠ” <code class="language-plaintext highlighter-rouge">DeBERTa</code> ì˜ <code class="language-plaintext highlighter-rouge">Disentangled Self-Attention</code>ì„ êµ¬í˜„í•œ ì½”ë“œì˜ ì¼ë¶€ë¶„ì´ë‹¤. ìì„¸í•œ ì›ë¦¬ëŠ” <code class="language-plaintext highlighter-rouge">DeBERTa</code> ë…¼ë¬¸ ë¦¬ë·° í¬ìŠ¤íŒ…ì—ì„œ í™•ì¸í•˜ë©´ ë˜ê³ , ìš°ë¦¬ê°€ ì§€ê¸ˆ ì£¼ëª©í•  ë¶€ë¶„ì€ ë°”ë¡œ <code class="language-plaintext highlighter-rouge">tmp_c2p</code>, <code class="language-plaintext highlighter-rouge">rel_pos_matrix</code> ê·¸ë¦¬ê³  ë§ˆì§€ë§‰ ì¤„ì— ìœ„ì¹˜í•œ <code class="language-plaintext highlighter-rouge">torch.gather</code> ë‹¤. <code class="language-plaintext highlighter-rouge">[10, 1024, 1024]</code> ëª¨ì–‘ì„ ê°€ì§„ ëŒ€ìƒ í…ì„œ <code class="language-plaintext highlighter-rouge">tmp_c2p</code> ì—ì„œ ë‚´ê°€ ì›í•˜ëŠ” ì›ì†Œë§Œ ì¶”ì¶œí•˜ë ¤ëŠ” ìƒí™©ì¸ë°, ì¶”ì¶œí•´ì•¼í•  ì›ì†Œì˜ ì¸ë±ìŠ¤ ê°’ì´ ë‹´ê¸´ í…ì„œë¥¼ <code class="language-plaintext highlighter-rouge">rel_pos_matrix</code> ë¡œ ì •ì˜í–ˆë‹¤. <code class="language-plaintext highlighter-rouge">rel_pos_matrix</code> ì˜ ì°¨ì›ì€ <code class="language-plaintext highlighter-rouge">[10, 1024, 1024]</code>ë¡œ <code class="language-plaintext highlighter-rouge">tmp_c2p</code>ì™€ ë™ì¼í•˜ë‹¤. ì°¸ê³ ë¡œ ì¶”ì¶œí•´ì•¼ í•˜ëŠ” ì°¨ì› ë°©í–¥ì€ ê°€ë¡œ ë°©í–¥(ë‘ ë²ˆì§¸ 1024)ì´ë‹¤.</p>

<p>ì´ì œ <code class="language-plaintext highlighter-rouge">torch.gather</code>ì˜ ë™ì‘ì„ ì‚´í´ë³´ì. ìš°ë¦¬ê°€ í˜„ì¬ ì¶”ì¶œí•˜ê³  ì‹¶ì€ ëŒ€ìƒì€ 3ì°¨ì› í…ì„œì˜ ê°€ë¡œ ë°©í–¥(ë‘ ë²ˆì§¸ 1024, í…ì„œì˜ í–‰ ë²¡í„°), ì¦‰ <code class="language-plaintext highlighter-rouge">2 * max_sequence_length</code> ë¥¼ ì˜ë¯¸í•˜ëŠ” ì°¨ì› ë°©í–¥ì˜ ì›ì†Œë‹¤. ë”°ë¼ì„œ <code class="language-plaintext highlighter-rouge">dim=-1</code>ìœ¼ë¡œ ì„¤ì •í•´ì¤€ë‹¤. ì´ì œ ë©”ì„œë“œê°€ ì˜ë„ëŒ€ë¡œ ì ìš©ë˜ì—ˆëŠ”ì§€ í™•ì¸í•´ë³´ì. <code class="language-plaintext highlighter-rouge">rel_pos_matrix</code> ì˜ 0ë²ˆ ë°°ì¹˜, 0ë²ˆì§¸ ì‹œí€€ìŠ¤ì˜ ê°€ì¥ ë§ˆì§€ë§‰ ì°¨ì›ì˜ ê°’ì€ <code class="language-plaintext highlighter-rouge">0</code>ìœ¼ë¡œ ì´ˆê¸°í™” ë˜ì–´ ìˆë‹¤. ë‹¤ì‹œ ë§í•´, ëŒ€ìƒ í…ì„œì˜ ëŒ€ìƒ ì°¨ì›ì—ì„œ 0ë²ˆì§¸ ì¸ë±ìŠ¤ì— í•´ë‹¹í•˜ëŠ” ê°’ì„ ê°€ì ¸ì˜¤ë¼ëŠ” ì˜ë¯¸ë¥¼ ë‹´ê³  ìˆë‹¤. ê·¸ë ‡ë‹¤ë©´ <code class="language-plaintext highlighter-rouge">torch.gather</code> ì‹¤í–‰ ê²°ê³¼ê°€ <code class="language-plaintext highlighter-rouge">tmp_c2p</code>ì˜ 0ë²ˆ ë°°ì¹˜, 0ë²ˆì§¸ ì‹œí€€ìŠ¤ì˜ 0ë²ˆì§¸ ì°¨ì› ê°’ê³¼ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸í•´ë³´ì. ë‘˜ ë‹¤ <code class="language-plaintext highlighter-rouge">-2.6477</code>, <code class="language-plaintext highlighter-rouge">-2.6477</code> ìœ¼ë¡œ ê°™ì€ ê°’ì„ ë‚˜íƒ€ë‚´ê³  ìˆë‹¤. ë”°ë¼ì„œ ìš°ë¦¬ ì˜ë„ëŒ€ë¡œ ì˜ ì‹¤í–‰ë˜ì—ˆë‹¤ëŠ” ì‚¬ì‹¤ì„ ì•Œ ìˆ˜ ìˆë‹¤.</p>

<h3 id="torchtriu-torchtril"><code class="language-plaintext highlighter-rouge">ğŸ‘©â€ğŸ‘©â€ğŸ‘§â€ğŸ‘¦Â torch.triu, torch.tril</code></h3>

<p>ê°ê° ì…ë ¥ í…ì„œë¥¼ <code class="language-plaintext highlighter-rouge">ìƒì‚¼ê°í–‰ë ¬</code>, <code class="language-plaintext highlighter-rouge">í•˜ì‚¼ê°í–‰ë ¬</code>ë¡œ ë§Œë“ ë‹¤. <code class="language-plaintext highlighter-rouge">triu</code>ë‚˜ <code class="language-plaintext highlighter-rouge">tril</code>ì€ ì‚¬ì‹¤ ë’¤ì§‘ìœ¼ë©´ ê°™ì€ ê²°ê³¼ë¥¼ ë°˜í™˜í•˜ê¸° ë•Œë¬¸ì— <code class="language-plaintext highlighter-rouge">tril</code>ì„ ê¸°ì¤€ìœ¼ë¡œ ì„¤ëª…ì„ í•˜ê² ë‹¤. ë©”ì„œë“œì˜ ë§¤ê°œë³€ìˆ˜ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># torch.triu, tril params
</span><span class="n">upper_tri_matrix</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">triu</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span> <span class="n">diagonal</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
<span class="n">lower_tri_matrix</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tril</span><span class="p">(</span><span class="n">input_tensors</span><span class="p">,</span> <span class="n">diagonal</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">diagonal</code> ì— ì£¼ëª©í•´ë³´ì. ì–‘ìˆ˜ë¥¼ ì „ë‹¬í•˜ë©´ ì£¼ëŒ€ê°ì„±ë¶„ì—ì„œ í•´ë‹¹í•˜ëŠ” ê°’ë§Œí¼ ë–¨ì–´ì§„ ê³³ì˜ ëŒ€ê°ì„±ë¶„ê¹Œì§€ ê·¸ ê°’ì„ ì‚´ë ¤ë‘”ë‹¤. í•œí¸ ìŒìˆ˜ë¥¼ ì „ë‹¬í•˜ë©´ ì£¼ëŒ€ê°ì„±ë¶„ì„ í¬í•¨í•´ ì£¼ì–´ì§„ ê°’ë§Œí¼ ë–¨ì–´ì§„ ê³³ê¹Œì§€ì˜ ëŒ€ê°ì„±ë¶„ì„ ëª¨ë‘ 0ìœ¼ë¡œ ë§Œë“¤ì–´ë²„ë¦°ë‹¤. ê¸°ë³¸ì€ 0ìœ¼ë¡œ ì„¤ì •ë˜ì–´ ìˆìœ¼ë©°, ì´ëŠ” ì£¼ëŒ€ê°ì„±ë¶„ë¶€í„° ì™¼ìª½ í•˜ë‹¨ì˜ ì›ì†Œë¥¼ ëª¨ë‘ ì‚´ë ¤ë‘ê² ë‹¤ëŠ” ì˜ë¯¸ê°€ ëœë‹¤.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># torch.tril usage example
</span><span class="o">&gt;&gt;&gt;</span> <span class="n">lm_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tril</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">ones</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">lm_mask</span>
<span class="mi">1</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span>
<span class="mi">1</span> <span class="mi">1</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span>
<span class="mi">1</span> <span class="mi">1</span> <span class="mi">1</span> <span class="mi">0</span> <span class="mi">0</span>
<span class="mi">1</span> <span class="mi">1</span> <span class="mi">1</span> <span class="mi">1</span> <span class="mi">0</span>
</code></pre></div></div>

<p>ë‘ ë©”ì„œë“œëŠ” ì„ í˜•ëŒ€ìˆ˜í•™ì´ í•„ìš”í•œ ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ ì‚¬ìš©ë˜ëŠ”ë°, í•„ìì˜ ê²½ìš°, <code class="language-plaintext highlighter-rouge">GPT</code>ì²˜ëŸ¼ <code class="language-plaintext highlighter-rouge">Transformer</code>ì˜ <code class="language-plaintext highlighter-rouge">Decoder</code> ë¥¼ ì‚¬ìš©í•˜ëŠ” ëª¨ë¸ì„ ë¹Œë“œí•  ë•Œ ê°€ì¥ ë§ì´ ì‚¬ìš©í–ˆë˜ ê²ƒ ê°™ë‹¤. <code class="language-plaintext highlighter-rouge">Decoder</code>ë¥¼ ì‚¬ìš©í•˜ëŠ” ëª¨ë¸ì€ ëŒ€ë¶€ë¶„ êµ¬ì¡°ìƒ <code class="language-plaintext highlighter-rouge">Language Modeling</code>ì„ ìœ„í•´ì„œ <code class="language-plaintext highlighter-rouge">Masked Multi-Head Self-Attention Block</code>ì„ ì‚¬ìš©í•˜ëŠ”ë° ì´ ë•Œ ë¯¸ë˜ ì‹œì ì˜ í† í° ì„ë² ë”© ê°’ì— ë§ˆìŠ¤í‚¹ì„ í•´ì£¼ê¸° ìœ„í•´ <code class="language-plaintext highlighter-rouge">torch.tril</code> ì„ ì‚¬ìš©í•˜ê²Œ ë˜ë‹ˆ ì°¸ê³ í•˜ì.</p>

<h3 id="torchtensormasked_fill"><code class="language-plaintext highlighter-rouge">ğŸ‘©â€ğŸ‘©â€ğŸ‘§â€ğŸ‘¦Â torch.Tensor.masked_fill</code></h3>

<p>ì‚¬ìš©ìê°€ ì§€ì •í•œ ê°’ì— í•´ë‹¹ë˜ëŠ” ì›ì†Œë¥¼ ëª¨ë‘ ë§ˆìŠ¤í‚¹ ì²˜ë¦¬í•´ì£¼ëŠ” ë©”ì„œë“œë‹¤. ë¨¼ì € ë§¤ê°œë³€ìˆ˜ë¥¼ í™•ì¸í•´ë³´ì.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># torch.Tensor.masked_fill params
</span><span class="n">input_tensors</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">6</span><span class="p">]])</span>
<span class="n">input_tensors</span><span class="p">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">mask</span><span class="p">:</span> <span class="n">BoolTensor</span><span class="p">,</span> <span class="n">value</span><span class="p">:</span> <span class="nb">float</span><span class="p">)</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">masked_fill</code> ì€ í…ì„œ ê°ì²´ì˜ ë‚´ë¶€ <code class="language-plaintext highlighter-rouge">attribute</code> ë¡œ ì •ì˜ë˜ê¸° ë•Œë¬¸ì— í•´ë‹¹ ë©”ì„œë“œë¥¼ ì‚¬ìš©í•˜ê³  ì‹¶ë‹¤ë©´ ë¨¼ì € ë§ˆìŠ¤í‚¹ ëŒ€ìƒ í…ì„œë¥¼ ë§Œë“¤ì–´ì•¼ í•œë‹¤. í…ì„œë¥¼ ì •ì˜í–ˆë‹¤ë©´ í…ì„œ ê°ì²´ì˜ <code class="language-plaintext highlighter-rouge">attributes</code> ì ‘ê·¼ì„ í†µí•´ <code class="language-plaintext highlighter-rouge">masked_fill()</code> ì„ í˜¸ì¶œí•œ ë’¤, í•„ìš”í•œ ë§¤ê°œë³€ìˆ˜ë¥¼ ì „ë‹¬í•´ì£¼ëŠ” ë°©ì‹ìœ¼ë¡œ ì‚¬ìš©í•˜ë©´ ëœë‹¤.</p>

<p><code class="language-plaintext highlighter-rouge">mask</code> ë§¤ê°œë³€ìˆ˜ì—ëŠ” ë§ˆìŠ¤í‚¹ í…ì„œë¥¼ ì „ë‹¬í•´ì•¼ í•˜ëŠ”ë°, ì´ ë•Œ ë‚´ë¶€ ì›ì†ŒëŠ” ëª¨ë‘ <code class="language-plaintext highlighter-rouge">boolean</code>ì´ì–´ì•¼ í•˜ê³  í…ì„œì˜ í˜•íƒœëŠ” ëŒ€ìƒ í…ì„œì™€ ë™ì¼í•´ì•¼ í•œë‹¤(ì™„ì „íˆ ê°™ì„ í•„ìš”ëŠ” ì—†ê³ , ë¸Œë¡œë“œ ìºìŠ¤íŒ…ë§Œ ê°€ëŠ¥í•˜ë©´ ìƒê´€ ì—†ìŒ).</p>

<p><code class="language-plaintext highlighter-rouge">value</code> ë§¤ê°œë³€ìˆ˜ì—ëŠ” ë§ˆìŠ¤í‚¹ ëŒ€ìƒ ì›ì†Œë“¤ì— ì¼ê´„ì ìœ¼ë¡œ ì ìš©í•´ì£¼ê³  ì‹¶ì€ ê°’ì„ ì „ë‹¬í•œë‹¤. ì´ê²Œ ë§ë¡œë§Œ ë“¤ìœ¼ë©´ ì´í•´í•˜ê¸° ì‰½ì§€ ì•Šë‹¤. ì•„ë˜ ì‚¬ìš© ì˜ˆì‹œë¥¼ í•¨ê»˜ ì²¨ë¶€í–ˆìœ¼ë‹ˆ ì°¸ê³  ë°”ë€ë‹¤.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># torch.masked_fill usage
</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">lm_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tril</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">ones</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">lm_mask</span>
<span class="mi">1</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span>
<span class="mi">1</span> <span class="mi">1</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span>
<span class="mi">1</span> <span class="mi">1</span> <span class="mi">1</span> <span class="mi">0</span> <span class="mi">0</span>
<span class="mi">1</span> <span class="mi">1</span> <span class="mi">1</span> <span class="mi">1</span> <span class="mi">0</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">attention_matrix</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">))</span> <span class="o">/</span> <span class="n">dot_scale</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">attention_matrix</span>
<span class="mf">1.22</span> <span class="mf">2.1</span> <span class="mf">3.4</span> <span class="mf">1.2</span> <span class="mf">1.1</span>
<span class="mf">1.22</span> <span class="mf">2.1</span> <span class="mf">3.4</span> <span class="mf">9.9</span> <span class="mf">9.9</span>
<span class="mf">1.22</span> <span class="mf">2.1</span> <span class="mf">3.4</span> <span class="mf">9.9</span> <span class="mf">9.9</span>
<span class="mf">1.22</span> <span class="mf">2.1</span> <span class="mf">3.4</span> <span class="mf">9.9</span> <span class="mf">9.9</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">attention_matrix</span> <span class="o">=</span> <span class="n">attention_matrix</span><span class="p">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">lm_mask</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="s">'-inf'</span><span class="p">))</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">attention_matrix</span>
<span class="mf">1.22</span> <span class="o">-</span><span class="n">inf</span> <span class="o">-</span><span class="n">inf</span> <span class="o">-</span><span class="n">inf</span> <span class="o">-</span><span class="n">inf</span>
<span class="mf">1.22</span> <span class="mf">2.1</span> <span class="o">-</span><span class="n">inf</span> <span class="o">-</span><span class="n">inf</span> <span class="o">-</span><span class="n">inf</span>
<span class="mf">1.22</span> <span class="mf">2.1</span> <span class="mf">3.4</span> <span class="o">-</span><span class="n">inf</span> <span class="o">-</span><span class="n">inf</span>
<span class="mf">1.22</span> <span class="mf">2.1</span> <span class="mf">3.4</span> <span class="mf">9.9</span> <span class="o">-</span><span class="n">inf</span>
</code></pre></div></div>

<h3 id="ï¸torchclone"><code class="language-plaintext highlighter-rouge">ğŸ—‚ï¸Â torch.clone</code></h3>

<p><code class="language-plaintext highlighter-rouge">inputs</code> ì¸ìë¡œ ì „ë‹¬í•œ í…ì„œë¥¼ ë³µì‚¬í•˜ëŠ” íŒŒì´í† ì¹˜ ë‚´ì¥ ë©”ì„œë“œë‹¤.  ì‚¬ìš©ë²•ì€ ì•„ë˜ì™€ ê°™ë‹¤.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="s">""" torch.clone """</span>
<span class="n">torch</span><span class="p">.</span><span class="n">clone</span><span class="p">(</span>
    <span class="nb">input</span><span class="p">,</span><span class="err">Â </span>
    <span class="o">*</span><span class="p">,</span>
   <span class="err">Â </span><span class="n">memory_format</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">preserve_format</span>
<span class="p">)</span><span class="err">Â â†’Â </span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span>
</code></pre></div></div>

<p>ë”¥ëŸ¬ë‹ íŒŒì´í”„ë¼ì¸ì„ ë§Œë“¤ë‹¤ ë³´ë©´ ë§ì´ ì‚¬ìš©í•˜ê²Œ ë˜ëŠ” ê¸°ë³¸ì ì¸ ë©”ì„œë“œì¸ë°, ì´ë ‡ê²Œ ë”°ë¡œ ì •ë¦¬í•˜ê²Œ ëœ ì´ìœ ê°€ ìˆë‹¤. ì…ë ¥ëœ í…ì„œë¥¼ ê·¸ëŒ€ë¡œ ë³µì‚¬í•œë‹¤ëŠ” íŠ¹ì„± ë•Œë¬¸ì— ì‚¬ìš©ì‹œ ì£¼ì˜í•´ì•¼ í•  ì ì´ ìˆê¸° ë•Œë¬¸ì´ë‹¤. í•´ë‹¹ ë©”ì„œë“œë¥¼ ì‚¬ìš©í•˜ê¸° ì „ì— ë°˜ë“œì‹œ ì…ë ¥í•  í…ì„œê°€ í˜„ì¬ ì–´ëŠ ë””ë°”ì´ìŠ¤(CPU, GPU) ìœ„ì— ìˆëŠ”ì§€, ê·¸ë¦¬ê³  í•´ë‹¹ í…ì„œê°€ ê³„ì‚° ê·¸ë˜í”„ë¥¼ ê°€ì§€ê³  ìˆëŠ”ì§€ë¥¼ <strong>ë°˜ë“œì‹œ</strong> íŒŒì•…í•´ì•¼ í•œë‹¤.</p>

<p>í•„ìëŠ” ELECTRA ëª¨ë¸ì„ ì§ì ‘ êµ¬í˜„í•˜ëŠ” ê³¼ì •ì—ì„œ <code class="language-plaintext highlighter-rouge">clone()</code> ë©”ì„œë“œë¥¼ ì‚¬ìš©í–ˆëŠ”ë°, Generator ëª¨ë¸ì˜ ê²°ê³¼ ë¡œì§“ì„ Discriminatorì˜ ì…ë ¥ìœ¼ë¡œ ë³€í™˜í•´ì£¼ê¸° ìœ„í•¨ì´ì—ˆë‹¤. ê·¸ ê³¼ì •ì—ì„œ Generatorê°€ ë°˜í™˜í•œ ë¡œì§“ì„ ê·¸ëŒ€ë¡œ <code class="language-plaintext highlighter-rouge">clone</code>í•œ ë’¤, ì…ë ¥ì„ ë§Œë“¤ì–´ ì£¼ì—ˆê³  ê·¸ ê²°ê³¼ ì•„ë˜ì™€ ê°™ì€ ì—ëŸ¬ë¥¼ ë§ˆì£¼í–ˆë‹¤.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">RuntimeError</span><span class="p">:</span> <span class="n">one</span> <span class="n">of</span> <span class="n">the</span> <span class="n">variables</span> <span class="n">needed</span> <span class="k">for</span> <span class="n">gradient</span> <span class="n">computation</span> <span class="n">has</span> <span class="n">been</span> <span class="n">modified</span> <span class="n">by</span> <span class="n">an</span> <span class="n">inplace</span> <span class="n">operation</span><span class="p">:</span> <span class="p">[</span>
<span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">LongTensor</span> <span class="p">[</span><span class="mi">8</span><span class="p">,</span> <span class="mi">511</span><span class="p">]]</span> <span class="ow">is</span> <span class="n">at</span> <span class="n">version</span> <span class="mi">1</span><span class="p">;</span> <span class="n">expected</span> <span class="n">version</span> <span class="mi">0</span> 
<span class="n">instead</span><span class="p">.</span> <span class="n">Hint</span><span class="p">:</span> <span class="n">the</span> <span class="n">backtrace</span> <span class="n">further</span> <span class="n">above</span> <span class="n">shows</span> <span class="n">the</span> <span class="n">operation</span> <span class="n">that</span> <span class="n">failed</span> <span class="n">to</span> <span class="n">compute</span> <span class="n">its</span> <span class="n">gradient</span><span class="p">.</span> 
<span class="n">The</span> <span class="n">variable</span> <span class="ow">in</span> <span class="n">question</span> <span class="n">was</span> <span class="n">changed</span> <span class="ow">in</span> <span class="n">there</span> <span class="ow">or</span> <span class="n">anywhere</span> <span class="n">later</span><span class="p">.</span> <span class="n">Good</span> <span class="n">luck</span><span class="err">!</span>
</code></pre></div></div>

<p>ì—ëŸ¬ ë¡œê·¸ë¥¼ ìì„¸íˆ ì½ì–´ë³´ë©´ í…ì„œ ë²„ì „ì˜ ë³€ê²½ìœ¼ë¡œ ì¸í•´ ê·¸ë¼ë””ì–¸íŠ¸ ê³„ì‚°ì´ ë¶ˆê°€í•˜ë‹¤ëŠ” ë‚´ìš©ì´ ë‹´ê²¨ìˆë‹¤. êµ¬ê¸€ë§í•´ë´ë„ ì˜ ì•ˆë‚˜ì™€ì„œ í¬ê¸°í•˜ë ¤ë˜ ì°°ë¼ì— ìš°ì—°íˆ <code class="language-plaintext highlighter-rouge">torch.clone()</code> ë©”ì„œë“œì˜ ì •í™•í•œ ì‚¬ìš©ë²•ì´ ê¶ê¸ˆí•´ ê³µì‹ Docsë¥¼ ì½ê²Œ ë˜ì—ˆê³ , ê±°ê¸°ì„œ ì—„ì²­ë‚œ ì‚¬ì‹¤ì„ ë°œê²¬í–ˆë‹¤. <code class="language-plaintext highlighter-rouge">clone()</code> ë©”ì„œë“œê°€ ì…ë ¥ëœ í…ì„œì˜ í˜„ì¬ ë””ë°”ì´ìŠ¤ ìœ„ì¹˜ì— ë˜‘ê°™ì´ ë³µì‚¬ë  ê²ƒì´ë€ ì˜ˆìƒì€ í–ˆì§€ë§Œ, ì…ë ¥ í…ì„œì˜ ê³„ì‚°ê·¸ë˜í”„ê¹Œì§€ ë³µì‚¬ë  ê²ƒì´ë€ ìƒê°ì€ ì „í˜€ í•˜ì§€ ëª»í–ˆê¸° ë•Œë¬¸ì´ë‹¤. ê·¸ë˜ì„œ ìœ„ì™€ ê°™ì€ ì—ëŸ¬ë¥¼ ë§ˆì£¼í•˜ì§€ ì•Šìœ¼ë ¤ë©´, <code class="language-plaintext highlighter-rouge">clone()</code>ì„ í˜¸ì¶œí•  ë•Œ ë’¤ì— ë°˜ë“œì‹œ <code class="language-plaintext highlighter-rouge">detach()</code>ë¥¼ í•¨ê»˜ í˜¸ì¶œí•´ì¤˜ì•¼ í•œë‹¤.</p>

<p><code class="language-plaintext highlighter-rouge">clone()</code> ë©”ì„œë“œëŠ” ì…ë ¥ëœ í…ì„œì˜ ëª¨ë“  ê²ƒì„ ë³µì‚¬í•œë‹¤ëŠ” ì ì„ ë°˜ë“œì‹œ ê¸°ì–µí•˜ì.</p>]]></content><author><name>qcqced</name><email>qcqced123@gmail.com</email></author><category term="Framework &amp; Library" /><category term="Pytorch" /><category term="Tensor" /><category term="Linear Algebra" /><summary type="html"><![CDATA[íŒŒì´í† ì¹˜ì—ì„œ ìì£¼ ì‚¬ìš©í•˜ëŠ” í…ì„œ ì¸ë±ì‹± ê´€ë ¨ ë©”ì„œë“œ ëª¨ìŒ]]></summary></entry><entry><title type="html">ğŸ”¢Â Product &amp;amp; Quotient Rule: ê³±ì˜ ë¯¸ë¶„, ëª«ì˜ ë¯¸ë¶„</title><link href="http://localhost:4000/optimization-theory/product_quotient_rule" rel="alternate" type="text/html" title="ğŸ”¢Â Product &amp;amp; Quotient Rule: ê³±ì˜ ë¯¸ë¶„, ëª«ì˜ ë¯¸ë¶„" /><published>2024-01-08T00:00:00+09:00</published><updated>2024-01-08T23:00:00+09:00</updated><id>http://localhost:4000/optimization-theory/product_quotient_rule</id><content type="html" xml:base="http://localhost:4000/optimization-theory/product_quotient_rule"><![CDATA[<p>ê³±ì˜ ë¯¸ë¶„, ëª«ì˜ ë¯¸ë¶„ì€ í•¨ìˆ˜ê°€ ê³±ì˜ ê¼´ í˜•íƒœ $f(x)g(x)$ í˜¹ì€ ë¶„ìˆ˜ ê¼´ í˜•íƒœ $\frac{f(x)}{g(x)}$ë¥¼ ê°€ì§€ê³  ìˆì„ ë•Œ ë„í•¨ìˆ˜ë¥¼ êµ¬í•˜ëŠ” ë°©ë²•ì´ë‹¤. ê³ ë“±í•™êµ ë¯¸ì ë¶„ ì‹œê°„(17~18í•™ë²ˆ ê¸°ì¤€)ì— ë°°ìš´ì ì´ ìˆì§€ë§Œ, í•©ì„±í•¨ìˆ˜ ë¯¸ë¶„ë²•ê³¼ ë”ë¶ˆì–´ ë‹¨ìˆœ ì•”ê¸°ì˜ íí•´ë¡œ ê¹Œë¨¹ê¸° ì¢‹ì€ ë¯¸ë¶„ë²•ë“¤ì´ë‹¤. í¬ë¡œìŠ¤ ì—”íŠ¸ë¡œí”¼, ì†Œí”„íŠ¸ë§¥ìŠ¤ ë¯¸ë¶„ì— ì“°ì´ë¯€ë¡œ í•©ì„±í•¨ìˆ˜ ë¯¸ë¶„ë²•ê³¼ ë§ˆì°¬ê°€ì§€ë¡œ ë”¥ëŸ¬ë‹, ë¨¸ì‹ ëŸ¬ë‹ì—ì„œ ë§¤ìš° ì¤‘ìš”í•˜ë‹¤.</p>

<h3 id="ï¸product-rule"><code class="language-plaintext highlighter-rouge">âœ–ï¸Â Product Rule</code></h3>

<p>ëª«ì˜ ë¯¸ë¶„ì€ ê³±ì˜ ë¯¸ë¶„ì˜ ì›ë¦¬ë¥¼ ì´í•´í•˜ë©´ ìë™ìœ¼ë¡œ ì•Œ ìˆ˜ ìˆê¸° ë•Œë¬¸ì— ê³±ì˜ ë¯¸ë¶„ë¶€í„° ì‚´í´ë³´ê² ë‹¤. ë¨¼ì € ì•„ë˜ì™€ ê°™ì´ ê³±ì˜ í˜•íƒœë¥¼ ê°€ì§€ëŠ” í•¨ìˆ˜ $p(x)$ê°€ ìˆë‹¤ê³  ê°€ì •í•´ë³´ì.</p>

\[p(x) = f(x)g(x)\ \ \ (0)\]

<p>ìš°ë³€ì˜ ë‘ í•­ì„ ë¶„ë¦¬í•˜ê¸° ì „ì— ë„í•¨ìˆ˜ì˜ ì •ì˜ë¥¼ ì´ìš©í•´ ë„í•¨ìˆ˜ $pâ€™(x)$ë¥¼ êµ¬í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.</p>

\[p'(x) = \lim_{h -&gt; 0} \frac{p(x+h) - p(x)}{h}\ \ \ (1)\]

<p>ì´ì œ ë‹¤ì‹œ $p(x+h),\  p(x)$ì— $f(x)g(x)$ë¥¼ ëŒ€ì…í•´ë³´ì.</p>

\[p'(x) = \lim_{h -&gt; 0} \frac{p(x+h) - p(x)}{h} = \lim_{h-&gt;0} \frac{f(x+h)g(x+h) - f(x)(g(x)}{h}\ \ \ (2)\]

<p>ì´ ì§€ì ì—ì„œ ìš°ë¦¬ê°€ ë­˜í•˜ë ¤ê³  ì§€ê¸ˆ ì´ë ‡ê²Œ ìˆ˜ì‹ì„ ì „ê°œí•˜ê³  ìˆëŠ”ì§€ ìƒê¸°í•  í•„ìš”ê°€ ìˆë‹¤. ìš°ë¦¬ëŠ” ê³±ì˜ í˜•íƒœë¥¼ ê°–ëŠ” í•¨ìˆ˜ì˜ <code class="language-plaintext highlighter-rouge">ë„í•¨ìˆ˜</code>ë¥¼ êµ¬í•˜ê³  ì‹¶ì€ ê²ƒì´ë‹¤. (1)ë²ˆì²˜ëŸ¼ í•¨ìˆ˜ì— ëŒ€ì…í•˜ëŠ” ì…ë ¥ê°’ì„ ëº€ ê²°ê³¼ê°€ $h$ê°€ ë˜ë„ë¡ ë§ì´ë‹¤. (1)ê³¼ ê°™ì€ ê¼´ì„ ë§Œë“¤ì–´ì£¼ê¸° ìœ„í•´ ì•½ê°„ì˜ íŠ¸ë¦­ì„ ì“¸ í•„ìš”ê°€ ìˆë‹¤. ì‚¬ìš©í•  íŠ¸ë¦­ì€ ëŒ€ìˆ˜í•™ì—ì„œ ì •ë§ ë¹ˆë²ˆí•˜ê²Œ ì‚¬ìš©ë˜ë¯€ë¡œ ì˜ ê¸°ì–µí•˜ê³  ìˆëŠ”ê²Œ ì¢‹ë‹¤. ë°”ë¡œ $A-A = 0$ì´ë¼ëŠ” ê²ƒì„ ì´ìš©í•˜ëŠ” ê²ƒì´ë‹¤. ì´ê²Œ ë¬´ìŠ¨ ë§ì¸ê°€ëŠ” ì•„ë˜ ìˆ˜ì‹ì„ ë³´ë©´ ì•Œ ìˆ˜ ìˆë‹¤.</p>

\[p'(x) = \lim_{h-&gt;0} \frac{f(x+h)g(x+h) - f(x)g(x+h) + f(x)g(x+h) - f(x)g(x)}{h}\ \ \ (3)\]

<p>(3)ë²ˆ ìˆ˜ì‹ì€ (2)ë²ˆ ìˆ˜ì‹ì˜ ë¶„ìì— $- f(x)g(x+h) + f(x)g(x+h)$ë§Œ ì¶”ê°€ëœ í˜•íƒœë‹¤. ë‘í•­ì„ ë”í•˜ë©´ 0ì´ ë˜ê¸° ë•Œë¬¸ì— ì‚¬ì‹¤ (2)ë²ˆê³¼ (3)ë²ˆì€ ê°™ì€ ìˆ˜ì‹ì´ë¼ê³  ë³¼ ìˆ˜ ìˆëŠ” ê²ƒì´ë‹¤. ê·¸ë˜ì„œ ë‘í•­ì„ ì¶”ê°€í•´ë„ ì „í˜€ ë¬¸ì œê°€ ì—†ë‹¤. ì´ì œ ìš°ë¦¬ê°€ ìµìˆ™í•œ ë„í•¨ìˆ˜ ì •ì˜ë¥¼ ë§Œì¡±í•˜ëŠ” í•­ë“¤ì´ ì§ê´€ì ìœ¼ë¡œ ë³´ì¸ë‹¤.</p>

\[p'(x) = f'(x)g(x) + f(x)g'(x)\ \ \ (4)\]

<p>ë”°ë¼ì„œ ìˆ˜ì‹ì„ ì •ë¦¬í•˜ë©´ ê²°êµ­ ê³±ì˜ í˜•íƒœë¥¼ ê°–ëŠ” í•¨ìˆ˜ì˜ ë„í•¨ìˆ˜ëŠ” (4)ë²ˆê³¼ ê°™ì€ ê³µì‹ì„ ê°–ê²Œ ë˜ëŠ” ê²ƒì´ë‹¤. í•œí¸, ê³±ì˜ ë¯¸ë¶„ë²•ì€ (0)ë²ˆ ìˆ˜ì‹ì„ ì§ì‚¬ê°í˜•ì˜ ë„“ì´ë¼ê³  ê°„ì£¼í•˜ë©´ ì¢€ ë” ì§ê´€ì ìœ¼ë¡œ ì´í•´í•  ìˆ˜ ìˆë‹¤. ë”°ë¼ì„œ ì§ì‚¬ê°í˜• ë„“ì´ $p(x)$ì— ëŒ€í•œ ë„í•¨ìˆ˜ $pâ€™(x)$ëŠ” ë„“ì´ì˜ ìˆœê°„ ë³€í™”ìœ¨ë¡œ í•´ì„í•  ìˆ˜ ìˆë‹¤.</p>

<p align="center">
<img src="/assets/images/optimization/product_rule_rectangle.jpeg" alt="ê³±ì˜ ë¯¸ë¶„ì˜ ì§ê´€ì  í•´ì„" class="align-center image-caption" width="50%&quot;, height=&quot;30%" />
<strong><em><a href="https://blog.naver.com/sodong212/220924875183">ê³±ì˜ ë¯¸ë¶„ì˜ ì§ê´€ì  í•´ì„</a></em></strong>
</p>

<p>ìš°ë³€ì˜ ì™¼ìª½í•­ì´ ì§ì‚¬ê°í˜•ì˜ ì•„ë˜ ë¶€ë¶„ì˜ ì¦ê°€í•˜ëŠ” ì˜ì—­ì´ ë˜ê³ , ìš°ì¸¡í•­ì´ íšŒìƒ‰ì´ ì¹ í•´ì§„ ì˜ì—­ì´ ë˜ëŠ” ê²ƒì´ë‹¤. ê·¸ë ‡ë‹¤ë©´ ê·¸ë¦¼ì˜ ìš°ì¸¡ í•˜ë‹¨ì— ìœ„ì¹˜í•œ ì‘ì€ ì‚¬ê°í˜•ì˜ ë„“ì´ëŠ” ì–´ë–»ê²Œ ì²˜ë¦¬í•´ì¤˜ì•¼ í• ê¹Œ?? ê³±ì˜ ë¯¸ë¶„ ê³µì‹ì—ëŠ” í•´ë‹¹ ì˜ì—­ì„ ë°˜ì˜í•˜ëŠ” í•­ì´ ì „í˜€ ì—†ë‹¤. ê·¸ ì´ìœ ëŠ” ì˜ì—­ì˜ ë„“ì´ê°€ ë„ˆë¬´ë‚˜ ì‘ì•„ì„œ ê·¼ì‚¬ì¹˜ë¡œ ê°„ì£¼í•˜ê³  ë¬´ì‹œí•´ë„ ë  ì •ë„ë¼ì„œ ê·¸ë ‡ë‹¤. í•´ë‹¹ ì‚¬ê°í˜•ì˜ ê°€ë¡œ ê¸¸ì´ëŠ” $gâ€™(h)$, ì„¸ë¡œ ê¸¸ì´ëŠ” $fâ€™(h)$ê°€ ëœë‹¤. ë„í•¨ìˆ˜ì˜ ì •ì˜ë¥¼ ë‹¤ì‹œ ë– ì˜¬ë ¤ë³´ë©´ $h$ëŠ” 0ì˜ ê·¹í•œìœ¼ë¡œ ê·¼ì‚¬í•˜ê¸° ë•Œë¬¸ì— ë‘ í•­ì˜ ê³±ì¸ ì˜ì—­ì˜ ë„“ì´ ì—­ì‹œ 0ì— ë§¤ìš° ê·¼ì ‘í•˜ê²Œ ëœë‹¤. ë”°ë¼ì„œ ê³ ë ¤í•  í•„ìš” ì—†ì´ ë¬´ì‹œí•´ë„ ëœë‹¤.</p>

<h3 id="quotient-rule"><code class="language-plaintext highlighter-rouge">â—Â Quotient Rule</code></h3>

<p>ëª«ì˜ ë¯¸ë¶„ì€ ê³±ì˜ ë¯¸ë¶„ ê³µì‹ì„ ì´ìš©í•˜ê³  ë‚˜ì„œ ë‚¨ì€ ì§€ì €ë¶„í•œ ìˆ˜ì‹ë§Œ ì˜ ì •ë¦¬í•˜ë©´ ëœë‹¤. ë‹¤ìŒê³¼ ê°™ì´ ë¶„ìˆ˜ ê¼´ í˜•íƒœì˜ í•¨ìˆ˜ $q(x)$ê°€ ìˆë‹¤ê³  ê°€ì •í•´ë³´ì.</p>

\[q(x) = \frac{f(x)}{g(x)} \ \ \ (0)\]

<p>ê³±ì˜ ë¯¸ë¶„ ê³µì‹ì„ ì´ìš©í•˜ê¸° ìœ„í•´ ë¶„ìˆ˜ ê¼´ì˜ í•¨ìˆ˜ë¥¼ ë‹¤ì‹œ ê³±ì˜ í˜•íƒœë¡œ ë°”ê¿”ë³´ì.</p>

\[f(x) = q(x)g(x) \ \ \ (1)\]

<p>ì´ì œ ê³µì‹ì„ ì´ìš©í•´ ì¢Œë³€ì— ëŒ€í•œ ë„í•¨ìˆ˜ë¥¼ êµ¬í•´ë³´ì.</p>

\[f'(x) = q'(x)g(x) + q(x)g'(x)\ \ \  (2)\]

<p>ìš°ë¦¬ê°€ êµ¬í•˜ê³  ì‹¶ì€ ê²ƒì€ ë¶„ìˆ˜ ê¼´ì„ ê°–ëŠ” í•¨ìˆ˜ $q(x)$ì˜ ë„í•¨ìˆ˜ $qâ€™(x)$ì´ë‹¤. ë”°ë¼ì„œ (2)ë²ˆ ìˆ˜ì‹ì„ $qâ€™(x)$ì— ëŒ€í•´ì„œ ì •ë¦¬í•´ì•¼ í•œë‹¤.</p>

\[q'(x) = \frac{f'(x) - q(x)g'(x)}{g(x)}\ \ \ (3)\]

<p>(3)ë²ˆ ìˆ˜ì‹ì„ ì˜ˆì˜ê²Œ ì •ë¦¬í•˜ê¸° ìœ„í•´ ê³µí†µ ë¶„ëª¨ $\frac{1}{g(x)}$ë¥¼ ì•ìœ¼ë¡œ ë¹¼ì£¼ê³ , $q(x)$ì— (0)ë²ˆ ìˆ˜ì‹ì„ ëŒ€ì…í•´ ì •ë¦¬í•˜ë©´ ëª«ì˜ ë¯¸ë¶„ë²• ê³µì‹, (5)ë²ˆì´ ë„ì¶œëœë‹¤.</p>

\[q'(x) = \frac{1}{g(x)}(\frac{f'(x)g(x)-f(x)g'(x)}{g(x)})\ \ \ (4) \\
q'(x) = \frac{f'(x)g(x)-f(x)g'(x)}{g(x)^2}\ \ \ (5)\]

<p>ì´ë ‡ê²Œ ê³±ì˜ ë¯¸ë¶„ë²•, ëª«ì˜ ë¯¸ë¶„ë²•ì˜ ê³µì‹ì´ ìœ ë„ë˜ëŠ” ê³¼ì •ì„ ëª¨ë‘ ì‚´í´ë³´ì•˜ë‹¤.</p>]]></content><author><name>qcqced</name><email>qcqced123@gmail.com</email></author><category term="Optimization Theory" /><category term="Calculus" /><category term="Product Rule" /><category term="Quotient Rule" /><summary type="html"><![CDATA[ê³±ì˜ ë¯¸ë¶„, ëª«ì˜ ë¯¸ë¶„ ê³µì‹ ìœ ë„]]></summary></entry><entry><title type="html">ğŸ“ˆÂ Chain Rule: í•©ì„±í•¨ìˆ˜ ë¯¸ë¶„ë²•</title><link href="http://localhost:4000/optimization-theory/chain-rule" rel="alternate" type="text/html" title="ğŸ“ˆÂ Chain Rule: í•©ì„±í•¨ìˆ˜ ë¯¸ë¶„ë²•" /><published>2024-01-08T00:00:00+09:00</published><updated>2024-01-08T23:00:00+09:00</updated><id>http://localhost:4000/optimization-theory/chain_rule</id><content type="html" xml:base="http://localhost:4000/optimization-theory/chain-rule"><![CDATA[<p><code class="language-plaintext highlighter-rouge">Chain Rule</code> ì´ë¼ê³  ë¶ˆë¦¬ê¸°ë„ í•˜ëŠ” í•©ì„±í•¨ìˆ˜ ë¯¸ë¶„ë²•ì€ ë¯¸ì ë¶„í•™ì—ì„œ íŠ¹íˆë‚˜ ì¤‘ìš”í•œ ê°œë… ì¤‘ í•˜ë‚˜ë‹¤. ê·¼ë˜ì—ëŠ” ì‹ ê²½ë§ì„ í™œìš©í•œ ë”¥ëŸ¬ë‹ì´ ì£¼ëª©ë°›ìœ¼ë©´ì„œ ê·¸ ì¤‘ìš”ì„±ì´ ë”ìš± ë¶€ê°ë˜ê³  ìˆë‹¤. ì‹ ê²½ë§ ëª¨ë¸ì€ ì‰½ê²Œ ìƒê°í•˜ë©´ ì •ë§ ë§ì€ 1ì°¨í•¨ìˆ˜ì™€ ì—¬ëŸ¬ í™œì„±í•¨ìˆ˜ë¥¼ í•©ì„±í•œ ê²ƒê³¼ ê°™ê¸° ë•Œë¬¸ì´ë‹¤. ë”°ë¼ì„œ ì˜¤ì°¨ ì—­ì „ì„ í†µí•´ ê°€ì¤‘ì¹˜ë¥¼ ìµœì í™” í•˜ëŠ” ê³¼ì •ì„ ì •í™•íˆ ì´í•´í•˜ë ¤ë©´ í•©ì„±í•¨ìˆ˜ ë¯¸ë¶„ë²•ì— ëŒ€í•œ ì´í•´ëŠ” í•„ìˆ˜ì ì´ë‹¤.</p>

<h3 id="concept"><code class="language-plaintext highlighter-rouge">ğŸ’¡Â Concept</code></h3>

<p>í•©ì„±í•¨ìˆ˜ ë¯¸ë¶„ì„ ì´í•´í•˜ê¸° ì „ì— ë¨¼ì € ë„í•¨ìˆ˜ í‘œê¸°ë²•ì— ëŒ€í•´ì„œ ìµìˆ™í•´ì§ˆ í•„ìš”ê°€ ìˆë‹¤. ë„í•¨ìˆ˜ í‘œê¸°ë²•ì€ í¬ê²Œ ë‰´í„´ í‘œê¸°ë²•ê³¼ ë¼ì´í”„ë‹ˆì¸  í‘œê¸°ë²•ìœ¼ë¡œ ë‚˜ë‰œë‹¤. ì•„ë˜ í‘œê¸°ëœ ìˆ˜ì‹ì„ ë³´ì. (1)ë²ˆ ìˆ˜ì‹ì´ ë‰´í„´ í‘œê¸°ë²•, (2)ë²ˆì´ ë¼ì´í”„ë‹ˆì¸ ì˜ í‘œê¸°ë²•ì´ë‹¤.</p>

\[y'= 2x\ \ \ \ (1) \\
f'(x) = 2x\ \ \ (2)\]

<p>ì´ì œ ì•„ë˜ì™€ ê°™ì€ í•©ì„±í•¨ìˆ˜ê°€ ìˆë‹¤ê³  ê°€ì •í•´ë³´ì. ë„í•¨ìˆ˜ì˜ ì •ì˜ì— ë”°ë¼ì„œ í•´ë‹¹ í•¨ìˆ˜ì— ëŒ€í•œ ë„í•¨ìˆ˜ëŠ” ë‹¤ìŒê³¼ ê°™ì´ í‘œí˜„í•  ìˆ˜ ìˆì„ ê²ƒì´ë‹¤.</p>

\[y = g(f(x))\ \ \ (3) \\
y' = {g(f(x))}' = \frac{dy}{dx}\ \ \ (4)\]

<p>ìœ„ì— ì‘ì„±í•œ ìˆ˜ì‹ë“¤ì€ ì ì‹œ ìŠê³  ì´ì œ $fâ€™(x)$ ë¥¼ ë¨¼ì € ìƒê°í•´ë³´ì. $f(x)$ì— ëŒ€í•œ ë„í•¨ìˆ˜ $fâ€™(x)$ëŠ” ë‹¤ìŒê³¼ ê°™ì´ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆë‹¤.</p>

\[u = f(x) \\
u' = f'(x) = \frac{du}{dx} \\\]

<p>ê·¸ë ‡ë‹¤ë©´ í•¨ìˆ˜ $g$ì— ëŒ€í•œ ë„í•¨ìˆ˜ëŠ” ì–´ë–»ê²Œ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆì„ê¹Œ?? ë‹µì€ ë°”ë¡œ ì¹˜í™˜ì„ ì´ìš©í•˜ëŠ” ê²ƒì´ë‹¤. ìœ„ì—ì„œ ìš°ë¦¬ëŠ” $f(x)$ê°€ $u$ì™€ ê°™ë‹¤ê³  ì •ì˜í–ˆë‹¤. ì´ê²ƒì„ ì´ìš©í•´ ë„í•¨ìˆ˜ $gâ€™$ëŠ” ë‹¤ìŒê³¼ ê°™ì´ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆê² ë‹¤.</p>

\[y' = g'(u) = \frac{dy}{du} \\\]

<p>ëˆˆì¹˜ê°€ ë¹ ë¥´ë‹¤ë©´ ë²Œì¨ ì™œ í•©ì„±í•¨ìˆ˜ ë¯¸ë¶„ë²•ì„ <code class="language-plaintext highlighter-rouge">Chain Rule</code>ì´ë¼ê³  ë¶€ë¥´ëŠ”ì§€ ê¹¨ë‹«ê²Œ ë˜ì—ˆì„ ê²ƒì´ë‹¤. (4)ë²ˆ ìˆ˜ì‹ì˜ ìš°ë³€ì€ ì‚¬ì‹¤ ë¶„ìì™€ ë¶„ëª¨ì— ìœ„ì¹˜í•œ $du$ê°€ ì•½ë¶„ëœ ê¼´ì´ë¼ê³  ë³¼ ìˆ˜ ìˆë‹¤. ì´ê²ƒì„ ë‰´í„´ í‘œê¸°ë²•ìœ¼ë¡œ ë‚˜íƒ€ë‚´ë©´ ì•„ë˜ì™€ ê°™ë‹¤.</p>

\[y' = \frac{dy}{du}â€¢\frac{du}{dx}\]

<p>ë‰´í„´ í‘œê¸°ë²•ì€ ì§ê´€ì ì´ì§€ ì•Šê¸° ë•Œë¬¸ì— ë¼ì´í”„ë‹ˆì¸  í‘œê¸°ë²•ìœ¼ë¡œ ë‹¤ì‹œ ì •ë¦¬í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.</p>

\[y' = g'(f(x))â€¢f'(x)\]

<p>ì•„ë§ˆ ê³ ë“±í•™êµ ë•ŒëŠ” í•©ì„±í•¨ìˆ˜ ë¯¸ë¶„ ê³µì‹ì„ ê²‰ë¯¸ë¶„â€¢ì†ë¯¸ë¶„ì´ë¼ëŠ” ëª…ì¹­ìœ¼ë¡œ ì²˜ìŒ ì ‘í–ˆì„ ê²ƒì´ë‹¤. ì•ˆì— ê°ì‹¸ì ¸ ìˆëŠ” í•¨ìˆ˜ë¥¼ ë¯¸ë¶„í•´ì„œ ë°–ìœ¼ë¡œ ë¹¼ë‚¸ë‹¤ í•´ì„œ ì†ë¯¸ë¶„ì´ë¼ ë¶€ë¥´ê³ , ë‹¤ì‹œ ì†ì€ ëƒ…ë‘ê³  ë°–ì˜ ë‘˜ëŸ¬ì ¸ ìˆëŠ” í•¨ìˆ˜ë§Œ ë¯¸ë¶„í•œë‹¤í•´ì„œ ê²‰ë¯¸ë¶„ì´ë¼ ì¹­í•˜ëŠ”ë°, ì´ë ‡ê²Œ ë‹¨ìˆœí•˜ê²Œ ì™¸ìš°ê¸°ë³´ë‹¤ëŠ” ê³µì‹ì´ ë„ì¶œë˜ëŠ” íë¦„ì„ ì´í•´í•˜ëŠ” ê²ƒì´ í›¨ì”¬ ì˜¤ë˜ ê¸°ì–µì— ë‚¨ëŠ”ë‹¤. ë¶€ë„ëŸ½ì§€ë§Œ í•„ìê°€ ë°”ë¡œ ê·¸ëŸ¬í–ˆë‹¤. ëŒ€í•™êµì— ì…í•™í•˜ê³  ìˆ˜ë„ ì—†ì´ í•©ì„±í•¨ìˆ˜ ë¯¸ë¶„ì„ í•´ì•¼ í–ˆì§€ë§Œ ì‚¬ìš©í•  ë•Œë§ˆë‹¤ ê¹Œë¨¹ì–´ì„œ ì¸í„°ë„·ì„ ì°¾ì•„ë³´ê±°ë‚˜ êµê³¼ì„œë¥¼ ë’¤ì ë’¤ì  í–ˆë˜ ê¸°ì–µì´ ìˆë‹¤. ì´ê¸€ì„ ì½ëŠ” ë…ìë“¤ì€ ë‚˜ì™€ ê°™ì€ ì‹¤ìˆ˜ë¥¼ ë°˜ë³µí•˜ì§€ ì•Šê¸°ë¥¼ ë°”ë¼ë©° í¬ìŠ¤íŒ…ì„ ë§ˆì¹œë‹¤.</p>]]></content><author><name>qcqced</name><email>qcqced123@gmail.com</email></author><category term="Optimization Theory" /><category term="Calculus" /><summary type="html"><![CDATA[í•©ì„±í•¨ìˆ˜ ë¯¸ë¶„ë²• ê³µì‹ ìœ ë„]]></summary></entry></feed>