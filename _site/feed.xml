<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2023-08-17T17:16:51+09:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">AI/Business Study Log</title><subtitle>NLP, Marketing</subtitle><author><name>qcqced</name><email>qcqced123@gmail.com</email></author><entry><title type="html">👨‍💻🐍 [Python] Object Attribute &amp;amp; Assertion Function</title><link href="http://localhost:4000/python/attribute_function" rel="alternate" type="text/html" title="👨‍💻🐍 [Python] Object Attribute &amp;amp; Assertion Function" /><published>2023-08-17T00:00:00+09:00</published><updated>2023-08-18T02:00:00+09:00</updated><id>http://localhost:4000/python/python_object_attribute_func</id><content type="html" xml:base="http://localhost:4000/python/attribute_function"><![CDATA[<h3 id="-attribute-function"><code class="language-plaintext highlighter-rouge">🧧 Attribute Function</code></h3>

<p>이번 포스팅은 <code class="language-plaintext highlighter-rouge">Python</code> 코드를 작성하면서 객체와 내부 메서드에 관련한 처리가 필요할 때 가장 많이 사용하게 되는 <code class="language-plaintext highlighter-rouge">getattr</code>, <code class="language-plaintext highlighter-rouge">setattr</code> , <code class="language-plaintext highlighter-rouge">delattr</code> , <code class="language-plaintext highlighter-rouge">hasttr</code> 함수들의 사용법에 대해 다뤄보려 한다. 특히 <code class="language-plaintext highlighter-rouge">getattr</code>, <code class="language-plaintext highlighter-rouge">setattr</code> 의 경우 머신러닝 혹은 딥러닝 관련 코드를 읽다가 심심치 않게 찾아볼 수 있다. 모델의 <code class="language-plaintext highlighter-rouge">hyper-parameter</code>를 튜닝하거나 기타 실험을 할 때 정의한 객체의 변수 혹은 메서드에 쉽고 간결하게 접근하기 위해 사용되고 있기 때문이다.</p>

<h4 id="-getattr"><strong><code class="language-plaintext highlighter-rouge">📌 getattr</code></strong></h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="s">""" getattr(object, attribute_name, default) """</span>

<span class="k">class</span> <span class="nc">CFG</span><span class="p">:</span>
    <span class="s">"""--------[Common]--------"""</span>
    <span class="n">wandb</span><span class="p">,</span> <span class="n">train</span><span class="p">,</span> <span class="n">competition</span><span class="p">,</span> <span class="n">seed</span><span class="p">,</span> <span class="n">cfg_name</span> <span class="o">=</span> <span class="bp">True</span><span class="p">,</span> <span class="bp">True</span><span class="p">,</span> <span class="s">'UPPPM'</span><span class="p">,</span> <span class="mi">42</span><span class="p">,</span> <span class="s">'CFG'</span>
    <span class="n">device</span><span class="p">,</span> <span class="n">gpu_id</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">device</span><span class="p">(</span><span class="s">'cuda'</span> <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s">'cpu'</span><span class="p">),</span> <span class="mi">0</span>
    <span class="n">num_workers</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="s">""" Mixed Precision, Gradient Check Point """</span>
    <span class="n">amp_scaler</span> <span class="o">=</span> <span class="bp">True</span>
    <span class="n">gradient_checkpoint</span> <span class="o">=</span> <span class="bp">True</span> <span class="c1"># save parameter
</span>    <span class="n">output_dir</span> <span class="o">=</span> <span class="s">'./output/'</span>
    <span class="s">""" Clipping Grad Norm, Gradient Accumulation """</span>
    <span class="n">clipping_grad</span> <span class="o">=</span> <span class="bp">True</span> <span class="c1"># clip_grad_norm
</span>    <span class="n">n_gradient_accumulation_steps</span> <span class="o">=</span> <span class="mi">1</span> <span class="c1"># Gradient Accumulation
</span>    <span class="n">max_grad_norm</span> <span class="o">=</span> <span class="n">n_gradient_accumulation_steps</span> <span class="o">*</span> <span class="mi">1000</span>
    <span class="s">""" Model """</span>
    <span class="n">model_name</span> <span class="o">=</span> <span class="s">'microsoft/deberta-v3-large'</span>
    <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>
<span class="c1">#    pooling = 'attention'
</span>    <span class="n">max_len</span> <span class="o">=</span> <span class="mi">512</span>
    <span class="s">""" CV, Epoch, Batch Size """</span>
    <span class="n">n_folds</span> <span class="o">=</span> <span class="mi">4</span>
    <span class="n">epochs</span> <span class="o">=</span> <span class="mi">180</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="mi">64</span>
</code></pre></div></div>

<p>위의 객체는 실제 제가 캐글 대회를 준비하면서 사용했던 <a href="http://config.py"><code class="language-plaintext highlighter-rouge">config.py</code></a> 를 가져왔다.</p>

<p><code class="language-plaintext highlighter-rouge">getattr(object: object, attribute_name: str, default: Any)</code> 함수는 사용자가 지정한 객체에 매개변수로 전달한 <code class="language-plaintext highlighter-rouge">attribute</code>가 존재하는지 여부를 판단하고, 존재한다면 해당 <code class="language-plaintext highlighter-rouge">attribute</code>의 <code class="language-plaintext highlighter-rouge">value</code>를 반환한다. 한편 존재하지 않으면 <code class="language-plaintext highlighter-rouge">default</code>로 세팅한 값을 반환한다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">getattr</span><span class="p">(</span><span class="n">CFG</span><span class="p">,</span> <span class="s">'epochs'</span><span class="p">,</span> <span class="s">"This Attribute doesn't find"</span><span class="p">)</span>
<span class="nb">getattr</span><span class="p">(</span><span class="n">CFG</span><span class="p">,</span> <span class="s">'MPL'</span><span class="p">,</span> <span class="s">"This Attribute doesn't find"</span><span class="p">)</span>
<span class="o">---------------</span> <span class="n">Result</span> <span class="o">---------------</span> 
<span class="mi">180</span>
<span class="n">This</span> <span class="n">Attribute</span> <span class="n">doesn</span><span class="s">'t find
</span></code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">if-else</code> 구문보다 훨씬 간결하게 객체의 메서드에 접근하는 것이 가능해졌으며, <code class="language-plaintext highlighter-rouge">default</code> 값을 매개변수로 전달 받기 때문에 클라이언트가 지정한 <code class="language-plaintext highlighter-rouge">attribute</code> 가 객체 내부에 없어도 <code class="language-plaintext highlighter-rouge">AttributeError</code> 를 발생시키지 않아 예외 처리를 별도로 지정할 필요가 사라져 코드 가독성 및 유지보수에 용이하다는 장점이 있다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Exmple</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">test1</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">test2</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">def</span> <span class="nf">A</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">print</span><span class="p">(</span><span class="s">"A"</span><span class="p">)</span>  
    <span class="k">def</span> <span class="nf">B</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">print</span><span class="p">(</span><span class="s">"B"</span><span class="p">)</span>  
    <span class="k">def</span> <span class="nf">C</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">print</span><span class="p">(</span><span class="s">"C"</span><span class="p">)</span>

<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="s">'__main__'</span><span class="p">:</span>
    <span class="n">exmple</span> <span class="o">=</span> <span class="n">Exmple</span><span class="p">()</span>
    <span class="n">class_list</span> <span class="o">=</span> <span class="p">[</span><span class="s">'A'</span><span class="p">,</span><span class="s">'B'</span><span class="p">,</span><span class="s">'C'</span><span class="p">]</span>

    <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">class_list</span><span class="p">:</span>
        <span class="nb">getattr</span><span class="p">(</span><span class="n">exmple</span><span class="p">,</span> <span class="n">c</span><span class="p">)()</span>
</code></pre></div></div>

<p>한편 <code class="language-plaintext highlighter-rouge">getattr()</code> 뒤에 괄호를 하나 더 붙여서 사용하기도(머신러닝, 딥러닝 훈련 루프 코드에 종종 보임) 하는데,  해당 괄호는 지정 <code class="language-plaintext highlighter-rouge">attribute</code> 의 호출에 필요한 매개변수를 전달하기 위한 용도로 쓰인다. 이번 예시의 객체 내부 메서드들은 호출에 필요한 매개변수가 정의되어 있지 않기 때문에 괄호 안을 비워뒀다.</p>

<h4 id="️-setattr"><strong><code class="language-plaintext highlighter-rouge">✂️ setattr</code></strong></h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="s">""" setattr(object, attribute_name, value) """</span>

<span class="k">class</span> <span class="nc">CFG</span><span class="p">:</span>
    <span class="s">"""--------[Common]--------"""</span>
    <span class="n">wandb</span><span class="p">,</span> <span class="n">train</span><span class="p">,</span> <span class="n">competition</span><span class="p">,</span> <span class="n">seed</span><span class="p">,</span> <span class="n">cfg_name</span> <span class="o">=</span> <span class="bp">True</span><span class="p">,</span> <span class="bp">True</span><span class="p">,</span> <span class="s">'UPPPM'</span><span class="p">,</span> <span class="mi">42</span><span class="p">,</span> <span class="s">'CFG'</span>
    <span class="n">device</span><span class="p">,</span> <span class="n">gpu_id</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">device</span><span class="p">(</span><span class="s">'cuda'</span> <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s">'cpu'</span><span class="p">),</span> <span class="mi">0</span>
    <span class="n">num_workers</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="s">""" Mixed Precision, Gradient Check Point """</span>
    <span class="n">amp_scaler</span> <span class="o">=</span> <span class="bp">True</span>
    <span class="n">gradient_checkpoint</span> <span class="o">=</span> <span class="bp">True</span> <span class="c1"># save parameter
</span>    <span class="n">output_dir</span> <span class="o">=</span> <span class="s">'./output/'</span>
    <span class="s">""" Clipping Grad Norm, Gradient Accumulation """</span>
    <span class="n">clipping_grad</span> <span class="o">=</span> <span class="bp">True</span> <span class="c1"># clip_grad_norm
</span>    <span class="n">n_gradient_accumulation_steps</span> <span class="o">=</span> <span class="mi">1</span> <span class="c1"># Gradient Accumulation
</span>    <span class="n">max_grad_norm</span> <span class="o">=</span> <span class="n">n_gradient_accumulation_steps</span> <span class="o">*</span> <span class="mi">1000</span>
    <span class="s">""" Model """</span>
    <span class="n">model_name</span> <span class="o">=</span> <span class="s">'microsoft/deberta-v3-large'</span>
    <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>
<span class="c1">#    pooling = 'attention'
</span>    <span class="n">max_len</span> <span class="o">=</span> <span class="mi">512</span>
    <span class="s">""" CV, Epoch, Batch Size """</span>
    <span class="n">n_folds</span> <span class="o">=</span> <span class="mi">4</span>
    <span class="n">epochs</span> <span class="o">=</span> <span class="mi">180</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="mi">64</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">setattr(object: object, attribute_name: str, value: Any)</code> 는 지정 객체의 지정 메서드 혹은 변수에 접근하고 제어하는 용도로 사용하는 함수다. 지정 객체 단위로 접근 가능하기 때문에 모델을 튜닝할 때 정말 많이 사용하게 된다. <code class="language-plaintext highlighter-rouge">setattr()</code> 를 활용해 상황에 맞는 파라미터를 모델에 주입하고 해당 <code class="language-plaintext highlighter-rouge">config</code>를 <code class="language-plaintext highlighter-rouge">json</code> 혹은 <code class="language-plaintext highlighter-rouge">yaml</code> 형식으로 저장해두면 모델의 버전별 파라미터 값을 효율적으로 관리할 수 있으니 기억해두자.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">CFG</span><span class="p">.</span><span class="n">wandb</span>
<span class="nb">setattr</span><span class="p">(</span><span class="n">CFG</span><span class="p">,</span> <span class="s">'wandb'</span><span class="p">,</span> <span class="bp">False</span><span class="p">)</span>
<span class="n">CFG</span><span class="p">.</span><span class="n">wandb</span>
<span class="nb">setattr</span><span class="p">(</span><span class="n">CFG</span><span class="p">,</span> <span class="s">'wandb'</span><span class="p">,</span> <span class="bp">True</span><span class="p">)</span>
<span class="n">CFG</span><span class="p">.</span><span class="n">wandb</span>

<span class="o">---------------</span> <span class="n">Result</span> <span class="o">---------------</span> 
<span class="bp">True</span>
<span class="bp">False</span>
<span class="bp">True</span>
</code></pre></div></div>

<h4 id="-hasattr"><strong><code class="language-plaintext highlighter-rouge">📌 hasattr</code></strong></h4>

<p><code class="language-plaintext highlighter-rouge">hasattr(object, attribute_name)</code> 는 지정 객체에 매개변수로 전달한 <code class="language-plaintext highlighter-rouge">attribute</code> 가 존재하면 <code class="language-plaintext highlighter-rouge">True</code>, 없다면 <code class="language-plaintext highlighter-rouge">False</code> 를 반환한다. 사용법은 <code class="language-plaintext highlighter-rouge">getattr()</code> 와 매우 유사하기 때문에 생략한다.</p>

<h4 id="️-delattr"><strong><code class="language-plaintext highlighter-rouge">✏️ delattr</code></strong></h4>

<p><code class="language-plaintext highlighter-rouge">delattr(object, attribute_name)</code> 는 지정 객체에 매개변수로 전달한 <code class="language-plaintext highlighter-rouge">attribute</code>를 객체 내부에서 삭제하는 역할을 한다. 사용 예시는 아래와 같다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">delattr</span><span class="p">(</span><span class="n">CFG</span><span class="p">,</span> <span class="s">'epochs'</span><span class="p">)</span>
<span class="nb">hasattr</span><span class="p">(</span><span class="n">CFG</span><span class="p">,</span> <span class="s">'epochs'</span><span class="p">)</span>

<span class="o">---------------</span> <span class="n">Result</span> <span class="o">---------------</span> 
<span class="bp">False</span>
</code></pre></div></div>

<p>한편, 모듈(ex: config,py, model.py, model_utils.py 등)도 객체로 간주되기 때문에 위에서 살펴본 4가지 function은 모듈 레벨에서도 동일하게 사용할 수 있다.</p>

<h3 id="️-assertion"><code class="language-plaintext highlighter-rouge">⚠️ Assertion</code></h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">assert</span> <span class="n">조건</span><span class="p">,</span> <span class="n">메세지</span> 
</code></pre></div></div>

<p>조건이 True이면 아무런 일이 일어나지 않는다. 하지만 조건이 False이면 AssertionError가 발생하고 지정한 메세지가 출력된다. 메세지를 지정하지 않았다면 <code class="language-plaintext highlighter-rouge">AssertionError</code>가 동일하게 발생하지만 구체적인 에러 명시란은 비워진 채로 로그가 출력된다.</p>

<p><code class="language-plaintext highlighter-rouge">assert</code>는 코드의 오류를 찾는 데 유용하다. 또한 코드의 의도를 명확하게 표현하는 데에도 유용하다. 예를 들어, 변수의 값이 특정 조건을 만족해야 한다는 것을 <code class="language-plaintext highlighter-rouge">assert</code>를 사용해 표현할 수 있다.</p>

<p><code class="language-plaintext highlighter-rouge">assert</code>는 에러 로그를 반환하면서 개발자가 프로그램을 만드는 과정에 관여한다. 원하는 조건의 변수 값을 보증받을 때까지 <code class="language-plaintext highlighter-rouge">assert</code>로 테스트 할 수 있다. 이는 데이터 유효성 검사처럼 단순히 에러를 찾는것이 아니라 값을 보증하기 위해 사용된다. 예를 들어 함수의 입력 값이 어떤 조건의 참임을 보증하기 위해 사용할 수 있고 함수의 반환 값이 어떤 조건에 만족하도록 만들 수 있다. 혹은 변수 값이 변하는 과정에서 특정 부분은 반드시 어떤 영역에 속하는 것을 보증하기 위해 가정 설정문을 통해 확인 할 수도 있다. <code class="language-plaintext highlighter-rouge">assert</code>는 실수를 가정해 값을 보증하는 방식으로 코딩 하기 때문에 <code class="language-plaintext highlighter-rouge">'방어적 프로그래밍'</code>에 속한다. 방어적 프로그래밍에 대한 자세한 내용은 다음 포스트에서 살펴보도록 하자.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Python assert 데이터 유효성 검사 예시
</span><span class="k">class</span> <span class="nc">DeBERTa</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,):</span>
    <span class="p">...</span><span class="n">중략</span><span class="p">...</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">mask</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">):</span>
        <span class="k">assert</span> <span class="n">inputs</span><span class="p">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">3</span><span class="p">,</span> <span class="sa">f</span><span class="s">'Expected (batch, sequence, vocab_size) got </span><span class="si">{</span><span class="n">inputs</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="s">'</span>
    <span class="p">...</span><span class="n">중략</span><span class="p">...</span>
</code></pre></div></div>

<p>위의 코드는 필자가 논문을 보고 따라 구현한 <code class="language-plaintext highlighter-rouge">DeBERTa</code> 모델 최상위 객체의 코드 일부분이다. 최상위 객체는 모델의 입력 임베딩 층과 위치 임베딩 층을 정의해줘야 하기 때문에 반드시 입력값을 미리 정해진 차원 형식에 맞게 객체의 매개 변수로 넘겨줘야 한다. 지정 형식에서 벗어난 텐서는 입력으로 사용될 수 없게 만들기 위해 객체의 <code class="language-plaintext highlighter-rouge">forward</code> 메서드 시작부분에 <code class="language-plaintext highlighter-rouge">assert</code> 함수를 두어 데이터 유효성 검사를 하도록 구현했다. 지정된 차원 형태에 맞지 않는 데이터를 입력하게 되면 <code class="language-plaintext highlighter-rouge">AssertionError</code>와 함께 필자가 지정한 에러 메세지를 반환 받게 될 것이다.</p>

<p>한편 <code class="language-plaintext highlighter-rouge">AssertionError</code>는 프로그래머가 의도에 맞지 않는 메서드 혹은 객체 사용을 막기 위해 선제적으로 대응한 것이라고 볼 수 있다. 이는 프로그래머가 만든 규칙에 해당할 뿐, 실제 파이썬이나 컴퓨터 내부 동작 문법에 틀렸다는 것을 의미하는 것은 아니다.</p>]]></content><author><name>qcqced</name><email>qcqced123@gmail.com</email></author><category term="Python" /><category term="Python" /><category term="Object" /><category term="Attribute" /><category term="Assertion" /><category term="ML" /><category term="Deep Learning" /><summary type="html"><![CDATA[getattr, setattr, delattr, hasattr, Assertion 사용방법]]></summary></entry><entry><title type="html">🔥 Pytorch Tensor Indexing 자주 사용하는 메서드 모음집</title><link href="http://localhost:4000/framework-library/torch-indexing-function" rel="alternate" type="text/html" title="🔥 Pytorch Tensor Indexing 자주 사용하는 메서드 모음집" /><published>2023-08-04T00:00:00+09:00</published><updated>2023-08-05T02:00:00+09:00</updated><id>http://localhost:4000/framework-library/Pytorch-Tensor-Indexing-Function</id><content type="html" xml:base="http://localhost:4000/framework-library/torch-indexing-function"><![CDATA[<p>파이토치에서 필자가 자주 사용하는 텐서 인덱싱 관련 메서드의 사용법 및 사용 예시를 한방에 정리한 포스트다. 메서드 하나당 하나의 포스트로 만들기에는 너무 길이가 짧다 생각해 한 페이지에 모두 넣게 되었다. 지속적으로 업데이트 될 예정이다. 또한 텐서 인덱싱 말고도 다른 주제로도 관련 메서드를 정리해 올릴 예정이니 많은 관심 부탁드린다.</p>

<h3 id="torchargmax"><code class="language-plaintext highlighter-rouge">🔎 torch.argmax</code></h3>

<p>입력 텐서에서 가장 큰 값을 갖고 있는 원소의 인덱스를 반환한다. 최대값을 찾을 차원을 지정해줄 수 있다. 아래 예시 코드를 확인해보자.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># torch.argmax params
</span><span class="n">torch</span><span class="p">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

<span class="c1"># torch.argmax example 1
</span><span class="n">test</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">29</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">45</span><span class="p">,</span><span class="mi">22</span><span class="p">,</span><span class="mi">3</span><span class="p">])</span>
<span class="n">torch</span><span class="p">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">test</span><span class="p">)</span>
<span class="n">torch</span><span class="p">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">test2</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="o">&lt;</span><span class="n">Result</span><span class="o">&gt;</span>
<span class="n">tensor</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>

<span class="c1"># torch.argmax example 2
</span><span class="n">test</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
                     <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">]])</span>

<span class="n">torch</span><span class="p">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">test</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="o">&lt;</span><span class="n">Result</span><span class="o">&gt;</span>
<span class="n">tensor</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>

<span class="c1"># torch.argmax example 3
</span><span class="n">test</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
                     <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">]])</span>

<span class="n">torch</span><span class="p">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">test</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">tensor</span><span class="p">([[</span><span class="mi">0</span><span class="p">],</span>
        <span class="p">[</span><span class="mi">2</span><span class="p">]])</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">dim</code> 매개변수에 원하는 차원을 입력하면 해당 차원 뷰에서 가장 큰 원소를 찾아 인덱스 값을 반환해줄 것이다. 이 때 <code class="language-plaintext highlighter-rouge">keepdim=True</code> 로 설정한다면 입력 차원에서 가장 큰 원소의 인덱스를 반환하되 원본 텐서의 차원과 동일한 형태로 출력해준다. <code class="language-plaintext highlighter-rouge">example 2</code> 의 경우 <code class="language-plaintext highlighter-rouge">dim=0</code> 라서 행이 누적된 방향으로 텐서를 바라봐야 한다. 행이 누적된 방향으로 텐서를 보게 되면 <code class="language-plaintext highlighter-rouge">tensor([[0, 1, 1]])</code>이 된다.</p>

<h3 id="torchstack"><code class="language-plaintext highlighter-rouge">📚 torch.stack</code></h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="s">"""
torch.stack
Args:
	tensors(sequence of Tensors): 텐서가 담긴 파이썬 시퀀스 객체
	dim(int): 추가할 차원 방향을 세팅, 기본값은 0
"""</span>
<span class="n">torch</span><span class="p">.</span><span class="n">stack</span><span class="p">(</span><span class="n">tensors</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</code></pre></div></div>

<p>매개변수로 주어진 파이썬 시퀀스 객체(리스트, 튜플)를 사용자가 지정한 새로운 차원에 쌓는 기능을 한다. 매개변수 <code class="language-plaintext highlighter-rouge">tensors</code> 는 텐서가 담긴 파이썬의 시퀀스 객체를 입력으로 받는다. <code class="language-plaintext highlighter-rouge">dim</code> 은 사용자가 텐서 적재를 하고 싶은 새로운 차원을 지정해주면 된다. 기본값은 0차원으로 지정 되어있으며, 텐서의 맨 앞차원이 새롭게 생기게 된다. <code class="language-plaintext highlighter-rouge">torch.stack</code> 은 기계학습, 특히 딥러닝에서 정말 자주 사용되기 때문에 사용법 및 사용상황을 익혀두면 도움이 된다. 예시를 통해 해당 메서드를 어떤 상황에서 어떻게 사용하는지 알아보자.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="s">""" torch.stack example """</span>

<span class="k">class</span> <span class="nc">Projector</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="s">"""
    Making projection matrix(Q, K, V) for each attention head
    When you call this class, it returns projection matrix of each attention head
    For example, if you call this class with 8 heads, it returns 8 set of projection matrices (Q, K, V)
    Args:
        num_heads: number of heads in MHA, default 8
        dim_head: dimension of each attention head, default 64
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span> <span class="n">dim_head</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">64</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Projector</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span> <span class="o">=</span> <span class="n">num_heads</span> <span class="o">*</span> <span class="n">dim_head</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dim_head</span> <span class="o">=</span> <span class="n">dim_head</span>

    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">fc_q</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">dim_head</span><span class="p">)</span>
        <span class="n">fc_k</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">dim_head</span><span class="p">)</span>
        <span class="n">fc_v</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">dim_head</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">fc_q</span><span class="p">,</span> <span class="n">fc_k</span><span class="p">,</span> <span class="n">fc_v</span>

<span class="n">num_heads</span> <span class="o">=</span> <span class="mi">8</span>
<span class="n">dim_head</span> <span class="o">=</span> <span class="mi">64</span>
<span class="n">projector</span> <span class="o">=</span> <span class="n">Projector</span><span class="p">(</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">dim_head</span><span class="p">)</span>  <span class="c1"># init instance
</span><span class="n">projector_list</span> <span class="o">=</span> <span class="p">[</span><span class="nb">list</span><span class="p">(</span><span class="n">projector</span><span class="p">())</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_heads</span><span class="p">)]</span>  <span class="c1"># call instance
</span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">512</span><span class="p">)</span> <span class="c1"># x.shape: [Batch_Size, Sequence_Length, Dim_model]
</span><span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">V</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[],</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_heads</span><span class="p">):</span>
    <span class="n">Q</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">projector_list</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">0</span><span class="p">](</span><span class="n">x</span><span class="p">))</span> <span class="c1"># [10, 512, 64]
</span>    <span class="n">K</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">projector_list</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">1</span><span class="p">](</span><span class="n">x</span><span class="p">))</span> <span class="c1"># [10, 512, 64]
</span>	  <span class="n">V</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">projector_list</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">2</span><span class="p">](</span><span class="n">x</span><span class="p">))</span> <span class="c1"># [10, 512, 64]
</span> 
<span class="n">Q</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">stack</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># Q.shape: [10, 8, 512, 64]
</span><span class="n">K</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">stack</span><span class="p">(</span><span class="n">K</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># K.shape: [10, 8, 512, 64]
</span><span class="n">V</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">stack</span><span class="p">(</span><span class="n">V</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># V.shape: [10, 8, 512, 64]
</span></code></pre></div></div>

<p>위 코드는 <code class="language-plaintext highlighter-rouge">Transformer</code> 의 <code class="language-plaintext highlighter-rouge">Multi-Head Attention</code> 구현체 일부를 발췌해온 것이다. <code class="language-plaintext highlighter-rouge">Multi-Head Attention</code> 은 개별 어텐션 해드별로 행렬 $Q, K, V$를 가져야 한다. 따라서 입력 임베딩을 개별 어텐션 헤드에 <code class="language-plaintext highlighter-rouge">Linear Combination</code> 해줘야 하는데 헤드 개수가 8개나 되기 때문에 개별적으로 <code class="language-plaintext highlighter-rouge">Projection Matrix</code> 를 선언해주는 것은 매우 비효율적이다. 따라서 객체  <code class="language-plaintext highlighter-rouge">Projector</code> 에 행렬 $Q, K, V$에 대한 <code class="language-plaintext highlighter-rouge">Projection Matrix</code> 를 정의해줬다. 이후 헤드 개수만큼 객체  <code class="language-plaintext highlighter-rouge">Projector</code> 를 호출해 리스트에 해드별 <code class="language-plaintext highlighter-rouge">Projection Matrix</code> 를 담아준다. 그 다음 <code class="language-plaintext highlighter-rouge">torch.stack</code>을 사용해 <code class="language-plaintext highlighter-rouge">Attention Head</code> 방향의 차원으로 리스트 내부 텐서들을 쌓아주면 된다.</p>

<h3 id="torcharange"><code class="language-plaintext highlighter-rouge">🔢 torch.arange</code></h3>

<p>사용자가 지정한 시작점부터 끝점까지 일정한 간격으로 텐서를 나열한다. Python의 내장 메서드 <code class="language-plaintext highlighter-rouge">range</code>와 동일한 역할을 하는데, 대신 텐서 그 결과를 텐서 구조체로 반환한다고 생각하면 되겠다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># torch.arange usage
</span><span class="n">torch</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="n">start</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">end</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
<span class="n">tensor</span><span class="p">([</span> <span class="mi">0</span><span class="p">,</span>  <span class="mi">1</span><span class="p">,</span>  <span class="mi">2</span><span class="p">,</span>  <span class="mi">3</span><span class="p">,</span>  <span class="mi">4</span><span class="p">])</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">tensor</span><span class="p">([</span> <span class="mi">1</span><span class="p">,</span>  <span class="mi">2</span><span class="p">,</span>  <span class="mi">3</span><span class="p">])</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">)</span>
<span class="n">tensor</span><span class="p">([</span> <span class="mf">1.0000</span><span class="p">,</span>  <span class="mf">1.5000</span><span class="p">,</span>  <span class="mf">2.0000</span><span class="p">])</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">step</code> 매개변수로 원소간 간격 조정을 할 수 있는데, 기본은 1로 지정 되어 있으니 참고하자. 필자의 경우에는 <code class="language-plaintext highlighter-rouge">nn.Embedding</code>의 입력 텐서를 만들 때 가장 많이 사용했다. <code class="language-plaintext highlighter-rouge">nn.Embedding</code> 의 경우 Input으로 <code class="language-plaintext highlighter-rouge">IntTensor</code>, <code class="language-plaintext highlighter-rouge">LongTensor</code>를 받게 되어 있으니 알아두자.</p>

<h3 id="torchrepeat"><code class="language-plaintext highlighter-rouge">🔁 torch.repeat</code></h3>

<p>입력값으로 주어진 텐서를 사용자가 지정한 반복 횟수만큼 특정 차원 방향으로 늘린다. 예를 들면 <code class="language-plaintext highlighter-rouge">[1,2,3] * 3</code>의 결과는 <code class="language-plaintext highlighter-rouge">[1, 2, 3, 1, 2, 3, 1, 2, 3]</code> 인데, 이것을 사용자가 지정한 반복 횟수만큼 특정 차원으로 수행하겠다는 것이다. 아래 사용 예제를 확인해보자.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># torch.repeat example
</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">x</span><span class="p">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">tensor</span><span class="p">([[</span> <span class="mi">1</span><span class="p">,</span>  <span class="mi">2</span><span class="p">,</span>  <span class="mi">3</span><span class="p">,</span>  <span class="mi">1</span><span class="p">,</span>  <span class="mi">2</span><span class="p">,</span>  <span class="mi">3</span><span class="p">],</span>
        <span class="p">[</span> <span class="mi">1</span><span class="p">,</span>  <span class="mi">2</span><span class="p">,</span>  <span class="mi">3</span><span class="p">,</span>  <span class="mi">1</span><span class="p">,</span>  <span class="mi">2</span><span class="p">,</span>  <span class="mi">3</span><span class="p">],</span>
        <span class="p">[</span> <span class="mi">1</span><span class="p">,</span>  <span class="mi">2</span><span class="p">,</span>  <span class="mi">3</span><span class="p">,</span>  <span class="mi">1</span><span class="p">,</span>  <span class="mi">2</span><span class="p">,</span>  <span class="mi">3</span><span class="p">],</span>
        <span class="p">[</span> <span class="mi">1</span><span class="p">,</span>  <span class="mi">2</span><span class="p">,</span>  <span class="mi">3</span><span class="p">,</span>  <span class="mi">1</span><span class="p">,</span>  <span class="mi">2</span><span class="p">,</span>  <span class="mi">3</span><span class="p">]])</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">x</span><span class="p">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">tensor</span><span class="p">([[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
         <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]],</span>

        <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
         <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]],</span>

        <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
         <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]],</span>

        <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
         <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]]])</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">x</span><span class="p">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">).</span><span class="n">size</span>
<span class="n">torch</span><span class="p">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">x</span><span class="p">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">tensor</span><span class="p">([[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
         <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]],</span>

        <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
         <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]],</span>

        <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
         <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]],</span>

        <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
         <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]]])</span>
</code></pre></div></div>

<p>$t$를 어떤 텐서 구조체 $x$의 최대 차원이라고 했을 , $x_t$를 가장 왼쪽에 넣고 가장 낮은 차원인 0차원에 대한 반복 횟수를 오른쪽 끝에 대입해서 사용하면 된다. (<code class="language-plaintext highlighter-rouge">torch.repeat(</code>$x_t, x_{t-1}, … x_2, x_1, x_0$<code class="language-plaintext highlighter-rouge">))</code>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># torch.arange &amp; torch.repeate usage example
</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">pos_x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">num_patches</span> <span class="o">+</span> <span class="mi">1</span><span class="p">).</span><span class="n">repeat</span><span class="p">(</span><span class="n">inputs</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]).</span><span class="n">to</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">pos_x</span><span class="p">.</span><span class="n">shape</span>
<span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">16</span><span class="p">,</span> <span class="mi">1025</span><span class="p">])</span>
</code></pre></div></div>

<p>필자의 경우, <code class="language-plaintext highlighter-rouge">position embedding</code>의 입력을 만들고 싶을 때 <code class="language-plaintext highlighter-rouge">torch.arange</code> 와 연계해 자주 사용 했던 것 같다. 위 코드를 참고하자.</p>

<h3 id="torchclamp"><code class="language-plaintext highlighter-rouge">🔬 torch.clamp</code></h3>

<p>입력 텐서의 원소값을 사용자가 지정한 최대•최소값 범위 이내로 제한하는 메서드다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># torch.clamp params
</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">clamp</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="nb">min</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span> <span class="err">→</span> <span class="n">Tensor</span>

<span class="c1"># torch.clamp usage example
</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">a</span>
<span class="n">tensor</span><span class="p">([</span><span class="o">-</span><span class="mf">1.7120</span><span class="p">,</span>  <span class="mf">0.1734</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0478</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0922</span><span class="p">])</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="nb">min</span><span class="o">=-</span><span class="mf">0.5</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">tensor</span><span class="p">([</span><span class="o">-</span><span class="mf">0.5000</span><span class="p">,</span>  <span class="mf">0.1734</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0478</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0922</span><span class="p">])</span>
</code></pre></div></div>

<p>입력된 텐서의 원소를 지정 최대•최소 설정값과 하나 하나 대조해서 텐서 내부의 모든 원소가 지정 범위 안에 들도록 만들어준다. <code class="language-plaintext highlighter-rouge">torch.clamp</code> 역시 다양한 상황에서 사용되는데, 필자의 경우 모델 레이어 중간에 제곱근이나 지수, 분수 혹은 각도 관련 연산이 들어가 <code class="language-plaintext highlighter-rouge">Backward Pass</code>에서 <code class="language-plaintext highlighter-rouge">NaN</code>이 발생할 수 있는 경우에 안전장치로 많이 사용하고 있다. (<a href="https://qcqced123.github.io/framework-library/backward-nan/">자세히 알고 싶다면 클릭</a>)</p>

<h3 id="torchgather"><code class="language-plaintext highlighter-rouge">👩‍👩‍👧‍👦 torch.gather</code></h3>

<p>텐서 객체 내부에서 원하는 인덱스에 위치한 원소만 추출하고 싶을 때 사용하면 매우 유용한 메서드다. 텐서 역시 <code class="language-plaintext highlighter-rouge">iterable</code> 객체라서 <code class="language-plaintext highlighter-rouge">loop</code> 를 사용해 접근하는 것이 직관적으로 보일 수 있으나, 통상적으로 텐서를 사용하는 상황이라면 객체의 차원이 어마무시 하기 때문에 루프로 접근해 관리하는 것은 매우 비효율적이다. 루프를 통해 접근하면 파이썬의 내장 리스트를 사용하는 것과 별반 다를게 없어지기 때문에, 텐서를 사용하는 메리트가 사라진다. 비교적 크지 않은 2~3차원의 텐서 정도라면 사용해도 크게 문제는 없을거라 생각하지만 그래도 코드의 일관성을 위해 <code class="language-plaintext highlighter-rouge">torch.gather</code> 사용을 권장한다. 이제 <code class="language-plaintext highlighter-rouge">torch.gather</code>의 사용법에 대해 알아보자.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># torch.gather params
</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">gather</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">sparse_grad</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">dim</code>과 <code class="language-plaintext highlighter-rouge">index</code>에 주목해보자. 먼저 <code class="language-plaintext highlighter-rouge">dim</code>은 사용자가 인덱싱을 적용하고 싶은 차원을 지정해주는 역할을 한다. <code class="language-plaintext highlighter-rouge">index</code> 매개변수로 전달하는 텐서 안에는 원소의 <code class="language-plaintext highlighter-rouge">‘인덱스’</code>를 의미하는 숫자들이 마구잡이로 담겨있는데, 해당 인덱스가 대상 텐서의 어느 차원을 가리킬 것인지를 컴퓨터에게 알려준다고 생각하면 된다. <code class="language-plaintext highlighter-rouge">index</code> 는 앞에서 설명했듯이 원소의 <code class="language-plaintext highlighter-rouge">‘인덱스’</code>를 의미하는 숫자들이 담긴 텐서를 입력으로 하는 매개변수다. 이 때 주의할 점은 대상 텐서(<code class="language-plaintext highlighter-rouge">input</code>)와 인덱스 텐서의 차원 형태가 반드시 동일해야 한다는 것이다. 역시 말로만 들으면 이해하기 힘드니 사용 예시를 함꼐 살펴보자.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># torch.gather usage example
</span><span class="o">&gt;&gt;&gt;</span> <span class="n">q</span><span class="p">,</span> <span class="n">kr</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1024</span><span class="p">,</span> <span class="mi">64</span><span class="p">),</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1024</span><span class="p">,</span> <span class="mi">64</span><span class="p">)</span> <span class="c1"># [batch, sequence, dim_head], [batch, 2*sequence, dim_head]
</span><span class="o">&gt;&gt;&gt;</span> <span class="n">tmp_c2p</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">kr</span><span class="p">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">))</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">tmp_c2p</span><span class="p">,</span> <span class="n">tmp_c2p</span><span class="p">.</span><span class="n">shape</span>
<span class="p">(</span><span class="n">tensor</span><span class="p">([[</span><span class="o">-</span><span class="mf">2.6477</span><span class="p">,</span> <span class="o">-</span><span class="mf">4.7478</span><span class="p">,</span> <span class="o">-</span><span class="mf">5.3250</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mf">1.6062</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.9717</span><span class="p">,</span>  <span class="mf">3.8004</span><span class="p">],</span>
         <span class="p">[</span> <span class="mf">0.0662</span><span class="p">,</span>  <span class="mf">1.5240</span><span class="p">,</span>  <span class="mf">0.1182</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mf">0.1653</span><span class="p">,</span>  <span class="mf">2.8476</span><span class="p">,</span>  <span class="mf">1.6337</span><span class="p">],</span>
         <span class="p">[</span><span class="o">-</span><span class="mf">0.5010</span><span class="p">,</span> <span class="o">-</span><span class="mf">4.2267</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.1179</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mf">1.1447</span><span class="p">,</span>  <span class="mf">1.7845</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1493</span><span class="p">],</span>
         <span class="p">...,</span>
         <span class="p">[</span><span class="o">-</span><span class="mf">2.1073</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2149</span><span class="p">,</span> <span class="o">-</span><span class="mf">4.8630</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mf">0.8238</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5833</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2066</span><span class="p">],</span>
         <span class="p">[</span> <span class="mf">2.1747</span><span class="p">,</span>  <span class="mf">3.2924</span><span class="p">,</span>  <span class="mf">6.5808</span><span class="p">,</span>  <span class="p">...,</span> <span class="o">-</span><span class="mf">0.2926</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2511</span><span class="p">,</span>  <span class="mf">2.6996</span><span class="p">],</span>
         <span class="p">[</span><span class="o">-</span><span class="mf">2.8362</span><span class="p">,</span>  <span class="mf">2.8700</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.9729</span><span class="p">,</span>  <span class="p">...,</span> <span class="o">-</span><span class="mf">4.9913</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3616</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1708</span><span class="p">]],</span>
        <span class="n">grad_fn</span><span class="o">=&lt;</span><span class="n">MmBackward0</span><span class="o">&gt;</span><span class="p">)</span>
<span class="n">torch</span><span class="p">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">1024</span><span class="p">,</span> <span class="mi">1024</span><span class="p">]))</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">max_seq</span><span class="p">,</span> <span class="n">max_relative_position</span> <span class="o">=</span> <span class="mi">1024</span><span class="p">,</span> <span class="mi">512</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">q_index</span><span class="p">,</span> <span class="n">k_index</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="n">max_seq</span><span class="p">),</span> <span class="n">torch</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">max_relative_position</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">q_index</span><span class="p">,</span> <span class="n">k_index</span>
<span class="p">(</span><span class="n">tensor</span><span class="p">([</span>   <span class="mi">0</span><span class="p">,</span>    <span class="mi">1</span><span class="p">,</span>    <span class="mi">2</span><span class="p">,</span>  <span class="p">...,</span> <span class="mi">1021</span><span class="p">,</span> <span class="mi">1022</span><span class="p">,</span> <span class="mi">1023</span><span class="p">]),</span>
 <span class="n">tensor</span><span class="p">([</span>   <span class="mi">0</span><span class="p">,</span>    <span class="mi">1</span><span class="p">,</span>    <span class="mi">2</span><span class="p">,</span>  <span class="p">...,</span> <span class="mi">1021</span><span class="p">,</span> <span class="mi">1022</span><span class="p">,</span> <span class="mi">1023</span><span class="p">]))</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">tmp_pos</span> <span class="o">=</span> <span class="n">q_index</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="o">-</span> <span class="n">k_index</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">rel_pos_matrix</span> <span class="o">=</span> <span class="n">tmp_pos</span> <span class="o">+</span> <span class="n">max_relative_position</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">rel_pos_matrix</span>
<span class="n">tensor</span><span class="p">([[</span> <span class="mi">512</span><span class="p">,</span>  <span class="mi">511</span><span class="p">,</span>  <span class="mi">510</span><span class="p">,</span>  <span class="p">...,</span> <span class="o">-</span><span class="mi">509</span><span class="p">,</span> <span class="o">-</span><span class="mi">510</span><span class="p">,</span> <span class="o">-</span><span class="mi">511</span><span class="p">],</span>
        <span class="p">[</span> <span class="mi">513</span><span class="p">,</span>  <span class="mi">512</span><span class="p">,</span>  <span class="mi">511</span><span class="p">,</span>  <span class="p">...,</span> <span class="o">-</span><span class="mi">508</span><span class="p">,</span> <span class="o">-</span><span class="mi">509</span><span class="p">,</span> <span class="o">-</span><span class="mi">510</span><span class="p">],</span>
        <span class="p">[</span> <span class="mi">514</span><span class="p">,</span>  <span class="mi">513</span><span class="p">,</span>  <span class="mi">512</span><span class="p">,</span>  <span class="p">...,</span> <span class="o">-</span><span class="mi">507</span><span class="p">,</span> <span class="o">-</span><span class="mi">508</span><span class="p">,</span> <span class="o">-</span><span class="mi">509</span><span class="p">],</span>
        <span class="p">...,</span>
        <span class="p">[</span><span class="mi">1533</span><span class="p">,</span> <span class="mi">1532</span><span class="p">,</span> <span class="mi">1531</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mi">512</span><span class="p">,</span>  <span class="mi">511</span><span class="p">,</span>  <span class="mi">510</span><span class="p">],</span>
        <span class="p">[</span><span class="mi">1534</span><span class="p">,</span> <span class="mi">1533</span><span class="p">,</span> <span class="mi">1532</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mi">513</span><span class="p">,</span>  <span class="mi">512</span><span class="p">,</span>  <span class="mi">511</span><span class="p">],</span>
        <span class="p">[</span><span class="mi">1535</span><span class="p">,</span> <span class="mi">1534</span><span class="p">,</span> <span class="mi">1533</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mi">514</span><span class="p">,</span>  <span class="mi">513</span><span class="p">,</span>  <span class="mi">512</span><span class="p">]])</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">rel_pos_matrix</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">rel_pos_matrix</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="o">*</span><span class="n">max_relative_position</span> <span class="o">-</span> <span class="mi">1</span><span class="p">).</span><span class="n">repeat</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">tmp_c2p</span> <span class="o">=</span> <span class="n">tmp_c2p</span><span class="p">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">rel_pos_matrix</span><span class="p">,</span> <span class="n">rel_pos_matrix</span><span class="p">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">tmp_c2p</span><span class="p">.</span><span class="n">shape</span> 
<span class="p">(</span><span class="n">tensor</span><span class="p">([[[</span> <span class="mi">512</span><span class="p">,</span>  <span class="mi">511</span><span class="p">,</span>  <span class="mi">510</span><span class="p">,</span>  <span class="p">...,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">],</span>
          <span class="p">[</span> <span class="mi">513</span><span class="p">,</span>  <span class="mi">512</span><span class="p">,</span>  <span class="mi">511</span><span class="p">,</span>  <span class="p">...,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">],</span>
          <span class="p">[</span> <span class="mi">514</span><span class="p">,</span>  <span class="mi">513</span><span class="p">,</span>  <span class="mi">512</span><span class="p">,</span>  <span class="p">...,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">],</span>
          <span class="p">...,</span>
          <span class="p">[</span><span class="mi">1023</span><span class="p">,</span> <span class="mi">1023</span><span class="p">,</span> <span class="mi">1023</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mi">512</span><span class="p">,</span>  <span class="mi">511</span><span class="p">,</span>  <span class="mi">510</span><span class="p">],</span>
          <span class="p">[</span><span class="mi">1023</span><span class="p">,</span> <span class="mi">1023</span><span class="p">,</span> <span class="mi">1023</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mi">513</span><span class="p">,</span>  <span class="mi">512</span><span class="p">,</span>  <span class="mi">511</span><span class="p">],</span>
          <span class="p">[</span><span class="mi">1023</span><span class="p">,</span> <span class="mi">1023</span><span class="p">,</span> <span class="mi">1023</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mi">514</span><span class="p">,</span>  <span class="mi">513</span><span class="p">,</span>  <span class="mi">512</span><span class="p">]],</span>
 
         <span class="p">[[</span> <span class="mi">512</span><span class="p">,</span>  <span class="mi">511</span><span class="p">,</span>  <span class="mi">510</span><span class="p">,</span>  <span class="p">...,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">],</span>
          <span class="p">[</span> <span class="mi">513</span><span class="p">,</span>  <span class="mi">512</span><span class="p">,</span>  <span class="mi">511</span><span class="p">,</span>  <span class="p">...,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">],</span>
          <span class="p">[</span> <span class="mi">514</span><span class="p">,</span>  <span class="mi">513</span><span class="p">,</span>  <span class="mi">512</span><span class="p">,</span>  <span class="p">...,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">],</span>
          <span class="p">...,</span>
          <span class="p">[</span><span class="mi">1023</span><span class="p">,</span> <span class="mi">1023</span><span class="p">,</span> <span class="mi">1023</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mi">512</span><span class="p">,</span>  <span class="mi">511</span><span class="p">,</span>  <span class="mi">510</span><span class="p">],</span>
          <span class="p">[</span><span class="mi">1023</span><span class="p">,</span> <span class="mi">1023</span><span class="p">,</span> <span class="mi">1023</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mi">513</span><span class="p">,</span>  <span class="mi">512</span><span class="p">,</span>  <span class="mi">511</span><span class="p">],</span>
          <span class="p">[</span><span class="mi">1023</span><span class="p">,</span> <span class="mi">1023</span><span class="p">,</span> <span class="mi">1023</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mi">514</span><span class="p">,</span>  <span class="mi">513</span><span class="p">,</span>  <span class="mi">512</span><span class="p">]],</span>
<span class="n">torch</span><span class="p">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1024</span><span class="p">,</span> <span class="mi">1024</span><span class="p">]),</span>
<span class="n">torch</span><span class="p">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1024</span><span class="p">,</span> <span class="mi">1024</span><span class="p">]))</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">gather</span><span class="p">(</span><span class="n">tmp_c2p</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">rel_pos_matrix</span><span class="p">)</span>
<span class="n">tensor</span><span class="p">([[[</span><span class="o">-</span><span class="mf">0.8579</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2178</span><span class="p">,</span>  <span class="mf">1.6323</span><span class="p">,</span>  <span class="p">...,</span> <span class="o">-</span><span class="mf">2.6477</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.6477</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.6477</span><span class="p">],</span>
         <span class="p">[</span> <span class="mf">1.1601</span><span class="p">,</span>  <span class="mf">2.1752</span><span class="p">,</span>  <span class="mf">0.7187</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mf">0.0662</span><span class="p">,</span>  <span class="mf">0.0662</span><span class="p">,</span>  <span class="mf">0.0662</span><span class="p">],</span>
         <span class="p">[</span> <span class="mf">3.4379</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2573</span><span class="p">,</span>  <span class="mf">0.1375</span><span class="p">,</span>  <span class="p">...,</span> <span class="o">-</span><span class="mf">0.5010</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5010</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5010</span><span class="p">],</span>
         <span class="p">...,</span>
         <span class="p">[</span><span class="o">-</span><span class="mf">1.2066</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2066</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2066</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mf">0.5943</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5169</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.0820</span><span class="p">],</span>
         <span class="p">[</span> <span class="mf">2.6996</span><span class="p">,</span>  <span class="mf">2.6996</span><span class="p">,</span>  <span class="mf">2.6996</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mf">0.2014</span><span class="p">,</span>  <span class="mf">1.1458</span><span class="p">,</span>  <span class="mf">3.2626</span><span class="p">],</span>
         <span class="p">[</span><span class="o">-</span><span class="mf">0.1708</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1708</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1708</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mf">1.9955</span><span class="p">,</span>  <span class="mf">4.1549</span><span class="p">,</span>  <span class="mf">2.6356</span><span class="p">]],</span>
</code></pre></div></div>

<p>위 코드는 <code class="language-plaintext highlighter-rouge">DeBERTa</code> 의 <code class="language-plaintext highlighter-rouge">Disentangled Self-Attention</code>을 구현한 코드의 일부분이다. 자세한 원리는 <code class="language-plaintext highlighter-rouge">DeBERTa</code> 논문 리뷰 포스팅에서 확인하면 되고, 우리가 지금 주목할 부분은 바로 <code class="language-plaintext highlighter-rouge">tmp_c2p</code>, <code class="language-plaintext highlighter-rouge">rel_pos_matrix</code> 그리고 마지막 줄에 위치한 <code class="language-plaintext highlighter-rouge">torch.gather</code> 다. <code class="language-plaintext highlighter-rouge">[10, 1024, 1024]</code> 모양을 가진 대상 텐서 <code class="language-plaintext highlighter-rouge">tmp_c2p</code> 에서 내가 원하는 원소만 추출하려는 상황인데, 추출해야할 원소의 인덱스 값이 담긴 텐서를 <code class="language-plaintext highlighter-rouge">rel_pos_matrix</code> 로 정의했다. <code class="language-plaintext highlighter-rouge">rel_pos_matrix</code> 의 차원은 <code class="language-plaintext highlighter-rouge">[10, 1024, 1024]</code>로 <code class="language-plaintext highlighter-rouge">tmp_c2p</code>와 동일하다. 참고로 추출해야 하는 차원 방향은 가로 방향(두 번째 1024)이다.</p>

<p>이제 <code class="language-plaintext highlighter-rouge">torch.gather</code>의 동작을 살펴보자. 우리가 현재 추출하고 싶은 대상은 3차원 텐서의 가로 방향(두 번째 1024, 텐서의 행 벡터), 즉 <code class="language-plaintext highlighter-rouge">2 * max_sequence_length</code> 를 의미하는 차원 방향의 원소다. 따라서 <code class="language-plaintext highlighter-rouge">dim=-1</code>으로 설정해준다. 이제 메서드가 의도대로 적용되었는지 확인해보자. <code class="language-plaintext highlighter-rouge">rel_pos_matrix</code> 의 0번 배치, 0번째 시퀀스의 가장 마지막 차원의 값은 <code class="language-plaintext highlighter-rouge">0</code>으로 초기화 되어 있다. 다시 말해, 대상 텐서의 대상 차원에서 0번째 인덱스에 해당하는 값을 가져오라는 의미를 담고 있다. 그렇다면 <code class="language-plaintext highlighter-rouge">torch.gather</code> 실행 결과가 <code class="language-plaintext highlighter-rouge">tmp_c2p</code>의 0번 배치, 0번째 시퀀스의 0번째 차원 값과 일치하는지 확인해보자. 둘 다 <code class="language-plaintext highlighter-rouge">-2.6477</code>, <code class="language-plaintext highlighter-rouge">-2.6477</code> 으로 같은 값을 나타내고 있다. 따라서 우리 의도대로 잘 실행되었다는 사실을 알 수 있다.</p>

<h3 id="torchtriu-torchtril"><code class="language-plaintext highlighter-rouge">👩‍👩‍👧‍👦 torch.triu, torch.tril</code></h3>

<p>각각 입력 텐서를 <code class="language-plaintext highlighter-rouge">상삼각행렬</code>, <code class="language-plaintext highlighter-rouge">하삼각행렬</code>로 만든다. <code class="language-plaintext highlighter-rouge">triu</code>나 <code class="language-plaintext highlighter-rouge">tril</code>은 사실 뒤집으면 같은 결과를 반환하기 때문에 <code class="language-plaintext highlighter-rouge">tril</code>을 기준으로 설명을 하겠다. 메서드의 매개변수는 다음과 같다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># torch.triu, tril params
</span><span class="n">upper_tri_matrix</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">triu</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span> <span class="n">diagonal</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
<span class="n">lower_tri_matrix</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tril</span><span class="p">(</span><span class="n">input_tensors</span><span class="p">,</span> <span class="n">diagonal</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">diagonal</code> 에 주목해보자. 양수를 전달하면 주대각성분에서 해당하는 값만큼 떨어진 곳의 대각성분까지 그 값을 살려둔다. 한편 음수를 전달하면 주대각성분을 포함해 주어진 값만큼 떨어진 곳까지의 대각성분을 모두 0으로 만들어버린다. 기본은 0으로 설정되어 있으며, 이는 주대각성분부터 왼쪽 하단의 원소를 모두 살려두겠다는 의미가 된다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># torch.tril usage example
</span><span class="o">&gt;&gt;&gt;</span> <span class="n">lm_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tril</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">ones</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">lm_mask</span>
<span class="mi">1</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span>
<span class="mi">1</span> <span class="mi">1</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span>
<span class="mi">1</span> <span class="mi">1</span> <span class="mi">1</span> <span class="mi">0</span> <span class="mi">0</span>
<span class="mi">1</span> <span class="mi">1</span> <span class="mi">1</span> <span class="mi">1</span> <span class="mi">0</span>
</code></pre></div></div>

<p>두 메서드는 선형대수학이 필요한 다양한 분야에서 사용되는데, 필자의 경우, <code class="language-plaintext highlighter-rouge">GPT</code>처럼 <code class="language-plaintext highlighter-rouge">Transformer</code>의 <code class="language-plaintext highlighter-rouge">Decoder</code> 를 사용하는 모델을 빌드할 때 가장 많이 사용했던 것 같다. <code class="language-plaintext highlighter-rouge">Decoder</code>를 사용하는 모델은 대부분 구조상 <code class="language-plaintext highlighter-rouge">Language Modeling</code>을 위해서 <code class="language-plaintext highlighter-rouge">Masked Multi-Head Self-Attention Block</code>을 사용하는데 이 때 미래 시점의 토큰 임베딩 값에 마스킹을 해주기 위해 <code class="language-plaintext highlighter-rouge">torch.tril</code> 을 사용하게 되니 참고하자.</p>

<h3 id="torchtensormasked_fill"><code class="language-plaintext highlighter-rouge">👩‍👩‍👧‍👦 torch.Tensor.masked_fill</code></h3>

<p>사용자가 지정한 값에 해당되는 원소를 모두 마스킹 처리해주는 메서드다. 먼저 매개변수를 확인해보자.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># torch.Tensor.masked_fill params
</span><span class="n">input_tensors</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">6</span><span class="p">]])</span>
<span class="n">input_tensors</span><span class="p">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">mask</span><span class="p">:</span> <span class="n">BoolTensor</span><span class="p">,</span> <span class="n">value</span><span class="p">:</span> <span class="nb">float</span><span class="p">)</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">masked_fill</code> 은 텐서 객체의 내부 <code class="language-plaintext highlighter-rouge">attribute</code> 로 정의되기 때문에 해당 메서드를 사용하고 싶다면 먼저 마스킹 대상 텐서를 만들어야 한다. 텐서를 정의했다면 텐서 객체의 <code class="language-plaintext highlighter-rouge">attributes</code> 접근을 통해 <code class="language-plaintext highlighter-rouge">masked_fill()</code> 을 호출한 뒤, 필요한 매개변수를 전달해주는 방식으로 사용하면 된다.</p>

<p><code class="language-plaintext highlighter-rouge">mask</code> 매개변수에는 마스킹 텐서를 전달해야 하는데, 이 때 내부 원소는 모두 <code class="language-plaintext highlighter-rouge">boolean</code>이어야 하고 텐서의 형태는 대상 텐서와 동일해야 한다(완전히 같을 필요는 없고, 브로드 캐스팅만 가능하면 상관 없음).</p>

<p><code class="language-plaintext highlighter-rouge">value</code> 매개변수에는 마스킹 대상 원소들에 일괄적으로 적용해주고 싶은 값을 전달한다. 이게 말로만 들으면 이해하기 쉽지 않다. 아래 사용 예시를 함께 첨부했으니 참고 바란다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># torch.masked_fill usage
</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">lm_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tril</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">ones</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">lm_mask</span>
<span class="mi">1</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span>
<span class="mi">1</span> <span class="mi">1</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span>
<span class="mi">1</span> <span class="mi">1</span> <span class="mi">1</span> <span class="mi">0</span> <span class="mi">0</span>
<span class="mi">1</span> <span class="mi">1</span> <span class="mi">1</span> <span class="mi">1</span> <span class="mi">0</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">attention_matrix</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">))</span> <span class="o">/</span> <span class="n">dot_scale</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">attention_matrix</span>
<span class="mf">1.22</span> <span class="mf">2.1</span> <span class="mf">3.4</span> <span class="mf">1.2</span> <span class="mf">1.1</span>
<span class="mf">1.22</span> <span class="mf">2.1</span> <span class="mf">3.4</span> <span class="mf">9.9</span> <span class="mf">9.9</span>
<span class="mf">1.22</span> <span class="mf">2.1</span> <span class="mf">3.4</span> <span class="mf">9.9</span> <span class="mf">9.9</span>
<span class="mf">1.22</span> <span class="mf">2.1</span> <span class="mf">3.4</span> <span class="mf">9.9</span> <span class="mf">9.9</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">attention_matrix</span> <span class="o">=</span> <span class="n">attention_matrix</span><span class="p">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">lm_mask</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="s">'-inf'</span><span class="p">))</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">attention_matrix</span>
<span class="mf">1.22</span> <span class="o">-</span><span class="n">inf</span> <span class="o">-</span><span class="n">inf</span> <span class="o">-</span><span class="n">inf</span> <span class="o">-</span><span class="n">inf</span>
<span class="mf">1.22</span> <span class="mf">2.1</span> <span class="o">-</span><span class="n">inf</span> <span class="o">-</span><span class="n">inf</span> <span class="o">-</span><span class="n">inf</span>
<span class="mf">1.22</span> <span class="mf">2.1</span> <span class="mf">3.4</span> <span class="o">-</span><span class="n">inf</span> <span class="o">-</span><span class="n">inf</span>
<span class="mf">1.22</span> <span class="mf">2.1</span> <span class="mf">3.4</span> <span class="mf">9.9</span> <span class="o">-</span><span class="n">inf</span>
</code></pre></div></div>]]></content><author><name>qcqced</name><email>qcqced123@gmail.com</email></author><category term="Framework &amp; Library" /><category term="Pytorch" /><category term="Tensor" /><category term="Linear Algebra" /><summary type="html"><![CDATA[파이토치에서 자주 사용하는 텐서 인덱싱 관련 메서드 모음]]></summary></entry><entry><title type="html">🪢 [DeBERTa] DeBERTa: Decoding-Enhanced BERT with Disentangled-Attention</title><link href="http://localhost:4000/nlp/deberta" rel="alternate" type="text/html" title="🪢 [DeBERTa] DeBERTa: Decoding-Enhanced BERT with Disentangled-Attention" /><published>2023-08-04T00:00:00+09:00</published><updated>2023-08-05T02:00:00+09:00</updated><id>http://localhost:4000/nlp/DeBERTa</id><content type="html" xml:base="http://localhost:4000/nlp/deberta"><![CDATA[<h3 id="overview"><code class="language-plaintext highlighter-rouge">🔭 Overview</code></h3>

<p><code class="language-plaintext highlighter-rouge">DeBERTa</code>는 2020년 <code class="language-plaintext highlighter-rouge">Microsoft</code>가 <code class="language-plaintext highlighter-rouge">ICLR</code>에서 발표한 자연어 처리용 신경망 모델이다. <code class="language-plaintext highlighter-rouge">Disentangled Self-Attention</code>, <code class="language-plaintext highlighter-rouge">Enhanced Mask Decoder</code>라는 두가지 새로운 테크닉을 <code class="language-plaintext highlighter-rouge">BERT</code>, <code class="language-plaintext highlighter-rouge">RoBERTa</code>에 적용해 당시 <code class="language-plaintext highlighter-rouge">SOTA</code>를 달성했으며, 특히 영어처럼 문장에서 자리하는 위치에 따라 단어의 의미, 형태가 결정되는 굴절어 계열에 대한 성능이 좋아 꾸준히 사랑받고 있는 모델이다. 또한 인코딩 가능한 최대 시퀀스 길이가 <code class="language-plaintext highlighter-rouge">4096</code>으로 매우 긴 편 (<code class="language-plaintext highlighter-rouge">DeBERTa-V3-Large</code>) 에 속해, <code class="language-plaintext highlighter-rouge">Kaggle Competition</code>에서 자주 활용된다. 출시된지 2년이 넘도록 <code class="language-plaintext highlighter-rouge">SuperGLUE</code> 대시보드에서 꾸준히 상위권을 유지하고 있다는 점도 <code class="language-plaintext highlighter-rouge">DeBERTa</code>가 얼마나 잘 설계된 모델인지 알 수 있는 대목이다.</p>

<p>한편, <code class="language-plaintext highlighter-rouge">DeBERTa</code>의 설계 철학은 <code class="language-plaintext highlighter-rouge">Inductive Bias</code> 다. 간단하게 <code class="language-plaintext highlighter-rouge">Inductive Bias</code>란, 주어진 데이터로부터 일반화 성능을 높이기 위해 <code class="language-plaintext highlighter-rouge">"입력되는 데이터는 ~ 할 것이다"</code>, <code class="language-plaintext highlighter-rouge">"이런 특징을 갖고 있을 것이다"</code>와 같은 가정, 가중치, 가설 등을 기계학습 알고리즘에 적용하는 것을 말한다. <strong><a href="https://qcqced123.github.io/cv/vit"><code class="language-plaintext highlighter-rouge">ViT</code> 논문 리뷰</a></strong>에서도 밝혔듯, 퓨어한 <code class="language-plaintext highlighter-rouge">Self-Attention</code> 의 <code class="language-plaintext highlighter-rouge">Inductive Bias</code> 는 사실상 없으며, 전체 <code class="language-plaintext highlighter-rouge">Transformer</code> 구조 레벨에서 봐도 <code class="language-plaintext highlighter-rouge">Absolute Position Embedding</code>을 사용해 토큰의 위치 정보를 모델에 주입해주는 것이 그나마 약한 <code class="language-plaintext highlighter-rouge">Iniductive Bias</code>라고 볼 수 있다. 다른 포스팅에서는 분명 <code class="language-plaintext highlighter-rouge">Inductive Bias</code> 가 적기 때문에 자연어 처리에서 <code class="language-plaintext highlighter-rouge">Transformer</code> 가 성공을 거둘 수 있다고 해놓고 이게 지금 와서 말을 뒤집는다고 생각할 수 있다. 하지만 <code class="language-plaintext highlighter-rouge">Self-Attention</code>과 <code class="language-plaintext highlighter-rouge">Absolute Position Embedding</code>의 의미를 다시 한 번 상기해보면, <code class="language-plaintext highlighter-rouge">Inductive Bias</code> 추가를 주장하는 저자들의 생각이 꽤나 합리적이었음을 알 수 있게 된다. 구체적인 모델 구조를 파악하기 전에 먼저 <code class="language-plaintext highlighter-rouge">Inductive Bias</code> 추가가 왜 필요하며, 어떠한 가정이 필요한지 알아보자.</p>

<h3 id="inducitve-bias-in-deberta"><code class="language-plaintext highlighter-rouge">🪢 Inducitve Bias in DeBERTa</code></h3>
<ul>
  <li><strong><code class="language-plaintext highlighter-rouge">Absolute Position + Relative Position</code>을 모두 활용해 풍부하고 깊은 임베딩 추출</strong></li>
  <li><strong><code class="language-plaintext highlighter-rouge">단어의 발생 순서</code> 임베딩과 <code class="language-plaintext highlighter-rouge">단어 분포 가설</code> 임베딩을 모두 추출하는 것을 목적으로 설계</strong></li>
</ul>

<p>본 논문 초록에는 다음과 같은 문장이 서술되어 있다.</p>

<p><code class="language-plaintext highlighter-rouge">motivated by the observation that the attention weight of a word pair depends on not only their contents but their relative positions. For example, the dependency between the words “deep” and “learning” is much stronger when they occur next to each other than when they occur in different sentences.</code></p>

<p>위의 두 문장이 <code class="language-plaintext highlighter-rouge">DeBERTa</code>의 <code class="language-plaintext highlighter-rouge">Inducitve Bias</code> 를 가장 잘 설명하고 있다고 생각한다. 저자가 추가를 주장하는 <code class="language-plaintext highlighter-rouge">Inductive Bias</code>란, <code class="language-plaintext highlighter-rouge">relative position</code> 정보라는 것과 기존 모델링으로는 <code class="language-plaintext highlighter-rouge">relative position</code>이 주는 문맥 정보 포착이 불가능하다는 사실을 알 수 있다.</p>

<p>그렇다면 <code class="language-plaintext highlighter-rouge">relative position</code> 가 제공하는 문맥 정보가 도대체 뭐길래 기존의 방식으로는 포착이 불가능하다는 것일까?? 자연어에서 포착 가능한 문맥들의 종류와 기존의 모델링 방식에 대한 정리부터 해보자. 여기서 말하는 기존 방식이란, 퓨어한 <code class="language-plaintext highlighter-rouge">Self-Attention</code>과 <code class="language-plaintext highlighter-rouge">Absolute Position Embedding</code> 을 사용하는 <code class="language-plaintext highlighter-rouge">Transformer-Encoder-Base</code>  모델(<code class="language-plaintext highlighter-rouge">BERT</code>, <code class="language-plaintext highlighter-rouge">RoBERTa</code>)을 뜻한다. 이번 포스팅에서는 <code class="language-plaintext highlighter-rouge">BERT</code>를 기준으로 설명하겠다.</p>

<h4 id="-types-of-embedding"><strong><code class="language-plaintext highlighter-rouge">📚 Types of Embedding</code></strong></h4>
<p><strong>먼저 현존하는 모든 임베딩(<code class="language-plaintext highlighter-rouge">벡터에 문맥을 주입하는</code>)기법들을 정리해보자. 다음과 같이 3가지 카테고리로 분류가 가능하다.</strong></p>

<ul>
  <li><strong>1) 단어의 빈도수:  시퀀스에서 사용된 토큰들의 빈도수를 측정(<code class="language-plaintext highlighter-rouge">Bag of words</code>)</strong></li>
  <li><strong>2) 단어의 발생 순서: <code class="language-plaintext highlighter-rouge">corpus</code> 내부의 특정 <code class="language-plaintext highlighter-rouge">sequence</code> 등장 빈도를 카운트(<code class="language-plaintext highlighter-rouge">N-Gram</code>), 주어진 시퀀스를 가지고 다음 시점에 등장할 토큰을 맞추는 방식(<code class="language-plaintext highlighter-rouge">LM</code>)</strong></li>
  <li><strong>3) 단어 분포 가설 :  단어의 의미는 주변 문맥에 의해 결정된다는 가정, 어떤 단어 쌍이 자주 같이 등장하는지 카운트해 <code class="language-plaintext highlighter-rouge">PMI</code>를 측정하는 방식(<code class="language-plaintext highlighter-rouge">Word2Vec</code>)</strong></li>
</ul>

<p>기존의 모델링 방식은 어디에 포함될까?? <code class="language-plaintext highlighter-rouge">BERT</code> 는 대분류 상 신경망에 포함되고, <code class="language-plaintext highlighter-rouge">Language Modeling</code>을 통해 시퀀스를 학습한다는 점 그리고 <code class="language-plaintext highlighter-rouge">Self-Attention</code>과 <code class="language-plaintext highlighter-rouge">Absolute Position Embedding</code> 을 사용한다는 점에서 2번, <code class="language-plaintext highlighter-rouge">단어의 발생 순서</code> 에 포함된다고 볼 수 있다. <code class="language-plaintext highlighter-rouge">Absolute Position Embedding</code> 과 <code class="language-plaintext highlighter-rouge">Self-Attention</code>의 사용이 퓨어한 <code class="language-plaintext highlighter-rouge">BERT</code>가 분류상 2번이라는 사실을 뒷받침하는 증거라는 점에서 의아할 수 있다. 하지만 잘 생각해보자.</p>

<p><code class="language-plaintext highlighter-rouge">Absolute Position Embedding</code>은 주어진 시퀀스의 길이를 측정한 뒤, 나열된 순서 그대로 <code class="language-plaintext highlighter-rouge">forward</code>하게 <code class="language-plaintext highlighter-rouge">0</code>부터 <code class="language-plaintext highlighter-rouge">길이-1</code>의 번호를 개별 토큰에 할당한다. 다시 말해, 단어가 시퀀스에서 발생한 순서를 수학적으로 표현해 모델에 주입한다는 의미가 된다. <code class="language-plaintext highlighter-rouge">Self-Attention</code>은 <code class="language-plaintext highlighter-rouge">Absolute Position Embedding</code> 정보가 주입된 시퀀스 전체를 한 번에 병렬 처리한다. 따라서 충분히 <code class="language-plaintext highlighter-rouge">BERT</code> 같은 <code class="language-plaintext highlighter-rouge">Self-Attention</code>, <code class="language-plaintext highlighter-rouge">Absolute Position Embedding</code> 기반 모델을 2번에 분류할 수 있겠다.</p>

<p>한편, 혹자는 <code class="language-plaintext highlighter-rouge">"BERT는 MLM 을 사용하는데 Language Modeling을 한다고 하는게 맞나요"</code>라고 말할 수 있다. 하지만 <code class="language-plaintext highlighter-rouge">MLM</code> 역시 대분류 상 <code class="language-plaintext highlighter-rouge">Language Modeling</code> 기법에 속한다. <strong>다만, <code class="language-plaintext highlighter-rouge">Bi-Directional</code>하게 문맥을 파악하고 <code class="language-plaintext highlighter-rouge">LM</code>을 하니까 정말 엄밀히 따지면 3번의 속성도 조금은 있다고 보는게 무리는 아니라 생각한다.</strong> <code class="language-plaintext highlighter-rouge">MLM</code> 사용으로 더 많은 정보를 포착해 임베딩을 만들기 때문에 초기 <code class="language-plaintext highlighter-rouge">BERT</code>가 <code class="language-plaintext highlighter-rouge">GPT</code>보다 <code class="language-plaintext highlighter-rouge">NLU</code>에서 상대적으로 강점을 가졌던 것 아닐까 싶다.</p>

<h4 id="-relative-position-embedding"><strong><code class="language-plaintext highlighter-rouge">🔢 Relative Position Embedding</code></strong></h4>
<p>이제 <code class="language-plaintext highlighter-rouge">Relative Position Embedding</code>이 무엇이고, 도대체 어떤 문맥 정보를 포착한다는 것인지 알아보자. <code class="language-plaintext highlighter-rouge">Relative Position Embedding</code> 이란, 시퀀스 내부 토큰 사이의 위치 관계 표현을 통해 토큰 사이의 <code class="language-plaintext highlighter-rouge">relation</code>을 <code class="language-plaintext highlighter-rouge">pairwise</code>하게 학습하는 위치 임베딩 기법을 말한다. 일반적으로 상대 위치 관계는 서로 다른 두 토큰의 시퀀스 인덱스 값의 차를 이용해 나타낸다. 포착하는 문맥 정보는 예시와 함깨 설명하겠다. 딥러닝이라는 단어는 영어로 <code class="language-plaintext highlighter-rouge">Deep Learning</code> 이다. 두 단어를 합쳐놓고 보면 <code class="language-plaintext highlighter-rouge">신경망을 사용하는 머신러닝 기법의 한 종류</code>라는 의미를 갖겠지만, 따로 따로 보면 <code class="language-plaintext highlighter-rouge">깊은</code>, <code class="language-plaintext highlighter-rouge">배움</code>이라는 개별적인 의미로 나뉜다.</p>

<ul>
  <li><strong><code class="language-plaintext highlighter-rouge">1) The Deep Learning is the Best Technique in Computer Science</code></strong></li>
  <li><strong><code class="language-plaintext highlighter-rouge">2) I’m learning how to swim in the deep ocean</code></strong></li>
</ul>

<p><code class="language-plaintext highlighter-rouge">Deep</code>과 <code class="language-plaintext highlighter-rouge">Learning</code>의 상대적인 거리에 주목하면서 두 문장을 해석해보자. 첫 번째 문장에서 두 단어는 이웃하게 위치해 <code class="language-plaintext highlighter-rouge">신경망을 사용하는 머신러닝 기법의 한 종류</code> 라는 의미를 만들어내고 있다. 한편 두 번째 문장에서 두 단어는 띄어쓰기 기준 5개의 토큰만큼 떨어져 위치해 각각 <code class="language-plaintext highlighter-rouge">배움</code>, <code class="language-plaintext highlighter-rouge">깊은</code> 이라는 의미를 만들어 내고 있다. 이처럼 개별 토큰 사이의 위치 관계에 따라서 파생되는 문맥적 정보를 포착하려는 의도로 설계된 기법이 바로 <code class="language-plaintext highlighter-rouge">Relative Position Embedding</code> 이다.</p>

<p><code class="language-plaintext highlighter-rouge">pairwise</code> 하게 <code class="language-plaintext highlighter-rouge">relation</code> 을 포착한다는 점으로 보아 <code class="language-plaintext highlighter-rouge">skip-gram</code>의 <code class="language-plaintext highlighter-rouge">negative sampling</code>과 매우 유사한 느낌의 정보를 포착할 것이라고 예상되며 카테고리 분류상 <strong>3번, <code class="language-plaintext highlighter-rouge">단어 분포 가설</code></strong>에 포함시킬 수 있을 것 같다. (필자의 개인적인 의견이니 이 부분에 대한 다른 의견이 있다면 꼭 댓글에 적어주시면 감사하겠습니당🥰).</p>

<p>위 예시만으로는 상대 위치 임베딩 개념이 와닿지 않을 수 있다. 그렇다면 옆에 링크를 먼저 읽고 오자. (<a href="https://qcqced123.github.io/nlp/deberta#-word-context-vs-relative-position-vs-absolute-position">링크1</a>)</p>

<p><code class="language-plaintext highlighter-rouge">Relative Position Embedding</code> 을 실제 어떻게 코드로 구현하는지, 본 논문에서는 위치 관계를 어떻게 정의했는지 <code class="language-plaintext highlighter-rouge">Absolute Position Embedding</code>와 비교를 통해 알아보자. 다음과 같은 두 개의 문장이 있을 때, 개별 위치 임베딩 방식이 문장의 위치 정보를 인코딩하는 과정을 파이썬 코드로 작성해봤다. 함께 살펴보자.</p>

<ul>
  <li><strong><code class="language-plaintext highlighter-rouge">A) I love studying deep learning so much</code></strong></li>
  <li><strong><code class="language-plaintext highlighter-rouge">B) I love deep cheeze burguer so much</code></strong></li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Absolute Position Embedding
</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">max_length</span> <span class="o">=</span> <span class="mi">7</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">position_embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Embedding</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">512</span><span class="p">)</span> <span class="c1"># [max_seq, dim_model]
</span><span class="o">&gt;&gt;&gt;</span> <span class="n">pos_x</span> <span class="o">=</span> <span class="n">position_embedding</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="n">max_length</span><span class="p">))</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">pos_x</span><span class="p">,</span> <span class="n">pos_x</span><span class="p">.</span><span class="n">shape</span>
<span class="p">(</span><span class="n">tensor</span><span class="p">([[</span> <span class="mf">0.4027</span><span class="p">,</span>  <span class="mf">0.9331</span><span class="p">,</span>  <span class="mf">1.0556</span><span class="p">,</span>  <span class="p">...,</span> <span class="o">-</span><span class="mf">1.7370</span><span class="p">,</span>  <span class="mf">0.7799</span><span class="p">,</span>  <span class="mf">1.9851</span><span class="p">],</span>  <span class="c1"># A,B의 0번 토큰: I
</span>         <span class="p">[</span><span class="o">-</span><span class="mf">0.2206</span><span class="p">,</span>  <span class="mf">2.1024</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6055</span><span class="p">,</span>  <span class="p">...,</span> <span class="o">-</span><span class="mf">1.1342</span><span class="p">,</span>  <span class="mf">1.3956</span><span class="p">,</span>  <span class="mf">0.9017</span><span class="p">],</span>  <span class="c1"># A,B의 1번 토큰: love
</span>         <span class="p">[</span><span class="o">-</span><span class="mf">0.9560</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0426</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.8587</span><span class="p">,</span>  <span class="p">...,</span> <span class="o">-</span><span class="mf">0.9406</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1467</span><span class="p">,</span>  <span class="mf">0.1762</span><span class="p">],</span>  <span class="c1"># A,B의 2번 토큰: studying, deep
</span>         <span class="p">...,</span>                                                           <span class="c1"># A,B의 3번 토큰: deep, cheeze
</span>         <span class="p">[</span> <span class="mf">0.5999</span><span class="p">,</span>  <span class="mf">0.5235</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3445</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mf">1.9020</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.5003</span><span class="p">,</span>  <span class="mf">0.7535</span><span class="p">],</span>  <span class="c1"># A,B의 4번 토큰: learning, burger
</span>         <span class="p">[</span> <span class="mf">0.0688</span><span class="p">,</span>  <span class="mf">0.5867</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0340</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mf">0.8547</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.9196</span><span class="p">,</span>  <span class="mf">1.1193</span><span class="p">],</span>  <span class="c1"># A,B의 5번 토큰: so
</span>         <span class="p">[</span><span class="o">-</span><span class="mf">0.0751</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4133</span><span class="p">,</span>  <span class="mf">0.0256</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mf">0.0788</span><span class="p">,</span>  <span class="mf">1.4665</span><span class="p">,</span>  <span class="mf">0.8196</span><span class="p">]],</span> <span class="c1"># A,B의 6번 토큰: much
</span>        <span class="n">grad_fn</span><span class="o">=&lt;</span><span class="n">EmbeddingBackward0</span><span class="o">&gt;</span><span class="p">),</span>
 <span class="n">torch</span><span class="p">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">7</span><span class="p">,</span> <span class="mi">512</span><span class="p">]))</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">Absolute Position Embedding</code>은 주위 문맥에 상관없이 같은 위치의 토큰이라면 같은 포지션 값으로 인코딩하기 때문에 <code class="language-plaintext highlighter-rouge">512</code>개의 원소로 구성된 행벡터들의 인덱스를 실제 문장에서 토큰의 등장 순서에 맵핑해주는 방식으로 위치 정보를 표현한다. 예를 들면, 문장에서 가장 먼저 등장하는 <code class="language-plaintext highlighter-rouge">0</code>번 토큰에 <code class="language-plaintext highlighter-rouge">0</code>번째 <code class="language-plaintext highlighter-rouge">행벡터</code>를 배정하고 가장 마지막에 등장하는 <code class="language-plaintext highlighter-rouge">N-1</code> 번째 토큰은 <code class="language-plaintext highlighter-rouge">N-1</code>번째 <code class="language-plaintext highlighter-rouge">행벡터</code>를 위치 정보값으로 갖는 방식이다. 전체 시퀀스 관점에서 개별 토큰에 번호를 부여하기 때문에 <code class="language-plaintext highlighter-rouge">syntactical</code>한 정보를 모델링 해주기 적합하다는 장점이 있다.</p>

<p><code class="language-plaintext highlighter-rouge">Absolute Position Embedding</code> 은 일반적으로 <code class="language-plaintext highlighter-rouge">Input Embedding</code>과 행렬합 연산을 통해 <code class="language-plaintext highlighter-rouge">Word Embedding</code> 으로 만들어 인코더의 입력으로 사용한다.</p>

<p>아래 코드는 저자가 논문에서 제시한 <code class="language-plaintext highlighter-rouge">DeBERTa</code>의 <code class="language-plaintext highlighter-rouge">Relative Position Embedding</code> 구현을 파이토치로 옮긴 것이다. <code class="language-plaintext highlighter-rouge">Relative Position Embedding</code> 은 절대 위치에 비해 꽤나 복잡한 과정을 거쳐야 하기 때문에 코드 역시 긴 편이다. 하나 하나 천천히 살펴보자.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Relative Position Embedding
</span><span class="o">&gt;&gt;&gt;</span> <span class="n">position_embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Embedding</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">max_length</span><span class="p">,</span> <span class="n">dim_model</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">x</span><span class="p">,</span> <span class="n">p_x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="n">max_length</span><span class="p">,</span> <span class="n">dim_model</span><span class="p">),</span> <span class="n">position_embedding</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">max_length</span><span class="p">))</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">fc_q</span><span class="p">,</span> <span class="n">fc_kr</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">dim_model</span><span class="p">,</span> <span class="n">dim_head</span><span class="p">),</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">dim_model</span><span class="p">,</span> <span class="n">dim_head</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">q</span><span class="p">,</span> <span class="n">kr</span> <span class="o">=</span> <span class="n">fc_q</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">fc_kr</span><span class="p">(</span><span class="n">p_x</span><span class="p">)</span> <span class="c1"># [batch, max_length, dim_head], [batch, 2*max_length, dim_head]
</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">tmp_c2p</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">kr</span><span class="p">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">))</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">tmp_c2p</span><span class="p">,</span> <span class="n">tmp_c2p</span><span class="p">.</span><span class="n">shape</span>
<span class="p">(</span><span class="n">tensor</span><span class="p">([[</span> <span class="mf">2.8118</span><span class="p">,</span>  <span class="mf">0.8449</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6240</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6516</span><span class="p">,</span>  <span class="mf">3.4009</span><span class="p">,</span>  <span class="mf">1.8296</span><span class="p">,</span>  <span class="mf">0.8304</span><span class="p">,</span>  <span class="mf">1.0164</span><span class="p">,</span>
           <span class="mf">3.5664</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.4208</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.0821</span><span class="p">,</span>  <span class="mf">1.5752</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.9469</span><span class="p">,</span> <span class="o">-</span><span class="mf">7.1767</span><span class="p">],</span>
         <span class="p">[</span><span class="o">-</span><span class="mf">2.1907</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.2801</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.0628</span><span class="p">,</span>  <span class="mf">0.4443</span><span class="p">,</span>  <span class="mf">2.2272</span><span class="p">,</span> <span class="o">-</span><span class="mf">5.6653</span><span class="p">,</span> <span class="o">-</span><span class="mf">4.6036</span><span class="p">,</span>  <span class="mf">1.4134</span><span class="p">,</span>
          <span class="o">-</span><span class="mf">1.1742</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3361</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4586</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.1827</span><span class="p">,</span>  <span class="mf">1.0878</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.5657</span><span class="p">],</span>
         <span class="p">[</span><span class="o">-</span><span class="mf">4.8952</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.5330</span><span class="p">,</span>  <span class="mf">0.0251</span><span class="p">,</span>  <span class="mf">3.5001</span><span class="p">,</span>  <span class="mf">4.1619</span><span class="p">,</span>  <span class="mf">1.7408</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5100</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.4616</span><span class="p">,</span>
          <span class="o">-</span><span class="mf">1.6101</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.8741</span><span class="p">,</span>  <span class="mf">1.1404</span><span class="p">,</span>  <span class="mf">4.9860</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.5350</span><span class="p">,</span>  <span class="mf">1.0999</span><span class="p">],</span>
         <span class="p">[</span><span class="o">-</span><span class="mf">3.3437</span><span class="p">,</span>  <span class="mf">4.2276</span><span class="p">,</span>  <span class="mf">0.4509</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.8911</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.1069</span><span class="p">,</span>  <span class="mf">0.9540</span><span class="p">,</span>  <span class="mf">1.2045</span><span class="p">,</span>  <span class="mf">2.2194</span><span class="p">,</span>
          <span class="o">-</span><span class="mf">2.6509</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.4076</span><span class="p">,</span>  <span class="mf">5.1599</span><span class="p">,</span>  <span class="mf">1.6591</span><span class="p">,</span>  <span class="mf">3.8764</span><span class="p">,</span>  <span class="mf">2.5126</span><span class="p">],</span>
         <span class="p">[</span> <span class="mf">0.8164</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.9171</span><span class="p">,</span>  <span class="mf">0.8217</span><span class="p">,</span>  <span class="mf">1.3953</span><span class="p">,</span>  <span class="mf">1.6260</span><span class="p">,</span>  <span class="mf">3.8104</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0303</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.1631</span><span class="p">,</span>
           <span class="mf">3.9008</span><span class="p">,</span>  <span class="mf">0.5856</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.6212</span><span class="p">,</span>  <span class="mf">1.7220</span><span class="p">,</span>  <span class="mf">2.7997</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.8802</span><span class="p">],</span>
         <span class="p">[</span> <span class="mf">3.4473</span><span class="p">,</span>  <span class="mf">0.9721</span><span class="p">,</span>  <span class="mf">3.9137</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.2055</span><span class="p">,</span>  <span class="mf">0.6963</span><span class="p">,</span>  <span class="mf">1.2761</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2266</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.7274</span><span class="p">,</span>
          <span class="o">-</span><span class="mf">1.4928</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.9257</span><span class="p">,</span> <span class="o">-</span><span class="mf">5.4422</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.8544</span><span class="p">,</span>  <span class="mf">1.8749</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.4923</span><span class="p">],</span>
         <span class="p">[</span> <span class="mf">2.6639</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.4392</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.8818</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.4120</span><span class="p">,</span>  <span class="mf">1.7542</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.8774</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.0795</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2156</span><span class="p">,</span>
          <span class="o">-</span><span class="mf">1.0852</span><span class="p">,</span>  <span class="mf">3.7825</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.5581</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.6989</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.6705</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2262</span><span class="p">]],</span>
        <span class="n">grad_fn</span><span class="o">=&lt;</span><span class="n">MmBackward0</span><span class="o">&gt;</span><span class="p">),</span>
 <span class="n">torch</span><span class="p">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">7</span><span class="p">,</span> <span class="mi">14</span><span class="p">]))</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">max_seq</span><span class="p">,</span> <span class="n">max_pos</span> <span class="o">=</span> <span class="mi">7</span><span class="p">,</span> <span class="n">max_seq</span> <span class="o">*</span> <span class="mi">2</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">q_index</span><span class="p">,</span> <span class="n">k_index</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="n">max_seq</span><span class="p">),</span> <span class="n">torch</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="n">max_seq</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">q_index</span><span class="p">,</span> <span class="n">k_index</span>
<span class="p">(</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">]),</span> <span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">]))</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">tmp_pos</span> <span class="o">=</span> <span class="n">q_index</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="o">-</span> <span class="n">k_index</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">rel_pos_matrix</span> <span class="o">=</span> <span class="n">tmp_pos</span> <span class="o">+</span> <span class="n">max_relative_position</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">rel_pos_matrix</span>
<span class="n">tensor</span><span class="p">([[</span> <span class="mi">7</span><span class="p">,</span>  <span class="mi">6</span><span class="p">,</span>  <span class="mi">5</span><span class="p">,</span>  <span class="mi">4</span><span class="p">,</span>  <span class="mi">3</span><span class="p">,</span>  <span class="mi">2</span><span class="p">,</span>  <span class="mi">1</span><span class="p">],</span>
        <span class="p">[</span> <span class="mi">8</span><span class="p">,</span>  <span class="mi">7</span><span class="p">,</span>  <span class="mi">6</span><span class="p">,</span>  <span class="mi">5</span><span class="p">,</span>  <span class="mi">4</span><span class="p">,</span>  <span class="mi">3</span><span class="p">,</span>  <span class="mi">2</span><span class="p">],</span>
        <span class="p">[</span> <span class="mi">9</span><span class="p">,</span>  <span class="mi">8</span><span class="p">,</span>  <span class="mi">7</span><span class="p">,</span>  <span class="mi">6</span><span class="p">,</span>  <span class="mi">5</span><span class="p">,</span>  <span class="mi">4</span><span class="p">,</span>  <span class="mi">3</span><span class="p">],</span>
        <span class="p">[</span><span class="mi">10</span><span class="p">,</span>  <span class="mi">9</span><span class="p">,</span>  <span class="mi">8</span><span class="p">,</span>  <span class="mi">7</span><span class="p">,</span>  <span class="mi">6</span><span class="p">,</span>  <span class="mi">5</span><span class="p">,</span>  <span class="mi">4</span><span class="p">],</span>
        <span class="p">[</span><span class="mi">11</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span>  <span class="mi">9</span><span class="p">,</span>  <span class="mi">8</span><span class="p">,</span>  <span class="mi">7</span><span class="p">,</span>  <span class="mi">6</span><span class="p">,</span>  <span class="mi">5</span><span class="p">],</span>
        <span class="p">[</span><span class="mi">12</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span>  <span class="mi">9</span><span class="p">,</span>  <span class="mi">8</span><span class="p">,</span>  <span class="mi">7</span><span class="p">,</span>  <span class="mi">6</span><span class="p">],</span>
        <span class="p">[</span><span class="mi">13</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span>  <span class="mi">9</span><span class="p">,</span>  <span class="mi">8</span><span class="p">,</span>  <span class="mi">7</span><span class="p">]])</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">rel_pos_matrix</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">rel_pos_matrix</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">max_pos</span> <span class="o">-</span> <span class="mi">1</span><span class="p">).</span><span class="n">repeat</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">tmp_c2p</span> <span class="o">=</span> <span class="n">tmp_c2p</span><span class="p">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">rel_pos_matrix</span><span class="p">,</span> <span class="n">rel_pos_matrix</span><span class="p">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">tmp_c2p</span><span class="p">.</span><span class="n">shape</span> 
<span class="p">(</span><span class="n">tensor</span><span class="p">([[[</span> <span class="mi">7</span><span class="p">,</span>  <span class="mi">6</span><span class="p">,</span>  <span class="mi">5</span><span class="p">,</span>  <span class="mi">4</span><span class="p">,</span>  <span class="mi">3</span><span class="p">,</span>  <span class="mi">2</span><span class="p">,</span>  <span class="mi">1</span><span class="p">],</span>
          <span class="p">[</span> <span class="mi">8</span><span class="p">,</span>  <span class="mi">7</span><span class="p">,</span>  <span class="mi">6</span><span class="p">,</span>  <span class="mi">5</span><span class="p">,</span>  <span class="mi">4</span><span class="p">,</span>  <span class="mi">3</span><span class="p">,</span>  <span class="mi">2</span><span class="p">],</span>
          <span class="p">[</span> <span class="mi">9</span><span class="p">,</span>  <span class="mi">8</span><span class="p">,</span>  <span class="mi">7</span><span class="p">,</span>  <span class="mi">6</span><span class="p">,</span>  <span class="mi">5</span><span class="p">,</span>  <span class="mi">4</span><span class="p">,</span>  <span class="mi">3</span><span class="p">],</span>
          <span class="p">[</span><span class="mi">10</span><span class="p">,</span>  <span class="mi">9</span><span class="p">,</span>  <span class="mi">8</span><span class="p">,</span>  <span class="mi">7</span><span class="p">,</span>  <span class="mi">6</span><span class="p">,</span>  <span class="mi">5</span><span class="p">,</span>  <span class="mi">4</span><span class="p">],</span>
          <span class="p">[</span><span class="mi">11</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span>  <span class="mi">9</span><span class="p">,</span>  <span class="mi">8</span><span class="p">,</span>  <span class="mi">7</span><span class="p">,</span>  <span class="mi">6</span><span class="p">,</span>  <span class="mi">5</span><span class="p">],</span>
          <span class="p">[</span><span class="mi">12</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span>  <span class="mi">9</span><span class="p">,</span>  <span class="mi">8</span><span class="p">,</span>  <span class="mi">7</span><span class="p">,</span>  <span class="mi">6</span><span class="p">],</span>
          <span class="p">[</span><span class="mi">13</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span>  <span class="mi">9</span><span class="p">,</span>  <span class="mi">8</span><span class="p">,</span>  <span class="mi">7</span><span class="p">]],</span>
<span class="n">torch</span><span class="p">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">10</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">14</span><span class="p">]),</span>
<span class="n">torch</span><span class="p">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">10</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">14</span><span class="p">]))</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">outputs</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">gather</span><span class="p">(</span><span class="n">tmp_c2p</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">rel_pos_matrix</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">.</span><span class="n">shape</span>
<span class="p">(</span><span class="n">tensor</span><span class="p">([[[</span> <span class="mf">1.0164</span><span class="p">,</span>  <span class="mf">0.8304</span><span class="p">,</span>  <span class="mf">1.8296</span><span class="p">,</span>  <span class="mf">3.4009</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6516</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6240</span><span class="p">,</span>  <span class="mf">0.8449</span><span class="p">],</span>
          <span class="p">[</span><span class="o">-</span><span class="mf">1.1742</span><span class="p">,</span>  <span class="mf">1.4134</span><span class="p">,</span> <span class="o">-</span><span class="mf">4.6036</span><span class="p">,</span> <span class="o">-</span><span class="mf">5.6653</span><span class="p">,</span>  <span class="mf">2.2272</span><span class="p">,</span>  <span class="mf">0.4443</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.0628</span><span class="p">],</span>
          <span class="p">[</span><span class="o">-</span><span class="mf">1.8741</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.6101</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.4616</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5100</span><span class="p">,</span>  <span class="mf">1.7408</span><span class="p">,</span>  <span class="mf">4.1619</span><span class="p">,</span>  <span class="mf">3.5001</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">5.1599</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.4076</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.6509</span><span class="p">,</span>  <span class="mf">2.2194</span><span class="p">,</span>  <span class="mf">1.2045</span><span class="p">,</span>  <span class="mf">0.9540</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.1069</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">1.7220</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.6212</span><span class="p">,</span>  <span class="mf">0.5856</span><span class="p">,</span>  <span class="mf">3.9008</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.1631</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0303</span><span class="p">,</span>  <span class="mf">3.8104</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">1.8749</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.8544</span><span class="p">,</span> <span class="o">-</span><span class="mf">5.4422</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.9257</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.4928</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.7274</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2266</span><span class="p">],</span>
          <span class="p">[</span><span class="o">-</span><span class="mf">1.2262</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.6705</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.6989</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.5581</span><span class="p">,</span>  <span class="mf">3.7825</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0852</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2156</span><span class="p">]],</span>
          <span class="p">.....</span>
          <span class="p">[[</span> <span class="mf">1.0164</span><span class="p">,</span>  <span class="mf">0.8304</span><span class="p">,</span>  <span class="mf">1.8296</span><span class="p">,</span>  <span class="mf">3.4009</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6516</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6240</span><span class="p">,</span>  <span class="mf">0.8449</span><span class="p">],</span>
          <span class="p">[</span><span class="o">-</span><span class="mf">1.1742</span><span class="p">,</span>  <span class="mf">1.4134</span><span class="p">,</span> <span class="o">-</span><span class="mf">4.6036</span><span class="p">,</span> <span class="o">-</span><span class="mf">5.6653</span><span class="p">,</span>  <span class="mf">2.2272</span><span class="p">,</span>  <span class="mf">0.4443</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.0628</span><span class="p">],</span>
          <span class="p">[</span><span class="o">-</span><span class="mf">1.8741</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.6101</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.4616</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5100</span><span class="p">,</span>  <span class="mf">1.7408</span><span class="p">,</span>  <span class="mf">4.1619</span><span class="p">,</span>  <span class="mf">3.5001</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">5.1599</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.4076</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.6509</span><span class="p">,</span>  <span class="mf">2.2194</span><span class="p">,</span>  <span class="mf">1.2045</span><span class="p">,</span>  <span class="mf">0.9540</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.1069</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">1.7220</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.6212</span><span class="p">,</span>  <span class="mf">0.5856</span><span class="p">,</span>  <span class="mf">3.9008</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.1631</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0303</span><span class="p">,</span>  <span class="mf">3.8104</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">1.8749</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.8544</span><span class="p">,</span> <span class="o">-</span><span class="mf">5.4422</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.9257</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.4928</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.7274</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2266</span><span class="p">],</span>
          <span class="p">[</span><span class="o">-</span><span class="mf">1.2262</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.6705</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.6989</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.5581</span><span class="p">,</span>  <span class="mf">3.7825</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0852</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2156</span><span class="p">]]],</span>
        <span class="n">grad_fn</span><span class="o">=&lt;</span><span class="n">GatherBackward0</span><span class="o">&gt;</span><span class="p">),</span>
 <span class="n">torch</span><span class="p">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">10</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">7</span><span class="p">]))</span>
</code></pre></div></div>

<p>일단 절대 위치와 동일하게 <code class="language-plaintext highlighter-rouge">nn.Embedding</code>을 사용해 임베딩 룩업 테이블(레이어)를 정의하지만, 입력 차원이 다르다. 절대 위치 임베딩은 <code class="language-plaintext highlighter-rouge">forward</code>하게 위치값을 맵핑해야 하는 반면에 상대 위치 임베딩 방식은 <code class="language-plaintext highlighter-rouge">Bi-Directional</code>한 맵핑을 해야 해서, 기존 <code class="language-plaintext highlighter-rouge">max_length</code> 값의 두 배를 입력 차원(<code class="language-plaintext highlighter-rouge">max_pos</code>)으로 사용했다. 예를 들어 <code class="language-plaintext highlighter-rouge">0</code>번 토큰과 나머지 토큰 사이의 위치 관계를 표현해야 하는 상황이다. 그렇다면 우리는 <code class="language-plaintext highlighter-rouge">0</code>번 토큰과 나머지 토큰과의 위치 관계를 <code class="language-plaintext highlighter-rouge">[0, -1, -2, -3, -4, -5, -6]</code> 으로 인코딩할 수 있다.</p>

<p>반대로 마지막 <code class="language-plaintext highlighter-rouge">6</code>번 토큰과 나머지 토큰 사이의 위치 관계를 표현하는 경우라면 어떻게 될까?? <code class="language-plaintext highlighter-rouge">[6, 5, 4, 3, 2, 1, 0]</code> 으로 인코딩 될 것이다. 다시 말해, 위치 임베딩 원소 값은 <code class="language-plaintext highlighter-rouge">[-max_seq:max_seq]</code> 사이에서 정의된다는 것이다. 그러나 원소값의 범위를 그대로 사용할 수는 없다. 이유는 파이썬의 리스트, 텐서 같은 배열형 자료구조는 음이 아닌 정수를 인덱스로 활용해야 <code class="language-plaintext highlighter-rouge">forward</code> 하게 원소에 접근할 수 있기 때문이다. 일반적으로 배열 형태의 자료형은 모두 인덱스 <code class="language-plaintext highlighter-rouge">0</code>부터 <code class="language-plaintext highlighter-rouge">N-1</code>까지 순차적으로 맵핑된다. 그래서 의도한대로 토큰에 접근하려면 역시 토큰의 인덱스를 <code class="language-plaintext highlighter-rouge">forward</code> 한 형태로 만들어줘야 한다.</p>

<p>따라서 기존 <code class="language-plaintext highlighter-rouge">[-max_seq:max_seq]</code> 에  <code class="language-plaintext highlighter-rouge">max_seq</code>를 더해준 <code class="language-plaintext highlighter-rouge">[0:2*max_seq]</code> (<code class="language-plaintext highlighter-rouge">2 * max_seq</code>)을 원소 값의 범위로 사용하게 된다. 여기까지가 통상적으로 말하는 <code class="language-plaintext highlighter-rouge">Relative Position Embedding</code> 에 해당한다. 위 코드상으로는 <code class="language-plaintext highlighter-rouge">rel_pos_matrix</code> 를 만든 부분에 해당한다.</p>

\[∂(i,j)=
\begin{cases}
\ 0 &amp; {(i - j ≤ k)} \\ 
\ 2k-1 &amp; {(i - j ≥ k)} \\
\ i - j + k &amp; {(others)} \\
\end{cases}\]

<p>이제부터 저자가 주장하는 위치 관계 표현 방식에 대해 알아보자. 일반적인 <code class="language-plaintext highlighter-rouge">Relative Position Embedding</code>과 거의 유사하지만, <code class="language-plaintext highlighter-rouge">rel_pos_matrix</code> 내부 원소 값이 음수가 되거나 <code class="language-plaintext highlighter-rouge">max_pos</code> 을 초과하는 경우를 처리 해주기 위해 후처리 과정을 도입해 사용했다. 예외 상황은 <strong><code class="language-plaintext highlighter-rouge">max_seq &gt; 1/2 * max_pos(==k)</code></strong> 일 때 발생한다. <code class="language-plaintext highlighter-rouge">official repo</code> 의 코드를 보면 <code class="language-plaintext highlighter-rouge">max_seq</code>와 <code class="language-plaintext highlighter-rouge">k</code>를 일치시켜 모델링 하기 때문에 파인튜닝 하는 상황이라면 이것을 몰라도 상관없겠지만, 하나 하나 모델을 직접 만드는 입장이라면 예외 상황을 반드시 기억하자.</p>

<p>한편, 이러한 인코딩 방식은 <code class="language-plaintext highlighter-rouge">word2vec</code> 의 <code class="language-plaintext highlighter-rouge">window size</code> 도입과 비슷한 원리(<code class="language-plaintext highlighter-rouge">의미는 주변 문맥에 의해 결정</code>)라고 생각하면 되는데, 윈도우 사이즈 범위에서 벗어난 토큰들은 주변 문맥으로 인정 하지 않겠다는(<code class="language-plaintext highlighter-rouge">negative sample</code>) 의도를 갖고 있다. 실제 구현은 텐서 내부 원소값의 범위를 사용자 지정 범위로 제한할 수 있는 <code class="language-plaintext highlighter-rouge">torch.clamp</code> 를 사용하면 <code class="language-plaintext highlighter-rouge">1</code>줄로 깔끔하게 만들 수 있으니 참고하자.</p>

<p><code class="language-plaintext highlighter-rouge">torch.clamp</code> 까지 적용하고 난 최종 결과를 살펴보자. 행백터, 열벡터 모두 <code class="language-plaintext highlighter-rouge">[0:2*max_seq]</code> 사이에서 정의되고 있으며, 개별 방향 벡터 원소의 최대값과 최소값의 차이가 항상 <code class="language-plaintext highlighter-rouge">k</code> 로 유지 된다. 의도대로 정확히 윈도우 사이즈만큼의 주변 맥락을 반영해 임베딩을 형성하고 있음을 알 수 있다.</p>

<p>정리하면, <code class="language-plaintext highlighter-rouge">Relative Position Embedding</code> 란 절대 위치 방식처럼 임베딩 룩업 테이블을 만들되, 사용자가 지정한 윈도우 사이즈에 해당하는 토큰의 임베딩 값만 추출해 새로운 행벡터를 여러 개 만들어 내는 기법이라고 할 수 있다. <strong>이 때 행벡터는 대상 토큰과 그 나머지 토큰 사이의 위치 변화에 따라 발생하는 파생적인 맥락 정보를 담고 있다.</strong></p>

<h4 id="-word-context-vs-relative-position-vs-absolute-position"><strong><code class="language-plaintext highlighter-rouge">🤔 Word Context vs Relative Position vs Absolute Position</code></strong></h4>

<p align="center">
<img src="/assets/images/deberta/line_people.png" alt="줄 서있는 사람들" class="align-center image-caption" width="40%&quot;, height=&quot;50%" />
<strong><em><a href="https://kr.freepik.com/premium-photo/people-standing-in-line-during-airport-check-in_8754408.htm">줄 서있는 사람들</a></em></strong>
</p>

<p>지금까지 <code class="language-plaintext highlighter-rouge">Relative Position Embedding</code>이 무엇이고, 도대체 어떤 문맥 정보를 포착한다는 것인지 알아봤다. 필자의 설명이 매끄럽지 못하기도 하고 예시를 텍스트로 들고 있어서 직관적으로 <code class="language-plaintext highlighter-rouge">word context</code>는 무엇인지, <code class="language-plaintext highlighter-rouge">Position</code> 정보와는 뭐가 다른지, 두 가지 <code class="language-plaintext highlighter-rouge">Position</code> 정보는 뭐가 어떻게 다른지 와닿지 않는 분들이 많으실 것 같다. 그래서 최대한 직관적인 예시를 통해 세가지 정보의 차이점을 설명해보려 한다. (필자 본인이 햇갈려서 쓰는 건 비밀이다)</p>

<p>사람 5명이 공항 체크인을 위해 서 있다. 모두 왼쪽을 보고 있는 것을 보아 왼쪽에 키가 제일 작은 여자가 가장 앞줄이라고 볼 수 있겠다. 우리는 줄 서있는 순서대로 5명의 사람에게 번호를 부여할 것이다. 편의상 0번부터 시작해 4번까지 번호를 주겠다. 1번에 해당하는 사람은 누구인가??  바로 줄의 2번째에 서있는 여자다. 그럼 2번에 해당하는 사람은 누구인가?? 사진 속 줄의 가장 중간에 있는 남자가 2번이다. 이렇게 그룹 단위(전체 줄)에서 개개인에 일련의 번호를 부여해 위치를 표현하는 방법이 바로 <code class="language-plaintext highlighter-rouge">Absolute Position Embedding</code>이다.</p>

<p>한편, 다시 2번 사람에게 주목해보자. 우리는 2번 남자를 전체 줄에서 가운데 위치한 사람이 아니라, 검정색 양복과 구두를 신고 손에 쥔 무언가를 응시하고 있는 사람이라고 표현할 수도 있다. 이것이 바로 토큰의 의미 정보를 담은 <code class="language-plaintext highlighter-rouge">word context</code>에 해당한다.</p>

<p>마지막으로 <code class="language-plaintext highlighter-rouge">Relative Position Embedding</code> 방식으로 2번 남자를 표현해보자. 오른손으로는 커피를 들고 다른 손으로는 캐리어를 잡고 있으며 검정색 하이힐과 베이지색 바지를 입은 <strong>1번 여자의 뒤에 있는 사람</strong>, 회색 양복과 검은 뿔테 안경을 쓰고 한 손에는 캐리어를 잡고 있는 <strong>4번 여자의 앞에 있는 사람</strong>, 검정색 자켓과 청바지를 입고 한 손에는 회색 코트를 들고 있는 줄의 <strong>맨 앞 여자로부터 2번째 뒤에 서있는 사람</strong>, 턱수염이 길고 머리가 긴 편이며 파란색 가디건을 입고 초록색과 검정색이 혼합된 가방을 왼쪽으로 메고 있는 <strong>남자로부터 2번째 앞에 있는 사람.</strong></p>

<p>이처럼 표현하는게 바로 <code class="language-plaintext highlighter-rouge">Relative Position Embedding</code>에 대응된다고 볼 수 있다. 이제 위 예시를 자연어 처리에 그대로 대입시켜보면 이해가 한결 수월할 것이다.</p>

<h4 id="-deberta-inductive-bias"><strong><code class="language-plaintext highlighter-rouge">🤔 DeBERTa Inductive Bias</code></strong></h4>
<p><strong>결국</strong> <code class="language-plaintext highlighter-rouge">DeBERTa</code><strong>는 두가지 위치 정보 포착 방식을 적절히 섞어서 모델이 더욱 풍부한 임베딩을 갖도록 하려는 의도로 설계 되었다.</strong> 또한 우리는 이미 모델이 다양한 맥락 정보를 포착할수록 <code class="language-plaintext highlighter-rouge">NLU Task</code> 에서 더 나은 성능을 기록한다는 사실을 <code class="language-plaintext highlighter-rouge">BERT</code>와 <code class="language-plaintext highlighter-rouge">GPT</code> 사례에서 알 수 있었다. 따라서 <code class="language-plaintext highlighter-rouge">Relative Position Embedding</code> 을 추가하여 <code class="language-plaintext highlighter-rouge">단어의 발생 순서</code> 를 포착하는 모델에 <code class="language-plaintext highlighter-rouge">단어 분포 가설</code> 적인 특징을 더해주려는 저자의 아이디어는 매우 타당하다고 볼 수 있겠다.</p>

<p>이제 관건은 <strong><code class="language-plaintext highlighter-rouge">“두가지 위치 정보를 어떤 방식으로 추출하고 섞어줄 것인가”</code></strong> 하는 물음에 답하는 것이다. 저자는 물음에 답하기 위해 <code class="language-plaintext highlighter-rouge">Disentangled Self-Attention</code> 과  <code class="language-plaintext highlighter-rouge">Enhanced Mask Decoder</code> 라는 새로운 기법 두가지를 제시한다. 전자는 <code class="language-plaintext highlighter-rouge">단어 분포 가설</code> 에 해당되는 맥락 정보를 추출하기 위한 기법이고, 후자는 <code class="language-plaintext highlighter-rouge">단어 발생 순서</code> 에 포함되는 임베딩을 모델에 주입하기 위해 설계되었다. 모델링 파트에서는 두가지 새로운 기법에 대해서 자세히 살펴본 뒤, 모델을 코드로 빌드하는 과정을 설명하려 한다.<br />
코드는 논문의 내용과 microsoft의 공식 git repo를 참고해 만들었음을 밝힌다. 다만, 논문에서 모델 구현과 관련해 세부적인 내용은 상당수 생략하고 있으며, repo에 공개된 코드는 hard coding되어 그 의도를 정확하게 파악하는데 많은 어려움이 있었다. 그래서 어느 정도는 필자의 주관적인 생각이 반영된 코드라는 점을 미리 밝힌다.</p>

<h3 id="modeling"><code class="language-plaintext highlighter-rouge">🌟 Modeling</code></h3>

<p align="center">
<img src="/assets/images/deberta/deberta_overview.png" alt="DeBERTa Model Structure" class="align-center image-caption" width="60%&quot;, height=&quot;50%" />
<strong><em><a href="https://www.youtube.com/watch?v=gcMyKUXbY8s&amp;t=838s&amp;ab_channel=%EA%B3%A0%EB%A0%A4%EB%8C%80%ED%95%99%EA%B5%90%EC%82%B0%EC%97%85%EA%B2%BD%EC%98%81%EA%B3%B5%ED%95%99%EB%B6%80DSBA%EC%97%B0%EA%B5%AC%EC%8B%A4">DeBERTa Model Structure</a></em></strong>
</p>

<ul>
  <li><strong>1) Disentangled Self-Attention Encoder Block for <code class="language-plaintext highlighter-rouge">Relative Position Embedding</code></strong></li>
  <li><strong>2) Enhanced Mask Decoder for <code class="language-plaintext highlighter-rouge">Absolute Position Embedding</code></strong></li>
</ul>

<p><code class="language-plaintext highlighter-rouge">DeBERTa</code> 의 전반적인 구조는 일반적인 <code class="language-plaintext highlighter-rouge">BERT</code>, <code class="language-plaintext highlighter-rouge">RoBERTa</code>와 크게 다른 점이 없다. 다만, 모델의 초반부 <code class="language-plaintext highlighter-rouge">Input Embedding</code> 에서 <code class="language-plaintext highlighter-rouge">Absolute Position</code> 정보를 추가하는 부분이 후반부 <code class="language-plaintext highlighter-rouge">Enhanced Mask Decoder</code>라 부르는 인코더 블록으로 옮겨간 것과 <code class="language-plaintext highlighter-rouge">Disentangled Self-Attention</code> 을 위해 개별 인코더 블록마다 상대 위치 정보를 출처로 하는 <code class="language-plaintext highlighter-rouge">linear projection</code> 레이어가 추가되었음을 명심하자. 또한, <code class="language-plaintext highlighter-rouge">DeBERTa</code>의 <code class="language-plaintext highlighter-rouge">pre-train</code> 은 <code class="language-plaintext highlighter-rouge">RoBERTa</code>처럼 <code class="language-plaintext highlighter-rouge">NSP</code>를 삭제하고 <code class="language-plaintext highlighter-rouge">MLM</code>만 사용한 점도 기억하자.</p>

<p align="center">
<img src="/assets/images/deberta/deberta_class_diagram.png" alt="DeBERTa Class Diagram" class="align-center image-caption" width="60%&quot;, height=&quot;50%" />
<strong><em>DeBERTa Class Diagram</em></strong>
</p>

<p>위 자료는 필자가 구현한 <code class="language-plaintext highlighter-rouge">DeBERTa</code>의 구조를 표현한 그림이다. 코드 리뷰에 참고하시면 좋을 것 같다 첨부했다. 가장 중요한 <code class="language-plaintext highlighter-rouge">Disentangled-Attention</code>과 <code class="language-plaintext highlighter-rouge">EMD</code>부터 살펴본 뒤, 나머지 객체에 대해서 살펴보자.</p>

<h4 id="disentangled-self-attention"><strong><code class="language-plaintext highlighter-rouge">🪢 Disentangled Self-Attention</code></strong></h4>

\[\tilde{A_{ij}} = Q_i^c•K_j^{cT} + Q_i^c•K_{∂(i,j)}^{rT} + K_j^c•Q_{∂(i,j)}^{rT} \\
Attention(Q_c,K_c,V_c,Q_r,K_r) = softmax(\frac{\tilde{A}}{\sqrt{3d_h}})*V_c\]

<p><code class="language-plaintext highlighter-rouge">Disentangled Self-Attention</code>은 저자가 퓨어한 <code class="language-plaintext highlighter-rouge">Input Embedding</code> 정보와 <code class="language-plaintext highlighter-rouge">Relative Position</code> 정보를 통합시키기 위해 고안한 변형 <code class="language-plaintext highlighter-rouge">Self-Attention</code> 기법이다. 기존의 <code class="language-plaintext highlighter-rouge">Self-Attention</code>과 다르게 <code class="language-plaintext highlighter-rouge">Position Embedding</code>을 <code class="language-plaintext highlighter-rouge">Input Embedding</code>와 더하지 않고 따로 사용한다. <strong>즉, 같은</strong> $d_h$ <strong>공간에 <code class="language-plaintext highlighter-rouge">Input Embedding</code>과 <code class="language-plaintext highlighter-rouge">Relative Position</code>이라는 서로 다른 두 벡터를 맵핑하고 그 관계성을 파악해보겠다는 뜻이다.</strong></p>

<p><code class="language-plaintext highlighter-rouge">Input</code>과 <code class="language-plaintext highlighter-rouge">Position</code> 정보를 서로 주체적인 입장에서 한 번씩 내적한다고 해서 <code class="language-plaintext highlighter-rouge">Disentangled</code>라는 이름이 붙게 되었다. <code class="language-plaintext highlighter-rouge">Transformer-XL</code>, <code class="language-plaintext highlighter-rouge">XLNet</code>에 제시된 <code class="language-plaintext highlighter-rouge">Cross-Attention</code>과 매우 유사한 개념이다. 첫번째 수식에서 가장 마지막 항을 제외하면 <code class="language-plaintext highlighter-rouge">Cross-Attention</code>과 포착하는 정보가 동일하다고 저자 역시 밝히고 있으니 참고하자.</p>

<p><code class="language-plaintext highlighter-rouge">Disentangled Self-Attention</code> 은 총 5가지 <code class="language-plaintext highlighter-rouge">linear projection matrix</code>를 사용한다. <code class="language-plaintext highlighter-rouge">Input Embedding</code> 을 출처로 하는 $Q^c, k^c, V^c$, 그리고 <code class="language-plaintext highlighter-rouge">Position Embedding</code>을 출처로 하는 $Q^r, K^r$이다. 첨자 $c,r$은 각각 <code class="language-plaintext highlighter-rouge">content</code>, <code class="language-plaintext highlighter-rouge">relative</code> 의 약자로 행렬의 출처를 뜻한다. 한편 행렬 아래 첨자에 써있는 $i,j$는 각각 현재 어텐션 대상 토큰의 인덱스와 그 나머지 토큰의 인덱스를 가리킨다. 그래서 $\tilde{A_{ij}}$는 <code class="language-plaintext highlighter-rouge">[NxN]</code> 크기 행렬(<code class="language-plaintext highlighter-rouge">기존 어텐션에서 쿼리와 키의 내적 결과에 해당</code>)의 $i$번째 행백터의 $j$번째 원소의 값을 의미한다. <code class="language-plaintext highlighter-rouge">Input Embedding</code> 정보와 <code class="language-plaintext highlighter-rouge">Relative Position</code> 정보를 따로 따로 관리하기 때문에 우리가 기존에 알고 있던 <code class="language-plaintext highlighter-rouge">Self-Attention</code>과는 사뭇 다른 수식이다. 이제부터 수식의 항 하나하나의 의미를 구체적인 예시와 함꼐 파헤쳐보자.</p>

<p><strong><code class="language-plaintext highlighter-rouge">☺️ c2c matrix</code></strong><br />
<code class="language-plaintext highlighter-rouge">content2content</code>의 약자로 첫번째 수식 우변의 첫번째 항을 가리키는 말이다. 이름의 의미는 내적에 사용하는 두 행렬의 출처가 모두 <code class="language-plaintext highlighter-rouge">Input Embedding</code> 이라는 사실을 내포하고 있다. 기존에 알고 있던 <code class="language-plaintext highlighter-rouge">Self-Attention</code> 의 두번째 단계인 $Q•K^T$와 거의 동일한 의미를 담고 있는 항이라고 생각하면 될 것 같다. 완전히 같다고 할 수 없는 이유는 <code class="language-plaintext highlighter-rouge">Absolute Position</code> 정보가 빠진채로 내적했기 때문이다.</p>

<p>따라서 연산의 의미 역시 우리가 기존에 알고 있던 바와 동일하다. 혹시 행렬 $Q,K,V$와 <code class="language-plaintext highlighter-rouge">Self-Attention</code>가 내포하는 의미에 대해 자세히 궁금하신 분이라면 필자가 작성한 <a href="https://qcqced123.github.io/nlp/transformer">Transformer논문 리뷰</a>를 보고 오시길 바란다. 그래도 어차피 뒤에 남은 두개의 항을 설명하려면 어차피 예시를 들어야 하기 때문에 <code class="language-plaintext highlighter-rouge">c2c</code>항에서부터 시작해보려 한다.</p>

<p>당신은 오늘 저녁 밥으로 <strong>차돌박이 된장 찌개, 삼겹살 그리고 후식으로 구운 달걀을 먹고 싶다.</strong> 집에 재료가 하나도 없지만 마트에 가기 귀찮으니 <strong>필요한 식자재를 남편에게 사오라고 시킬 생각이다.</strong> 당신은 그래서 필요한 재료 리스트를 적고 있다. <strong>그렇다면 필요한 재료를 어떤 식으로 표현해서 적어줘야 남편이 가장 빠르고 정확하게 필요한 모든 식자재를 사올 수 있을까??</strong></p>

<p>이것을 고민하는게 바로 행렬 $Q^c$와 <code class="language-plaintext highlighter-rouge">linear projector</code> 인 $W_{Q^c}$의 역할이다.예를 들어 같은 앞다리살이라도 구이용이 있고 찌개용이 있다. 달걀도 구운 달걀이 있고 날달걀이 있다. 정확히 용도를 적어주는게 남편 입장에서는 아내의 의도대로 정확하게 장을 보기 훨씬 편할 것이다.</p>

<p>한편, 내적은 본래 파라미터가 필요한 연산은 아니라서 실제 손실함수 오차 역전을 통해 최적화(학습)되는 대상은 바로 $W_{Q^c}$가 된다. 남편이 장을 빠르고 정확하게 보는데 과연 당신이 적어준 리스트만 영향을 미칠까??</p>

<p>아니다. 당신이 어떤 음식을 위해 어떤 재료가 필요한지 그 의도를 잘 적어주는 것도 중요하지만 실제 마트에 적혀 있는 상품명과 상품설명 역시 중요하다. 좀 억지스러운 예시처럼 보이긴 하지만 달걀의 경우 육안으로만 보면 이것이 구운 달걀인지 날달걀인지 구분할 수 없다. 그런데 마트에 별다른 설명없이 상품명으로 <code class="language-plaintext highlighter-rouge">“달걀”</code> 이라고만 적혀있다 생각해보자.</p>

<p>아무리 당신이 좋은 행렬 $Q^c$를 표현해줘도 남편이 날달걀을 사올 확률이 꽤나 높을 것이다. 이렇게 마트에 적혀있는 상품명과 상품설명이 바로 행렬 $K^c$에 대응된다. 그리고 물건을 사기 위해 당신이 적어준 식자재 리스트와 매장에 적힌 상품명과 상품설명을 대조하며 이것이 의도에 맞는 상품인지 따져보는 작업이 바로 $Q_i^c•K_j^{cT}$, <code class="language-plaintext highlighter-rouge">c2c matrix</code>가 된다.</p>

<p>다만, 전역 어텐션을 사용하기 때문에 달걀을 사기 위해 매장에 있는 모든 상품과 대조를 한다고 생각하면 된다. 특히 기존 전역 어텐션의 $Q_i^c•K_j^{cT}$의 경우 모든 상품과 대조하는 과정에서 대조군이 매장에 전시된 위치, 카테고리 분류상 어느 코너에 속하는지 등의 위치 정보를 한꺼번에 고려하지만, 우리의(<code class="language-plaintext highlighter-rouge">c2c matrix</code>) 경우 여기서 이런 위치 정보를 전혀 고려하지 않고 뒤에 두 개의 항에서 따로 고려한다.</p>

<p>정리하면, <code class="language-plaintext highlighter-rouge">c2c</code>는 매장에 진열된 식자재의 상품명 및 설명만 가지고 내가 사야 하는 식재료인지 아닌지 판단하는 작업을 수학적으로 모델링 했다고 볼 수 있겠다. 자연어 처리 맥락에서 바라보면, 특정 토큰의 의미를 알기 위해서 <code class="language-plaintext highlighter-rouge">syntactical</code>한 정보없이 순수하게 나머지 다른 토큰들의 의미를 가중합으로 반영하는 행위에 대응된다.</p>

<p><strong><code class="language-plaintext highlighter-rouge">🗂️ c2p matrix</code></strong></p>

\[c2p = Q_i^c•K_{∂(i,j)}^{rT}\]

<p><code class="language-plaintext highlighter-rouge">content2position</code>의 약자로 수식 우변의 두번째 항, $Q_i^c•K_{∂(i,j)}^{rT}$를 가리킨다. <code class="language-plaintext highlighter-rouge">c2c</code>때와는 다르게 서로 출처가 다른 두 행렬을 사용해 <code class="language-plaintext highlighter-rouge">c2p</code>라는 이름을 붙였다. 내적 대상의 쿼리는 <code class="language-plaintext highlighter-rouge">Input Embedding</code>으로부터 만든 행렬 $Q_i^c$, 키는 <code class="language-plaintext highlighter-rouge">Position Embedding</code>으로부터 만든 행렬 $K_{∂(i,j)}^{rT}$ 을 사용했다. <code class="language-plaintext highlighter-rouge">word context</code>와 <code class="language-plaintext highlighter-rouge">relative position</code>을 서로 대조한다는 것이 무슨 의미를 갖는지 직관적으로 알기 힘드니 장보기 예시를 통해 이해해보자.</p>

<p>구운 달걀과 날달걀의 예시를 들면서 상품명과 설명이 장보기에 중요한 영향을 미친다고 언급했다. 하지만 상품명과 설명이 여전히 단순 <code class="language-plaintext highlighter-rouge">“달걀”</code>으로 적혀 있어도 우리는 이것을 구분해 낼 방법이 있다. 바로 주변에 진열된 상품이 무엇인지 살펴보는 것이다. <code class="language-plaintext highlighter-rouge">“달걀”</code> 바로 옆에 우유, 치즈, 생선, 정육과 같은 신선식품류가 배치되어 있다고 가정해보자. 우리는 우리 눈 앞에 있는 <code class="language-plaintext highlighter-rouge">“달걀”</code>이 날달걀이라고 기대해 봄직하다. 만약 <code class="language-plaintext highlighter-rouge">“달걀”</code> 옆에 쥐포, 말린 오징어, 육포, 과자 같은 간식류 상품들이 배치되어 있다면 어떨까?? 그럼 이 <code class="language-plaintext highlighter-rouge">“달걀”</code>은 충분히 구운 달걀이라고 해석해볼 수 있다. 이처럼 주위에 어떤 다른 상품들이 배치 되어 있는가를 통해 우리가 사려는 물건이 맞는지 대조해보는 행위가 바로 <code class="language-plaintext highlighter-rouge">c2p</code> 에 대응된다. 그렇다면 주위에 어떤 다른 상품들이 배치 되어 있는가 정보를 모아 놓은 것이 바로 $K_{∂(i,j)}^{rT}$가 된다.</p>

<p><strong><code class="language-plaintext highlighter-rouge">🔬 p2c matrix</code></strong></p>

\[p2c = K_j^c•Q_{∂(i,j)}^{rT}\]

<p><code class="language-plaintext highlighter-rouge">Disentangled Self-Attention</code>이 여타 다른 어텐션 기법들과 가장 차별화되는 부분이다. 저자가 논문에서 가장 강조하는 부분이기도 하다. 사실 그런 것치고는 논문 속 설명이 상당히 불친절해 이해하기 참 난해한 개념이다. 이거 설명하고 싶어서 장보기 예시를 생각해내게 되었다. 다시 남편에게 줄 장보기 리스트를 작성하던 시점으로 돌아가보자.</p>

<p>오늘 저녁 메뉴는 차돌박이 된장찌개와 구운 삼겹살이다. 먼저 차돌박이 된장찌개를 만들려면 어떤 재료가 필요할까?? 차돌박이, 된장, 청양고추, 양파, 다진 마늘, 호박과 같은 식자재가 필요할 것이다. 그리고 삼겹살에 필요한 재료를 생각해보자. 생삼겹살과 잡내를 없애는데 필요한 후추와 소금 그리고 구워 먹을 통마늘이  필요하다고 당신은 생각했다. 그럼 이제 이것을 바탕으로 리스트를 작성할 것이다. 어떤 식으로 리스트를 작성하는게 가장 최적일까??</p>

<p><code class="language-plaintext highlighter-rouge">c2c</code>, <code class="language-plaintext highlighter-rouge">c2p</code> 예시와 함께 생각해보면 알 수 있다. <code class="language-plaintext highlighter-rouge">c2c</code>에서는 같은 재료라도 그 용도에 따라서 사야할 품목이 달라진다고 언급한 바있다. <code class="language-plaintext highlighter-rouge">c2p</code> 에서는 정확한 설명이 없어도 주변에 나열된 품목들을 보면서 어떤 상품인지 유추가 가능하다고 했다. 이것을 합쳐보자. 만약 당신이 아래와 같은 순서로 리스트를 적었다고 가정해보겠다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 장보기 리스트 예시1
</span>
<span class="n">차돌박이</span><span class="p">,</span> <span class="n">된장</span><span class="p">,</span> <span class="n">마늘</span><span class="p">,</span> <span class="n">청양고추</span><span class="p">,</span> <span class="n">양파</span><span class="p">,</span> <span class="n">호박</span><span class="p">,</span> <span class="n">삼겹살</span><span class="p">,</span> <span class="n">후추</span><span class="p">,</span> <span class="n">소금</span>
</code></pre></div></div>

<p>아까 필요한 품목을 나열했을 때 분명히 다진 마늘과 통마늘을 동시에 생각했었다. 근데 위처럼 리스트를 작성해서 남편에게 줬다면 남편은 어떤 마늘을 사올까?? 당연히 차돌박이와 된장 그리고 양파 사이에 마늘이 위치한 것을 보고 남편은 국물용 마늘이 필요하구나 싶어서 다진 마늘을 사올 것이다.</p>

<p>그렇다면 반대로 당신이 아래처럼 리스트를 작성했다고 생각해보자.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 장보기 리스트 예시2
</span>
<span class="n">차돌박이</span><span class="p">,</span> <span class="n">된장</span><span class="p">,</span> <span class="n">청양고추</span><span class="p">,</span> <span class="n">양파</span><span class="p">,</span> <span class="n">호박</span><span class="p">,</span> <span class="n">삼겹살</span><span class="p">,</span> <span class="n">마늘</span><span class="p">,</span> <span class="n">후추</span><span class="p">,</span> <span class="n">소금</span>
</code></pre></div></div>

<p>이번에는 삼겹살 구울 때, 같이 구워먹을 통마늘이 필요하구나를 남편이 느낄 수 있을 것이다. 한편 아래와 같은 상황이라면 어떨까??</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 장보기 리스트 예시3
</span>
<span class="n">차돌박이</span><span class="p">,</span> <span class="n">된장</span><span class="p">,</span> <span class="n">마늘</span><span class="p">,</span> <span class="n">청양고추</span><span class="p">,</span> <span class="n">양파</span><span class="p">,</span> <span class="n">호박</span><span class="p">,</span> <span class="n">삼겹살</span><span class="p">,</span> <span class="n">마늘</span><span class="p">,</span> <span class="n">후추</span><span class="p">,</span> <span class="n">소금</span>
</code></pre></div></div>

<p>조금 센스가 있는 남편이라면 된장찌개 국물용 다진마늘과 삼겹살 구이용 통마늘이 동시에 필요하구나라고 유추하고 매장에서 다진마늘, 통마늘이라 써있는 품목을 찾아서 둘 다 사올 것이다. 물론 센스있는 아내라면 애초에 저렇게 애매하게 <code class="language-plaintext highlighter-rouge">마늘</code>이라고 2번 안적고 <code class="language-plaintext highlighter-rouge">다진마늘</code>, <code class="language-plaintext highlighter-rouge">통마늘</code>이라고 용도를 함께 적어줬겠지만 말이다.</p>

<p>이러한 일련의 상황이 바로 <code class="language-plaintext highlighter-rouge">p2c</code>에 대응된다. 그렇다면 아내가 적어준 리스트에서 주변에 위치한 품목들에 따라서 포착되는 대상 품목의 용도나 쓰임새, 의미 등이 바로 행렬 $Q_{∂(i,j)}^{rT}$가 된다.</p>

<p><strong><code class="language-plaintext highlighter-rouge">⚒️ DeBERTa Scale Factor</code></strong><br />
처음에 나열한 수식을 다시 보면 <code class="language-plaintext highlighter-rouge">DeBERTa</code>의 <code class="language-plaintext highlighter-rouge">scale factor</code>는 기존 <code class="language-plaintext highlighter-rouge">Self-Attention</code> 과 다르게 $\sqrt{3d_h}$를 사용한다. 이유가 뭘까?? 기존 방식은 <code class="language-plaintext highlighter-rouge">softmax layer</code>에 전달하는 행렬의 종류가 $Q•K^T$ 한 개다. <code class="language-plaintext highlighter-rouge">DeBERTa</code>의 경우는 3개를 전달하게 된다. 그래서 $d_h$앞에 3을 곱해준 것이다. official repo의 코드를 확인해보면 확실히 알 수 있는데, 어텐션에 사용하는 행렬 종류의 개수를 $d_h$앞에 곱해준다. 아래는 <code class="language-plaintext highlighter-rouge">repo</code>에 올라와 있는 코드의 일부를 발췌한 것이다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># official Disentangled Self-Attention by microsoft from official repo
</span>
<span class="p">...</span><span class="n">중략</span><span class="p">...</span>
<span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">,</span> <span class="n">return_att</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">query_states</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">relative_pos</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">rel_embeddings</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">query_states</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">query_states</span> <span class="o">=</span> <span class="n">hidden_states</span>
    <span class="n">query_layer</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">transpose_for_scores</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">query_proj</span><span class="p">(</span><span class="n">query_states</span><span class="p">),</span> <span class="bp">self</span><span class="p">.</span><span class="n">num_attention_heads</span><span class="p">).</span><span class="nb">float</span><span class="p">()</span>
    <span class="n">key_layer</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">transpose_for_scores</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">key_proj</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">),</span> <span class="bp">self</span><span class="p">.</span><span class="n">num_attention_heads</span><span class="p">).</span><span class="nb">float</span><span class="p">()</span>
    <span class="n">value_layer</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">transpose_for_scores</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">value_proj</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">),</span> <span class="bp">self</span><span class="p">.</span><span class="n">num_attention_heads</span><span class="p">)</span>
    
    <span class="n">rel_att</span> <span class="o">=</span> <span class="bp">None</span>
    <span class="c1"># Take the dot product between "query" and "key" to get the raw attention scores.
</span>    <span class="n">scale_factor</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="k">if</span> <span class="s">'c2p'</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">pos_att_type</span><span class="p">:</span>
        <span class="n">scale_factor</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">if</span> <span class="s">'p2c'</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">pos_att_type</span><span class="p">:</span>
        <span class="n">scale_factor</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">if</span> <span class="s">'p2p'</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">pos_att_type</span><span class="p">:</span>
        <span class="n">scale_factor</span> <span class="o">+=</span> <span class="mi">1</span>
</code></pre></div></div>

<p><strong><code class="language-plaintext highlighter-rouge">👩‍💻 Implementation</code></strong><br />
이렇게 <code class="language-plaintext highlighter-rouge">Disentangled Self-Attention</code>에 대한 모든 내용을 살펴봤다. 실제 구현은 어떻게 해야 하는지 필자가 작성한 파이토치 코드와 함께 알아보자.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Pytorch Implementation of DeBERTa Disentangled Self-Attention
</span>
<span class="k">def</span> <span class="nf">build_relative_position</span><span class="p">(</span><span class="n">x_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
    <span class="s">"""
    Build Relative Position Matrix for Disentangled Self-Attention in DeBERTa
    Args:
        x_size: sequence length of query matrix
    Reference:
        https://github.com/microsoft/DeBERTa/blob/master/DeBERTa/deberta/da_utils.py#L29
        https://arxiv.org/abs/2006.03654
    """</span>
    <span class="n">x_index</span><span class="p">,</span> <span class="n">y_index</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="n">x_size</span><span class="p">),</span> <span class="n">torch</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="n">x_size</span><span class="p">)</span>  <span class="c1"># same as rel_pos in official repo
</span>    <span class="n">rel_pos</span> <span class="o">=</span> <span class="n">x_index</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="o">-</span> <span class="n">y_index</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">rel_pos</span>

<span class="k">def</span> <span class="nf">disentangled_attention</span><span class="p">(</span><span class="n">q</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">k</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">v</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">qr</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">kr</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">dropout</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Dropout</span><span class="p">,</span> <span class="n">mask</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="bp">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
    <span class="s">"""
    Disentangled Self-Attention for DeBERTa, same role as Module "DisentangledSelfAttention" in official Repo
    Args:
        q: content query matrix, shape (batch_size, seq_len, dim_head)
        k: content key matrix, shape (batch_size, seq_len, dim_head)
        v: content value matrix, shape (batch_size, seq_len, dim_head)
        qr: position query matrix, shape (batch_size, 2*max_relative_position, dim_head), r means relative position
        kr: position key matrix, shape (batch_size, 2*max_relative_position, dim_head), r means relative position
        dropout: dropout for attention matrix, default rate is 0.1 from official paper
        mask: mask for attention matrix, shape (batch_size, seq_len, seq_len), apply before softmax layer
    Math:
        c2c = torch.matmul(q, k.transpose(-1, -2))  # A_c2c
        c2p = torch.gather(torch.matmul(q, kr.transpose(-1, -2)), dim=-1, index=c2p_pos)
        p2c = torch.gather(torch.matmul(qr, k.transpose(-1, -2)), dim=-2, index=c2p_pos)
        Attention Matrix = c2c + c2p + p2c
        A = softmax(Attention Matrix/sqrt(3*D_h)), SA(z) = Av
    Notes:
        dot_scale(range 1 ~ 3): scale factor for Q•K^T result, sqrt(3*dim_head) from official paper by microsoft,
        3 means that use full attention matrix(c2c, c2p, p2c), same as number of using what kind of matrix
        default 1, c2c is always used and c2p &amp; p2c is optional
    References:
        https://github.com/microsoft/DeBERTa/blob/master/DeBERTa/deberta/disentangled_attention.py
        https://arxiv.org/pdf/1803.02155.pdf
        https://arxiv.org/abs/2006.03654
        https://arxiv.org/abs/2111.09543
        https://arxiv.org/abs/1901.02860
        https://arxiv.org/abs/1906.08237
    """</span>
    <span class="n">scale_factor</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">c2c</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">))</span>  <span class="c1"># A_c2c
</span>
    <span class="n">c2p_att</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">kr</span><span class="p">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">))</span>
    <span class="n">c2p_pos</span> <span class="o">=</span> <span class="n">build_relative_position</span><span class="p">(</span><span class="n">q</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="o">+</span> <span class="n">kr</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">/</span> <span class="mi">2</span>  <span class="c1"># same as rel_pos in official repo
</span>    <span class="n">c2p_pos</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">c2p_pos</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">kr</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span><span class="p">).</span><span class="n">repeat</span><span class="p">(</span><span class="n">q</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">c2p</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">gather</span><span class="p">(</span><span class="n">c2p_att</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">c2p_pos</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">c2p</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">scale_factor</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="n">p2c_att</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">qr</span><span class="p">,</span> <span class="n">k</span><span class="p">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">))</span>
    <span class="n">p2c</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">gather</span><span class="p">(</span><span class="n">p2c_att</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">2</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">c2p_pos</span><span class="p">)</span>  <span class="c1"># same as torch.gather(k•qr^t, dim=-1, index=c2p_pos)
</span>    <span class="k">if</span> <span class="n">p2c</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">scale_factor</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="n">dot_scale</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">scale_factor</span> <span class="o">*</span> <span class="n">q</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]))</span>  <span class="c1"># from official paper by microsoft
</span>    <span class="n">attention_matrix</span> <span class="o">=</span> <span class="p">(</span><span class="n">c2c</span> <span class="o">+</span> <span class="n">c2p</span> <span class="o">+</span> <span class="n">p2c</span><span class="p">)</span> <span class="o">/</span> <span class="n">dot_scale</span>  <span class="c1"># Attention Matrix = A_c2c + A_c2r + A_r2c
</span>    <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">attention_matrix</span> <span class="o">=</span> <span class="n">attention_matrix</span><span class="p">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">mask</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="s">'-inf'</span><span class="p">))</span>  <span class="c1"># Padding Token Masking
</span>    <span class="n">attention_dist</span> <span class="o">=</span> <span class="n">dropout</span><span class="p">(</span>
        <span class="n">F</span><span class="p">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">attention_matrix</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="p">)</span>
    <span class="n">attention_matrix</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">attention_dist</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">attention_matrix</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">p2c</code> 를 구하는 과정의 코드라인에 주목해보자. 논문에 기재된 수식($K_j^c•Q_{∂(i,j)}^{rT}$)과 다르게, 쿼리와 키의 순서를 뒤집었다. 그래서 <code class="language-plaintext highlighter-rouge">torch.gather</code>의 차원 매개변수 <code class="language-plaintext highlighter-rouge">dim</code>를 <code class="language-plaintext highlighter-rouge">c2p</code>의 상황과 다르게 -<code class="language-plaintext highlighter-rouge">2</code>로 초기화하게 되었다. 내적하는 항의 순서를 뒤집은 것으로 인해 우리가 추출하고 싶은 대상 값인 상대 위치 임베딩이 <code class="language-plaintext highlighter-rouge">-2</code>번째 차원에 위치 하게 되기 때문이다.</p>

<h4 id="enhanced-mask-decoder"><strong><code class="language-plaintext highlighter-rouge">😷 Enhanced Mask Decoder</code></strong></h4>
<p><code class="language-plaintext highlighter-rouge">DeBERTa</code>의 설계 목적은 2가지 위치 정보를 적절히 섞어서 최대한 풍부한 임베딩을 만드는 것이라고 했다. 상대 위치 임베딩은 <code class="language-plaintext highlighter-rouge">Disentangled Self-Attention</code>을 통해 포착한다는 것을 이제 알았다. 그럼 절대 위치 임베딩은 어떤 식으로 모델링해줘야 할까?? 그 물음에 답은 바로 <code class="language-plaintext highlighter-rouge">EMD</code>라 불리는 <code class="language-plaintext highlighter-rouge">Enhanced Mask Decoder</code>에 있다. <code class="language-plaintext highlighter-rouge">EMD</code>의 원리에 대해 공부하기 전에 왜 절대 위치 임베딩이 <code class="language-plaintext highlighter-rouge">NLU</code>에 필요한지 짚고 넘어가자.</p>

<p><strong><center><i>a new <u>"store"</u> opened beside the new <u>"mall”</u></i></center></strong></p>

<p>위 문장은 저자가 논문에서 <code class="language-plaintext highlighter-rouge">Absolute Position Embedding</code>의 필요성을 역설할 때 사용한 예시 문장이다. 과연 상대 위치 임베딩만 사용해서 <strong><code class="language-plaintext highlighter-rouge">store</code></strong>와 <strong><code class="language-plaintext highlighter-rouge">mall</code></strong>의 차이를 잘 구별할 수 있을까 생각해보자. 앞서 우리는 상대 위치 임베딩을 <strong>대상 토큰과 그 나머지 토큰 사이의 위치 변화에 따라 발생하는 파생적인 맥락 정보를 담은 행렬</strong>이라고 정의한 바 있다. 다시 말해, 대상 토큰의 의미를 주변에 어떤 <code class="language-plaintext highlighter-rouge">context</code>가 있는지 파악해 통해 이해해보겠다는 것이다.</p>

<p>예시 문장을 다시 보자. 두 대상 단어 모두 주위에 비슷한 의미를 갖는 단어들이 위치해 있다. 이런 경우 상대 위치 임베딩만으로는 시퀀스 내부에서 <strong><code class="language-plaintext highlighter-rouge">store</code></strong>와 <strong><code class="language-plaintext highlighter-rouge">mall</code></strong>의 의미 차이를 모델이 명확하게 이해하기 매우 어려울 것이다. 현재 상황에서 두 단어의 뉘앙스 차이는 결국 문장의 주어냐 목적어냐 하는 <code class="language-plaintext highlighter-rouge">syntactical</code>한 정보에 의해서 결정된다. <code class="language-plaintext highlighter-rouge">syntactical</code>한 정보의 필요성은 바로 절대 위치 임베딩이 <code class="language-plaintext highlighter-rouge">NLU</code>에 꼭 필요한 이유에 대응된다.</p>

<p align="center">
<img src="/assets/images/deberta/emd_overview.png" alt="Enhanced Mask Decoder Overview" class="align-center image-caption" width="60%&quot;, height=&quot;50%" />
<strong><em><a href="https://arxiv.org/abs/2006.03654">Enhanced Mask Decoder Overview</a></em></strong>
</p>

<p><strong><code class="language-plaintext highlighter-rouge">🤔 why named decoder</code></strong><br />
필자는 처음 논문을 읽었을 때 <code class="language-plaintext highlighter-rouge">Decoder</code>라는 이름을 보면서 참 의아했다. 분명 <code class="language-plaintext highlighter-rouge">Only-Encoder</code> 모델로 알고 있는데 어찌하여 이름에 디코더가 붙는 모듈이 있는 것인가. 그렇다고 이름을 저렇게 붙인 의도를 설명하는 것도 아니다. 그래서 필자가 스스로 추측해봤다.</p>

<p><code class="language-plaintext highlighter-rouge">DeBERTa</code>는 <code class="language-plaintext highlighter-rouge">pre-train task</code> 로 <code class="language-plaintext highlighter-rouge">MLM</code>을 사용했다. <code class="language-plaintext highlighter-rouge">MLM</code>이 무엇인가?? 바로 마스킹된 자리에 적절한 토큰을 찾는 빈칸 채우기 문제다. 영미권에서는 이것을 <code class="language-plaintext highlighter-rouge">denoising</code>한다고 표현하기도 하는데, <code class="language-plaintext highlighter-rouge">Absolute Position Embedding</code>이 바로 이 <code class="language-plaintext highlighter-rouge">denoising</code>에 지대한 영향력을 미친다는 언급을 논문에서 찾아볼 수 있다. 따라서 <code class="language-plaintext highlighter-rouge">denoising</code> 성능에 큰 영향을 주는 <code class="language-plaintext highlighter-rouge">Absolute Position Embedding</code>을 활용한다고 해서 이름에 <code class="language-plaintext highlighter-rouge">decoder</code>를 붙였지 않았나 예상해본다.</p>

<p>논문에 같이 실린 그림을 통해서도 추측이 가능하다. <code class="language-plaintext highlighter-rouge">EMD</code> 의 구조를 설명하면서 옆에 BERT의 모식도도 함께 제공하는데, <code class="language-plaintext highlighter-rouge">BERT</code>에는 <code class="language-plaintext highlighter-rouge">Decoder</code>가 전혀 없다. 그런데도 이름을 <code class="language-plaintext highlighter-rouge">BERT decoding layer</code>라고 부르는 것보면 필자의 추측에 좀 더 정당성을 부여하는 것 같다.</p>

<p>(+ 추가) offical repo code에서도 <code class="language-plaintext highlighter-rouge">EMD</code>가 우리가 아는 그 <code class="language-plaintext highlighter-rouge">Encoder</code>를 사용한다는 사실을 확인할 수 있다.</p>

<p><strong><code class="language-plaintext highlighter-rouge">🤷‍♂️ How to add Absolute Position</code></strong></p>

<p align="center">
<img src="/assets/images/deberta/deberta_overview.png" alt="DeBERTa Model Structure" class="align-center image-caption" width="45%&quot;, height=&quot;50%" />
<strong><em><a href="https://www.youtube.com/watch?v=gcMyKUXbY8s&amp;t=838s&amp;ab_channel=%EA%B3%A0%EB%A0%A4%EB%8C%80%ED%95%99%EA%B5%90%EC%82%B0%EC%97%85%EA%B2%BD%EC%98%81%EA%B3%B5%ED%95%99%EB%B6%80DSBA%EC%97%B0%EA%B5%AC%EC%8B%A4">DeBERTa Model Structure</a></em></strong>
</p>

<p>이제 <code class="language-plaintext highlighter-rouge">EMD</code>가 무엇이며, <code class="language-plaintext highlighter-rouge">Absolute Position</code>을 어떻게 모델에 추가하는지 알아보자. <code class="language-plaintext highlighter-rouge">EMD</code>는 <code class="language-plaintext highlighter-rouge">MLM</code> 성능을 높이기 위해 고안된 구조다. 그래서 토큰 예측을 위한 <code class="language-plaintext highlighter-rouge">feedforward &amp; softmax</code> 레이어 직전에 쌓는다. 몇개의 <code class="language-plaintext highlighter-rouge">EMD</code>를 쌓을 것인지는 하이퍼파리미터이며, 저자의 실험 결과 <code class="language-plaintext highlighter-rouge">2</code>개 사용하는게 가장 효율적이라고 한다. 새롭게 인코더 블럭을 쌓지 않고 <code class="language-plaintext highlighter-rouge">Disentangled-Attention</code> 레이어의 가장 마지막 인코더 블럭과 가중치를 공유하는 형태로 구현한다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># EMD Implementation Example
</span>
<span class="k">class</span> <span class="nc">EnhancedMaskDecoder</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">encoder</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">nn</span><span class="p">.</span><span class="n">ModuleList</span><span class="p">],</span> <span class="n">dim_model</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1024</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">EnhancedMaskDecoder</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">emd_layers</span> <span class="o">=</span> <span class="n">encoder</span>

<span class="k">class</span> <span class="nc">DeBERTa</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">max_seq</span><span class="p">:</span> <span class="mi">512</span><span class="p">,</span> <span class="n">N</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">24</span><span class="p">,</span> <span class="n">N_EMD</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="n">dim_model</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1024</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">16</span><span class="p">,</span> <span class="n">dim_ffn</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">4096</span><span class="p">,</span> <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
			<span class="c1"># Init Sub-Blocks &amp; Modules
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">DeBERTaEncoder</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">max_seq</span><span class="p">,</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">N</span><span class="p">,</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span><span class="p">,</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">num_heads</span><span class="p">,</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">dim_ffn</span><span class="p">,</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">dropout_prob</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">emd_layers</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="p">.</span><span class="n">encoder</span><span class="p">.</span><span class="n">encoder_layers</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">N_EMD</span><span class="p">)].</span> <span class="c1"># weight share
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">emd_encoder</span> <span class="o">=</span> <span class="n">EnhancedMaskDecoder</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">emd_layers</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span><span class="p">)</span>
</code></pre></div></div>

<p>따라서 <code class="language-plaintext highlighter-rouge">N_EMD=2</code> 로 설정한다는 것은 결국, <code class="language-plaintext highlighter-rouge">Disentangled-Attention</code> 레이어의 가장 마지막 인코더 블럭을 2개 더 쌓는 것과 동치다. 대신 인코더의 <code class="language-plaintext highlighter-rouge">linear projection</code> 레이어의 입력값이 다르다. <code class="language-plaintext highlighter-rouge">Disentangled-Attention</code> 의 행렬 $Q^c, K^c, V^c$는 이전 블럭의 <code class="language-plaintext highlighter-rouge">hidden_states</code> 값인 행렬 $H$를 입력으로, 행렬 $Q^r, K^r$은 레이어의 위치에 상관없이 모두 같은 값의 <code class="language-plaintext highlighter-rouge">Relative Position Embedding</code> 을 입력으로 사용한다.</p>

<p>반면, <code class="language-plaintext highlighter-rouge">EMD</code> 맨 처음 인코더 블럭의 행렬 $Q^c$는 바로 직전 블럭의 <code class="language-plaintext highlighter-rouge">hidden_states</code> 에 <code class="language-plaintext highlighter-rouge">Absolute Position Embedding</code>을 더한 값을 입력으로 사용한다. 이후 나머지 블럭에는 <code class="language-plaintext highlighter-rouge">Disentangled-Attention</code> 와 마찬가지로 이전 블럭의 <code class="language-plaintext highlighter-rouge">hidden_states</code> 를 사용한다. 행렬 $K^c, V^c$는 블럭 순서에 상관없이 이전 블럭의 <code class="language-plaintext highlighter-rouge">hidden_states</code> 만 가지고 <code class="language-plaintext highlighter-rouge">linear projection</code>을 수행한다. 그리고 행렬 $Q^r, K^r$ 역시 같은 값의 <code class="language-plaintext highlighter-rouge">Relative Position Embedding</code> 을 입력으로 사용한다.</p>

<p>사실 필자는 논문만 읽었을 때 <code class="language-plaintext highlighter-rouge">EMD</code>도 <code class="language-plaintext highlighter-rouge">Relative Position</code> 정보를 주입해 <code class="language-plaintext highlighter-rouge">Disengtanled-Attention</code>을 수행한다고 전혀 생각하지 못했다. 이는 논문의 설명이 상당히 불친절한 덕분인데, 논문에 이와 관련해서 자세한 설명도 없고 <code class="language-plaintext highlighter-rouge">Absolute Position Embedding</code>을 사용하는 레이어라서 당연히 일반적인 <code class="language-plaintext highlighter-rouge">Self-Attention</code>을 사용할 것이라고 생각했던 것이다.</p>

<p>필자는 여기서 <code class="language-plaintext highlighter-rouge">Absolute Position</code>이 왜 필요한지도 알겠고 그래서 행렬합으로 더해서 어텐션을 수행하는 것도 잘 알겠는데 왜 굳이 가장 마지막 레이어에서 이걸 할까?? 하는 의문이 들었다. 일반 <code class="language-plaintext highlighter-rouge">Self-Attention</code>처럼 맨 처음에 더하고 시작하면 안될까??</p>

<p>저자의 실험에 따르면 <code class="language-plaintext highlighter-rouge">Absolute Position</code> 을 처음에 추가하는 것보다 <code class="language-plaintext highlighter-rouge">EMD</code>처럼 가장 마지막에 더해주는게 성능이 더 좋았다고 한다. 그 이유로 <code class="language-plaintext highlighter-rouge">Absolute Position</code>를 초반에 추가하면 모델이 <code class="language-plaintext highlighter-rouge">Relative Position</code>을 학습하는데 방해가 되는 것 같다는 추측을 함께 서술하고 있다. <strong>그렇다면 왜 방해가 되는 것일까??</strong></p>

<p>필자의 뇌피셜이지만 이것 역시 <code class="language-plaintext highlighter-rouge">blessing of dimensionality</code> 에서 파생된 문제라고 생각한다. 일단 용어의 뜻부터 알아보자. <code class="language-plaintext highlighter-rouge">blessing of dimensionality</code> 란, 고차원 공간에서 무작위로 서로 다른 벡터 두개를 선택하면 두 벡터는 거의 대부분 <code class="language-plaintext highlighter-rouge">approximate orthogonality</code>를 갖는 현상을 설명하는 용어다. 무조건 성립하는 성질은 아니고 확률론적인 접근이라는 것을 명심하자. 아무튼 직교하는 두 벡터는 내적값이 0에 수렴한다. 즉, 두 벡터는 서로에게 영향을 미치지 못한다는 것이다.</p>

<p>이것은 <code class="language-plaintext highlighter-rouge">Transformer</code>에서 <code class="language-plaintext highlighter-rouge">Input Embedding</code>과 <code class="language-plaintext highlighter-rouge">Absolute Position Embedding</code>을 행렬합으로 더해도 좋은 학습 결과를 얻을 수 있는 이유가 된다. 다시 말해서, <code class="language-plaintext highlighter-rouge">hidden states space</code> 에서 <code class="language-plaintext highlighter-rouge">Input Embedding</code> 과 <code class="language-plaintext highlighter-rouge">Absolute Position Embedding</code> 역시 개별 벡터가 <code class="language-plaintext highlighter-rouge">span</code> 하는 부분 공간 끼리는 서로 직교할 가능성이 매우 높다는 것을 의미한다. 따라서 서로 다른 출처를 통해 만들어진 두 행렬을 더해도 서로에게 영향을 미치지 못할 것이고 그로 인해 모델이 <code class="language-plaintext highlighter-rouge">Input</code>과 <code class="language-plaintext highlighter-rouge">Position</code> 정보를 따로 잘 학습할 수 있을 것이라 기대해볼 수 있다.</p>

<p align="center">
<img src="/assets/images/deberta/latent_space.png" alt="hidden states vector space example" class="align-center image-caption" width="60%&quot;, height=&quot;50%" />
<strong><em>hidden states vector space example</em></strong>
</p>

<p>이제 다시 <code class="language-plaintext highlighter-rouge">DeBERTa</code> 경우로 돌아와보자. 위 그림의 파란색 직선을 <code class="language-plaintext highlighter-rouge">Input Embedding</code>, 빨간색 직선을 <code class="language-plaintext highlighter-rouge">Absolute Position Embedding</code>, 왼쪽의 보라색 직선을 <code class="language-plaintext highlighter-rouge">Relative Position Embedding</code>이라고 가정하자. <code class="language-plaintext highlighter-rouge">blessing of dimensionality</code>에 의해 <code class="language-plaintext highlighter-rouge">word con text</code> 정보(파란 직선)와 <code class="language-plaintext highlighter-rouge">position</code> 정보(빨강, 보라 직선)는 그림처럼 서로 근사 직교할 가능성이 매우 높다. 여기부터 필자의 뇌피셜이 들어가는데, 보라색 직선과 빨강색 직선은 성격이 좀 다르지만 결국 둘 다 시퀀스의 <code class="language-plaintext highlighter-rouge">position</code> 정보를 나타낸다는 점에서 뿌리는 같다고 볼 수 있다. 따라서 실제 <code class="language-plaintext highlighter-rouge">hidden states</code> 공간에서 어떤 식으로 맵핑될지는 잘 모르겠지만, 서로 직교하는 형태는 아닐 것이라 추측할 수 있다.</p>

<p>그렇다면 <code class="language-plaintext highlighter-rouge">Absolute Position</code>을 모델 극초반에 더해준다고 생각해보자. 인코더에 들어가는 행렬은 결국 위 그림의 초록색 직선으로 표현될 것이다. 파란색 직선과 빨간색 직선이 근사 직교한다는 가정하에 두 백터의 합은 두 벡터의 45도 정도 되는 곳에 위치하게(초록색 직선) 될 것이다. 그렇다면 보라색 직선과 초록색 직선의 관계 역시 근사 직교에서 서로 간섭하는 형태로 변화한다. 따서 <code class="language-plaintext highlighter-rouge">EMD</code>를 극초반에 사용하면 간섭이 발생해 모델이 <code class="language-plaintext highlighter-rouge">Relative Position</code> 정보를 제대로 학습하지 못할 것이다.</p>

<p><strong><code class="language-plaintext highlighter-rouge">👩‍💻 Implementation</code></strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Pytorch Implementation of DeBERTa Enhanced Mask Decoder
</span>
<span class="k">class</span> <span class="nc">EnhancedMaskDecoder</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="s">"""
    Class for Enhanced Mask Decoder module in DeBERTa, which is used for Masked Language Model (Pretrain Task)
    Word 'Decoder' means that denoise masked token by predicting masked token
    In official paper &amp; repo, they might use 2 EMD layers for MLM Task
    And this layer's key &amp; value input is output from last disentangled self-attention encoder layer,
    Also, all of them can share parameters and this layer also do disentangled self-attention
    In official repo, they implement this layer so hard coding that we can't understand directly &amp; easily
    So, we implement this layer with our own style, as closely as possible to paper statement
    Notes:
        Also we temporarily implement only extract token embedding, not calculating logit, loss for MLM Task yet
        MLM Task will be implemented ASAP
    Args:
        encoder: list of nn.ModuleList, which is (N_EMD * last encoder layer) from DeBERTaEncoder
    References:
        https://arxiv.org/abs/2006.03654
        https://arxiv.org/abs/2111.09543
        https://github.com/microsoft/DeBERTa/blob/master/DeBERTa/apps/models/masked_language_model.py
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">encoder</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">nn</span><span class="p">.</span><span class="n">ModuleList</span><span class="p">],</span> <span class="n">dim_model</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1024</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">EnhancedMaskDecoder</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">emd_layers</span> <span class="o">=</span> <span class="n">encoder</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">layer_norm</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">dim_model</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">emd_context_layer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">,</span> <span class="n">abs_pos_emb</span><span class="p">,</span> <span class="n">rel_pos_emb</span><span class="p">,</span> <span class="n">mask</span><span class="p">):</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">query_states</span> <span class="o">=</span> <span class="n">hidden_states</span> <span class="o">+</span> <span class="n">abs_pos_emb</span>  <span class="c1"># "I" in official paper,
</span>        <span class="k">for</span> <span class="n">emd_layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">emd_layers</span><span class="p">:</span>
            <span class="n">query_states</span> <span class="o">=</span> <span class="n">emd_layer</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">pos_x</span><span class="o">=</span><span class="n">rel_pos_emb</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="n">mask</span><span class="p">,</span> <span class="n">emd</span><span class="o">=</span><span class="n">query_states</span><span class="p">)</span>
            <span class="n">outputs</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">query_states</span><span class="p">)</span>
        <span class="n">last_hidden_state</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">layer_norm</span><span class="p">(</span><span class="n">query_states</span><span class="p">)</span>  <span class="c1"># because of applying pre-layer norm
</span>        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">stack</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">).</span><span class="n">to</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">last_hidden_state</span><span class="p">,</span> <span class="n">hidden_states</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">abs_pos_emb</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">rel_pos_emb</span><span class="p">,</span> <span class="n">mask</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
        <span class="s">"""
        hidden_states: output from last disentangled self-attention encoder layer
        abs_pos_emb: absolute position embedding
        rel_pos_emb: relative position embedding
        """</span>
        <span class="n">last_hidden_state</span><span class="p">,</span> <span class="n">emd_hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">emd_context_layer</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">abs_pos_emb</span><span class="p">,</span> <span class="n">rel_pos_emb</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">last_hidden_state</span><span class="p">,</span> <span class="n">emd_hidden_states</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">emd_context_layer</code> 메서드에서 <code class="language-plaintext highlighter-rouge">Absolute Position</code> 정보를 추가해주는 부분을 제외하면 일반 <code class="language-plaintext highlighter-rouge">Encoder</code> 객체의 동작과 동일하다. 또한 DeBERTa는 모든 레이어가 같은 시점의 forward pass 때, 동일한 가중치의 <code class="language-plaintext highlighter-rouge">Relative Position Embedding</code>을 사용해야 하는데, <code class="language-plaintext highlighter-rouge">EMD</code> 역시 예외는 아니기 때문에 반드시 최상위 객체에서 초기화한 <code class="language-plaintext highlighter-rouge">Relative Position Embedding</code>을 똑같이 매개변수로 전달해줘야 한다.</p>

<p>그리고 마지막으로 객체에서 사용하는 <code class="language-plaintext highlighter-rouge">emd_layers</code> 는 모두 <code class="language-plaintext highlighter-rouge">Disentangled-Attention</code> 레이어의 가장 마지막 인코더라는 사실을 잊지 말자.</p>

<h4 id="multi-head-attention"><strong><code class="language-plaintext highlighter-rouge">👩‍👩‍👧‍👦 Multi-Head Attention</code></strong></h4>

<p>이제 나머지 블럭들에 대해서 살펴보겠다. 원리나 의미는 이미 <code class="language-plaintext highlighter-rouge">Transformer</code> 리뷰에서 모두 살펴봤기 때문에 생략하고, 구현상 특이점에 대해서만 언급하려고 한다. 먼저 <code class="language-plaintext highlighter-rouge">Single-Head Atttention</code> 코드를 보자.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Pytorch Implementation of Single Attention Head
</span>
<span class="k">class</span> <span class="nc">AttentionHead</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="s">"""
    In this class, we implement workflow of single attention head in DeBERTa-Large
    This class has same role as Module "BertAttention" in official Repo (bert.py)
    Args:
        dim_model: dimension of model's latent vector space, default 1024 from official paper
        dim_head: dimension of each attention head, default 64 from official paper (1024 / 16)
        dropout: dropout rate for attention matrix, default 0.1 from official paper
    Math:
        Attention Matrix = c2c + c2p + p2c
        A = softmax(Attention Matrix/sqrt(3*D_h)), SA(z) = Av
    Reference:
        https://arxiv.org/abs/1706.03762
        https://arxiv.org/abs/2006.03654
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim_model</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1024</span><span class="p">,</span> <span class="n">dim_head</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">64</span><span class="p">,</span> <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">AttentionHead</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span> <span class="o">=</span> <span class="n">dim_model</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dim_head</span> <span class="o">=</span> <span class="n">dim_head</span>  <span class="c1"># 1024 / 16 = 64
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">dot_scale</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">dim_head</span><span class="p">))</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">fc_q</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">dim_head</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">fc_k</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">dim_head</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">fc_v</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">dim_head</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">fc_qr</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">dim_head</span><span class="p">)</span>  <span class="c1"># projector for Relative Position Query matrix
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">fc_kr</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">dim_head</span><span class="p">)</span>  <span class="c1"># projector for Relative Position Key matrix
</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">pos_x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">mask</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">emd</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="bp">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">qr</span><span class="p">,</span> <span class="n">kr</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">fc_q</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="bp">self</span><span class="p">.</span><span class="n">fc_k</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="bp">self</span><span class="p">.</span><span class="n">fc_v</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="bp">self</span><span class="p">.</span><span class="n">fc_qr</span><span class="p">(</span><span class="n">pos_x</span><span class="p">),</span> <span class="bp">self</span><span class="p">.</span><span class="n">fc_kr</span><span class="p">(</span><span class="n">pos_x</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">emd</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">q</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">fc_q</span><span class="p">(</span><span class="n">emd</span><span class="p">)</span>
        <span class="n">attention_matrix</span> <span class="o">=</span> <span class="n">disentangled_attention</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">qr</span><span class="p">,</span> <span class="n">kr</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">dropout</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">attention_matrix</span>
</code></pre></div></div>

<p>동작 자체는 동일하지만, 상대 위치 정보에 대한 <code class="language-plaintext highlighter-rouge">linear projection</code> 레이어가 추가 되었다. 그리고 <code class="language-plaintext highlighter-rouge">Enhanced Mask Decoder</code> 를 위해 <code class="language-plaintext highlighter-rouge">forward</code> 메서드에 조건문을 활용하여 <code class="language-plaintext highlighter-rouge">Decoding</code>하는 시점에는 <code class="language-plaintext highlighter-rouge">hidden_states + absolute position embedding</code> 으로 행렬 $Q^c$를 표현하게 구현했다. 이렇게 구현하면 <code class="language-plaintext highlighter-rouge">EMD</code> 를 위해 따로 <code class="language-plaintext highlighter-rouge">AttentionHead</code>를 구현할 필요가 없어서 코드 간소화가 된다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Pytorch Implementation of Multi-Head Attention
</span>
<span class="k">class</span> <span class="nc">MultiHeadAttention</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="s">"""
    In this class, we implement workflow of Multi-Head Self-Attention for DeBERTa-Large
    This class has same role as Module "BertAttention" in official Repo (bert.py)
    In official repo, they use post-layer norm, but we use pre-layer norm which is more stable &amp; efficient for training
    Args:
        dim_model: dimension of model's latent vector space, default 1024 from official paper
        num_heads: number of heads in MHSA, default 16 from official paper for Transformer
        dim_head: dimension of each attention head, default 64 from official paper (1024 / 16)
        dropout: dropout rate, default 0.1
    Math:
        Attention Matrix = c2c + c2p + p2c
        A = softmax(Attention Matrix/sqrt(3*D_h)), SA(z) = Av
    Reference:
        https://arxiv.org/abs/1706.03762
        https://arxiv.org/abs/2006.03654
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim_model</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1024</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">16</span><span class="p">,</span> <span class="n">dim_head</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">64</span><span class="p">,</span> <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MultiHeadAttention</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span> <span class="o">=</span> <span class="n">dim_model</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dim_head</span> <span class="o">=</span> <span class="n">dim_head</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">dropout</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">attention_heads</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">ModuleList</span><span class="p">(</span>
            <span class="p">[</span><span class="n">AttentionHead</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">dim_head</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">dropout</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">num_heads</span><span class="p">)]</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">fc_concat</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">rel_pos_emb</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">mask</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">emd</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="bp">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="s">""" x is already passed nn.Layernorm """</span>
        <span class="k">assert</span> <span class="n">x</span><span class="p">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">3</span><span class="p">,</span> <span class="sa">f</span><span class="s">'Expected (batch, seq, hidden) got </span><span class="si">{</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="s">'</span>
        <span class="n">attention_output</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">fc_concat</span><span class="p">(</span>
            <span class="n">torch</span><span class="p">.</span><span class="n">cat</span><span class="p">([</span><span class="n">head</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">rel_pos_emb</span><span class="p">,</span> <span class="n">mask</span><span class="p">,</span> <span class="n">emd</span><span class="p">)</span> <span class="k">for</span> <span class="n">head</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">attention_heads</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">attention_output</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">MultiHeadAttention</code> 객체는 단일 <code class="language-plaintext highlighter-rouge">AttentionHead</code> 객체를 호출할 때 <code class="language-plaintext highlighter-rouge">rel_pos_emb</code> 를 매개변수로 전달해야 한다는 점만 기억하면 된다.</p>

<h4 id="feed-forward-network"><strong><code class="language-plaintext highlighter-rouge">🔬 Feed Forward Network</code></strong></h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Pytorch Implementation of FeedForward Network
</span>
<span class="k">class</span> <span class="nc">FeedForward</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="s">"""
    Class for Feed-Forward Network module in transformer
    In official paper, they use ReLU activation function, but GELU is better for now
    We change ReLU to GELU &amp; add dropout layer
    Args:
        dim_model: dimension of model's latent vector space, default 512
        dim_ffn: dimension of FFN's hidden layer, default 2048 from official paper
        dropout: dropout rate, default 0.1
    Math:
        FeedForward(x) = FeedForward(LN(x))+x
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim_model</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">512</span><span class="p">,</span> <span class="n">dim_ffn</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2048</span><span class="p">,</span> <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">FeedForward</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">ffn</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">dim_model</span><span class="p">,</span> <span class="n">dim_ffn</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">GELU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">dropout</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">dim_ffn</span><span class="p">,</span> <span class="n">dim_model</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">dropout</span><span class="p">),</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">ffn</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</code></pre></div></div>

<p>역시 기존 <code class="language-plaintext highlighter-rouge">Transformer</code>, <code class="language-plaintext highlighter-rouge">BERT</code>와 다른게 없다.</p>

<h4 id="debertaencoderlayer"><strong><code class="language-plaintext highlighter-rouge">📘 DeBERTaEncoderLayer</code></strong></h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Pytorch Implementation of DeBERTaEncoderLayer(single Disentangled-Attention Encoder Block)
</span>
<span class="k">class</span> <span class="nc">DeBERTaEncoderLayer</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="s">"""
    Class for encoder model module in DeBERTa-Large
    In this class, we stack each encoder_model module (Multi-Head Attention, Residual-Connection, LayerNorm, FFN)
    This class has same role as Module "BertEncoder" in official Repo (bert.py)
    In official repo, they use post-layer norm, but we use pre-layer norm which is more stable &amp; efficient for training
    References:
        https://github.com/microsoft/DeBERTa/blob/master/DeBERTa/deberta/bert.py
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim_model</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1024</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">16</span><span class="p">,</span> <span class="n">dim_ffn</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">4096</span><span class="p">,</span> <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">DeBERTaEncoderLayer</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">self_attention</span> <span class="o">=</span> <span class="n">MultiHeadAttention</span><span class="p">(</span>
            <span class="n">dim_model</span><span class="p">,</span>
            <span class="n">num_heads</span><span class="p">,</span>
            <span class="nb">int</span><span class="p">(</span><span class="n">dim_model</span> <span class="o">/</span> <span class="n">num_heads</span><span class="p">),</span>
            <span class="n">dropout</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">layer_norm1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">dim_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">layer_norm2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">dim_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">dropout</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">ffn</span> <span class="o">=</span> <span class="n">FeedForward</span><span class="p">(</span>
            <span class="n">dim_model</span><span class="p">,</span>
            <span class="n">dim_ffn</span><span class="p">,</span>
            <span class="n">dropout</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">pos_x</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Embedding</span><span class="p">,</span> <span class="n">mask</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">emd</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="bp">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="s">""" rel_pos_emb is fixed for all layer in same forward pass time """</span>
        <span class="n">ln_x</span><span class="p">,</span> <span class="n">ln_pos_x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">layer_norm1</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="bp">self</span><span class="p">.</span><span class="n">layer_norm1</span><span class="p">(</span><span class="n">pos_x</span><span class="p">)</span>  <span class="c1"># pre-layer norm, weight share
</span>        <span class="n">residual_x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">dropout</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">self_attention</span><span class="p">(</span><span class="n">ln_x</span><span class="p">,</span> <span class="n">ln_pos_x</span><span class="p">,</span> <span class="n">mask</span><span class="p">,</span> <span class="n">emd</span><span class="p">))</span> <span class="o">+</span> <span class="n">x</span>

        <span class="n">ln_x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">layer_norm2</span><span class="p">(</span><span class="n">residual_x</span><span class="p">)</span>
        <span class="n">fx</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">ffn</span><span class="p">(</span><span class="n">ln_x</span><span class="p">)</span> <span class="o">+</span> <span class="n">residual_x</span>
        <span class="k">return</span> <span class="n">fx</span>
</code></pre></div></div>

<p>official code와 다르게 <code class="language-plaintext highlighter-rouge">pre-layernorm</code> 을 사용해 구현했다. <code class="language-plaintext highlighter-rouge">pre-layernorm</code>에 대해 궁금하다면 <a href="https://qcqced123.github.io/nlp/transformer#encoderlayer"><strong><em>여기</em></strong></a>를 클릭해 확인해보자.</p>

<h4 id="debertaencoder"><strong><code class="language-plaintext highlighter-rouge">📚 DeBERTaEncoder</code></strong></h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Pytorch Implementation of DeBERTaEncoderr(N stacked DeBERTaEncoderLayer)
</span>
<span class="k">class</span> <span class="nc">DeBERTaEncoder</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="s">"""
    In this class, 1) encode input sequence, 2) make relative position embedding, 3) stack N DeBERTaEncoderLayer
    This class's forward output is not integrated with EMD Layer's output
    Output have ONLY result of disentangled self-attention
    All of ops order is from official paper &amp; repo by microsoft, but ops operating is slightly different,
    Because they use custom ops, e.g. XDropout, XSoftmax, ..., we just apply pure pytorch ops
    Args:
        max_seq: maximum sequence length, named "max_position_embedding" in official repo, default 512
                 in official paper, this value is called 'k'
        N: number of EncoderLayer, default 24 for large model
    Notes:
        self.rel_pos_emb: P in paper, this matrix is fixed during forward pass in same time,
                          all layer &amp; all module must share this layer from official paper
    References:
        https://github.com/microsoft/DeBERTa/blob/master/DeBERTa/deberta/ops.py
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">max_seq</span><span class="p">:</span> <span class="mi">512</span><span class="p">,</span> <span class="n">N</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">24</span><span class="p">,</span> <span class="n">dim_model</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1024</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">16</span><span class="p">,</span> <span class="n">dim_ffn</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">4096</span><span class="p">,</span> <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">DeBERTaEncoder</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">max_seq</span> <span class="o">=</span> <span class="n">max_seq</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">num_layers</span> <span class="o">=</span> <span class="n">N</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span> <span class="o">=</span> <span class="n">dim_model</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dim_ffn</span> <span class="o">=</span> <span class="n">dim_ffn</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">dropout</span><span class="p">)</span>  <span class="c1"># dropout is not learnable
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">encoder_layers</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">ModuleList</span><span class="p">(</span>
            <span class="p">[</span><span class="n">DeBERTaEncoderLayer</span><span class="p">(</span><span class="n">dim_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">dim_ffn</span><span class="p">,</span> <span class="n">dropout</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">num_layers</span><span class="p">)]</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">layer_norm</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">dim_model</span><span class="p">)</span>  <span class="c1"># for final-Encoder output
</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">rel_pos_emb</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">mask</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
        <span class="s">"""
        inputs: embedding from input sequence
        rel_pos_emb: relative position embedding
        mask: mask for Encoder padded token for speeding up to calculate attention score
        """</span>
        <span class="n">layer_output</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">pos_x</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">rel_pos_emb</span>  <span class="c1"># x is same as word_embeddings or embeddings in official repo
</span>        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">encoder_layers</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">pos_x</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span>
            <span class="n">layer_output</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">last_hidden_state</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">layer_norm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># because of applying pre-layer norm
</span>        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">stack</span><span class="p">(</span><span class="n">layer_output</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">).</span><span class="n">to</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>  <span class="c1"># shape: [N, BS, SEQ_LEN, DIM_Model]
</span>        <span class="k">return</span> <span class="n">last_hidden_state</span><span class="p">,</span> <span class="n">hidden_states</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">EMD</code>와 마찬가지로 레이어의 위치에 상관없이 같은 시점에는 모두 동일한 <code class="language-plaintext highlighter-rouge">Relative Position Embedding</code> 을 사용해 <code class="language-plaintext highlighter-rouge">linear projection</code> 하도록 구현해주는 것이 중요 포인트다. <code class="language-plaintext highlighter-rouge">forward</code> 메서드를 확인하자!</p>

<h4 id="deberta"><strong><code class="language-plaintext highlighter-rouge">🤖 DeBERTa</code></strong></h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Pytorch Implementation of DeBERTa
</span>
<span class="k">class</span> <span class="nc">DeBERTa</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="s">"""
    Main class for DeBERTa, having all of sub-blocks &amp; modules such as Disentangled Self-Attention, DeBERTaEncoder, EMD
    Init Scale of DeBERTa Hyper-Parameters, Embedding Layer, Encoder Blocks, EMD Blocks
    And then make 3-types of Embedding Layer, Word Embedding, Absolute Position Embedding, Relative Position Embedding
    Args:
        max_seq: maximum sequence length
        N: number of Disentangled-Encoder layers
        N_EMD: number of EMD layers
        dim_model: dimension of model
        num_heads: number of heads in multi-head attention
        dim_ffn: dimension of feed-forward network, same as intermediate size in official repo
        dropout: dropout rate
    Notes:
        MLM Task is not implemented yet, will be implemented ASAP, but you can get token encode output (embedding)
    References:
        https://arxiv.org/abs/2006.03654
        https://arxiv.org/abs/2111.09543
        https://github.com/microsoft/DeBERTa/blob/master/experiments/language_model/deberta_xxlarge.json
        https://github.com/microsoft/DeBERTa/blob/master/DeBERTa/deberta/config.py
        https://github.com/microsoft/DeBERTa/blob/master/DeBERTa/deberta/deberta.py
        https://github.com/microsoft/DeBERTa/blob/master/DeBERTa/deberta/bert.py
        https://github.com/microsoft/DeBERTa/blob/master/DeBERTa/deberta/disentangled_attention.py
        https://github.com/microsoft/DeBERTa/blob/master/DeBERTa/apps/models/masked_language_model.py
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span>
            <span class="n">vocab_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
            <span class="n">max_seq</span><span class="p">:</span> <span class="mi">512</span><span class="p">,</span>
            <span class="n">N</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">24</span><span class="p">,</span>
            <span class="n">N_EMD</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span>
            <span class="n">dim_model</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1024</span><span class="p">,</span>
            <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">16</span><span class="p">,</span>
            <span class="n">dim_ffn</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">4096</span><span class="p">,</span>
            <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">DeBERTa</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="c1"># Init Scale of DeBERTa
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">max_seq</span> <span class="o">=</span> <span class="n">max_seq</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">max_rel_pos</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">max_seq</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">N</span> <span class="o">=</span> <span class="n">N</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">N_EMD</span> <span class="o">=</span> <span class="n">N_EMD</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span> <span class="o">=</span> <span class="n">dim_model</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dim_ffn</span> <span class="o">=</span> <span class="n">dim_ffn</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dropout_prob</span> <span class="o">=</span> <span class="n">dropout</span>

        <span class="c1"># Init Embedding Layer
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">word_embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">dim_model</span><span class="p">)</span>  <span class="c1"># Word Embedding which is not add Absolute Position
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">rel_pos_emb</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Embedding</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">max_rel_pos</span><span class="p">,</span> <span class="n">dim_model</span><span class="p">)</span>  <span class="c1"># Relative Position Embedding
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">abs_pos_emb</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">max_seq</span><span class="p">,</span> <span class="n">dim_model</span><span class="p">)</span>  <span class="c1"># Absolute Position Embedding for EMD Layer
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">layer_norm1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">dim_model</span><span class="p">)</span>  <span class="c1"># for word embedding
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">layer_norm2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">dim_model</span><span class="p">)</span>  <span class="c1"># for rel_pos_emb
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">dropout_prob</span><span class="p">)</span>

        <span class="c1"># Init Sub-Blocks &amp; Modules
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">DeBERTaEncoder</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">max_seq</span><span class="p">,</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">N</span><span class="p">,</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span><span class="p">,</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">num_heads</span><span class="p">,</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">dim_ffn</span><span class="p">,</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">dropout_prob</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">emd_layers</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="p">.</span><span class="n">encoder</span><span class="p">.</span><span class="n">encoder_layers</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">N_EMD</span><span class="p">)]</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">emd_encoder</span> <span class="o">=</span> <span class="n">EnhancedMaskDecoder</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">emd_layers</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">mask</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">):</span>
        <span class="k">assert</span> <span class="n">inputs</span><span class="p">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">3</span><span class="p">,</span> <span class="sa">f</span><span class="s">'Expected (batch, sequence, vocab_size) got </span><span class="si">{</span><span class="n">inputs</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="s">'</span>
        <span class="c1"># Embedding Layer
</span>        <span class="n">word_embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">dropout</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">layer_norm1</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">word_embedding</span><span class="p">(</span><span class="n">inputs</span><span class="p">))</span>
        <span class="p">)</span>
        <span class="n">rel_pos_emb</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">dropout</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">layer_norm2</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">rel_pos_emb</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">max_rel_pos</span><span class="p">).</span><span class="n">repeat</span><span class="p">(</span><span class="n">inputs</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]).</span><span class="n">to</span><span class="p">(</span><span class="n">inputs</span><span class="p">)))</span>
        <span class="p">)</span>
        <span class="n">abs_pos_emb</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">abs_pos_emb</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">max_seq</span><span class="p">).</span><span class="n">repeat</span><span class="p">(</span><span class="n">inputs</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]).</span><span class="n">to</span><span class="p">(</span><span class="n">inputs</span><span class="p">))</span>  <span class="c1"># "I" in paper
</span>
        <span class="c1"># Disentangled Self-Attention Encoder Layer
</span>        <span class="n">last_hidden_state</span><span class="p">,</span> <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">word_embeddings</span><span class="p">,</span> <span class="n">rel_pos_emb</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span>

        <span class="c1"># Enhanced Mask Decoder Layer
</span>        <span class="n">emd_hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span>
        <span class="n">emd_last_hidden_state</span><span class="p">,</span> <span class="n">emd_hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">emd_encoder</span><span class="p">(</span><span class="n">emd_hidden_states</span><span class="p">,</span> <span class="n">abs_pos_emb</span><span class="p">,</span> <span class="n">rel_pos_emb</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">last_hidden_state</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">,</span> <span class="n">emd_last_hidden_state</span><span class="p">,</span> <span class="n">emd_hidden_states</span>
</code></pre></div></div>]]></content><author><name>qcqced</name><email>qcqced123@gmail.com</email></author><category term="NLP" /><category term="Natural Language Process" /><category term="DeBERTa" /><category term="BERT" /><category term="RoBERTa" /><category term="Transformer" /><category term="Self-Attention" /><category term="Disentangled-Attention" /><category term="Relative Position Embedding" /><category term="EMD" /><category term="Encoder" /><summary type="html"><![CDATA[Transformer Official Paper Review with Pytorch Implementation]]></summary></entry><entry><title type="html">🤖 [Transformer] Attention Is All You Need</title><link href="http://localhost:4000/nlp/transformer" rel="alternate" type="text/html" title="🤖 [Transformer] Attention Is All You Need" /><published>2023-08-04T00:00:00+09:00</published><updated>2023-08-04T01:00:00+09:00</updated><id>http://localhost:4000/nlp/Transformer</id><content type="html" xml:base="http://localhost:4000/nlp/transformer"><![CDATA[<h3 id="overview"><code class="language-plaintext highlighter-rouge">🔭 Overview</code></h3>

<p><code class="language-plaintext highlighter-rouge">Transformer</code>는 2017년 Google이 NIPS에서 발표한 자연어 처리용 신경망으로 기존 <code class="language-plaintext highlighter-rouge">RNN</code> 계열(LSTM, GRU) 신경망이 가진 문제를 해결하고 최대한 인간의 자연어 이해 방식을 수학적으로 모델링 하려는 의도로 설계 되었다. 이 모델은 초기 <code class="language-plaintext highlighter-rouge">Encoder-Decoder</code> 를 모두 갖춘 <code class="language-plaintext highlighter-rouge">seq2seq</code> 형태로 고안 되었으며, 다양한 번역 테스크에서 <code class="language-plaintext highlighter-rouge">SOTA</code>를 달성해 주목을 받았다. 이후에는 여러분도 잘 아시는 것처럼  <code class="language-plaintext highlighter-rouge">BERT</code>, <code class="language-plaintext highlighter-rouge">GPT</code>, <code class="language-plaintext highlighter-rouge">ViT</code>의 베이스 라인으로 채택 되며, 현대 딥러닝 역사에 한 획을 그은 모델로 평가 받고 있다.</p>

<p>현대 딥러닝의 전성기를 열어준 <code class="language-plaintext highlighter-rouge">Transformer</code>는 어떤 아이디어로 기존 <code class="language-plaintext highlighter-rouge">Recurrent</code> 계열이 가졌던 문제들을 해결했을까?? 이것을 제대로 이해하려면 먼저 기존 순환 신경망 모델들이 가졌던 문제부터 짚고 넘어갈 필요가 있다.</p>

<h3 id="limitation-of-recurrent-structure"><strong><code class="language-plaintext highlighter-rouge">🤔 Limitation of Recurrent Structure</code></strong></h3>

<ul>
  <li><strong>1) 인간과 다른 메커니즘의 Vanishing Gradient 발생 (Activation Function with Backward)</strong></li>
  <li><strong>2) 점점 흐려지는 Inputs에 Attention (Activation Function with Forward)</strong></li>
  <li><strong>3) 디코더가 가장 마지막 단어만 열심히 보고 <code class="language-plaintext highlighter-rouge">denoising</code> 수행 (Seq2Seq with Bi-Directional RNN)</strong></li>
</ul>

<p><strong><code class="language-plaintext highlighter-rouge">📈 1) 인간과 다른 메커니즘의 Vanishing Gradient 발생 (Activation Function with Backward)</code></strong></p>

\[h(t) = tanh(x_tW_x + h_{t-1}W_h + b)\]

<p><code class="language-plaintext highlighter-rouge">RNN</code>의 활성 함수인 <code class="language-plaintext highlighter-rouge">Hyperbolic Tangent</code> 는 $y$값이 <code class="language-plaintext highlighter-rouge">[-1, 1]</code> 사이에서 정의되며 기울기의 최대값은 1이다. 따라서 이전 시점 정보는 시점이 지나면 지날수록 (더 많은 셀을 통과할수록) 그라디언트 값이 작아져 미래 시점의 학습에 매우 작은 영향력을 갖게 된다. 이것이 바로 그 유명한 <code class="language-plaintext highlighter-rouge">RNN</code>의 <code class="language-plaintext highlighter-rouge">Vanishing Gradient</code> 현상이다. 사실 현상의 발생 자체는 그렇게 큰 문제가 되지 않는다. <code class="language-plaintext highlighter-rouge">RNN</code>에서 발생하는 <code class="language-plaintext highlighter-rouge">Vanishing Gradient</code> 가 문제가 되는 이유는 바로 인간이 자연어를 이해하는 메커니즘과 다른 방식으로 현상이 발생하기 때문이다. 우리가 글을 읽는 과정을 잘 떠올려 보자. 어떤 단어의 의미를 알기 위해 가까운 주변 단어의 문맥을 활용할 때도 있지만, 저 멀리 떨어진 문단의 문맥을 활용할 때도 있다. 이처럼 단어 혹은 시퀀스를 구성하는 <code class="language-plaintext highlighter-rouge">원소 사이의 관계성</code>이나 <code class="language-plaintext highlighter-rouge">어떤 다른 의미론적인 이유</code>로 <code class="language-plaintext highlighter-rouge">불균형</code>하게 현재 시점의 학습에 영향력을 갖게 되는게 아니라, 단순 <code class="language-plaintext highlighter-rouge">입력 시점</code> 때문에 불균형이 발생하기 때문에 <code class="language-plaintext highlighter-rouge">RNN</code>의 <code class="language-plaintext highlighter-rouge">Vanishing Gradient</code>가 낮은 성능의 원인으로 지목되는 것이다.</p>

<p>다시 말해, 실제 자연어의 문맥을 파악해 그라디언트에 반영하는게 아니라 단순히 시점에 따라서 그 영향력을 반영하게 된다는 것이다. 멀리 떨어진 시퀀스의 문맥이 필요한 경우를 <code class="language-plaintext highlighter-rouge">Recurrent</code> 구조는 정확히 학습할 수 없다.</p>

<p>그렇다면 활성 함수를 <code class="language-plaintext highlighter-rouge">relu</code> 혹은 <code class="language-plaintext highlighter-rouge">gelu</code> 를 사용하면 위 문제를 해결할 수 있을까? <code class="language-plaintext highlighter-rouge">Vanishing Graident</code> 문제는 해결할 수도 있으나 <code class="language-plaintext highlighter-rouge">hidden_state</code> 값이 발산할 것이다. 그 이유는 두 활성 함수 모두 양수 구간에서 선형인데, 이전 정보를 누적해서 가중치와 곱하고 현재 입력값에 더하는 <code class="language-plaintext highlighter-rouge">RNN</code>의 구조를 생각해보면 넘어오는 이전 정보는 누적되면서 점점 커질 것이고 그러다 결국 발산하게 된다.</p>

<p>결론적으로 <code class="language-plaintext highlighter-rouge">Vanishing Gradient</code> 현상 자체가 문제는 아니지만 모델이 자연어의 문맥을 파악해 그라디언트에 반영하는게 아니라 단순히 시점에 따라서 불균형하게 발생하기 때문에 낮은 성능의 원인으로 지목 받는 것이다. 이것을 <code class="language-plaintext highlighter-rouge">long-term dependency</code>라고 부르기도 한다.</p>

<p><strong><code class="language-plaintext highlighter-rouge">✏️ 2) 점점 흐려지는 Inputs에 Attention (Activation Function with Forward)</code></strong></p>

<p align="center">
<img src="/assets/images/transformer/tanh.png" alt="tanh function" class="align-center image-caption" width="75%&quot;, height=&quot;50%" />
<strong><em>tanh function</em></strong>
</p>

<p><code class="language-plaintext highlighter-rouge">Hyperbolic Tangent</code> 은  $y$값이 <code class="language-plaintext highlighter-rouge">[-1, 1]</code> 사이에서 정의된다고 했다. 다시 말해 셀의 출력값이 항상 일정 범위값( <code class="language-plaintext highlighter-rouge">[-1,1]</code> )으로 제한(가중치, 편향 더하는 것은 일단 제외) 된다는 것이다. 따라서 한정된 좁은 범위에 출력값들이 맵핑되는데, 이는 결국 입력값의 정보는 대부분 소실된 채 일부 특징만 정제 되어 출력되고 다음 레이어로 <code class="language-plaintext highlighter-rouge">forward</code> 됨을 의미한다. 그래프를 한 번 살펴보자. 특히 <code class="language-plaintext highlighter-rouge">Inputs</code> 값이 2.5 이상인 경우부터는 출력값이 거의 1에 수렴해 그 차이를 직관적으로 파악하기 힘들다. 이러한 활성함수가 수십개, 수백개 쌓인다면 결국 원본 정보는 매우 흐려지고 뭉개져서 다른 인스턴스와 구별이 힘들어 질 것이다.</p>

<p><strong><code class="language-plaintext highlighter-rouge">🔬 3) 디코더가 가장 마지막 단어만 열심히 보고 denoising 수행 (Seq2Seq with Bi-Directional RNN)</code></strong><br />
<code class="language-plaintext highlighter-rouge">“쓰다”</code> ($t_7$)라는 단어의 뜻을 이해하려면 <code class="language-plaintext highlighter-rouge">“돈을”</code>, <code class="language-plaintext highlighter-rouge">“모자를”</code>, <code class="language-plaintext highlighter-rouge">“맛이”</code>, <code class="language-plaintext highlighter-rouge">“글을”</code>($t_1$)과 같이 멀리 있는 앞 단어를 봐야 알 수 있는데, $h_7$ 에는 $t_1$이 흐려진 채로 들어가 있어서 $t_7$의 제대로 된 의미를 포착하지 못한다. 심지어 언어가 영어라면 뒤를 봐야 정확한 문맥을 알 수 있는데 <code class="language-plaintext highlighter-rouge">Vanilla RNN</code>은 단방향으로만 학습을 하게 되어 문장의 뒷부분 문맥은 반영조차(뒤에 위치한 목적어에 따라서 쓰다라는 단어의 뉘앙스는 달라짐) 할 수 없다. 그래서 <code class="language-plaintext highlighter-rouge">Bi-directional RNN</code> 써야하는데, 이것도 역시도 여전히 <code class="language-plaintext highlighter-rouge">“거리”</code>에 영향 받는다는 건 변하지 않기 때문에 근본적인 해결책이라 볼 수 없다.</p>

<p>한편, 디코더의 <code class="language-plaintext highlighter-rouge">Next Token Prediction</code> 성능은 무조건 인코더로부터 받는 <code class="language-plaintext highlighter-rouge">Context Vector</code>의 품질에 따라 좌지우지 된다. 그러나 Recurrent 구조의 인코더로부터 나온 Context Vector는 앞서 서술한 것처럼 좋은 품질(뒤쪽 단어가 상대적으로 선명함)이 아니다. 따라서 디코더의 번역(다음 단어 예측) 성능 역시 좋을리가 없다.</p>

<p>결국 <code class="language-plaintext highlighter-rouge">Recurrent</code> 구조 자체에 명확한 한계가 존재하여 인간이 자연어를 사용하고 이해하는 맥락과 다른 방식으로 동작햐게 되었다. <code class="language-plaintext highlighter-rouge">LSTM</code>, <code class="language-plaintext highlighter-rouge">GRU</code>의 제안으로 어느 정도 문제를 완화 시켰으나, 앞에서 서술했듯이 태생이 <code class="language-plaintext highlighter-rouge">Recurrent Structure</code>을 가지기 때문에 근본적인 해결책이 되지는 못했다. 그렇다면 이제 <code class="language-plaintext highlighter-rouge">Transformer</code>가 어떻게 위에 서술한 3가지 문제를 해결하고 현재의 위상을 갖게 되었는지 알아보자.</p>

<h3 id="modeling"><strong><code class="language-plaintext highlighter-rouge">🌟 Modeling</code></strong></h3>

<p align="center">
<img src="/assets/images/transformer/transformer_overview.png" alt="Attention Is All You Need" class="align-center image-caption" width="50%&quot;, height=&quot;50%" />
<strong><em><a href="https://arxiv.org/abs/1706.03762">Attention Is All You Need</a></em></strong>
</p>

<p>앞서 <code class="language-plaintext highlighter-rouge">Recurrent</code> 구조의 <code class="language-plaintext highlighter-rouge">Vanishing Gradient</code> 을 설명하면서 시점에 따라 정보를 소실하게 되는 현상은 인간의 자연어 이해 방식이 아니라는 점을 언급한 적 있다. 따라서 <code class="language-plaintext highlighter-rouge">Transformer</code>는 최대한 인간의 자연어 이해 방식을 수학적으로 모델링 하는 것에 초점을 맞췄다. 우리가 쓰여진 글을 이해하기 위해 하는 행동들을 떠올려 보자. <strong><code class="language-plaintext highlighter-rouge">“Apple”</code><u>이란 단어가 사과를 말하는 것인지, 브랜드 애플을 지칭하는 것인지 파악하기 위해 같은 문장에 속한 주변 단어를 살피기도 하고 그래도 파악하기 힘들다면 앞뒤 문장, 나아가 문서 전체 레벨에서 맥락을 파악하기 위해 노력한다.</u></strong> <code class="language-plaintext highlighter-rouge">Transformer</code> 연구진은 바로 이 과정에 주목했으며 이것을 모델링하여 그 유명한 <code class="language-plaintext highlighter-rouge">Self-Attention</code>을 고안해낸다.</p>

<p align="center">
<img src="/assets/images/transformer/word_embedding.png" alt="Word Embedding Space" class="align-center image-caption" width="50%&quot;, height=&quot;50%" />
<strong><em><a href="https://www.researchgate.net/figure/Visualization-of-the-word-embedding-space_fig4_343595281/download">Word Embedding Space</a></em></strong>
</p>

<p>다시 말해 <code class="language-plaintext highlighter-rouge">Self-Attention</code>은 토큰의 의미를 이해하기 위해 <code class="language-plaintext highlighter-rouge">전체 입력 시퀀스</code> 중에서 어떤 단어에 주목해야할지를 수학적으로 표현한 것이라 볼 수 있다. <strong><u>좀 더 구체적으로는 시퀀스에 속한 여러 토큰 벡터(행백터)를 임베딩 공간 어디에 배치할 것인가에 대해 훈련하는 행위다.</u></strong></p>

<p align="center">
<img src="/assets/images/transformer/scaled_dot_attention.png" alt="Scaled Dot-Product Attention" class="align-center image-caption" width="75%&quot;, height=&quot;50%" />
<strong><em><a href="https://arxiv.org/abs/1706.03762">Scaled Dot-Product Attention</a></em></strong>
</p>

<p>그렇다면 이제부터 <code class="language-plaintext highlighter-rouge">Transformer</code> 가 어떤 아이데이션을 통해 기존 순환 신경망 모델의 단점을 해결하고 딥러닝계의 <code class="language-plaintext highlighter-rouge">G.O.A.T</code> 자리를 차지했는지 알아보자. 모델은 크게 인코더와 디코더 부분으로 나뉘는데, 하는 역할과 미세한 구조상의 차이만 있을뿐 두 모듈 모두 <code class="language-plaintext highlighter-rouge">Self-Attention</code>이 제일 중요하다는 본질은 변하지 않는다. 따라서 <code class="language-plaintext highlighter-rouge">Input Embedding</code>부터 차례대로 살펴보되,  <code class="language-plaintext highlighter-rouge">Self-Attention</code> 은 특별히 사용된 하위 블럭 단위를 빠짐 없이, 세세하게 살펴볼 것이다.</p>

<p align="center">
<img src="/assets/images/transformer/class_diagram.png" alt="Class Diagram" class="align-center image-caption" width="35%&quot;, height=&quot;50%" />
<strong><em>Class Diagram</em></strong>
</p>

<p>이렇게 하위 모듈에 대한 설명부터 쌓아 나가 마지막에는 실제 구현 코드와 함께 전체적인 구조 측면에서도 모델을 해석해볼 것이다. 끝까지 포스팅을 읽어주시길 바란다.</p>

<h4 id="-input-embedding"><code class="language-plaintext highlighter-rouge">🔬 Input Embedding</code></h4>

\[X_E \in R^{B * S_E * V_E} \\
X_D \in R^{B * S_D * V_D}\]

<p><code class="language-plaintext highlighter-rouge">Transformer</code>는 인코더와 디코더로 이뤄진 <code class="language-plaintext highlighter-rouge">seq2seq</code> 구조를 가지고 있다. 즉, 대상 언어를 타겟 언어로 번역하는데 목적을 두고 있기 때문에 입력으로 대상 언어 시퀀스와 타겟 언어 시퀀스 모두 필요하다. $X_E$는 <code class="language-plaintext highlighter-rouge">인코더</code>의 입력 행렬을 나타내고, $X_D$는 <code class="language-plaintext highlighter-rouge">디코더</code>의 입력 행렬을 의미한다. 이 때, $B$는 <code class="language-plaintext highlighter-rouge">batch size</code>, $S$는 <code class="language-plaintext highlighter-rouge">max_seq</code>, $V$는 개별 모듈이 가진 <code class="language-plaintext highlighter-rouge">Vocab</code>의 사이즈를 가리킨다. 위 수식은 사실 논문에 입력에 대한 수식이 따로 서술 되어 있지 않아, 필자가 직접 만든 것이다. 앞으로도 해당 기호를 이용해 수식을 표현할 예정이니 참고 바란다.</p>

\[W_E \in R^{V_E * d} \\
W_D \in R^{V_D * d} \\\]

<p>이렇게 정의된 입력값을 개별 모듈의 임베딩 레이어에 통과 시킨 결과물이 바로 <code class="language-plaintext highlighter-rouge">Input Embedding</code>이 된다. $d$는 <code class="language-plaintext highlighter-rouge">Transformer</code> 모델의 은닉층의 크기를 의미한다. 따라서 <code class="language-plaintext highlighter-rouge">Position Embedding</code> 과 더해지기 전, 임베딩 레이어를 통과한 <code class="language-plaintext highlighter-rouge">Input Embedding</code>의 모양은 아래 수식과 같다.</p>

\[X_E \in R^{B*S_E*d} \\
X_D \in R^{B*S_D*d} \\\]

<p>그렇다면 실제 구현은 어떻게 할까?? <code class="language-plaintext highlighter-rouge">Transformer</code> 의 <code class="language-plaintext highlighter-rouge">Input Embedding</code>은 <code class="language-plaintext highlighter-rouge">nn.Embedding</code>으로 레이어를 정의해 사용한다. <code class="language-plaintext highlighter-rouge">nn.Linear</code>도 있는데 왜 굳이 <code class="language-plaintext highlighter-rouge">nn.Embedding</code>을 사용하는 것일까??</p>

<p>자연어 처리에서 입력 임베딩을 만들때는 모델의 토크나이저에 의해 사전 정의된 <code class="language-plaintext highlighter-rouge">vocab</code>의 사이즈가 입력 시퀀스에 속한 토큰 개수보다 훨씬 크기 때문에 데이터 룩업 테이블 방식의 <code class="language-plaintext highlighter-rouge">nn.Embedding</code> 을 사용하게 된다. 이게 무슨 말이냐면, 토크나이저에 의해 사전에 정의된 <code class="language-plaintext highlighter-rouge">vocab</code> 전체가 <code class="language-plaintext highlighter-rouge">nn.Embedding(vocab_size, dim_model)</code>로 투영 되어 가로는 <code class="language-plaintext highlighter-rouge">vocab</code> 사이즈, 세로는 모델의 차원 크기에 해당하는 룩업 테이블이 생성되고, 내가 입력한 토큰들은 전체 <code class="language-plaintext highlighter-rouge">vocab</code>의 일부분일테니 전체 임베딩 룩업 테이블에서 내가 임베딩하고 싶은 토큰들의 인덱스만 알아낸다는 것이다. 그래서 <code class="language-plaintext highlighter-rouge">nn.Embedding</code> 은 레이어에 정의된 차원과 실제 입력 데이터의 차원이 맞지 않아도 함수가 동작하게 된다. <code class="language-plaintext highlighter-rouge">nn.Linear</code> 와 입력 차원에 대한 조건 빼고는 동일한 동작을 수행하기 때문에 사전 정의된 <code class="language-plaintext highlighter-rouge">vocab</code> 사이즈와 입력 시퀀스의 토큰 개수가 같다면 <code class="language-plaintext highlighter-rouge">nn.Linear</code>를 사용해도 무방하다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Input Embedding Example
</span>
<span class="k">class</span> <span class="nc">Transformer</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
	<span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">enc_vocab_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">dec_vocab_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">max_seq</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">512</span><span class="p">,</span>
        <span class="n">enc_N</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">6</span><span class="p">,</span>
        <span class="n">dec_N</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">6</span><span class="p">,</span>
        <span class="n">dim_model</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">512</span><span class="p">,</span> <span class="c1"># latent vector space
</span>        <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span>
        <span class="n">dim_ffn</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2048</span><span class="p">,</span>
        <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Transformer</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">enc_input_embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">enc_vocab_size</span><span class="p">,</span> <span class="n">dim_model</span><span class="p">)</span> <span class="c1"># Encoder Input Embedding Layer
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">dec_input_embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">dec_vocab_size</span><span class="p">,</span> <span class="n">dim_model</span><span class="p">)</span> <span class="c1"># Decoder Input Embedding Layer
</span>	
	<span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">enc_inputs</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">dec_inputs</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">enc_pad_index</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">dec_pad_index</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
            <span class="n">enc_x</span><span class="p">,</span> <span class="n">dec_x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">enc_input_embedding</span><span class="p">(</span><span class="n">enc_inputs</span><span class="p">),</span> <span class="bp">self</span><span class="p">.</span><span class="n">dec_input_embedding</span><span class="p">(</span><span class="n">dec_inputs</span><span class="p">)</span>
</code></pre></div></div>

<p>위의 예시 코드를 함께 살펴보자. <code class="language-plaintext highlighter-rouge">__init__</code> 의 <code class="language-plaintext highlighter-rouge">self.enc_input_embedding</code>, <code class="language-plaintext highlighter-rouge">self._dec_input_embedding</code>이 바로 $W_E, W_D$에 대응된다. 한편 <code class="language-plaintext highlighter-rouge">forward</code> 메서드에 정의된 <code class="language-plaintext highlighter-rouge">enc_x</code>, <code class="language-plaintext highlighter-rouge">dec_x</code> 는 임베딩 레이어를 거치고 나온 $X_E, X_D$에 해당된다.</p>

<p>한편, $X_E, X_D$은 각각 인코더, 디코더 모듈로 흘러 들어가 <code class="language-plaintext highlighter-rouge">Absolute Position Embedding</code>과 더해진(행렬 합) 뒤, 개별 모듈의 입력값으로 활용된다.</p>

<p><strong><code class="language-plaintext highlighter-rouge">🔢 Absolute Position Embedding(Encoding)</code></strong><br />
입력 시퀀스에 위치 정보를 맵핑해주는 역할을 한다. 필자는 개인적으로 <code class="language-plaintext highlighter-rouge">Transformer</code>에서 가장 중요한 요소를 뽑으라고 하면 세 손가락 안에 들어가는 파트라고 생각한다. 다음 파트에서 자세히 기술하겠지만, <code class="language-plaintext highlighter-rouge">Self-Attention(내적)</code>은 입력 시퀀스를 병렬로 한꺼번에 처리할 수 있다는 장점을 갖고 있지만, 그 자체로는 토큰의 위치 정보를 인코딩할 수 없다. 우리가 따로 위치 정보를 알려주지 않는 이상 쿼리 행렬의 2번째 행벡터가 입력 시퀀스에서 몇 번째 위치한 토큰인지 모델은 알 길이 없다.</p>

<p>그런데, 텍스트는 <code class="language-plaintext highlighter-rouge">Permutation Equivariant</code>한 <code class="language-plaintext highlighter-rouge">Bias</code> 가 있기 때문에 토큰의 위치 정보는 <code class="language-plaintext highlighter-rouge">NLP</code>에서 매우 중요한 요소로 꼽힌다. <strong>직관적으로도 토큰의 순서는 시퀀스가 내포하는 의미에 지대한 영향을 끼친다는 것을 알 수 있다.</strong> 예를 들어 <code class="language-plaintext highlighter-rouge">“철수는 영희를 좋아한다”</code>라는 문장과 <code class="language-plaintext highlighter-rouge">“영희는 철수를 좋아한다”</code>라는 문장의 의미가 같은가 생각해보자. 주어와 목적어 위치가 바뀌면서 정반대의 뜻이 되어버린다.</p>

<p align="center">
<img src="/assets/images/transformer/positional_encoding.png" alt="Positional Encoding Example" class="align-center image-caption" width="75%&quot;, height=&quot;50%" />
<strong><em><a href="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/56cf1596-c770-410c-8053-5876c3c66fff/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2022-10-09_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_5.13.48.png">Positional Encoding Example</a></em></strong>
</p>

<p>따라서 저자는 입력 입베딩에 위치 정보를 추가하고자 <code class="language-plaintext highlighter-rouge">Position Encoding</code> 을 제안한다. 사실 <code class="language-plaintext highlighter-rouge">Position Encoding</code> 은 여러 단점 때문에 후대 <code class="language-plaintext highlighter-rouge">Transformer</code>  파생 모델에서는 잘 사용되지 않는 추세다. 대신 모델이 학습을 통해 최적값을 찾아주는 <code class="language-plaintext highlighter-rouge">Position Embedding</code> 방식을 대부분 차용하고 있다. 필자 역시 <code class="language-plaintext highlighter-rouge">Position Embedding</code> 을 사용해 위치 임베딩을 구현했기 때문에 원리와 단점에 대해서만 간단히 소개하고 넘어가려 한다. 또한 저자 역시 논문에서 두 방식 중 어느 것을 써도 비슷한 성능을 보여준다고 언급하고 있다.</p>

\[P_E \in R^{B*S_E*D} \\
 P_D \in R^{B*S_D*D} \\
P(pos, 2i) = sin(pos/\overset{}
  {10000_{}^{2i/dmodel}}) \\
P(pos, 2i+1) = cos(pos/\overset{}
  {10000_{}^{2i/dmodel}})\]

<p><strong>원리는 매우 간단하다. 사인함수와 코사인 함수의 주기성을 이용해 개별 인덱스의 행벡터 값을 표현하는 것이다.</strong> 행벡터의 원소 중에서 짝수번째 인덱스에 위치한 원소는 (짝수번째 열벡터) \(sin(pos/\overset{}{10000_{}^{2i/dmodel}})\) 의 함숫값을 이용해 채워넣고, 홀수번째 원소는 \(cos(pos/\overset{}{10000_{}^{2i/dmodel}})\)를 이용해 채워넣는다.</p>

<p align="center">
<img src="/assets/images/transformer/sin_cos_graph.png" alt="periodic function graph" class="align-center image-caption" width="75%&quot;, height=&quot;50%" />
<strong><em>periodic function graph</em></strong>
</p>

<p>초록색 그래프는 \(sin(pos/\overset{}{10000_{}^{2i/dmodel}})\), 주황색 그래프는 \(cos(pos/\overset{}{10000_{}^{2i/dmodel}})\)를 시각화했다. 지면의 제한으로 <code class="language-plaintext highlighter-rouge">max_seq=512</code> 만큼의 변화량을 담지는 못했지만, x축이 커질수록 두 함수 모두 진동 주기가 조금씩 커지는 양상을 보여준다. 따라서 개별 인덱스(행벡터)를 중복되는 값 없이 표현하는 것이 가능하다고 저자는 주장한다.</p>

<p align="center">
<img src="/assets/images/transformer/positional_encoding_result.png" alt="Positional Encoding Result" class="align-center image-caption" width="75%&quot;, height=&quot;50%" />
<strong><em><a href="https://wikidocs.net/162099">Positional Encoding Result</a></em></strong>
</p>

<p>위 그림은 토큰 <code class="language-plaintext highlighter-rouge">256</code>개로 구성된 시퀀스에 대해 <code class="language-plaintext highlighter-rouge">Positional Encoding</code>한 결과를 시각화한 자료다. 그래프의 $x$축은 <code class="language-plaintext highlighter-rouge">행벡터의 원소</code>이자 <code class="language-plaintext highlighter-rouge">Transformer</code>의 은닉 벡터 차원을 가리키고, $y$축은 <code class="language-plaintext highlighter-rouge">시퀀스의 인덱스</code>(행벡터)를 의미한다. 육안으로 정확하게 차이를 인식하기 쉽지는 않지만, 행벡터가 모두 유니크하게 표현된다는 사실(직접 실수값을 확인해보면 정말 미세한 차이지만 개별 토큰의 희소성이 보장)을 알 수 있다. 작은 차이를 시각화 자료로 파악하기는 쉽지 않기 때문에 진짜 그런가 궁금하신 분들은 직접 실수값을 구해보는 것을 추천드린다.</p>

<p><strong>여기서 행벡터의 희소성이란 개별 행벡터 원소의 희소성을 말하는게 아니다.</strong> 0번 토큰, 4번 토큰, 9번 토큰의 행벡터 1번째 원소의 값은 같을 수 있다. 하지만 진동 주기가 갈수록 커지는 주기함수를 사용하기 때문에 다른 원소(차원)값은 다를 것이라 기대할 수 있는데, <strong>바로 이것을 행벡터의 희소성이라고 정의하는 것이다.</strong> 만약 1번 토큰과 2번 토큰의 모든 행벡터 원소값이 같다면 그것은 희소성 원칙에 위배되는 상황이다.</p>

<p align="center">
<img src="/assets/images/transformer/encoding.png" alt="Positional Encoding" class="align-center image-caption" width="75%&quot;, height=&quot;50%" />
</p>

<p align="center">
<img src="/assets/images/transformer/embedding.png" alt="Compare Performance between Encoding and Embedding" class="align-center image-caption" width="75%&quot;, height=&quot;50%" />
<strong><em><a href="https://arxiv.org/abs/1706.03762">Compare Performance between Encoding and Embedding</a></em></strong>
</p>

<p>비록 개별 행벡터의 희소성이 보장된다고 해도 <code class="language-plaintext highlighter-rouge">Position Encoding</code>은 <code class="language-plaintext highlighter-rouge">not trainable</code>해서 <code class="language-plaintext highlighter-rouge">static</code>하다는 단점이 있다. 모든 배치의 시퀀스가 동일한 위치 정보값을 갖게 된다는 것이다. <code class="language-plaintext highlighter-rouge">512</code>개의 토큰으로 구성된 시퀀스 A와 B가 있다고 가정해보자. 이 때 시퀀스 A는 문장 <code class="language-plaintext highlighter-rouge">5</code>개로 구성 되어 있고, B는 문장 <code class="language-plaintext highlighter-rouge">12</code>개로 만들어졌다. 두 시퀀스의 <code class="language-plaintext highlighter-rouge">11</code>번째 토큰의 문장 성분은 과연 같을까?? 아마도 대부분의 경우에 다를 것이다. 텍스트 데이터에서 순서 정보가 중요한 이유 중 하나는 바로 <code class="language-plaintext highlighter-rouge">syntactical</code> 한 정보를 포착하기 위함이다. <code class="language-plaintext highlighter-rouge">Position Encoding</code>은 <code class="language-plaintext highlighter-rouge">static</code> 하기 때문에 이러한 타입의 정보를 인코딩 하기 쉽지 않다. 그래서 좀 더 풍부한 표현을 담을 수 있는 <code class="language-plaintext highlighter-rouge">Position Embedding</code>을 사용하는 것이 최근 추세다.</p>

<p><strong><code class="language-plaintext highlighter-rouge">✏️ Position Embedding</code></strong></p>

<p>그렇다면 이제 <code class="language-plaintext highlighter-rouge">Position Embedding</code>에 대해 알아보자. <code class="language-plaintext highlighter-rouge">Position Embedding</code> 은 <code class="language-plaintext highlighter-rouge">Input Embedding</code>을 정의한 방식과 거의 유사하다. 먼저 입력값과 <code class="language-plaintext highlighter-rouge">weight</code> 의 모양부터 확인해보자.</p>

\[P_E \in R^{B*S_E*d} \\
P_D \in R^{B*S_d*d} \\
W_{P_E} \in R^{S_E * d} \\
W_{P_D} \in R^{S_D * d} \\\]

<p>$P_E, P_D$는 개별 모듈의 위치 임베딩 레이어 입력을 가리키며, $W_{P_E}, W_{P_D}$가 개별 모듈의 위치 임베딩 레이어가 된다. 이제 이것을 코드로 어떻게 구현하는지 살펴보자.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Absolute Position Embedding Example
</span>
<span class="k">class</span> <span class="nc">Encoder</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="s">"""
    In this class, encode input sequence and then we stack N EncoderLayer
    First, we define "positional embedding" and then add to input embedding for making "word embedding"
    Second, forward "word embedding" to N EncoderLayer and then get output embedding
    In official paper, they use positional encoding, which is base on sinusoidal function(fixed, not learnable)
    But we use "positional embedding" which is learnable from training
    Args:
        max_seq: maximum sequence length, default 512 from official paper
        N: number of EncoderLayer, default 6 for base model
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">max_seq</span><span class="p">:</span> <span class="mi">512</span><span class="p">,</span> <span class="n">N</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">6</span><span class="p">,</span> <span class="n">dim_model</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">512</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span> <span class="n">dim_ffn</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2048</span><span class="p">,</span> <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Encoder</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">max_seq</span> <span class="o">=</span> <span class="n">max_seq</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">scale</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">dim_model</span><span class="p">))</span>  <span class="c1"># scale factor for input embedding from official paper
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">positional_embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">max_seq</span><span class="p">,</span> <span class="n">dim_model</span><span class="p">)</span>  <span class="c1"># add 1 for cls token
</span>
		<span class="p">...</span> <span class="n">중략</span> <span class="p">...</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">mask</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
        <span class="s">"""
        inputs: embedding from input sequence, shape =&gt; [BS, SEQ_LEN, DIM_MODEL]
        mask: mask for Encoder padded token for speeding up to calculate attention score
        """</span>
        <span class="n">layer_output</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">pos_x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">max_seq</span><span class="p">).</span><span class="n">repeat</span><span class="p">(</span><span class="n">inputs</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]).</span><span class="n">to</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">dropout</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">scale</span> <span class="o">*</span> <span class="n">inputs</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">positional_embedding</span><span class="p">(</span><span class="n">pos_x</span><span class="p">)</span>  <span class="c1"># layernorm 적용하고
</span>        <span class="p">)</span>
		<span class="p">...</span> <span class="n">중략</span> <span class="p">...</span> 
</code></pre></div></div>

<p>위 코드는 <code class="language-plaintext highlighter-rouge">Transformer</code>의 인코더 모듈을 구현한 것이다. 그래서 <code class="language-plaintext highlighter-rouge">forward</code> 메서드의 <code class="language-plaintext highlighter-rouge">pos_x</code> 가 바로 $P_E$가 되며, <code class="language-plaintext highlighter-rouge">__init__</code>의 <code class="language-plaintext highlighter-rouge">self.positional_embedding</code>이 바로 $W_{P_E}$에 대응된다. 이렇게 정의한 <code class="language-plaintext highlighter-rouge">Position Embedding</code>은 <code class="language-plaintext highlighter-rouge">Input Embedding</code>과 더해서 <code class="language-plaintext highlighter-rouge">Word Embedding</code> 을 만든다. <code class="language-plaintext highlighter-rouge">Word Embedding</code> 은 다시 개별 모듈의 <code class="language-plaintext highlighter-rouge">linear projection</code> 레이어에 대한 입력 $X$로 사용 된다.</p>

<p><strong>한편,</strong> <code class="language-plaintext highlighter-rouge">Input Embedding</code> <strong>과</strong> <code class="language-plaintext highlighter-rouge">Position Embedding</code><strong>을 더한다는 것에 주목해보자. 필자는 본 논문을 보며 가장 의문이 들었던 부분이다. 도대체 왜 완전히 서로 다른 출처에서 만들어진 행렬 두개를</strong> <code class="language-plaintext highlighter-rouge">concat</code> <strong>하지 않고 더해서 사용했을까??</strong> <code class="language-plaintext highlighter-rouge">concat</code><strong>을 이용하면 <code class="language-plaintext highlighter-rouge">Input</code>과 <code class="language-plaintext highlighter-rouge">Position</code> 정보를 서로 다른 차원에 두고 학습하는게 가능했을텐데 말이다.</strong></p>

<p><strong><code class="language-plaintext highlighter-rouge">🤔 Why Sum instead of Concatenate</code></strong><br />
행렬합을 사용하는 이유에 대해 저자가 특별히 언급하지는 않아서 때문에 정확한 의도를 알 수 없지만, <strong>추측하건데 <code class="language-plaintext highlighter-rouge">blessing of dimensionality</code> 효과를 의도했지 않았나 싶다.</strong> <code class="language-plaintext highlighter-rouge">blessing of dimensionality</code> 란, 고차원 공간에서 무작위로 서로 다른 벡터 두개를 선택하면 두 벡터는 거의 대부분 <code class="language-plaintext highlighter-rouge">approximate orthogonality</code>를 갖는 현상을 설명하는 용어다. 무조건 성립하는 성질은 아니고 확률론적인 접근이라는 것을 명심하자. 아무튼 직교하는 두 벡터는 내적값이 0에 수렴한다. 즉, 두 벡터는 서로에게 영향을 미치지 못한다는 것이다. 이것은 전체 모델의 <code class="language-plaintext highlighter-rouge">hidden states space</code> 에서 <code class="language-plaintext highlighter-rouge">Input Embedding</code> 과 <code class="language-plaintext highlighter-rouge">Position Embedding</code> 역시 개별 벡터가 <code class="language-plaintext highlighter-rouge">span</code> 하는 부분 공간 끼리는 서로 직교할 가능성이 매우 높다는 것을 의미한다. 따라서 서로 다른 출처를 통해 만들어진 두 행렬을 더해도 서로에게 영향을 미치지 못할 것이고 그로 인해 모델이 <code class="language-plaintext highlighter-rouge">Input</code>과 <code class="language-plaintext highlighter-rouge">Position</code> 정보를 따로 잘 학습할 수 있을 것이라 기대해볼 수 있다. 가정대로만 된다면, <code class="language-plaintext highlighter-rouge">concat</code> 을 사용해 모델의 <code class="language-plaintext highlighter-rouge">hidden states space</code> 를 늘려 <code class="language-plaintext highlighter-rouge">Computational Overhead</code> 를 유발하는 것보다 훨씬 효율적이라고 볼 수 있겠다.</p>

<p>한편 <code class="language-plaintext highlighter-rouge">blessing of dimensionality</code>에 대한 설명과 증명은 꽤나 많은 내용이 필요해 여기서는 자세히 다루지 않고, 다른 포스트에서 따로 다루겠다. 관련하여 좋은 내용을 담고 있는 글의 링크를 같이 첨부했으니 읽어보실 것을 권한다(<a href="https://softwaredoug.com/blog/2022/12/26/surpries-at-hi-dimensions-orthoginality.html">링크1</a>, <a href="https://www.reddit.com/r/MachineLearning/comments/cttefo/comment/exs7d08/">링크2</a>).</p>

<h4 id="-self-attention-with-linear-projection"><code class="language-plaintext highlighter-rouge">🚀 Self-Attention with linear projection</code></h4>

<p>왜 이름이 <code class="language-plaintext highlighter-rouge">self-attention</code>일까 먼저 고민해보자. 사실 <code class="language-plaintext highlighter-rouge">attention</code> 개념은 본 논문이 발표되기 이전부터 사용되던 개념이다. <code class="language-plaintext highlighter-rouge">attention</code>은 <code class="language-plaintext highlighter-rouge">seq2seq</code> 구조에서 처음 나왔는데, <code class="language-plaintext highlighter-rouge">seq2seq</code> 은 번역 성능을 높이는 것을 목적으로 고안된 구조라서, 목표인 디코더의 <code class="language-plaintext highlighter-rouge">hidden_states</code> 값을 쿼리로, 인코더의 <code class="language-plaintext highlighter-rouge">hidden_states</code>를 키, 벨류의 출처로 사용했다. 즉, 서로 다른 출처에서 나온 <code class="language-plaintext highlighter-rouge">hidden_states</code> 을 사용해 내적 연산을 수행했던 것이다. 이런 개념에 이제 <code class="language-plaintext highlighter-rouge">“self"</code> 라는 이름이 붙었다. 결국 같은 출처에서 나온 <code class="language-plaintext highlighter-rouge">hidden_states</code> 를 내적하겠다는 의미를 내포하고 있는 것이다. 내적은 두 벡터의 <code class="language-plaintext highlighter-rouge">“닮은 정도”</code> 를 수학적으로 계산한다. 따라서 <code class="language-plaintext highlighter-rouge">self-attention</code> 이란 간단하게, 같은 출처에서 만들어진 $Q$(쿼리), $K$(키), $V$(벨류)가 <code class="language-plaintext highlighter-rouge">서로 얼마나 닮았는지</code> 계산해보겠다는 것이다.</p>

<p align="center">
<img src="/assets/images/transformer/linear_projection.png" alt="self-attention with linear projection" class="align-center image-caption" width="50%&quot;, height=&quot;50%" />
<strong><em><a href="https://jalammar.github.io/illustrated-transformer/">self-attention with linear projection</a></em></strong>
</p>

<p>그렇다면 이제 $Q$(쿼리), $K$(키), $V$(벨류)의 정체, 같은 출처에서 나왔다는 말의 의미 그리고 입력 행렬 $X$를 <code class="language-plaintext highlighter-rouge">linear projection</code> 하여 $Q$(쿼리), $K$(키), $V$(벨류) 행렬을 만드는 이유를 <strong>구체적인 예시를 통해 이해해보자.</strong> 추가로 $Q$(쿼리), $K$(키), $V$(벨류) 개념은 <code class="language-plaintext highlighter-rouge">Information Retrieval</code>에서 먼저 파생된 개념이라서 예시 역시 정보 검색과 관련된 것으로 준비했다.</p>

<p>당신이 만약 <code class="language-plaintext highlighter-rouge">“에어컨 필터 청소하는 방법”</code>이 궁금해 구글에 검색하는 상황이라고 가정해보겠다. <strong>목표는 가장 빠르고 정확하게 내가 원하는 필터 청소 방법에 대한 지식을 획득하는 것이다.</strong> <strong><code class="language-plaintext highlighter-rouge">그렇다면 당신은 뭐라고 구글 검색창에 검색할 것인가??</code></strong> <strong>이것이 바로</strong> $Q$<strong>(쿼리)에 해당한다.</strong> 당신은 검색창에 <code class="language-plaintext highlighter-rouge">“에어컨 필터 청소하는 방법”</code>을 입력해 검색 결과를 반환 받았다. <strong>반환 받은 결과물의 집합이 바로</strong> $K$<strong>(키)가 된다.</strong> 당신은 총 100개의 블로그 게시물을 키 값으로 받았다. 그래서 당신이 사용하는 삼성 무풍 에어컨의 필터 청소법이 정확히 적힌 게시물을 찾기 위해 하나 하나 링크를 타고 들어가 보았다. 하지만 정확하게 원하는 정보가 없어서 계속 찾다보니 결국 4페이지 쯤에서 원하던 정보가 담긴 게시물을 찾을 수 있었다. <strong>이렇게 내가 원하는 정보인지 아닌지 대조하는 과정이 바로</strong> $Q$<strong>(쿼리)와</strong> $K$<strong>(키) 행렬을</strong> <code class="language-plaintext highlighter-rouge">내적</code><strong>하는 행위가 된다.</strong> 곧바로 에어컨 청소를 하려고 보니, 방법을 까먹어서 매년 여름마다 검색을 해야할 것 같아 해당 게시물을 북마크에 저장해두었다. <strong>여기서 북마크가 바로</strong> $V$<strong>(벨류) 행렬이 된다.</strong></p>

<p>이 모든 과정에 10분이 걸렸다. 겨우 필터 청소 방법을 찾는데 10분이라니 당신은 자존심이 상했다. <code class="language-plaintext highlighter-rouge">더 빨리 원하는 정보(손실 함수 최적화)</code>를 찾을 수 있는 방법이 없을까 고민해보다가 <code class="language-plaintext highlighter-rouge">당신이 사용하는 에어컨 브랜드명(삼성 Bespoke 에어컨)을 검색어에 추가하기로 했다</code>. 그랬더니 1페이지 최하단에서 아까 4페이지에서 찾은 정보를 곧바로 찾을 수 있었다. 그 덕분에 시간을 <code class="language-plaintext highlighter-rouge">10분</code>에서 <code class="language-plaintext highlighter-rouge">1분 30초</code>로 단축시킬 수 있었다. <strong>이렇게 검색 시간을 단축(손실 줄이기)하기 위해 더 나은 검색 표현을 고민하고 수정하는 행위가 바로 입력</strong> $X$에 $W_{Q}$<strong>를 곱해 행렬</strong> $Q$ <strong>을 만드는 수식으로 표현된다.</strong></p>

<p>1년 뒤 여름, 당신은 브라우저를 바꾼 탓에 북마크가 초기화 되어 다시 한 번 검색을 해야 했다. 하지만 여전히 검색어는 기억하고 있어서, 1년전 최적의 결과를 얻었던 그대로 다시 검색을 했다. 분명 똑같이 검색을 했는데 같은 결과가 1페이지 최상단에서 반환되고 있었다. 당신은 이게 어떻게 된 일인지 궁금해 포스트를 천천히 보던 중, 제목에 1년전에는 없던 <code class="language-plaintext highlighter-rouge">삼성 Bespoke 에어컨</code> 이라는 키워드가 포함 되어 있었다. 게시물의 주인장이 <code class="language-plaintext highlighter-rouge">SEO 최적화</code>를 위해 추가했던 것이었다. 덕분에 당신은 소요 시간을 <code class="language-plaintext highlighter-rouge">1분 30초</code>에서 <code class="language-plaintext highlighter-rouge">20초</code>로 줄일 수 있었다. <strong>이런 상황이 바로 입력</strong> $X$에 $W_{K}$<strong>를 곱해 행렬</strong> $K$ <strong>를 만드는 수식에 대응된다.</strong></p>

<p>우리는 위 예시를 통해 원하는 정보를 빠르고 정확하게 찾는 행위란, 답변자가 이해하기 좋은 질문과 질문자의 질문 의도에 부합하는 좋은 답변으로 완성된다는 것을 알 수 있었다. 뿐만 아니라, 좋은 질문과 좋은 답변이라는 것은 처음부터 완성되는게 아니라 <strong>검색 시간을 단축하려는 끊임없는 노력</strong>을 통해 성취된다는 것 역시 깨우쳤다. 두가지 인사이트가 바로 <code class="language-plaintext highlighter-rouge">linear projection</code>으로 행렬 $Q, K,V$을 정의한 이유다. <strong>내가 원하는 정보인지 아닌지 대조하는 내적 연산은 수행하는데 가중치 행렬이 필요 없기 때문에 손실함수의 오차 역전을 활용한 수치 최적화를 수행할 수 없다.</strong> 그래서 손실함수 미분에 의한 최적화가 가능하도록  <code class="language-plaintext highlighter-rouge">linear projection matrix</code>를 활용해 행렬 $Q, K,V$를 정의해준 것이다. <strong>이렇게 하면 모델이 우리의 목적에 가장 적합한 질문과 답변을 알아서 표현 해줄 것이라 기대할 수 있게 된다.</strong> 한편, 같은 출처에서 나왔다는 말은 방금 예시에서 행렬 $Q, K,V$를 만드는데 동일하게 입력 $X$를 사용 것과 같은 상황을 의미한다.</p>

<p>이제 다시 자연어 처리 맥락으로 돌아와보자. <code class="language-plaintext highlighter-rouge">Transformer</code> 는 좋은 번역기를 만들기 위해 고안된 <code class="language-plaintext highlighter-rouge">seq2seq</code> 구조의 모델이다. 즉, 빠르고 정확하게 대상 언어에서 타겟 언어로 번역하는 것에 목표를 두고 만들어졌다는 것이다. 번역을 잘하기 위해서는 어떻게 해야 할까?? <strong>1) 대상 언어로 쓰인 시퀀스의 의미를 정확하게 파악해야 하고, 2) 파악한 의미와 가장 유사한 시퀀스를 타겟 언어로 만들어 내야 한다.</strong> <code class="language-plaintext highlighter-rouge">그래서 1번의 역할은 Encoder가 그리고 2번은 Decoder가 맡게 된다</code>. 인코더는 결국 (번역하는데 적합한 형태로) 대상 언어 시퀀스의 의미를 정확히 이해하는 방향(숫자로 표현, 임베딩 추출)으로 학습을 수행하게 되며, 디코더는 인코더의 학습 결과와 가장 유사한 문장을 타겟 언어로 생성해내는 과정을 배우게 된다. 따라서 인코더는 대상 언어를 출처로, 디코더는 타겟 언어를 출처로 행렬 $Q, K,V$를 만든다. 정확히 <code class="language-plaintext highlighter-rouge">self</code> 라는 단어를 이름에 갖다 붙인 의도와 일맥상통하는 모습이다.</p>

<p><strong>결국</strong> <code class="language-plaintext highlighter-rouge">Transformer</code> <strong>의 성능을 좌지우지 하는 것은 누가 얼마나 더</strong> <code class="language-plaintext highlighter-rouge">linear projection weight</code><strong>을 잘 최적화 하는가에 달렸다고 볼 수 있다.</strong></p>

<p><strong>한편 필자는 처음 이 논문을 읽었을 때</strong> <code class="language-plaintext highlighter-rouge">linear projection</code> <strong>자체의 필요성은 공감했으나, 굳이 3개의 행렬로 나눠서</strong> <code class="language-plaintext highlighter-rouge">train</code> <strong>시켜야 하는</strong> <code class="language-plaintext highlighter-rouge">param</code> <strong>숫자를 늘리는 것보다는</strong> <code class="language-plaintext highlighter-rouge">weight share</code> <strong>하는 형태로 만드는게 더 효율적일 것 같다는 추측을 했었다.</strong></p>

<p>그러나 이번 리뷰를 위해 다시 논문을 읽던 중, 좋은 질문을 하기 위한 노력과 좋은 답변을 하기 위한 노력, 그리고 필요한 정보를 정확히 추출해내는 행위를 각각 서로 다른 3개의 벡터로 표현했을 때 <strong>벡터들이 가지는 방향성이 서로 다를텐데</strong> 그것을 하나의 벡터로 표현하려면 모델이 학습을 하기 힘들 것 같다는 생각이 들었다. 방금 위에서 든 예시만 봐도 그렇다. 서로 다른 3개의 행위 사이의 최적 지점을 찾으라는 것과 마찬가진데 그런 스팟이 있다고 해도 언어 모델이 잘 찾을 수 있을까?? 인간도 찾기 힘든 것을 모델이 잘 찾을리가 없다.</p>

<h4 id="scaled-dot-product-attention"><strong><code class="language-plaintext highlighter-rouge">📐 Scaled Dot-Product Attention</code></strong></h4>

\[Attention(Q,K,V) = softmax(\frac{Q·K^T}{\sqrt{d_k}})V\]

<p>이번에는 <code class="language-plaintext highlighter-rouge">Self-Attention</code> 의 두 번째 하위 블럭인 <code class="language-plaintext highlighter-rouge">Scaled Dot-Product Attention</code> 차례다. 사실 우리는 <code class="language-plaintext highlighter-rouge">Linear Projection</code> 파트에서 이미 우리도 모르게 <code class="language-plaintext highlighter-rouge">Scaled Dot-Product Attention</code> 에 대해 공부했다. 예시를 다시 한 번 상기시켜보자. 질의를 통해 얻은 결과 리스트(키)에서 내가 원하는 정보를 찾기 위해 쿼리와 키를 대조한다고 했던 것 기억나는가?? 바로 그 대조하는 행위를 수학적으로 모델링한 것이 바로 <code class="language-plaintext highlighter-rouge">Scaled Dot-Product Attention</code> 에 해당한다.</p>

<p align="center">
<img src="/assets/images/transformer/dot_attention.png" alt="Attention is All You Need" class="align-center image-caption" width="50%&quot;, height=&quot;50%" />
<strong><em><a href="https://arxiv.org/abs/1706.03762">Attention is All You Need</a></em></strong>
</p>

<p><code class="language-plaintext highlighter-rouge">Scaled Dot-Product Attention</code> 은 총 5단계를 거쳐 완성된다. 단계마다 어떤 연산을 왜 하는지 그리고 무슨 인사이트가 담겨 있는지 알아보자. 이 중에서 마스킹 단계는 인코더와 디코더의 동작을 자세히 알아야하기 때문에 전체적인 구조 관점에서 모델을 바라볼 때 함께 설명하도록 하겠다.</p>

<p><strong><code class="language-plaintext highlighter-rouge">✖️ Stage 1. Q•K^T Dot-Product</code></strong></p>

\[Q•K^T\]

<p>인간은 문장이나 어떤 표현의 의미를 파악하는데 바로 주변 맥락을 참고하거나, 더 멀리 떨어진 곳의 단어•시퀀스를 이용하기도 한다. <strong>즉, 주어진 시퀀스 내부의 모든 맥락을 이용해 특정 부분의 의미를 이해한다는 것이다.</strong> 그렇다고 모든 정보가 동일하게 특정 표현의 의미에 영향을 미치는 것은 또 아닌데, 수능 영어에 킬러 문항으로 등장하는 빈칸 채우기 문제를 어떻게 풀었나 떠올려보자. 디테일한 풀이 방식에는 사람마다 차이가 있겠지만, 일반적으로 지문은 모두 훑어 보되 빈칸에 들어갈 정답의 근거가 되는 특정 문장 혹은 표현 1~2개를 찾아내어 비슷한 의미를 지닌 선지를 골라 내는 방식을 사용한다. <strong>다시 말해, 주어진 전체 단락에서 의미를 이해하는데 중요한 역할을 하는 표현이나 문장을 골라내어 <code class="language-plaintext highlighter-rouge">중요도</code> 만큼 <code class="language-plaintext highlighter-rouge">가중치</code> 를 주겠다는 것이다.</strong></p>

<p align="center">
<img src="/assets/images/transformer/attention_visualization.png" alt="Q•K^T Dot Product Visualization" class="align-center image-caption" width="50%&quot;, height=&quot;50%" />
<strong><em><a href="https://jalammar.github.io/illustrated-transformer/">Q•K^T Dot Product Visualization</a></em></strong>
</p>

<p>그렇다면 이것을 어떻게 수학적으로 모델링했을까?? 바로 행렬 $Q$와 $K^T$의 <code class="language-plaintext highlighter-rouge">내적</code>을 활용한다. 행렬 $Q$는 모델이 의미를 파악해야 하는 대상이 담겨 있고, 행렬 $K$에는 의미 파악에 필요한 단서들이 담겨있다. 내적은 두 벡터의 서로 <code class="language-plaintext highlighter-rouge">“닮은 정도”</code> 를 의미한다고 했다. <code class="language-plaintext highlighter-rouge">“닮은 정도”</code> 가 바로 <code class="language-plaintext highlighter-rouge">중요도•가중치</code>에 대응된다. 따라서 연산 결과는 전체 시퀀스에 속한 토큰들 사이의 <code class="language-plaintext highlighter-rouge">“닮은 정도”</code> 가 수치로 변환되어 행렬에 담긴다.</p>

<p>왜 <code class="language-plaintext highlighter-rouge">내적 결과</code>가 <code class="language-plaintext highlighter-rouge">중요도</code>와 같은 의미를 갖게 되는 것일까?? 아까 <code class="language-plaintext highlighter-rouge">Input Embedding</code>과 <code class="language-plaintext highlighter-rouge">Position Embedding</code>을 행렬합 하는 것에 대한 당위성을 설명하면서 고차원으로 갈수록 대부분의 벡터 쌍은 <code class="language-plaintext highlighter-rouge">직교성</code>을 갖게 된다고 언급한 바 있다. 그래서 두 벡터가 비슷한 방향성을 갖는다는 것 자체가 매우 드문일이다. 희귀하고 드문 사건은 그만큼 중요하다고 말할 수 있기 때문에 <code class="language-plaintext highlighter-rouge">내적 결과</code>를 <code class="language-plaintext highlighter-rouge">중요도</code>에 맵핑하는 것이다.</p>

<p>한편, 행렬 $Q,K$ 모두 차원이 <code class="language-plaintext highlighter-rouge">[Batch, Max_Seq, Dim_Head]</code> 인 텐서라서 내적한 결과의 모양은 <code class="language-plaintext highlighter-rouge">[Batch, Max_Seq, Max_Seq]</code> 이 될 것이다.</p>

<p><strong><code class="language-plaintext highlighter-rouge">🔭 Stage 2. Scale</code></strong></p>

\[Q•K^T = \begin{bmatrix}
56.8 &amp; 12.1 &amp; 43.5 \\
30.4 &amp; 100.8 &amp; 24.2 \\
11.11 &amp; 7.34 &amp; 20.23 \\
\end{bmatrix}\]

<p><code class="language-plaintext highlighter-rouge">“I am dog”</code> 라는 문장을 $Q•K^T$하면 위와 같은 <code class="language-plaintext highlighter-rouge">3x3</code> 짜리 행렬이 나올 것이다. 행렬을 행벡터로 바라보자. <strong>행 사이의 값의 분포가 고르지 못하다는 것을 알 수 있다.</strong> 이렇게 분산이 큰 상태로 <code class="language-plaintext highlighter-rouge">softmax</code> 에 통과시키게 되면 역전파 과정에서 <code class="language-plaintext highlighter-rouge">softmax</code> 의 미분값이 줄어 들어 학습 속도가 느려지고 나아가 <code class="language-plaintext highlighter-rouge">vanishing gradient</code> 현상이 발생할 수 있다. 따라서 행벡터 사이의 분산을 줄여주기 위해서 <code class="language-plaintext highlighter-rouge">Scale Factor</code> 를 정의하게 된다. 그렇다면 어떤 <code class="language-plaintext highlighter-rouge">Scale Factor</code> 를 써야할까??</p>

\[\frac{Q•K^T}{\sqrt{d_h}}\]

<p>애초에 <code class="language-plaintext highlighter-rouge">Dim Head</code> 차원에 속한 값들의 분산이 큰 것도 문제가 되지만 이것은 <code class="language-plaintext highlighter-rouge">Input Embedding</code>이나 <code class="language-plaintext highlighter-rouge">Position Embedding</code>에 <code class="language-plaintext highlighter-rouge">layernorm</code> 을 적용하면 해결할 수 있기 때문에 논의 대상이 아니다. 그것보다는 내적 과정에 주목해보자. 우리는 내적을 하다보면 <code class="language-plaintext highlighter-rouge">Dim Head</code>의 차원이 커질수록 더해줘야 하는 스칼라 값의 개수가 늘어나게 된다는 사실을 알 수 있다. 만약 위에서 예시로 든 수식의 <code class="language-plaintext highlighter-rouge">Dim Head</code>가 64라고 가정해보자. 그럼 우리는 1행 1열의 값을 얻기 위해 64개의 스칼라 값을 더해줘야 한다. 만약 <code class="language-plaintext highlighter-rouge">512</code>차원이라면 <code class="language-plaintext highlighter-rouge">512</code>개로 불어난다. <strong>더해줘야 하는 스칼라 값이 많아진다면 행벡터 끼리의 분산이 커질 우려가 있다.</strong> 따라서 차원 크기의 스케일에 따라 <code class="language-plaintext highlighter-rouge">softmax</code>의 미분값이 줄어드는 것을 방지하기 위해 $Q•K^T$결과에 $\sqrt{d_h}$를 나눠 준다.</p>

<p>여담으로 이러한 <code class="language-plaintext highlighter-rouge">scale factor</code> 의 존재 때문에 <code class="language-plaintext highlighter-rouge">Self-Attention</code>을 <code class="language-plaintext highlighter-rouge">Scaled Dot-Product Attention</code> 이라고 부르기도 한다.</p>

<p><strong><code class="language-plaintext highlighter-rouge">🎭 Stage 3. masking</code></strong><br />
마스킹은 인코더 <code class="language-plaintext highlighter-rouge">Input Padding</code>, 디코더 <code class="language-plaintext highlighter-rouge">Masked Multi-Head Attention</code>, 인코더-디코더 <code class="language-plaintext highlighter-rouge">Self-Attention</code> 을 위해 필요한 계층이다. 뒤에 두개는 디코더의 동작을 알아야 이해가 가능하기 때문에 여기서는 인코더의 마스킹에 대해서만 알아보자.</p>

<p align="center">
<img src="/assets/images/transformer/encoder_mask.png" alt="Encoder Padding Mask" class="align-center image-caption" width="50%&quot;, height=&quot;50%" />
<strong><em><a href="https://paul-hyun.github.io/transformer-02/">Encoder Padding Mask</a></em></strong>
</p>

<p>실제 텍스트 데이터는 배치된 시퀀스마다 그 길이가 제각각이다. 효율성을 위해 행렬을 사용하는 컴퓨터 연산 특성상 배치된 시퀀스의 길이가 모두 다르다면 연산을 진행할 수가 없다. 따라서 배치 내부의 모든 시퀀스의 길이를 통일해주는 작업을 하게 되는데, 이 때 기준 길이보다 짧은 시퀀스에 대해서는 <code class="language-plaintext highlighter-rouge">0</code>값을 채워넣는 <code class="language-plaintext highlighter-rouge">padding</code> 작업을 한다.  행렬 연산에는 꼭 필요했던 <code class="language-plaintext highlighter-rouge">padding</code>은 오히려 <code class="language-plaintext highlighter-rouge">softmax</code> 레이어를 계산할 때 방해가 된다. 따라서 모든 <code class="language-plaintext highlighter-rouge">padding</code> 값을 <code class="language-plaintext highlighter-rouge">softmax</code>의 확률 계산에서 완전히 제외시키기 위해 <code class="language-plaintext highlighter-rouge">Input Embedding</code>에서 <code class="language-plaintext highlighter-rouge">padding token</code>의 인덱스를 저장하고 해당되는 모든 원소를 <code class="language-plaintext highlighter-rouge">-∞</code> 로 마스킹하는 과정이 필요하다.</p>

<p>이 때 마스킹 처리는 열벡터에만 적용한다. 그 이유는 바로 <code class="language-plaintext highlighter-rouge">softmax</code> 계산을 어차피 행벡터 방향으로만 할 것이기 때문이다. 행벡터 방향의 <code class="language-plaintext highlighter-rouge">padding token</code>에도 동일하게 마스킹 적용하는 것은 상관 없으나 열벡터와 행벡터 동시에 마스킹 적용하는 동작을 구현하는 것은 생각보다 많이 까다로우며, 나중에 손실값 계산하는 단계에서 <code class="language-plaintext highlighter-rouge">ignore_index</code> 옵션을 사용해 행벡터의 <code class="language-plaintext highlighter-rouge">padding token</code>을 무시하는 것이 훨씬 효율적이다. 한편, <code class="language-plaintext highlighter-rouge">ignore_index</code> 옵션은 <code class="language-plaintext highlighter-rouge">nn.CrossEntropyLoss</code> 에 매개변수로 구현 되어 있다.</p>

<p><strong><code class="language-plaintext highlighter-rouge">📈 Stage 4. Softmax &amp; Score•V</code></strong></p>

\[Score = \begin{bmatrix}
  0.90 &amp; 0.07 &amp; 0.03 \\
  0.025 &amp; 0.95 &amp; 0.025 \\
  0.21 &amp; 0.03 &amp; 0.76 
\end{bmatrix}, \ \  V=\begin{bmatrix}
  67.85 &amp; 90 &amp; 91 &amp; ..... \\
  62 &amp; 40 &amp; 50 &amp; ..... \\
  37 &amp; 41 &amp; 20 &amp; .....
\end{bmatrix},\ \  Z = score \ • \ V\]

\[{\overset{}{z_{1}^{}}} = {\overset{}{Score_{11}^{}}}({\overset{}{V_{11}^{}}}\ + \ {\overset{}{V_{12}^{}}}\ + \ ...) \ + \ {\overset{}{Score_{12}^{}}}({\overset{}{V_{21}^{}}}\ + \ {\overset{}{V_{22}^{}}}\ + \ ...)\ + \ ....... \\
{\overset{}{z_{2}^{}}} = {\overset{}{Score_{21}^{}}}({\overset{}{V_{11}^{}}}\ + \ {\overset{}{V_{12}^{}}}\ + \ ...) \ + \ {\overset{}{Score_{22}^{}}}({\overset{}{V_{21}^{}}}\ + \ {\overset{}{V_{22}^{}}}\ + \ ...)\ + \ ....... \\
{\overset{}{z_{3}^{}}} = {\overset{}{Score_{31}^{}}}({\overset{}{V_{11}^{}}}\ + \ {\overset{}{V_{12}^{}}}\ + \ ...) \ + \ {\overset{}{Score_{32}^{}}}({\overset{}{V_{21}^{}}}\ + \ {\overset{}{V_{22}^{}}}\ + \ ...)\ + \ ....... \\\]

<p>계산된 <code class="language-plaintext highlighter-rouge">유사도(내적 결과, 중요도, 가중치)</code>, $\frac{Q•K^T}{\sqrt{d_h}}$는 이후에 행렬 $V$와 다시 곱해져 행벡터 $Z_n$(n번째 토큰)에서 토큰에 대한 어텐션 정도를 나타내는 <code class="language-plaintext highlighter-rouge">가중치</code>의 역할을 하게 된다. 그러나 계산된 유사도는 비정규화된 형태다. 수식에는 편의상 이미 <code class="language-plaintext highlighter-rouge">softmax</code>를 적용한 형태의 행렬을 적었지만, 실제로는 원소값의 분산이 너무 커서 가중치로는 쓰기 힘든 수준이다. 따라서 행벡터 단위로 <code class="language-plaintext highlighter-rouge">softmax</code>에 통과시켜 결과의 합이 1인 확률값으로 <code class="language-plaintext highlighter-rouge">변환(정규화)</code>해 행렬 $V$의 가중치로 사용한다.</p>

<p>이제 두번째 수식을 보자. $Score_{11}$에 해당하는 <code class="language-plaintext highlighter-rouge">0.90</code>가 행렬 $V$의 첫번째 행벡터와 곱해지고 있다. 행렬 $V$의 첫번째 행벡터는 토큰 <code class="language-plaintext highlighter-rouge">“I”</code> 를 <code class="language-plaintext highlighter-rouge">512</code>차원으로 표현한 것이다. 그 다음 $Score_{12}$는 행렬 $V$의 두번째 행벡터와, $Score_{13}$은 행렬 $V$의 세번째 행벡터와 각각 곱해진다.</p>

<p>이 행위의 의미는 무엇일까?? $Score_{11}$, $Score_{12}$, $Score_{13}$은 모두 첫번째 토큰인 <code class="language-plaintext highlighter-rouge">“I”</code>에 의미를 파악하는데 <code class="language-plaintext highlighter-rouge">“I”</code>, <code class="language-plaintext highlighter-rouge">“am”</code>, <code class="language-plaintext highlighter-rouge">“dog”</code>를 어느 정도로 어텐션해야 하는지, 즉 <code class="language-plaintext highlighter-rouge">“I”</code>의 의미를 표현하는데 세 토큰의 의미를 어느 정도 반영할지 수치로 표현한 것이다. 당연히 자기 자신인 <code class="language-plaintext highlighter-rouge">“I”</code>와 <code class="language-plaintext highlighter-rouge">가중치(유사도, 중요도)</code>가 가장 높기 때문에 행렬 $V$에서 <code class="language-plaintext highlighter-rouge">“I”</code> 에 해당하는 행벡터 가중치에 가장 큰 값이 들어간다고 생각해볼 수 있다. 이렇게 각 토큰마다 가중합을 반복해주면 최종적으로 <code class="language-plaintext highlighter-rouge">“I”</code>, <code class="language-plaintext highlighter-rouge">“am”</code>, <code class="language-plaintext highlighter-rouge">“dog”</code> 을 인코딩한 $Z_1, \ Z_2, \  Z_3$ 값을 얻을 수 있다.</p>

<p><strong><code class="language-plaintext highlighter-rouge">👩‍💻 Implementation</code></strong></p>

<p>이렇게 <code class="language-plaintext highlighter-rouge">Scaled Dot-Product Attention</code> 을 모두 살펴보았다. 해당 레이어는 모델이 손실값이 가장 작아지는 방향으로 최적화한 행렬 $Q, K, V$ 을 이용해, 토큰의 의미를 이해하는데 어떤 맥락과 표현에 좀 더 집중하고 덜 집중해야 하는지를 유사도를 기준으로 판단한다는 것을 꼭 기억하자. 그렇다면 실제 코드는 어떻게 작성 해야하는지 함께 알아보자. 상단의 <code class="language-plaintext highlighter-rouge">class diagram</code> 을 다시 한 번 보고 돌아오자.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Pytorch Implementation of Scaled Dot-Product Self-Attention
</span>
<span class="k">def</span> <span class="nf">scaled_dot_product_attention</span><span class="p">(</span><span class="n">q</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">k</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">v</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">dot_scale</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">mask</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="bp">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
    <span class="s">"""
    Scaled Dot-Product Attention with Masking for Decoder
    Args:
        q: query matrix, shape (batch_size, seq_len, dim_head)
        k: key matrix, shape (batch_size, seq_len, dim_head)
        v: value matrix, shape (batch_size, seq_len, dim_head)
        dot_scale: scale factor for Q•K^T result
        mask: there are three types of mask, mask matrix shape must be same as single attention head
              1) Encoder padded token
              2) Decoder Masked-Self-Attention
              3) Decoder's Encoder-Decoder Attention
    Math:
        A = softmax(q•k^t/sqrt(D_h)), SA(z) = Av
    """</span>
    <span class="n">attention_matrix</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">))</span> <span class="o">/</span> <span class="n">dot_scale</span>
    <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">attention_matrix</span> <span class="o">=</span> <span class="n">attention_matrix</span><span class="p">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">mask</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="s">'-inf'</span><span class="p">))</span>
    <span class="n">attention_dist</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">attention_matrix</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">attention_matrix</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">attention_dist</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">attention_matrix</span>
</code></pre></div></div>

<p>마스킹 옵션의 경우 주석에 정리된 3가지 상황 중에서 한 개 이상에 해당되면 실행되도록 코드를 작성했다. 3가지  상황과 구체적인 마스킹 방법에 대해서는 전체 모델 구조를 보는 때 소개하도록 하겠다.</p>

<p>한편, 인코더나 디코더나 모두 사용하는 입력과 마스킹 방식에 차이는 있지만, <code class="language-plaintext highlighter-rouge">Scaled Dot-Product Attention</code> 연산 자체는 동일한 것을 사용한다. 따라서 여러개의 인코더나 디코더 객체들 혹은 어텐션 해드 객체들이 모두 쉽게 연산에 접근할 수 있게 클래스 외부에 메서드 형태로 구현하게 되었다.</p>

<h4 id="multi-head-attention-block"><strong><code class="language-plaintext highlighter-rouge">👩‍👩‍👧‍👦 Multi-Head Attention Block</code></strong></h4>

<p>지금까지 살펴본 <code class="language-plaintext highlighter-rouge">Self-Attention</code>의 동작은 모두 한 개의 <code class="language-plaintext highlighter-rouge">Attention-Head</code>에서 일어나는 일을 서술한 것이다. 사실 실제 모델에서는 같은 동작이 <code class="language-plaintext highlighter-rouge">N-1</code>개의 다른 해드에서 동시에 일어나는데, 이것이 바로 <code class="language-plaintext highlighter-rouge">Multi-Head Attention</code>이다.</p>

<p><code class="language-plaintext highlighter-rouge">Official Paper</code> 기준으로 <code class="language-plaintext highlighter-rouge">Transformer-base</code>의 <code class="language-plaintext highlighter-rouge">hidden states</code> 차원은 <code class="language-plaintext highlighter-rouge">512</code>이다. 이것을 개당 <code class="language-plaintext highlighter-rouge">64</code>차원을 갖는 <code class="language-plaintext highlighter-rouge">8</code>개의 <code class="language-plaintext highlighter-rouge">Attention-Head</code> 로 쪼갠 뒤, 8개의 <code class="language-plaintext highlighter-rouge">Attention-Head</code> 에서 동시에 <code class="language-plaintext highlighter-rouge">Self-Attention</code> 을 수행한다. 이후 결과를 <code class="language-plaintext highlighter-rouge">concat</code>하여 다시 <code class="language-plaintext highlighter-rouge">hidden states</code> 를 <code class="language-plaintext highlighter-rouge">512</code> 로 만든 뒤, 여러 해드에서 만든 결과를 연결하고 섞어주기 위해 입출력 차원이 <code class="language-plaintext highlighter-rouge">hidden states</code>와 동일한 <code class="language-plaintext highlighter-rouge">linear projection layer</code>에 통과시킨다. 이것이 인코더(혹은 디코더) 블럭 한 개의 최종 <code class="language-plaintext highlighter-rouge">Self-Attention</code> 결과가 된다.</p>

<p align="center">
<img src="/assets/images/transformer/multi_head_result.png" alt="Multi-Head Attention Result Visualization" class="align-center image-caption" width="50%&quot;, height=&quot;50%" />
<strong><em><a href="https://arxiv.org/abs/1706.03762">Multi-Head Attention Result Visualization</a></em></strong>
</p>

<p><strong>그럼 왜 이렇게 여러 해드를 사용했을까?? 바로 집단지성의 효과를 누리기 위함이다.</strong> 생각해보자. 책 하나를 읽어도 사람마다 정말 다양한 해석이 나온다. 모델도 마찬가지다. 여러 해드를 사용해서 좀 더 다양하고 풍부한 의미를 임베딩에 담고 싶었던 것이다. Kaggle을 해보신 독자라면, 여러 전략을 사용해 여러 개의 결과를 도출한 뒤, 마지막에 모두 앙상블하면 전략 하나 하나의 결과보다 더 높은 성적을 얻어본 경험이 있을 것이다. 이것도 비슷한 효과를 의도했다고 생각한다. Vision에서 Conv Filter를 여러 종류 사용해 다양한 Feature Map을 추출하는 것도 비슷한 현상이라 볼 수 있겠다.</p>

<p>위 그림은 저자가 제시한 <code class="language-plaintext highlighter-rouge">Multi-Head Attention</code>의 시각화 결과다. 중간에 있는 여러 색깔의 띠는 개별 해드가 어텐션하는 방향을 가리킨다. 토큰 <code class="language-plaintext highlighter-rouge">“making”</code> 에 대해서 해드들이 서로 다른 토큰에 어텐션하고 있다.</p>

<p align="center">
<img src="/assets/images/transformer/vit_multi_head_result.png" alt="ViT Multi-Head Attention Result Visualization" class="align-center image-caption" width="50%&quot;, height=&quot;50%" />
<strong><em><a href="https://arxiv.org/abs/2010.11929">ViT Multi-Head Attention Result Visualization</a></em></strong>
</p>

<p>위 그림은 Vision Transformer 논문에서 발췌한 그림<a href="https://qcqced123.github.io/cv/vit">(그림의 자세한 의미는 여기서)</a>이다. 역시 마찬가지로 모델의 초반부 인코더에 속한 Multi-Head들이 서로 다양한 토큰에 어텐션을 하고 있음을 알 수 있다. 추가로 후반으로 갈수록 점점 <code class="language-plaintext highlighter-rouge">Attention Distance</code> 가 일정한 수준에 수렴하는 모습을 볼 수 있는데, 이것을 레이어를 통과할수록 개별 해드가 자신이 어떤 토큰에 주의를 기울여야할지 구체적으로 알아가는 과정이라고 해석할 수 있다. 초반부에는 어찌할 바를 몰라서 이토큰 저토큰에 죄다 어텐션하는 것이다.</p>

<p>그래서 <code class="language-plaintext highlighter-rouge">Transformer</code>는 <code class="language-plaintext highlighter-rouge">Bottom Layer</code>에서는 <code class="language-plaintext highlighter-rouge">Global</code>하고 <code class="language-plaintext highlighter-rouge">General</code>한 정보를 포착하고, <code class="language-plaintext highlighter-rouge">Output</code>과 가까운 <code class="language-plaintext highlighter-rouge">Top Layer</code>에서는 <code class="language-plaintext highlighter-rouge">Local</code>하고 <code class="language-plaintext highlighter-rouge">Specific</code>한 정보를 포착한다.</p>

<p><strong><code class="language-plaintext highlighter-rouge">👩‍💻 Implementation</code></strong></p>

<p>이제 구현을 실제로 구현을 해보자. 역시 구현은 파이토치로 진행했다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Pytorch Implemenation of Single Attention Head
</span>
<span class="k">class</span> <span class="nc">AttentionHead</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="s">"""
    In this class, we implement workflow of single attention head
    Args:
        dim_model: dimension of model's latent vector space, default 512 from official paper
        dim_head: dimension of each attention head, default 64 from official paper (512 / 8)
        dropout: dropout rate, default 0.1
    Math:
        [q,k,v]=z•U_qkv, A = softmax(q•k^t/sqrt(D_h)), SA(z) = Av
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim_model</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">512</span><span class="p">,</span> <span class="n">dim_head</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">64</span><span class="p">,</span> <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">AttentionHead</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span> <span class="o">=</span> <span class="n">dim_model</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dim_head</span> <span class="o">=</span> <span class="n">dim_head</span>  <span class="c1"># 512 / 8 = 64
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">dropout</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dot_scale</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">dim_head</span><span class="p">))</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">fc_q</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">dim_head</span><span class="p">)</span>  <span class="c1"># Linear Projection for Query Matrix
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">fc_k</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">dim_head</span><span class="p">)</span>  <span class="c1"># Linear Projection for Key Matrix
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">fc_v</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">dim_head</span><span class="p">)</span>  <span class="c1"># Linear Projection for Value Matrix
</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">mask</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">enc_output</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="bp">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">fc_q</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="bp">self</span><span class="p">.</span><span class="n">fc_k</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="bp">self</span><span class="p">.</span><span class="n">fc_v</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># x is previous layer's output
</span>        <span class="k">if</span> <span class="n">enc_output</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="s">""" For encoder-decoder self-attention """</span>
            <span class="n">k</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">fc_k</span><span class="p">(</span><span class="n">enc_output</span><span class="p">)</span>
            <span class="n">v</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">fc_v</span><span class="p">(</span><span class="n">enc_output</span><span class="p">)</span>
        <span class="n">attention_matrix</span> <span class="o">=</span> <span class="n">scaled_dot_product_attention</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">dot_scale</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="n">mask</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">attention_matrix</span>
</code></pre></div></div>

<p>똑같은 <code class="language-plaintext highlighter-rouge">Attention-Head</code>를 <code class="language-plaintext highlighter-rouge">N</code>개 사용하기 때문에 먼저 <code class="language-plaintext highlighter-rouge">Single Attention Head</code>의 동작을 따로 객체로 만들었다. 이렇게 하면 <code class="language-plaintext highlighter-rouge">MultiHeadAttention</code> 객체에서 <code class="language-plaintext highlighter-rouge">nn.ModuleList</code> 를 사용해 <code class="language-plaintext highlighter-rouge">N</code>개의 해드를 이어붙일 수 있어서 구현이 훨씬 간편해지기 때문이다. <code class="language-plaintext highlighter-rouge">Single Attention Head</code> 객체가 하는 일은 다음과 같다.</p>

<ul>
  <li><strong>1) Linear Projection by Dimension of Single Attention Head</strong></li>
  <li><strong>2) Maksing</strong></li>
  <li><strong>3) Scaled Dot-Product Attention</strong></li>
</ul>

<p>한편, 여러 <code class="language-plaintext highlighter-rouge">Transformer</code> 구현 Git Repo를 살펴보면 구현 방법은 크게 필자처럼 <code class="language-plaintext highlighter-rouge">Single Attention Head</code>를 추상화하거나 <code class="language-plaintext highlighter-rouge">MultiHeadAttention</code> 객체 하나에 모든 동작을 때려넣는 방식으로 나뉘는 것 같다. 사실 구현에 정답은 없지만 개인적으로 후자의 방식은 비효율적이라 생각한다. 저렇게 구현하면 <code class="language-plaintext highlighter-rouge">3*N</code>개의 <code class="language-plaintext highlighter-rouge">linear projector</code>를 클래스 <code class="language-plaintext highlighter-rouge">__init__</code> 에 만들고 관리해줘야 하는데 쉽지 않을 것이다. 물론 <code class="language-plaintext highlighter-rouge">3</code>개의 <code class="language-plaintext highlighter-rouge">linear projector</code> 만 초기화해서 사용하고 대신 출력 차원을 <code class="language-plaintext highlighter-rouge">Dim_Head</code>가 아닌 <code class="language-plaintext highlighter-rouge">Dim_Model</code>로 구현한 뒤, <code class="language-plaintext highlighter-rouge">N</code>개로 차원을 분할하는 방법도 있다. 하지만 차원을 쪼개는 동작을 구현하는 것도 사실 쉽지 않다. 그래서 필자는 전자의 방식을 추천한다.</p>

<p>한편, <code class="language-plaintext highlighter-rouge">forward</code> 메서드에  <code class="language-plaintext highlighter-rouge">if enc_output is not None:</code> 부분은 추후에 디코더에서 <code class="language-plaintext highlighter-rouge">Multi-Head Attention</code>을 구현하기 위해 추가한 코드다. 디코더는 인코더와 다르게 하나의 디코더 블럭에서 <code class="language-plaintext highlighter-rouge">Self-Attention</code>동작을 두번하는데, 두번째 동작은 서로 다른 출처의 값을 이용해 <code class="language-plaintext highlighter-rouge">linear projection</code>을 수행한다. 따라서 그 경우를 처리해주기 위해 구현하게 되었다.</p>

<p>아래는 <code class="language-plaintext highlighter-rouge">MultiHeadAttention</code> 을 구현한 파이토치 코드다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Pytorch Implemenation of Single Attention Head
</span>
<span class="k">class</span> <span class="nc">MultiHeadAttention</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="s">"""
    In this class, we implement workflow of Multi-Head Self-Attention
    Args:
        dim_model: dimension of model's latent vector space, default 512 from official paper
        num_heads: number of heads in MHSA, default 8 from official paper for Transformer
        dim_head: dimension of each attention head, default 64 from official paper (512 / 8)
        dropout: dropout rate, default 0.1
    Math:
        MSA(z) = [SA1(z); SA2(z); · · · ; SAk(z)]•Umsa
    Reference:
        https://arxiv.org/abs/1706.03762
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim_model</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">512</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span> <span class="n">dim_head</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">64</span><span class="p">,</span> <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MultiHeadAttention</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span> <span class="o">=</span> <span class="n">dim_model</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dim_head</span> <span class="o">=</span> <span class="n">dim_head</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">dropout</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">attention_heads</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">ModuleList</span><span class="p">(</span>
            <span class="p">[</span><span class="n">AttentionHead</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">dim_head</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">dropout</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">num_heads</span><span class="p">)]</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">fc_concat</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">mask</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">enc_output</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="bp">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="s">""" x is already passed nn.Layernorm """</span>
        <span class="k">assert</span> <span class="n">x</span><span class="p">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">3</span><span class="p">,</span> <span class="sa">f</span><span class="s">'Expected (batch, seq, hidden) got </span><span class="si">{</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="s">'</span>
        <span class="n">attention_output</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">fc_concat</span><span class="p">(</span>
            <span class="n">torch</span><span class="p">.</span><span class="n">cat</span><span class="p">([</span><span class="n">head</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">mask</span><span class="p">,</span> <span class="n">enc_output</span><span class="p">)</span> <span class="k">for</span> <span class="n">head</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">attention_heads</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">attention_output</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">MultiHeadAttention</code> 객체는 개별 해드들이 도출한 어텐션 결과를 <code class="language-plaintext highlighter-rouge">concat</code>하고 그것을 <code class="language-plaintext highlighter-rouge">connect &amp; mix</code>하려고 <code class="language-plaintext highlighter-rouge">linear projection</code>을 수행한다.</p>

<h4 id="-feed-forward-network"><strong><code class="language-plaintext highlighter-rouge">🔬 Feed Forward Network</code></strong></h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Pytorch Implementation of FeedForward Network
</span>
<span class="k">class</span> <span class="nc">FeedForward</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="s">"""
    Class for Feed-Forward Network module in transformer
    In official paper, they use ReLU activation function, but GELU is better for now
    We change ReLU to GELU &amp; add dropout layer
    Args:
        dim_model: dimension of model's latent vector space, default 512
        dim_ffn: dimension of FFN's hidden layer, default 2048 from official paper
        dropout: dropout rate, default 0.1
    Math:
        FeedForward(x) = FeedForward(LN(x))+x
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim_model</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">512</span><span class="p">,</span> <span class="n">dim_ffn</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2048</span><span class="p">,</span> <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">FeedForward</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">ffn</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">dim_model</span><span class="p">,</span> <span class="n">dim_ffn</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">GELU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">dropout</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">dim_ffn</span><span class="p">,</span> <span class="n">dim_model</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">dropout</span><span class="p">),</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">ffn</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</code></pre></div></div>

<p>피드 포워드는 모델에 <code class="language-plaintext highlighter-rouge">non-linearity</code>를 추가하기 위해 사용하는 레이어다. 원본 모델은 <code class="language-plaintext highlighter-rouge">ReLU</code> 를 사용하지만 최근 <code class="language-plaintext highlighter-rouge">Transformer</code>류 모델에는 <code class="language-plaintext highlighter-rouge">GeLU</code>를 사용하는 것이 좀 더 안정적인 학습을 하는데 도움이 된다고 밝혀져, 필자 역시 <code class="language-plaintext highlighter-rouge">GeLU</code>를 사용해 구현했다. 또한 논문에는 <code class="language-plaintext highlighter-rouge">dropout</code>에 대한 언급이 전혀 없는데, 은닉층의 차원을 저렇게 크게 키웠다 줄이는데 <code class="language-plaintext highlighter-rouge">overfitting</code> 이슈가 있을 것 같아서 <code class="language-plaintext highlighter-rouge">ViT</code> 논문을 참고해 따로 추가해줬다.</p>

<h4 id="add--norm"><strong><code class="language-plaintext highlighter-rouge">➕ Add &amp; Norm</code></strong></h4>

<p><code class="language-plaintext highlighter-rouge">Residual Connection</code>과 <code class="language-plaintext highlighter-rouge">Layernorm</code>을 의미한다.  따로 객체를 만들어서 사용하지는 않고, <code class="language-plaintext highlighter-rouge">EncoderLayer</code> 객체에 라인으로 추가해 구현하기 때문에 여기서는 역할과 의미만 설명하고 넘어가겠다.</p>

<p>먼저 <code class="language-plaintext highlighter-rouge">Skip-Connection</code>으로도 불리는 <code class="language-plaintext highlighter-rouge">Residual Connection</code>은 어떤 레이어를 통과하기 전, 입력 $x$ 를 레이어를 통과하고 나온 결과값 $fx$ 에 더해준다. 따라서 다음 레이어에 통과되는 입력값은 $x+fx$ 가 된다. 왜 이렇게 더해줄까?? 바로 모델의 안정적인 학습을 위해서다. 일단 그전에 명심하고 가야할 전제가 하나 있다. 모델의 레이어가 깊어질수록 레이어마다 값을 조금씩 바꿔나가는 것이 <code class="language-plaintext highlighter-rouge">Robust</code>하고 <code class="language-plaintext highlighter-rouge">Stable</code>한 결과를 도출할 수 있다는 것이다. 직관적으로 레이어마다 결과가 널뛰기하는 모델보다 안정적으로 차근차근 학습해나가는 모델의 일반화 성능이 더 좋을 것이라고 추측해볼 수 있다. 그래서 <code class="language-plaintext highlighter-rouge">Residual Connection</code> 은 입력 $x$ 와 레이어의 이상적인 출력값 $H(x)$ 의 차이가 크지 않음을 가정한다. 만약, 입력 $X$ 를 <code class="language-plaintext highlighter-rouge">10.0</code> , $H(x)$ 를 <code class="language-plaintext highlighter-rouge">10.4</code> 라고 해보자. 그럼 <code class="language-plaintext highlighter-rouge">Residual Connection</code> 을 사용하는 모델은 <code class="language-plaintext highlighter-rouge">0.4</code>에 대해서만 학습을 하면 된다. 한편 이것을 사용하지 않는 모델은 0에서부터 시작해 무려 <code class="language-plaintext highlighter-rouge">10.4</code>를 학습해야 한다. 어떤 모델이 학습하기 쉬울까?? 당연히 전자일 것이다. 이렇게 모델이 이상적인 값과 입력의 차이만 학습하면 되기 때문에 이것을 <code class="language-plaintext highlighter-rouge">잔차 학습</code>이라고 부르는 것이다.</p>

<p align="center">
<img src="/assets/images/transformer/layernorm.png" alt="Layernorm vs Batchnorm" class="align-center image-caption" width="50%&quot;, height=&quot;50%" />
<strong><em><a href="https://paperswithcode.com/method/layer-normalization">Layernorm vs Batchnorm</a></em></strong>
</p>

<p><code class="language-plaintext highlighter-rouge">Batchnorm</code>은 <code class="language-plaintext highlighter-rouge">“Mini-Batch”</code> 단위를 <code class="language-plaintext highlighter-rouge">Channel(Feature)</code>별로 평균과 표준편차를 구한다면, <code class="language-plaintext highlighter-rouge">Layernorm</code>은  <code class="language-plaintext highlighter-rouge">Channel(Feature)</code> 단위를 <code class="language-plaintext highlighter-rouge">개별 인스턴스</code>별로 평균과 표준편차를 구하여 정규화하는 방식이다.</p>

<p>예를 들어 배치로 4개의 문장을 은닉층의 사이즈가 <code class="language-plaintext highlighter-rouge">512</code>인 모델에 입력해줬다고 생각해보자. 그럼 4개의 문장은 각각 <code class="language-plaintext highlighter-rouge">512</code>개의 원소를 갖게 되는데, 이것에 대한 평균과 표준편차를 구한다는 것이다. 한 개의 문장당 평균과 표준편차를 1개씩 구해서, 4개의 문장이니까 총 8개가 나오겠다.</p>

<p>그렇다면 왜 <code class="language-plaintext highlighter-rouge">Transformer</code>는 <code class="language-plaintext highlighter-rouge">Layernorm</code>을 사용했을까?? 자연어 처리는 배치마다 시퀀스의 길이가 고정되어 있지 않아 패딩이나 절삭을 수행한다. 절삭보다는 패딩이 문제가 된다. 패딩은 일반적으로 문장의 끝부분에 해준다. 여기서 <code class="language-plaintext highlighter-rouge">Batchnorm</code>을 사용하면 끝쪽에 위치한 다른 시퀀스에 속한 정상적인 토큰들은 패딩에 의해 값이 왜곡될 가능성이 있다. 그래서 <code class="language-plaintext highlighter-rouge">Batchnorm</code> 대신 <code class="language-plaintext highlighter-rouge">Layernorm</code>을 사용한다. 또한 <code class="language-plaintext highlighter-rouge">Batchnorm</code> 은 배치 크기에 종속적이라서 테스트 상황에서는 그대로 사용할 수 없다. 따라서 배치 사이즈에 독립적인 <code class="language-plaintext highlighter-rouge">Layernorm</code>을 사용하기도 한다.</p>

<p>한편 이러한 정규화를 왜 사용하는지 궁금하시다면 다른 포스트에 정리를 해뒀으니 참고하시길 바란다. <strong>간단하게만 언급하면,</strong> <code class="language-plaintext highlighter-rouge">모델의 비선형성</code><strong>과 그라디언트 크기 사이의 최적의</strong> <code class="language-plaintext highlighter-rouge">Trade-Off</code><strong>를 인간이 아닌 모델보고 찾게 만드는게 목적이라 볼 수 있다.</strong></p>

<h4 id="encoderlayer"><strong><code class="language-plaintext highlighter-rouge">📘 EncoderLayer</code></strong></h4>
<p>이제 <code class="language-plaintext highlighter-rouge">Single Encoder Block</code>을 정의하기에 필요한 모든 재료를 살펴봤다. 지금까지의 내용을 종합해 한 개의 인코더 블럭을 만들어보자.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Pytorch Implementation of Single Encoder Block
</span>
<span class="k">class</span> <span class="nc">EncoderLayer</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="s">"""
    Class for encoder model module in Transformer
    In this class, we stack each encoder_model module (Multi-Head Attention, Residual-Connection, LayerNorm, FFN)
    We apply pre-layernorm, which is different from original paper
    In common sense, pre-layernorm are more effective &amp; stable than post-layernorm
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim_model</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">512</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span> <span class="n">dim_ffn</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2048</span><span class="p">,</span> <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">EncoderLayer</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">self_attention</span> <span class="o">=</span> <span class="n">MultiHeadAttention</span><span class="p">(</span>
            <span class="n">dim_model</span><span class="p">,</span>
            <span class="n">num_heads</span><span class="p">,</span>
            <span class="nb">int</span><span class="p">(</span><span class="n">dim_model</span> <span class="o">/</span> <span class="n">num_heads</span><span class="p">),</span>
            <span class="n">dropout</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">layer_norm1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">dim_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">layer_norm2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">dim_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">dropout</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">ffn</span> <span class="o">=</span> <span class="n">FeedForward</span><span class="p">(</span>
            <span class="n">dim_model</span><span class="p">,</span>
            <span class="n">dim_ffn</span><span class="p">,</span>
            <span class="n">dropout</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">mask</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="n">ln_x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">layer_norm1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">residual_x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">dropout</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">self_attention</span><span class="p">(</span><span class="n">ln_x</span><span class="p">,</span> <span class="n">mask</span><span class="p">))</span> <span class="o">+</span> <span class="n">x</span>

        <span class="n">ln_x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">layer_norm2</span><span class="p">(</span><span class="n">residual_x</span><span class="p">)</span>
        <span class="n">fx</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">ffn</span><span class="p">(</span><span class="n">ln_x</span><span class="p">)</span> <span class="o">+</span> <span class="n">residual_x</span>
        <span class="k">return</span> <span class="n">fx</span>
</code></pre></div></div>

<p>지금까지의 내용을 객체 하나에 모아둔거라 특별히 설명이 필요한 부분은 없지만, 필자가 <code class="language-plaintext highlighter-rouge">add &amp; norm</code>을 언제 사용했는지 주목해보자. 원본 논문은 <code class="language-plaintext highlighter-rouge">Multi-Head Attention</code>과 <code class="language-plaintext highlighter-rouge">FeedForward</code> <code class="language-plaintext highlighter-rouge">Layer</code>를 통과한 이후에 <code class="language-plaintext highlighter-rouge">add &amp; norm</code>을 하는 <code class="language-plaintext highlighter-rouge">post-layernorm</code> 방식을 적용했다. 하지만 필자는 두 레이어 통과 이전에 미리 <code class="language-plaintext highlighter-rouge">add &amp; norm</code> 을 해주는 <code class="language-plaintext highlighter-rouge">pre-layernorm</code> 방식을 채택했다.</p>

<p align="center">
<img src="/assets/images/transformer/prelayernorm.png" alt="pre-layernorm vs post-layernorm" class="align-center image-caption" width="50%&quot;, height=&quot;50%" />
<strong><em><a href="https://github.com/rickiepark/nlp-with-transformers/blob/main/images/chapter03_layer-norm.png">pre-layernorm vs post-layernorm</a></em></strong>
</p>

<p>최근 <code class="language-plaintext highlighter-rouge">Transformer</code>류의 모델에 <code class="language-plaintext highlighter-rouge">pre-layernorm</code>을 적용하는 것이 좀 더 안정적이고 효율적인 학습을 유도할 수 있다고 실험을 통해 밝혀지고 있다. <code class="language-plaintext highlighter-rouge">pre-layernorm</code> 을 사용하면 별다른 <code class="language-plaintext highlighter-rouge">Gradient Explode</code> 현상이 현저히 줄어들어 복잡한 스케줄러(<code class="language-plaintext highlighter-rouge">warmup</code> 기능이 있는 스케줄러)를 사용할 필요가 없어진다고 하니 참고하자.</p>

<p>이렇게 구현한 <code class="language-plaintext highlighter-rouge">Single Encoder Block</code>을 이제 N개 쌓기만 하면 드디어 인코더를 완성할 수 있게 된다.</p>

<h4 id="-encoder"><strong><code class="language-plaintext highlighter-rouge">📚 Encoder</code></strong></h4>

<p>드디어 대망의 <code class="language-plaintext highlighter-rouge">Encoder</code> 객체 구현을 살펴볼 시간이다.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Pytorch Implementation of Encoder(Stacked N EncoderLayer)
</span>
<span class="k">class</span> <span class="nc">Encoder</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="s">"""
    In this class, encode input sequence and then we stack N EncoderLayer
    First, we define "positional embedding" and then add to input embedding for making "word embedding"
    Second, forward "word embedding" to N EncoderLayer and then get output embedding
    In official paper, they use positional encoding, which is base on sinusoidal function(fixed, not learnable)
    But we use "positional embedding" which is learnable from training
    Args:
        max_seq: maximum sequence length, default 512 from official paper
        N: number of EncoderLayer, default 6 for base model
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">max_seq</span><span class="p">:</span> <span class="mi">512</span><span class="p">,</span> <span class="n">N</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">6</span><span class="p">,</span> <span class="n">dim_model</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">512</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span> <span class="n">dim_ffn</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2048</span><span class="p">,</span> <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Encoder</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">max_seq</span> <span class="o">=</span> <span class="n">max_seq</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">scale</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">dim_model</span><span class="p">))</span>  <span class="c1"># scale factor for input embedding from official paper
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">positional_embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">max_seq</span><span class="p">,</span> <span class="n">dim_model</span><span class="p">)</span>  <span class="c1"># add 1 for cls token
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">num_layers</span> <span class="o">=</span> <span class="n">N</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span> <span class="o">=</span> <span class="n">dim_model</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dim_ffn</span> <span class="o">=</span> <span class="n">dim_ffn</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">dropout</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">encoder_layers</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">ModuleList</span><span class="p">(</span>
            <span class="p">[</span><span class="n">EncoderLayer</span><span class="p">(</span><span class="n">dim_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">dim_ffn</span><span class="p">,</span> <span class="n">dropout</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">num_layers</span><span class="p">)]</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">layer_norm</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">dim_model</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">mask</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
        <span class="s">"""
        inputs: embedding from input sequence, shape =&gt; [BS, SEQ_LEN, DIM_MODEL]
        mask: mask for Encoder padded token for speeding up to calculate attention score
        """</span>
        <span class="n">layer_output</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">pos_x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">max_seq</span><span class="p">).</span><span class="n">repeat</span><span class="p">(</span><span class="n">inputs</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]).</span><span class="n">to</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">dropout</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">scale</span> <span class="o">*</span> <span class="n">inputs</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">positional_embedding</span><span class="p">(</span><span class="n">pos_x</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">encoder_layers</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span>
            <span class="n">layer_output</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">encoded_x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">layer_norm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># from official paper &amp; code by Google Research
</span>        <span class="n">layer_output</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">stack</span><span class="p">(</span><span class="n">layer_output</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">).</span><span class="n">to</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>  <span class="c1"># For Weighted Layer Pool: [N, BS, SEQ_LEN, DIM]
</span>        <span class="k">return</span> <span class="n">encoded_x</span><span class="p">,</span> <span class="n">layer_output</span>
</code></pre></div></div>

<p>역시 지금까지 내용을 종합한 것뿐이라서 크게 특이한 내용은 없고, 구현상 놓치기 쉬운 부분만 알고 넘어가면 된다. <code class="language-plaintext highlighter-rouge">forward</code> 메서드의 변수 <code class="language-plaintext highlighter-rouge">x</code>를 초기화하는 코드 라인을 주목해보자. 이것이 바로 <code class="language-plaintext highlighter-rouge">Input Embedding</code>과 <code class="language-plaintext highlighter-rouge">Position Embedding</code>을 더하는(행렬 합) 연산을 구현한 것이다. 이 때 놓치기 쉬운 부분이 바로 <code class="language-plaintext highlighter-rouge">Input Embedding</code>에 <code class="language-plaintext highlighter-rouge">scale factor</code>를 곱해준다는 것이다. 저자의 주장에 따르면 <code class="language-plaintext highlighter-rouge">Input Embedding</code>에만 <code class="language-plaintext highlighter-rouge">scale factor</code>를 사용하는 것이 안정적인 학습에 도움이 된다고 하니 참고하자.</p>

<p>한편, 마지막 인코더 블럭에서 나온 임베딩을 다시 한 번 <code class="language-plaintext highlighter-rouge">layernorm</code>에 통과하도록 구현했다. 이 부분도 원본 논문에 있는 내용은 아니고  <code class="language-plaintext highlighter-rouge">ViT</code>의 논문 내용을 참고해 추가했다.</p>

<h4 id="decoderlayer"><strong><code class="language-plaintext highlighter-rouge">📘 DecoderLayer</code></strong></h4>

<p>이번에는 디코더에 사용된 블럭의 동작 방식과 의미 그리고 구현까지 알아보자. 사실 디코더도 지금까지 공부한 내용과 크게 다른게 없다. 다만 인코더와는 목적이 다르기 때문에 발생하는 미세한 동작의 차이에 주목해보자. 먼저 <code class="language-plaintext highlighter-rouge">Single Decoder Block</code>은 <code class="language-plaintext highlighter-rouge">Single Encoder Block</code>과 다르게 <code class="language-plaintext highlighter-rouge">Self-Attention</code>을 두 번 수행한다. 지겹겠지만 다시 한 번 Transformer의 목적을 상기시켜보자. 바로 대상 언어를 타겟 언어로 잘 번역하는 것이었다. 일단 인코더를 통해 대상 언어는 잘 이해하게 되었다. 그럼 이제 타겟 언어도 잘 이해해야하지 않은가?? 그래서 타겟 언어를 이해하기 위해 <code class="language-plaintext highlighter-rouge">Self-Attention</code>을 한 번, 그리고 대상 언어를 타겟 언어로 번역하기 위해 <code class="language-plaintext highlighter-rouge">Self-Attention</code>을 한 번, 총 2번 수행하는 것이다. 첫번째  <code class="language-plaintext highlighter-rouge">Self-Attention</code> 을 <code class="language-plaintext highlighter-rouge">Masked Multi-Head Attention</code>, 두번째를 <code class="language-plaintext highlighter-rouge">Encoder-Decoder Multi-Head Attention</code>이라고 부른다.</p>

<p><strong><code class="language-plaintext highlighter-rouge">🎭 Masked Multi-Head Attention</code></strong><br />
인코더의 <code class="language-plaintext highlighter-rouge">Multi-Head Attention와</code> 행렬 $Q,K,V$ 의 출처가 다르다. 디코더는 출처가 타겟 언어인 <code class="language-plaintext highlighter-rouge">linear projection matrix</code>를 사용한다. 또한 인코더와 다르게 개별 시점에 맞는 마스킹 행렬이 필요하다. 디코더의 과업은 결국 대상 언어를 잘 이해하고 그것에 가장 잘 들어맞는 타겟 언어 시퀀스를 <code class="language-plaintext highlighter-rouge">generate</code>하는 것이다. 즉, <code class="language-plaintext highlighter-rouge">Next Token Prediction</code>을 통해 시퀀스를 만들어내야 한다. 그런데 현재 시점에서 미래 시점에 디코더가 예측해야할 토큰을 미리 알고 있으면 그것을 예측이라고 할 수 있을까?? 디코더가 현재 시점의 토큰을 예측하는데 미래 시점의 <code class="language-plaintext highlighter-rouge">Context</code>를 반영하지 못하도록 막기 위해 미리 마스킹 행렬을 정의해 <code class="language-plaintext highlighter-rouge">Word_Embedding</code>에 적용해준다. 이렇게 마스킹이 적용된 임베딩 행렬을 가지고 <code class="language-plaintext highlighter-rouge">linear projection &amp; self-attention</code>을 수행하기 때문에 이름 앞에 <code class="language-plaintext highlighter-rouge">masked</code>를 붙이게 되었다.</p>

<p align="center">
<img src="/assets/images/transformer/decoder_mask.png" alt="Decoder Language Modeling Mask" class="align-center image-caption" width="50%&quot;, height=&quot;50%" />
<strong><em><a href="https://paul-hyun.github.io/transformer-02/">Decoder Language Modeling Mask</a></em></strong>
</p>

<p>위 그림은 마스킹을 적용한 <code class="language-plaintext highlighter-rouge">Word_Embedding</code>의 모습이다. 첫 번째 시점에서 모델은 자기 자신을 제외한 나머지 <code class="language-plaintext highlighter-rouge">Context</code>를 예측에 활용할 수 없다. 그래서 이하 나머지 토큰을 전부 마스킹 처리해줬다. 두번째 시점에서는 직전 시점인 첫번째 토큰과 자기 자신만 참고할 수 있다. 한편, 이렇게 직전 <code class="language-plaintext highlighter-rouge">Context</code>만 가지고 현재 토큰을 추론하는 것을 <code class="language-plaintext highlighter-rouge">Language Modeling</code>이라 부른다. 그리고 마찬가지로 디코더 역시 시퀀스에 패딩 처리를 해주기 때문에 인코더와 동일한 원리로 만든 <code class="language-plaintext highlighter-rouge">decoder padding mask</code> 역시 필요하다.</p>

<p>마스킹 행렬 구현은 최상위 객체인 <code class="language-plaintext highlighter-rouge">Transformer</code>의 내부 메서드로 만들었으니, 그 때 자세히 설명하겠다. 이하 나머지 디테일은 인코더의 것과 동일하다.</p>

<p><strong><code class="language-plaintext highlighter-rouge">🪢 Encoder-Decoder Multi-Head Attention</code></strong><br />
인코더를 통해 이해한 대상 언어 시퀀스와 바로 직전 <code class="language-plaintext highlighter-rouge">Self-Attention</code>을 통해 이해한 타겟 언어 시퀀스를 서로 대조하는 레이어다. 우리의 지금 목적은 <code class="language-plaintext highlighter-rouge">타겟 언어</code>와 가장 유사한 <code class="language-plaintext highlighter-rouge">대상 언어</code>를 찾아 문장을 완성하는 것이다. 따라서 어텐션 계산에 사용될 행렬 $Q$ 의 출처는 직전 레이어인 <code class="language-plaintext highlighter-rouge">Masked Multi-Head Attention</code> 의 반환값을 사용하고, 행렬 $K,V$ 는 인코더의 최종 반환값을 사용한다.</p>

<p>한편, 여기 레이어에는 마스킹 행렬이 세 종류나 필요하다. 그 이유는 서로 출처가 다른 두가지 행렬을 계산에 사용하기 때문이다. 지금은 여전히 디코더의 역할을 수행하는 것이기 때문에 직전 레이어에서 사용한 2개의 마스킹 행렬이 그대로 필요하다. 그리고 인코더에서 넘어온 값을 사용한다는 것은 인코더의 패딩 역시 처리가 필요하다는 의미다. 따라서 <code class="language-plaintext highlighter-rouge">lm_mask</code>, <code class="language-plaintext highlighter-rouge">dec_pad_mask</code>, <code class="language-plaintext highlighter-rouge">enc_pad_mask</code>가 필요하다. 역시 마스킹 구현은 최상위 객체 설명 때 함께 살펴보겠다.</p>

<p><strong><code class="language-plaintext highlighter-rouge">👩‍💻 Implementation</code></strong><br />
이제 <code class="language-plaintext highlighter-rouge">Single Decoder Block</code>의 구현을 살펴보자. 역시 파이토치로 구현했다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Pytorch Implementation of Single Decoder Block
</span>
<span class="k">class</span> <span class="nc">DecoderLayer</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="s">"""
    Class for decoder model module in Transformer
    In this class, we stack each decoder_model module (Masked Multi-Head Attention, Residual-Connection, LayerNorm, FFN)
    We apply pre-layernorm, which is different from original paper
    References:
        https://arxiv.org/abs/1706.03762
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim_model</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">512</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span> <span class="n">dim_ffn</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2048</span><span class="p">,</span> <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">DecoderLayer</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">masked_attention</span> <span class="o">=</span> <span class="n">MultiHeadAttention</span><span class="p">(</span>
            <span class="n">dim_model</span><span class="p">,</span>
            <span class="n">num_heads</span><span class="p">,</span>
            <span class="nb">int</span><span class="p">(</span><span class="n">dim_model</span> <span class="o">/</span> <span class="n">num_heads</span><span class="p">),</span>
            <span class="n">dropout</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">enc_dec_attention</span> <span class="o">=</span> <span class="n">MultiHeadAttention</span><span class="p">(</span>
            <span class="n">dim_model</span><span class="p">,</span>
            <span class="n">num_heads</span><span class="p">,</span>
            <span class="nb">int</span><span class="p">(</span><span class="n">dim_model</span> <span class="o">/</span> <span class="n">num_heads</span><span class="p">),</span>
            <span class="n">dropout</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">layer_norm1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">dim_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">layer_norm2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">dim_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">layer_norm3</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">dim_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">dropout</span><span class="p">)</span>  <span class="c1"># dropout is not learnable layer
</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">ffn</span> <span class="o">=</span> <span class="n">FeedForward</span><span class="p">(</span>
            <span class="n">dim_model</span><span class="p">,</span>
            <span class="n">dim_ffn</span><span class="p">,</span>
            <span class="n">dropout</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">dec_mask</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">enc_dec_mask</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">enc_output</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="n">ln_x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">layer_norm1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">residual_x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">dropout</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">masked_attention</span><span class="p">(</span><span class="n">ln_x</span><span class="p">,</span> <span class="n">dec_mask</span><span class="p">))</span> <span class="o">+</span> <span class="n">x</span>

        <span class="n">ln_x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">layer_norm2</span><span class="p">(</span><span class="n">residual_x</span><span class="p">)</span>
        <span class="n">residual_x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">dropout</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">enc_dec_attention</span><span class="p">(</span><span class="n">ln_x</span><span class="p">,</span> <span class="n">enc_dec_mask</span><span class="p">,</span> <span class="n">enc_output</span><span class="p">))</span> <span class="o">+</span> <span class="n">x</span>  <span class="c1"># for enc_dec self-attention
</span>
        <span class="n">ln_x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">layer_norm3</span><span class="p">(</span><span class="n">residual_x</span><span class="p">)</span>
        <span class="n">fx</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">ffn</span><span class="p">(</span><span class="n">ln_x</span><span class="p">)</span> <span class="o">+</span> <span class="n">residual_x</span>
        <span class="k">return</span> <span class="n">fx</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">Self-Attention</code> 레이어가 인코더보다 하나 더 추가되어 <code class="language-plaintext highlighter-rouge">add &amp; norm</code> 을 총 3번 해줘야 한다는 것을 제외하고는 크게 구현상의 특이점은 없다. 그저 지금까지 살펴본 블럭을 요리조리 다시 쌓으면 된다.</p>

<h4 id="decoder"><strong><code class="language-plaintext highlighter-rouge">📚 Decoder</code></strong></h4>

<p><code class="language-plaintext highlighter-rouge">Single Decoder Block</code>을 <code class="language-plaintext highlighter-rouge">N</code>개 쌓고 전체 디코더 동작을 수행하는 <code class="language-plaintext highlighter-rouge">Decoder</code> 객체의 구현을 알아보자.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Pytorch Implementation of Decoder(N Stacked Single Decoder Block)
</span>
<span class="k">class</span> <span class="nc">Decoder</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="s">"""
    In this class, decode encoded embedding from encoder by outputs (target language, Decoder's Input Sequence)
    First, we define "positional embedding" for Decoder's Input Sequence,
    and then add them to Decoder's Input Sequence for making "decoder word embedding"
    Second, forward "decoder word embedding" to N DecoderLayer and then pass to linear &amp; softmax for OutPut Probability
    Args:
        vocab_size: size of vocabulary for output probability
        max_seq: maximum sequence length, default 512 from official paper
        N: number of EncoderLayer, default 6 for base model
    References:
        https://arxiv.org/abs/1706.03762
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">vocab_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">max_seq</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">512</span><span class="p">,</span>
        <span class="n">N</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">6</span><span class="p">,</span>
        <span class="n">dim_model</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">512</span><span class="p">,</span>
        <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span>
        <span class="n">dim_ffn</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2048</span><span class="p">,</span>
        <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Decoder</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">max_seq</span> <span class="o">=</span> <span class="n">max_seq</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">scale</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">dim_model</span><span class="p">))</span>  <span class="c1"># scale factor for input embedding from official paper
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">positional_embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">max_seq</span><span class="p">,</span> <span class="n">dim_model</span><span class="p">)</span>  <span class="c1"># add 1 for cls token
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">num_layers</span> <span class="o">=</span> <span class="n">N</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span> <span class="o">=</span> <span class="n">dim_model</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dim_ffn</span> <span class="o">=</span> <span class="n">dim_ffn</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">dropout</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">decoder_layers</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">ModuleList</span><span class="p">(</span>
            <span class="p">[</span><span class="n">DecoderLayer</span><span class="p">(</span><span class="n">dim_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">dim_ffn</span><span class="p">,</span> <span class="n">dropout</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">num_layers</span><span class="p">)]</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">layer_norm</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">dim_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">fc_out</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">dim_model</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">)</span>  <span class="c1"># In Pytorch, nn.CrossEntropyLoss already has softmax function
</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">dec_mask</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">enc_dec_mask</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">enc_output</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
        <span class="s">"""
        inputs: embedding from input sequence, shape =&gt; [BS, SEQ_LEN, DIM_MODEL]
        dec_mask: mask for Decoder padded token for Language Modeling
        enc_dec_mask: mask for Encoder-Decoder Self-Attention, from encoder padded token
        """</span>
        <span class="n">layer_output</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">pos_x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">max_seq</span><span class="p">).</span><span class="n">repeat</span><span class="p">(</span><span class="n">inputs</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]).</span><span class="n">to</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">dropout</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">scale</span> <span class="o">*</span> <span class="n">inputs</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">positional_embedding</span><span class="p">(</span><span class="n">pos_x</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">decoder_layers</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dec_mask</span><span class="p">,</span> <span class="n">enc_dec_mask</span><span class="p">,</span> <span class="n">enc_output</span><span class="p">)</span>
            <span class="n">layer_output</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">decoded_x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">fc_out</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">layer_norm</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>  <span class="c1"># Because of pre-layernorm
</span>        <span class="n">layer_output</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">stack</span><span class="p">(</span><span class="n">layer_output</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">).</span><span class="n">to</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>  <span class="c1"># For Weighted Layer Pool: [N, BS, SEQ_LEN, DIM]
</span>        <span class="k">return</span> <span class="n">decoded_x</span><span class="p">,</span> <span class="n">layer_output</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">Encoder</code> 객체와 모든 부분이 동일하다. 디테일한 설정만 디코더에 맞게 변경되었을 뿐이다. <code class="language-plaintext highlighter-rouge">self.fc_out</code> 에 주목해보자. 디코더는 현재 시점에 가장 적합한 토큰을 예측해야 하기 때문에 디코더의 출력부분에 로짓 계산을 위한 레이어가 필요하다. 그 역할을 하는 것이 바로 <code class="language-plaintext highlighter-rouge">self.fc_out</code>이다. 한편, <code class="language-plaintext highlighter-rouge">self.fc_out</code>의 출력 차원이 <code class="language-plaintext highlighter-rouge">vocab_size</code>으로 되어있는데, 디코더는 디코더가 가진 전체 <code class="language-plaintext highlighter-rouge">vocab</code> 을 현재 시점에 적합한 토큰 후보군으로 사용하기 때문이다.</p>

<h4 id="-transformer"><strong><code class="language-plaintext highlighter-rouge">🦾 Transformer</code></strong></h4>
<p>이제 대망의 마지막… 모델의 가장 최상위 객체인 <code class="language-plaintext highlighter-rouge">Transformer</code>에 대해서 살펴보자. 객체의 동작은 정리하면 다음과 같다.</p>

<ul>
  <li><strong>1) Make <code class="language-plaintext highlighter-rouge">Input Embedding</code> for Encoder &amp; Decoder respectively, Init <code class="language-plaintext highlighter-rouge">Encoder &amp; Decoder</code> Class</strong></li>
  <li><strong>2) Make 3 types of Masking: <code class="language-plaintext highlighter-rouge">Encoder Padding Mask</code>, <code class="language-plaintext highlighter-rouge">Decoder LM &amp; Padding Mask</code>, <code class="language-plaintext highlighter-rouge">Encoder-Decoder Mask</code></strong></li>
  <li><strong>3) Return <code class="language-plaintext highlighter-rouge">Output</code> from Encoder &amp; Decoder</strong></li>
</ul>

<p>특히 계속 미뤄왔던 마스킹 구현에 대해서 살펴보자. 나머지는 이미 앞에서 많이 설명했으니까 넘어가도록 하겠다. 일단 먼저 코드를 읽어보자. 추가로 <code class="language-plaintext highlighter-rouge">Input Embedding</code> 구현은 사용자의 <code class="language-plaintext highlighter-rouge">vocab</code> 구축 방식에 따라 달라진다. 필자의 경우 대상 언어와 타겟 언어의 <code class="language-plaintext highlighter-rouge">vocab</code>을 분리해 사용하는 것을 가정하고 코드를 만들어 임베딩 레이어를 따로 따로 구현해줬다. <code class="language-plaintext highlighter-rouge">vocab</code>을 통합으로 구축하시는 분이라면 하나만 정의해도 상관없다. 대신 나중에 디코더의 로짓값 계산을 위해 개별 언어의 토큰 사이즈는 알고 있어야 할 것이다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Pytorch Implementation of Transformer
</span>
<span class="k">class</span> <span class="nc">Transformer</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="s">"""
    Main class for Pure Transformer, Pytorch implementation
    There are two Masking Method for padding token
        1) Row &amp; Column masking
        2) Column masking only at forward time, Row masking at calculating loss time
    second method is more efficient than first method, first method is complex &amp; difficult to implement
    Args:
        enc_vocab_size: size of vocabulary for Encoder Input Sequence
        dec_vocab_size: size of vocabulary for Decoder Input Sequence
        max_seq: maximum sequence length, default 512 from official paper
        enc_N: number of EncoderLayer, default 6 for base model
        dec_N: number of DecoderLayer, default 6 for base model
    Reference:
        https://arxiv.org/abs/1706.03762
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">enc_vocab_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">dec_vocab_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">max_seq</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">512</span><span class="p">,</span>
        <span class="n">enc_N</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">6</span><span class="p">,</span>
        <span class="n">dec_N</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">6</span><span class="p">,</span>
        <span class="n">dim_model</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">512</span><span class="p">,</span>
        <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span>
        <span class="n">dim_ffn</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2048</span><span class="p">,</span>
        <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Transformer</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">enc_input_embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">enc_vocab_size</span><span class="p">,</span> <span class="n">dim_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dec_input_embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">dec_vocab_size</span><span class="p">,</span> <span class="n">dim_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">Encoder</span><span class="p">(</span><span class="n">max_seq</span><span class="p">,</span> <span class="n">enc_N</span><span class="p">,</span> <span class="n">dim_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">dim_ffn</span><span class="p">,</span> <span class="n">dropout</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="n">Decoder</span><span class="p">(</span><span class="n">dec_vocab_size</span><span class="p">,</span> <span class="n">max_seq</span><span class="p">,</span> <span class="n">dec_N</span><span class="p">,</span> <span class="n">dim_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">dim_ffn</span><span class="p">,</span> <span class="n">dropout</span><span class="p">)</span>

    <span class="o">@</span><span class="nb">staticmethod</span>
    <span class="k">def</span> <span class="nf">enc_masking</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">enc_pad_index</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="s">""" make masking matrix for Encoder Padding Token """</span>
        <span class="n">enc_mask</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span> <span class="o">!=</span> <span class="n">enc_pad_index</span><span class="p">).</span><span class="nb">int</span><span class="p">().</span><span class="n">repeat</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]).</span><span class="n">view</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
        <span class="k">return</span> <span class="n">enc_mask</span>

    <span class="o">@</span><span class="nb">staticmethod</span>
    <span class="k">def</span> <span class="nf">dec_masking</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">dec_pad_index</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="s">""" make masking matrix for Decoder Masked Multi-Head Self-Attention """</span>
        <span class="n">pad_mask</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span> <span class="o">!=</span> <span class="n">dec_pad_index</span><span class="p">).</span><span class="nb">int</span><span class="p">().</span><span class="n">repeat</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]).</span><span class="n">view</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
        <span class="n">lm_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tril</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">ones</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>
        <span class="n">dec_mask</span> <span class="o">=</span> <span class="n">pad_mask</span> <span class="o">*</span> <span class="n">lm_mask</span>
        <span class="k">return</span> <span class="n">dec_mask</span>

    <span class="o">@</span><span class="nb">staticmethod</span>
    <span class="k">def</span> <span class="nf">enc_dec_masking</span><span class="p">(</span><span class="n">enc_x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">dec_x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">enc_pad_index</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="s">""" make masking matrix for Encoder-Decoder Multi-Head Self-Attention in Decoder """</span>
        <span class="n">enc_dec_mask</span> <span class="o">=</span> <span class="p">(</span><span class="n">enc_x</span> <span class="o">!=</span> <span class="n">enc_pad_index</span><span class="p">).</span><span class="nb">int</span><span class="p">().</span><span class="n">repeat</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">dec_x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]).</span><span class="n">view</span><span class="p">(</span>
            <span class="n">enc_x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">dec_x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">enc_x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">enc_dec_mask</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">enc_inputs</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">dec_inputs</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">enc_pad_index</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">dec_pad_index</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
        <span class="n">enc_mask</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">enc_masking</span><span class="p">(</span><span class="n">enc_inputs</span><span class="p">,</span> <span class="n">enc_pad_index</span><span class="p">)</span>  <span class="c1"># enc_x.shape[1] == encoder input sequence length
</span>        <span class="n">dec_mask</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">dec_masking</span><span class="p">(</span><span class="n">dec_inputs</span><span class="p">,</span> <span class="n">dec_pad_index</span><span class="p">)</span>  <span class="c1"># dec_x.shape[1] == decoder input sequence length
</span>        <span class="n">enc_dec_mask</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">enc_dec_masking</span><span class="p">(</span><span class="n">enc_inputs</span><span class="p">,</span> <span class="n">dec_inputs</span><span class="p">,</span> <span class="n">enc_pad_index</span><span class="p">)</span>

        <span class="n">enc_x</span><span class="p">,</span> <span class="n">dec_x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">enc_input_embedding</span><span class="p">(</span><span class="n">enc_inputs</span><span class="p">),</span> <span class="bp">self</span><span class="p">.</span><span class="n">dec_input_embedding</span><span class="p">(</span><span class="n">dec_inputs</span><span class="p">)</span>

        <span class="n">enc_output</span><span class="p">,</span> <span class="n">enc_layer_output</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">enc_x</span><span class="p">,</span> <span class="n">enc_mask</span><span class="p">)</span>
        <span class="n">dec_output</span><span class="p">,</span> <span class="n">dec_layer_output</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">decoder</span><span class="p">(</span><span class="n">dec_x</span><span class="p">,</span> <span class="n">dec_mask</span><span class="p">,</span> <span class="n">enc_dec_mask</span><span class="p">,</span> <span class="n">enc_output</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">enc_output</span><span class="p">,</span> <span class="n">dec_output</span><span class="p">,</span> <span class="n">enc_layer_output</span><span class="p">,</span> <span class="n">dec_layer_output</span>
</code></pre></div></div>

<p>마스킹의 필요성이나 동작 방식은 이미 위에서 모두 설명했기 때문에 구현상 특징만 설명하려한다. 세가지 마스킹 모두 공통적으로 구현 코드 라인에 <code class="language-plaintext highlighter-rouge">.int()</code> 가 들어가 있다. 그 이유는 $\frac{Q•K^T}{\sqrt{d_h}}$에 마스킹을 적용할 때 <code class="language-plaintext highlighter-rouge">torch.masked_fill</code> 메서드를 사용하기 때문이다. 무슨 이유 때문인지는 모르겠으나 <code class="language-plaintext highlighter-rouge">torch.masked_fill</code>의 경우 마스킹 조건으로 <code class="language-plaintext highlighter-rouge">boolean</code>을 전달하면 마스킹이 제대로 구현되지 않는 현상이 있었다. 한편, 정수값으로 조건을 구현하면 의도한대로 구현이 되는 것을 확인했다. 그래서 패딩에 해당되는 토큰이 위치한 곳의 원소값을 정수형 <code class="language-plaintext highlighter-rouge">Binary</code> 로 만들어주기 위해 <code class="language-plaintext highlighter-rouge">int()</code> 를 사용한 것이다.</p>

<p><strong><code class="language-plaintext highlighter-rouge">🎭 Decoder Mask</code></strong><br />
디코더는 총 2가지의 마스킹이 필요하다고 언급했었다. <code class="language-plaintext highlighter-rouge">pad_mask</code>의 경우는 인코더의 것과 동일한 원리를 사용하기 때문에 설명을 생략하겠다. <code class="language-plaintext highlighter-rouge">lm_mask</code> 의 경우는 <code class="language-plaintext highlighter-rouge">torch.tril</code>을 이용해 하삼각행렬 형태로 마스킹 행렬 정의가 쉽게 가능하다.<br />
한편, 2개의 마스킹을 동시에 디코더 객체에 넘기는 것은 매우 비효율적이다. 따라서 <code class="language-plaintext highlighter-rouge">pad_mask</code> 와 <code class="language-plaintext highlighter-rouge">lm_mask</code>의 합집합에 해당하는 행렬을 만들어 최종 디코더의 마스킹으로 전달한다.</p>

<p><strong><code class="language-plaintext highlighter-rouge">🙌 Encoder-Decoder Mask</code></strong><br />
이번 경우는 마스킹의 행방향 차원은 디코더 입력값의 시퀀스 길이, 열방향 차원은 인코더 입력값의 시퀀스 길이로 설정해야 한다. 그 이유는 다른 <code class="language-plaintext highlighter-rouge">Self-Attention</code> 레이어와 다르게 서로 다른 출처를 통해 만든 행렬을 사용하기 때문에 $\frac{Q•K^T}{\sqrt{d_h}}$의 모양이 정사각행렬이 아닐 수도 있다. 예를 들어 한국어 문장을 영어로 바꾸는 경우를 생각해보자. 같은 뜻이 담긴 문장이라고 해서 두 문장의 길이가 같은가?? 아니다. 서로 다른 언어라면 거의 대부분의 경우 길이가 다를 것이다. 따라서 $\frac{Q•K^T}{\sqrt{d_h}}$의 행방향은 디코더의 시퀀스 길이에 따르고 열방향은 인코더의 시퀀스 길이에 따르도록 마스킹 역시 구현해줘야 한다.<br />
그리고 이번 마스킹을 만드는 목적이 인코더의 패딩을 마스킹 처리해주기 위함이기 때문에 <code class="language-plaintext highlighter-rouge">enc_pad_index</code> 매개변수에는 인코더 <code class="language-plaintext highlighter-rouge">vocab</code>에서 정의한 <code class="language-plaintext highlighter-rouge">pad_token_ID</code>를 전달하면 된다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># scaled_dot_product_attention의 일부
</span>
<span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
		<span class="n">attention_matrix</span> <span class="o">=</span> <span class="n">attention_matrix</span><span class="p">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">mask</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="s">'-inf'</span><span class="p">))</span>
</code></pre></div></div>

<p>이렇게 구현된 마스킹은 <code class="language-plaintext highlighter-rouge">scaled_dot_product_attention</code> 메서드에 구현된 조건문을 통해 마스킹 대상을 -∞으로 변환하는 역할을 하게 된다.</p>]]></content><author><name>qcqced</name><email>qcqced123@gmail.com</email></author><category term="NLP" /><category term="Natural Language Process" /><category term="Transformer" /><category term="Self-Attention" /><category term="Seq2Seq" /><category term="Encoder" /><category term="Decoder" /><summary type="html"><![CDATA[Transformer Official Paper Review with Pytorch Implementation]]></summary></entry><entry><title type="html">📈 Gradient: Directional Derivative</title><link href="http://localhost:4000/calculus/gradient" rel="alternate" type="text/html" title="📈 Gradient: Directional Derivative" /><published>2023-07-31T00:00:00+09:00</published><updated>2023-07-31T23:00:00+09:00</updated><id>http://localhost:4000/calculus/gradient</id><content type="html" xml:base="http://localhost:4000/calculus/gradient"><![CDATA[<h3 id="concept-of-gradient"><code class="language-plaintext highlighter-rouge">🤔 Concept of Gradient</code></h3>

<p>그라디언트는 다변수 함수의 기울기를 나타내는 벡터를 말한다. 그라디언트의 원소는 함수에 존재하는 모든 변수를 대상으로 편미분한 결과로 구성되는데, 예를 들어 변수가 $x_1, x_2$ 2개인 다변수 함수 $f(x_1, x_2)$가 있다고 가정해보자. 다변수 함수 $f$의 그라디언트는 아래 수식처럼 표현할 수 있다.</p>

\[f'(x_1, x_2) = \begin{vmatrix}
  \frac{∂f}{∂x_1} \\
  \frac{∂f}{∂x_2}
\end{vmatrix}\]

<p>이러한 그라디언트는 머신 러닝, 수치 최적화 학문에서 매우 중요한 개념으로 꼽힌다. 그라디언트 벡터가 가리키는 방향이 바로 다변수 함수가 특정 지점에서 가장 가파르게 증가하는 방향을 가리키기 때문이다. 이처럼 그라디언트는 함수의 입력 공간을 따라 함수가 어떻게 변화하는지를 알려주는 길잡이 역할을 하기 때문에, 그라디언트 방향을 따라 변수값을 튜닝하다 보면 함수의 최대값•최소값에 도달하여 최적화 문제를 해결할 수 있게 된다. 그렇다면 왜 그라디언트 벡터의 방향이 특정 지점에서 함수가 가장 가파르게 증가하는 방향을 나타내는 것일까?? 편미분, 도함수 정의 그리고 내적을 활용해 증명할 수 있다.</p>

<h3 id="-proof-of-gradient"><code class="language-plaintext highlighter-rouge">🪪 Proof of Gradient</code></h3>

<p align="center">
<img src="/assets/images/gradient/gradient.jpg" alt="Example of multivariate function" class="align-center image-caption" width="60%&quot;, height=&quot;25%" />
<strong><em><a href="https://m.blog.naver.com/PostView.naver?isHttpsRedirect=true&amp;blogId=galaxyenergy&amp;logNo=221431325545">Example of multivariate function</a></em></strong>
</p>

<p>그라디언트 벡터의 방향이 함수가 가장 가파르게 증가하는 방향과 일치한다는 명제를 증명하기 위해 최단 경로로 산 정상에 오르는 과정을 떠올려보려 한다. 우리는 현재 이변수 함수로 정의되는 산 중턱 어딘가, 점 $(x_1^0, x_2^0)$를 지나고 있다. 산 정상을 최단 경로로 오르려면 어떻게 해야할까?? 가장 경사가 가파른 급경사 지대를 향해 나아가면 될 것이다. 하지만 산 중턱에 있는 우리가 어느 방향이 가장 가파른 급경사 지대인지 직관적으로 알 길이 없다. 그래서 방향 도함수를 도입해 급경사 지대로 향할 수 있는 방향을 구해 보기로 했다. 아래 수식을 보자.</p>

\[\lim_{\Delta{x}-&gt;0}\frac{f(x+\Delta{x}) - f(x)}{\Delta{x}} =    \frac{df}{dx}= f'(x) \\
df = f'(x)dx\]

<p>너무나도 익숙한 형태 아닌가?? 우리가 일반적으로 알고 있는 일변수 함수의 미분 정의 그리고 좌변의 $dx$를 우변으로 넘겨 살짝 변형한 식이다. 이것을 이제 다변수 함수에 적용하면 바로 방향 도함수가 된다. 다시 우리가 오르려는 산(이변수 함수)으로 돌아와 보자.</p>

\[f(x_1 + dx_1, x_2) = f(x_1, x_2) + f'(x_1, x_2)dx_1 \\
f(x_1, x_2 + dx_2) = f(x_1, x_2) + f'(x_1, x_2)dx_2 \\\]

<p>위에서 서술한 도함수 정의를 활용해 우리가 다음에 발걸음을 옮길 위치를 점  $A$를 $(x_1^0 + dx_1, x_2^0+dx_2)$ 이라고 표현할 수 있다. 이 표현을 활용해 다변수 함수의 미분을 정의해보자. 우리는 이미 다변수 함수의 개별 변수에 편미분을 취하고 행벡터로 쌓은 결과가 바로 전미분이라는 것을 알고 있다.</p>

\[f(x_1 + dx_1, x_2 + dx_2) - f(x_1, x_2) = f'(x_1)dx_1 + f'(x_2)dx_2\]

<p>다시 편미분의 정의를 활용해 수식을 정리하면 방향 벡터와 편미분 결과의 내적으로 표현할 수 있다.</p>

\[dL = \frac{∂L}{∂{x_1}}dx_1 + \frac{∂L}{∂{x_2}}dx_2 \\
dL = [dx_1, dx_2]\ •\ \begin{vmatrix}
  \frac{∂L}{∂x_1} \\
  \frac{∂L}{∂x_2}
\end{vmatrix}\]

<p>쏟아지는 수식 속에 우리의 본래 목적을 잊어서는 안된다. 우리는 지금 가장 빠르게 산 정상에 도달할 수 있는 방법을 찾기 위해 지금까지 달려왔다. 산 정상에 가장 빠르게 도달하기 위해 가장 가파른 급경사 지대만 찾아서 올라가는 전략을 세웠었다. 다시 말해, 다변수 함수 $f(x)$의 극소 변화량 $dL$이 최대가 되는 방향으로 발걸음을 옮기면 된다는 것이다. 그렇다면 극소 변화량 $dL$은 언제 최대가 될까??</p>

<p>이제 까먹고 있었던 내적의 개념을 다시 한 번 상기시켜보자. 내적은 다양하게 해석되지만, 본디 서로 다른 두 벡터의 <code class="language-plaintext highlighter-rouge">닮은 정도</code>를 나타낸다. 극소 변화량 $dL$이 최대가 되려면 우변의 내적 결과가 최대가 되어야 한다. 내적의 최대값은 서로 다른 두 벡터 사이의 끼인각도가 0˚일 때 즉, 두 벡터가 동일한 방향을 나타낼 때 정의된다. <strong><u>따라서 방향 벡터가 그라디언트(편미분의 행벡터) 방향일 때</u></strong> <code class="language-plaintext highlighter-rouge">내적 결과</code>(극소 변화량 $dL$)<strong><u>가 최대가 된다.</u></strong></p>

<p><strong><u>한편, 실제 기계학습에서는 손실함수의 최적화를 목적 함수로 사용하기 때문에 그라디언트(손실함수의 전미분) 방향에 음수를 취해준 값을 사용하게 된다.</u></strong></p>]]></content><author><name>qcqced</name><email>qcqced123@gmail.com</email></author><category term="Calculus" /><category term="Calculus" /><category term="Partial Derivative" /><category term="Total Derivative" /><category term="loss function" /><category term="Gradient" /><category term="Gradient Descent" /><category term="Machine Learning" /><summary type="html"><![CDATA[Proof of gradient direction with Total Derivative]]></summary></entry><entry><title type="html">🌆 [ViT] An Image Is Worth 16x16 Words: Transformers For Image Recognition At Scale</title><link href="http://localhost:4000/cv/vit" rel="alternate" type="text/html" title="🌆 [ViT] An Image Is Worth 16x16 Words: Transformers For Image Recognition At Scale" /><published>2023-07-26T00:00:00+09:00</published><updated>2023-07-27T02:00:00+09:00</updated><id>http://localhost:4000/cv/ViT</id><content type="html" xml:base="http://localhost:4000/cv/vit"><![CDATA[<h3 id="overview"><code class="language-plaintext highlighter-rouge">🔭 Overview</code></h3>

<p>시작하기 앞서, 본 논문 리뷰를 수월하게 읽으려면 <code class="language-plaintext highlighter-rouge">Transformer</code> 에 대한 선이해가 필수적이다. 아직 <code class="language-plaintext highlighter-rouge">Transformer</code> 에 대해서 잘 모른다면 필자가 작성한 포스트를 읽고 오길 권장한다. 또한 본문 내용을 작성하면서 참고한 논문과 여러 포스트의 링크를 맨 밑 하단에 첨부했으니 참고 바란다. 시간이 없으신 분들은 중간의 코드 구현부를 생략하고 <code class="language-plaintext highlighter-rouge">Insight</code> 부터 읽기를 권장한다.</p>

<p><code class="language-plaintext highlighter-rouge">Vision Transformer</code>(이하 <code class="language-plaintext highlighter-rouge">ViT</code>)는 2020년 10월 Google에서 발표한 컴퓨터 비전용 모델이다. 자연어 처리에서 대성공을 거둔 트렌스포머 구조와 기법을 거의 그대로 비전 분야에 이식했다는 점에서 큰 의의가 있으며, 이후 컴퓨터 비전 분야의 트렌스포머 전성시대가 열리게 된 계기로 작용한다.</p>

<p>한편, <code class="language-plaintext highlighter-rouge">ViT</code> 의 설계 철학은 바로 <code class="language-plaintext highlighter-rouge">scalability(범용성)</code>이다. 신경망 설계에서 범용성이란, 모델의 확장 가능성을 말한다. 예를 들면 학습 데이터보다 더 크고 복잡한 데이터 세트를 사용하거나 모델의 파라미터를 늘려 사이즈를 키워도 여전히 유효한 추론 결과를 도출하거나 더 나은 성능을 보여주고 나아가 개선의 여지가 여전히 남아있을 때 <code class="language-plaintext highlighter-rouge">“확장성이 높다”</code> 라고 표현한다. 저자들은 논문 초반에 콕 찝어서 컴퓨터 비전 분야의 <code class="language-plaintext highlighter-rouge">scalability</code> 높이는 것이 이번 모델 설계의 목표였다고 밝히고 있다. <code class="language-plaintext highlighter-rouge">범용성</code>은 신경망 모델 설계에서 가장 큰 화두가 되는데 도메인마다 정의하는 의미에 차이가 미세하게 존재한다. 따라서  <code class="language-plaintext highlighter-rouge">ViT</code>의 저자들이 말하는 <code class="language-plaintext highlighter-rouge">범용성</code>이 무엇을 의미하는지 알아보는 것은 구체적인 모델 구조를 이해하는데 큰 도움이 될 것이다.</p>

<h3 id="scalability-in-vit"><code class="language-plaintext highlighter-rouge">🧠 Scalability in ViT</code></h3>

<p>논문 초반부에서 다음과 같은 문장이 서술 되어있다.</p>

<p><code class="language-plaintext highlighter-rouge">“Our Vision Transformer (ViT) attains excellent results when pre-trained at sufficient scale and transferred to tasks with fewer datapoints"</code></p>

<p>이 구문이 <code class="language-plaintext highlighter-rouge">ViT</code> 의 <code class="language-plaintext highlighter-rouge">Scalability</code>를 가장 잘 설명하고 있다고 생각한다. 저자들이 말하는 범용성은 결국 <code class="language-plaintext highlighter-rouge">backbone</code> 구조의 활용을 의미한다. 자연어 처리에 익숙한 독자라면 쉽게 이해가 가능할 것이다. <code class="language-plaintext highlighter-rouge">Transformer</code>, <code class="language-plaintext highlighter-rouge">GPT</code>, <code class="language-plaintext highlighter-rouge">BERT</code>의 등장 이후, 자연어 처리는 범용성을 갖는 데이터 세트로 사전 훈련한 모델을 활용해 <code class="language-plaintext highlighter-rouge">Task-Agnostic</code>하게 하나의 <code class="language-plaintext highlighter-rouge">backbone</code>으로 거의 모든 Task를 수행할 수 있으며, 작은 사이즈의 데이터라도 상당히 높은 수준의 추론 성능을 낼 수 있었다. 그러나 당시 컴퓨터 비전의 메인이었던 <code class="language-plaintext highlighter-rouge">Conv</code> 기반 모델들은 파인튜닝해도 데이터 크기가 작으면 일반화 성능이 매우 떨어지고, Task에 따라서 다른 아키텍처를 갖는 모델을 새롭게 정의하거나 불러와 사용해야 하는 번거로움이 있었다. 예를 들면 <code class="language-plaintext highlighter-rouge">Image Classfication</code> 에는 <code class="language-plaintext highlighter-rouge">ResNet</code>, <code class="language-plaintext highlighter-rouge">Segmentation</code> 에는 <code class="language-plaintext highlighter-rouge">U-Net</code>, <code class="language-plaintext highlighter-rouge">Object Detection</code> 은 <code class="language-plaintext highlighter-rouge">YOLO</code> 를 사용하는 것처럼 말이다. 반면 자연어 처리는 사전 학습된 모델 하나로 모든 NLU, 심지어는 NLG Task도 수행할 수 있다. 저자들은 이러한 범용성을 컴퓨터 비전에도 이식 시키고 싶었던 것 같다. 그렇다면 먼저 자연어 처리에서 트랜스포머 계열이 범용성을 가질 수 있었던 이유는 무엇인지 간단히 살펴보자.</p>

<p>저자들은 <code class="language-plaintext highlighter-rouge">self-attention</code>(내적)의 효율성, 모델의 구조적 탁월성 그리고 <code class="language-plaintext highlighter-rouge">self-supervised task</code>의 존재를 꼽는다. 그럼 이것들이 왜 범용성을 높이는데 도움이 될까??</p>

<p><code class="language-plaintext highlighter-rouge">self-attention(내적)</code>은 행렬 간 곱셉으로 정의 되어 설계가 매우 간편하고 병렬로 한번에 처리하는 것이 가능하기 때문에 효율적으로 전체 데이터를 모두 고려한 연산 결과를 얻을 수 있다.</p>

<p><code class="language-plaintext highlighter-rouge">Multi-Head Attention</code> 구조는 여러 차원의 의미 관계를 동시에 포착하고 그것을 앙상블한 것과 같은(실제로는 MLP) 결과를 얻을 수 있다는 점에서 구조적으로 탁월하다.</p>

<p>마지막으로 <code class="language-plaintext highlighter-rouge">MLM</code>, <code class="language-plaintext highlighter-rouge">Auto-Regression(LM) Task</code>는 데이터 세트에 별도의 인간의 개입(라벨링)이 필요하지 않기 때문에 가성비 있게 데이터와 모델의 사이즈를 늘릴 수 있게 된다.<br />
이제 논문에서 트랜스포머 계열이 가진 범용성을 어떻게 비전 분야에 적용했는지 주목하면서 모델 구조를 하나 하나 살펴보자.</p>

<h3 id="modeling"><code class="language-plaintext highlighter-rouge">🌟 Modeling</code></h3>

<p align="center">
<img src="/assets/images/vision_transformer/modeling_overview.png" alt="ViT Model Structure" class="align-center image-caption" width="100%&quot;, height=&quot;50%" />
<strong><em><a href="https://arxiv.org/abs/2010.11929">ViT Model Structure</a></em></strong>
</p>

<ul>
  <li><strong>1) Transfer <code class="language-plaintext highlighter-rouge">Scalability</code> from pure <code class="language-plaintext highlighter-rouge">Transformer</code> to Computer Vision</strong>
    <ul>
      <li><strong>Overcome <code class="language-plaintext highlighter-rouge">reliance</code> on Convolution(<code class="language-plaintext highlighter-rouge">Inductive Bias</code>) in Computer Vision</strong></li>
      <li><strong>Apply Self-Attention &amp; Architecture from vanilla NLP Transformers as <code class="language-plaintext highlighter-rouge">closely</code> as possible</strong></li>
      <li><strong>Treat Image as sequence of text token</strong></li>
      <li><strong>Make $P$ sub-patches from whole image, playing same role as token in NLP Transformer</strong></li>
    </ul>
  </li>
</ul>

<p>저자들은 먼저 <code class="language-plaintext highlighter-rouge">Conv</code> 에 대한 의존을 버릴 것을 주장한다. <code class="language-plaintext highlighter-rouge">Conv</code>가 가진 <code class="language-plaintext highlighter-rouge">Inductive Bias</code> 때문에 파인튜닝 레벨에서 데이터 크기가 작으면 일반화 성능이 떨어지는 것이라고 설명하고 있다. 이 말을 이해하려면 <code class="language-plaintext highlighter-rouge">Inductive Bias</code>에 대해서 먼저 알아야 한다. <code class="language-plaintext highlighter-rouge">Inductive Bias</code>란, 주어진 데이터로부터 일반화 성능을 높이기 위해 <code class="language-plaintext highlighter-rouge">‘입력되는 데이터는 ~ 할 것이다’</code>, <code class="language-plaintext highlighter-rouge">‘이런 특징을 갖고 있을 것이다’</code>와 같은 가정, 가중치, 가설 등을 기계학습 알고리즘에 적용하는 것을 말한다.</p>

<p><code class="language-plaintext highlighter-rouge">Conv</code> 연산 자체 (가중치 공유, 풀링 있는 <code class="language-plaintext highlighter-rouge">Conv Block</code>이 <code class="language-plaintext highlighter-rouge">Invariance</code>)의 기본 가정은 <code class="language-plaintext highlighter-rouge">translation equivariance</code>, <code class="language-plaintext highlighter-rouge">locality</code>이다. 사실 저자의 주장을 이해하는데 <code class="language-plaintext highlighter-rouge">equivariance</code>와 <code class="language-plaintext highlighter-rouge">locality</code>의 뜻이 무엇인지 파악하는 것은 크게 의미가 없다 (<code class="language-plaintext highlighter-rouge">equivariance</code>와 <code class="language-plaintext highlighter-rouge">invariance</code>에 대해서는 다른 포스팅에서 자세히 살펴보도록 하겠다). <strong><u>중요한 것은 입력 데이터에 가정을 더한다는 점이다.</u></strong> 만약 주어진 입력이 미리 가정한 <code class="language-plaintext highlighter-rouge">Inductive Bias</code> 에 벗어난다면 어떻게 될까??</p>

<p>아마 오버피팅 되거나 모델 학습이 수렴성을 갖지 못하게 될 것이다. 이미지 데이터도 Task에 따라 필요한 <code class="language-plaintext highlighter-rouge">Inductive Bias</code>가 달라진다. 예를 들어 <code class="language-plaintext highlighter-rouge">Segmentation</code>, <code class="language-plaintext highlighter-rouge">Detection</code> 의 경우는 이미지 속 객체의 위치, 픽셀 사이의 <code class="language-plaintext highlighter-rouge">spatial variance</code> 정보가 매우 중요하다. 한편, <code class="language-plaintext highlighter-rouge">Classification</code>은 <code class="language-plaintext highlighter-rouge">spatial invariance</code>가 중요하다. 목표 객체의 위치와 주변 특징보다 타겟 자체를 신경망이 인식하는 것이 중요하기 때문이다. 따라서  <code class="language-plaintext highlighter-rouge">ViT</code> 저자들은 어떤 Bias던 상관없이 편향을 갖고 데이터를 본다는 것 자체에 의문을 표하며, 이미지 역시 <code class="language-plaintext highlighter-rouge">Inductive Bias</code>에서 벗어나, 주어진 데이터 전체 특징(패치) 사이의 관계를 파악하는 과정에서 <code class="language-plaintext highlighter-rouge">scalability</code>를 획득할 수 있다고 주장한다.</p>

<p>그래서 <code class="language-plaintext highlighter-rouge">Conv</code>의 대안으로 상대적으로 <code class="language-plaintext highlighter-rouge">Inductive Bias</code> 가 부족한 <code class="language-plaintext highlighter-rouge">Self-Attention</code>, <code class="language-plaintext highlighter-rouge">Transformer Architecture</code>를 사용한다. 두가지의 효용성에 대해서는 이미 위에서 언급했기 때문에 생략하고, 여기서 짚고 넘어가야할 점은 <code class="language-plaintext highlighter-rouge">Self-Attention</code>이 <code class="language-plaintext highlighter-rouge">Conv</code> 대비 <code class="language-plaintext highlighter-rouge">Inductive Bias</code>가 적다는 점이다. Self-Attention 과정에는 여러 연산, 스케일 조정값들이 포함되지만 본질적으로 <code class="language-plaintext highlighter-rouge">“내적”</code> 이 중심이다. 내적은 그 어떤 편향 (<code class="language-plaintext highlighter-rouge">Conv</code>와 대조하려고 이렇게 서술했지만 사실 <code class="language-plaintext highlighter-rouge">Position Embedding</code> 더하는 것도 일종의 약한 <code class="language-plaintext highlighter-rouge">Inductive Bias</code>)이 존재하지 않는다. 일단 주어진 모든 데이터에 대해서 내적값을 산출하고 그 다음에 관계가 있다고 생각되는 정보를 추리기 때문이다. <code class="language-plaintext highlighter-rouge">Conv</code> 때와 달리 <code class="language-plaintext highlighter-rouge">‘입력되는 데이터는 ~ 할 것이다’</code>, <code class="language-plaintext highlighter-rouge">‘이런 특징을 갖고 있을 것이다’</code> 라는 가정이 없다. 이번 포스팅의 마지막 쯤에서 다시 다루겠지만 그래서 <code class="language-plaintext highlighter-rouge">ViT</code>는 인스턴스 사이의 모든 관계를 뽑아보는 <code class="language-plaintext highlighter-rouge">Self-Attention(내적)</code> 을 기반으로 만들어졌기 때문에 이미지의 <code class="language-plaintext highlighter-rouge">Global Information</code>을 포착하는데 탁월한 성능을 보이고, <code class="language-plaintext highlighter-rouge">Conv</code> 는 <strong><u>“중요한 정보는 근처 픽셀에 몰려있다라는”</u></strong> <code class="language-plaintext highlighter-rouge">Inductive Bias</code>  덕분에 <code class="language-plaintext highlighter-rouge">Local Information</code>을 포착하는데 탁월한 성능을 낸다.</p>

<p>그렇다면 픽셀 하나 하나끼리 내적해준다는 것일까?? 아니다 여기서 논문의 제목이 <code class="language-plaintext highlighter-rouge">An Image Is Worth 16x16 Words</code> 인 이유가 드러난다. 일단 픽셀 하나 하나끼리 유사도를 측정하는 것이 유의미할까 생각해보자. 자연어의 토큰과 달리 이미지의 단일 픽셀 한 개는 큰 인사이트를 얻기 힘들다. 픽셀은 말 그대로 점 하나일 뿐이다. 픽셀을 여러 개 묶어 패치 단위로 묶는다면 이야기는 달라진다. 일정 크기 이상의 패치라면 자연어의 토큰처럼 그 자체로 어떤 의미를 담을 수 있다. 따라서 저자는 전체 이미지를 여러 개의 16x16 혹은 14x14 사이즈 패치로 나누어 하나 하나를 토큰으로 간주해 이미지 시퀀스를 만들고 그것을 모델의 Input으로 사용한다.</p>

<p align="center">
<img src="/assets/images/vision_transformer/class_diagram.png" alt="Class Diagram" class="align-center image-caption" width="50%&quot;, height=&quot;50%" />
<strong><em>Class Diagram</em></strong>
</p>

<p>모델 구조의 뼈대가 되는 내용들을 모두 살펴보았고, 위에서 서술한 내용을 구현하기 위해 어떤 블록들을 사용했는지 필자가 직접 논문을 보고 따라 구현한 코드와 함께 알아보도록 하자. 위에 첨부한 모델 모식도에 나와 있는 블록들 하나 하나 살펴볼 예정이다. 여담으로 Google Research의 Official Repo 역시 함께 참고했는데, 코드가 모두 구글이 요새 새롭게 미는 <code class="language-plaintext highlighter-rouge">Jax</code>, <code class="language-plaintext highlighter-rouge">Flax</code> 로 구현 되어 있었다. 파이토치나 좀 써본 필자 입장에서는 정말 … 지옥불을 경험했다. 오늘도 다시 한 번 페이스북 파이토치 개발팀에 큰절 드리고 싶다.</p>

<h4 id="linear-projection-of-flattened-patches"><code class="language-plaintext highlighter-rouge">🔬 Linear Projection of Flattened Patches</code></h4>

\[x_p \in R^{N * (P^2•C)}\]

\[z_{0} = [x_{class}; x_p^1E;x_p^2E;x_p^3E....x_p^NE]\]

\[N = \frac{H*W}{P*P}\]

<p><code class="language-plaintext highlighter-rouge">ViT</code>의 입력 임베딩을 생성하는 역할을 한다. <code class="language-plaintext highlighter-rouge">ViT</code>는 $x \in R^{H * W * C}$(H: height, W: width, C: channel)의 형상을 갖는 이미지를 입력으로 받아 가로 세로 길이가 $P$, 채널 개수 $C$인 $N$개의 패치로 <code class="language-plaintext highlighter-rouge">reshape</code> 한다. 필자가 코드 구현 중 가장 혼동한 부분이 바로 패치 개수 $N$이었다. 직관적으로 패치 개수라고 하면, 전체 이미지 사이즈에서 패치 크기를 나눈 값이라고 생각하기 쉽기 때문이다. 예를 들면 <code class="language-plaintext highlighter-rouge">512x512</code>짜리 이미지를 <code class="language-plaintext highlighter-rouge">16x16</code> 사이즈의 패치로 나눈다고 해보자. 필자는 단순히 <code class="language-plaintext highlighter-rouge">512/16=32</code> 라는 결과를 이용해 $N=32$로 설정하고 실험을 진행하다가 텐서 차원이 맞지 않아 발생하는 에러 로그를 마주했었다. 그러나 논문 속 수식을 확인해보면,  $H * W / P^2$이 바로 패치 개수$N$으로 정의된다. 그래서 만약 <code class="language-plaintext highlighter-rouge">512x512</code> 사이즈의 <code class="language-plaintext highlighter-rouge">RGB</code> 이미지 <code class="language-plaintext highlighter-rouge">10장</code>을 ViT 입력 임베딩에 맞게 차원 변환한다면 결과는 <code class="language-plaintext highlighter-rouge">[10, 3, 1024, 768]</code> 이 될 것이다. (이 예시를 앞으로 계속 이용하겠다)</p>

<p>이렇게 차원을 바꿔준 이미지를 <code class="language-plaintext highlighter-rouge">nn.Linear((channels * patch_size**2), dim_model)</code> 를 통해 <code class="language-plaintext highlighter-rouge">ViT</code>의 임베딩 레이어에 선형 투영해준다. 여기서 자연어 처리와 파이토치를 자주 사용하시는 독자라면 왜 <code class="language-plaintext highlighter-rouge">nn.Embedding</code>을 사용하지 않았는가 의문을 가질 수 있다.</p>

<p>자연어 처리에서 입력 임베딩을 만들때는 모델의 토크나이저에 의해 사전 정의된 vocab의 사이즈가 입력 문장에 속한 토큰 개수보다 훨씬 크기 때문에 데이터 룩업 테이블 방식의 <code class="language-plaintext highlighter-rouge">nn.Embedding</code> 을 사용하게 된다. 이게 무슨 말이냐면, 토크나이저에 의해 사전에 정의된 <code class="language-plaintext highlighter-rouge">vocab</code> 전체가 <code class="language-plaintext highlighter-rouge">nn.Embedding(vocab_size, dim_model)</code>로 투영 되어 가로는 vocab 사이즈, 세로는 모델의 차원 크기에 해당하는 룩업 테이블이 생성되고, 내가 입력한 토큰들은 전체 <code class="language-plaintext highlighter-rouge">vocab</code>의 일부분일테니 전체 임베딩 룩업 테이블에서 내가 임베딩하고 싶은 토큰들의 인덱스만 알아낸다는 것이다.</p>

<p>그래서 <code class="language-plaintext highlighter-rouge">nn.Embedding</code> 에 정의된 차원과 실제 입력 데이터의 차원이 맞지 않아도 함수가 동작하게 되는 것이다. 그러나 비전의 경우, 사전에 정의된 <code class="language-plaintext highlighter-rouge">vocab</code>이라는 개념이 전혀 없고 입력 이미지 역시 항상 고정된 크기의 차원으로 들어오기 때문에 <code class="language-plaintext highlighter-rouge">nn.Embedding</code>이 아닌  <code class="language-plaintext highlighter-rouge">nn.Linear</code> 을 사용해 곧바로 선형 투영을 구현한 것이다. 두 메서드에 대한 자세한 비교는 파이토치 관련 포스트에서 다시 한 번 자세히 다루도록 하겠다.</p>

<p>한편, <code class="language-plaintext highlighter-rouge">Position Embedding</code>을 더하기 전, <code class="language-plaintext highlighter-rouge">Input Embedding</code>의 차원은 <code class="language-plaintext highlighter-rouge">[10, 1024, 1024]</code> 이 된다. 지금까지 설명한 부분(<code class="language-plaintext highlighter-rouge">Linear Projection of Flattened Patches</code> )을 파이토치 코드로 구현하면 다음과 같다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">VisionTransformer</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="p">...</span>
    <span class="n">중략</span>
    <span class="p">...</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">num_patches</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">image_size</span> <span class="o">/</span> <span class="n">patch_size</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">input_embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">((</span><span class="n">channels</span> <span class="o">*</span> <span class="n">patch_size</span><span class="o">**</span><span class="mi">2</span><span class="p">),</span> <span class="n">dim_model</span><span class="p">)</span> <span class="c1"># Projection Layer for Input Embedding
</span>    <span class="p">...</span>
    <span class="n">중략</span>
    <span class="p">...</span>  
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">any</span><span class="p">:</span>
        <span class="s">""" For cls pooling """</span>
        <span class="k">assert</span> <span class="n">inputs</span><span class="p">.</span><span class="n">ndim</span> <span class="o">!=</span> <span class="mi">4</span><span class="p">,</span> <span class="sa">f</span><span class="s">"Input shape should be [BS, CHANNEL, IMAGE_SIZE, IMAGE_SIZE], but got </span><span class="si">{</span><span class="n">inputs</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="s">"</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">inputs</span> 
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">input_embedding</span><span class="p">(</span>
            <span class="n">x</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="p">.</span><span class="n">num_patches</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">patch_size</span><span class="o">**</span><span class="mi">2</span> <span class="o">*</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span> <span class="c1"># Projection Layer for Input Embedding
</span>        <span class="p">)</span>
        <span class="n">cls_token</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">,</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>  <span class="c1"># can change init method
</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cat</span><span class="p">([</span><span class="n">cls_token</span><span class="p">,</span> <span class="n">x</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="p">...</span>
</code></pre></div></div>

<p>임베딩 레이어를 객체로 따로 구현해도 되지만, 필자는 굳이 추상화가 필요하지 않다고 생각해 ViT의 최상위 클래스인 <code class="language-plaintext highlighter-rouge">VisionTransformer</code>의 <code class="language-plaintext highlighter-rouge">forward</code> 메서드 맨 초반부에 구현하게 되었다. 입력 받은 이미지 텐서를 <code class="language-plaintext highlighter-rouge">torch.reshape</code> 을 통해 <code class="language-plaintext highlighter-rouge">[패치 개수, 픽셀개수*채널개수]</code> 로 바꾼 뒤, 미리 정의해둔 <code class="language-plaintext highlighter-rouge">self.input_embedding</code> 에 매개변수로 전달해 <code class="language-plaintext highlighter-rouge">“위치 임베딩”</code> 값이 더해지기 전 <code class="language-plaintext highlighter-rouge">Input Embedding</code>을 만든다.</p>

<p>한편, <code class="language-plaintext highlighter-rouge">CLS Pooling</code>을 위해 마지막에 <code class="language-plaintext highlighter-rouge">[batch, 1, image_size]</code> 의 차원을 갖는 <code class="language-plaintext highlighter-rouge">cls_token</code> 을 정의해 패치 시퀀스와 <code class="language-plaintext highlighter-rouge">concat</code> (맨 앞에)해준다. 이 때 논문에 제시된 수식 상, <code class="language-plaintext highlighter-rouge">CLS Token</code>은 선형 투영하지 않으며, 패치 시퀀스에 선형 투영이 이뤄지고 난 뒤에 맨 앞에 <code class="language-plaintext highlighter-rouge">Concat</code> 하게 된다.</p>

<p><code class="language-plaintext highlighter-rouge">CLS Token</code>까지 더한 최종 <code class="language-plaintext highlighter-rouge">Input Embedding</code> 의 텐서 차원은 <code class="language-plaintext highlighter-rouge">[10, 1025, 1024]</code> 가 된다.</p>

<h4 id="positional-embedding"><code class="language-plaintext highlighter-rouge">🔢 Positional Embedding</code></h4>

\[E_{pos} \in R^{(N+1)*D}\]

<p>이미지를 패치 단위의 임베딩으로 만들었다면 이제 위치 임베딩을 정의해서 더해주면 모식도 속 <code class="language-plaintext highlighter-rouge">Embedded Patches</code> , 즉 인코더에 들어갈 최종 <code class="language-plaintext highlighter-rouge">Patch Embedding</code> 이 완성 된다. 위치 임베딩을 만드는 방식은 기존 <code class="language-plaintext highlighter-rouge">Transformer</code>, <code class="language-plaintext highlighter-rouge">BERT</code> 와 동일하다. 아래 <code class="language-plaintext highlighter-rouge">VisionEncoder</code> 클래스를 구현한 코드를 살펴보자.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="k">class</span> <span class="nc">VisionEncoder</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="p">...</span>
    <span class="n">중략</span>
    <span class="p">...</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">positional_embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Embedding</span><span class="p">((</span><span class="bp">self</span><span class="p">.</span><span class="n">num_patches</span> <span class="o">+</span> <span class="mi">1</span><span class="p">),</span> <span class="n">dim_model</span><span class="p">)</span>  <span class="c1"># add 1 for cls token
</span>    <span class="p">...</span>
    <span class="n">중략</span>
    <span class="p">...</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
        <span class="n">layer_output</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">pos_x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">num_patches</span> <span class="o">+</span> <span class="mi">1</span><span class="p">).</span><span class="n">repeat</span><span class="p">(</span><span class="n">inputs</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]).</span><span class="n">to</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>  <span class="c1"># inputs.shape[0] = Batch Size of Input
</span>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">dropout</span><span class="p">(</span>
            <span class="n">inputs</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">positional_embedding</span><span class="p">(</span><span class="n">pos_x</span><span class="p">)</span>
        <span class="p">)</span>
    <span class="p">...</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">Input Embedding</code>과 다르게 위치 임베딩은 <code class="language-plaintext highlighter-rouge">nn.Embedding</code>으로 구현했는데, 여기서도 사실 <code class="language-plaintext highlighter-rouge">nn.Linear</code>를 사용해도 무방하다. 그것보다 <code class="language-plaintext highlighter-rouge">nn.Embedding</code>의 입력 차원인 <code class="language-plaintext highlighter-rouge">self.num_patches + 1</code> 에 주목해보자. 왜 1을 더해준 값을 사용했을까??</p>

<p><code class="language-plaintext highlighter-rouge">ViT</code>는 BERT의 <code class="language-plaintext highlighter-rouge">CLS Token Pooling</code> 을 차용하기 위해 패치 시퀀스 맨 앞에 CLS 토큰을 추가하기 때문이다. 이렇게 추가된 <code class="language-plaintext highlighter-rouge">CLS Token</code>은 인코더를 거쳐 최종 <code class="language-plaintext highlighter-rouge">MLP Head</code>에 흘러들어가 로짓으로 변환된다. 만약 독자께서 <code class="language-plaintext highlighter-rouge">CLS Token Pooling</code> 대신 다른 풀링 방식을 사용할거라면 1을 추가해줄 필요는 없다.</p>

<p>애초에 객체 인스턴스 초기화 당시에 <code class="language-plaintext highlighter-rouge">CLS Token</code> 을 추가를 반영한 값을 전달하면 되지 않는가하는 의문이 들 수도 있다. 하지만 <code class="language-plaintext highlighter-rouge">VisionEncoder</code> 객체 인스턴스 초기화 당시에는 <code class="language-plaintext highlighter-rouge">num_patches</code> 값으로 <code class="language-plaintext highlighter-rouge">CLS Token</code>이 추가되기 이전 값(+1 반영이 안되어 있음)을 전달하도록 설계 되어 있어서  <code class="language-plaintext highlighter-rouge">CLS Pooling</code>을 사용할거라면 1 추가를 꼭 해줘야 한다.</p>

<p align="center">
<img src="/assets/images/vision_transformer/insight5.png" alt="Performance Table by making Position Embedding method" class="align-center image-caption" width="75%&quot;, height=&quot;50%" />
<strong><em><a href="https://arxiv.org/abs/2010.11929">Performance Table by making Position Embedding method</a></em></strong>
</p>

<p>한편 저자는 <code class="language-plaintext highlighter-rouge">2D Postion Embedding</code>, <code class="language-plaintext highlighter-rouge">Relative Position Embedding</code> 방식도 적용해봤지만, 구현 복잡도 &amp; 연산량 대비 성능 향상 폭이 매우 미미해 일반적인 <code class="language-plaintext highlighter-rouge">1D Position Embedding</code>을 사용할 것을 추천하고 있다.</p>

<h4 id="-multi-head-attention"><code class="language-plaintext highlighter-rouge">👩‍👩‍👧‍👦 Multi-Head Attention</code></h4>

\[z_t^{'} = MSA(LN(z_{t-1}) + z_{t-1})\]

\[MSA(z) = [SA_1();SA_2();SA_3()...SA_k()]*U_{msa}, \ \ U_{msa} \in R^{(k*D_h)*D} \\\]

<p>트랜스포머 계열 모델의 핵심 <code class="language-plaintext highlighter-rouge">Multi-Head Self-Attention</code> 모듈에 대해서 알아보자. 사실 기존 자연어 처리 <code class="language-plaintext highlighter-rouge">Transformer</code>, <code class="language-plaintext highlighter-rouge">BERT</code> 등의 동작 방식과 완전히 동일하며, 코드로 구현할 때 역시 동일하게 만들어주면 된다. 자세한 원리와 동작 방식은 <strong><u>Attention Is All You Need</u></strong> 리뷰 포스트에서 설명했기 때문에 생략하고 넘어가겠다. 한편 파이토치로 구현한 <code class="language-plaintext highlighter-rouge">Multi-Head Self-Attention</code> 블럭에 대한 코드는 다음과 같다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">scaled_dot_product_attention</span><span class="p">(</span><span class="n">q</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">k</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">v</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">dot_scale</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
    <span class="s">"""
    Scaled Dot-Product Attention
    Args:
        q: query matrix, shape (batch_size, seq_len, dim_head)
        k: key matrix, shape (batch_size, seq_len, dim_head)
        v: value matrix, shape (batch_size, seq_len, dim_head)
        dot_scale: scale factor for Q•K^T result, same as pure transformer
    Math:
        A = softmax(q•k^t/sqrt(D_h)), SA(z) = Av
    """</span>
    <span class="n">attention_dist</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="n">softmax</span><span class="p">(</span>
        <span class="n">torch</span><span class="p">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">))</span> <span class="o">/</span> <span class="n">dot_scale</span><span class="p">,</span>
        <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span>
    <span class="p">)</span>
    <span class="n">attention_matrix</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">attention_dist</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">attention_matrix</span>

<span class="k">class</span> <span class="nc">AttentionHead</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="s">"""
    In this class, we implement workflow of single attention head
    Args:
        dim_model: dimension of model's latent vector space, default 1024 from official paper
        dim_head: dimension of each attention head, default 64 from official paper (1024 / 16)
        dropout: dropout rate, default 0.1
    Math:
        [q,k,v]=z•U_qkv, A = softmax(q•k^t/sqrt(D_h)), SA(z) = Av
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim_model</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span>  <span class="mi">1024</span><span class="p">,</span> <span class="n">dim_head</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">64</span><span class="p">,</span> <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">AttentionHead</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span> <span class="o">=</span> <span class="n">dim_model</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dim_head</span> <span class="o">=</span> <span class="n">dim_head</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">dropout</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dot_scale</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">dim_head</span><span class="p">))</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">fc_q</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">dim_head</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">fc_k</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">dim_head</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">fc_v</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">dim_head</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="n">attention_matrix</span> <span class="o">=</span> <span class="n">scaled_dot_product_attention</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">fc_q</span><span class="p">(</span><span class="n">x</span><span class="p">),</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">fc_k</span><span class="p">(</span><span class="n">x</span><span class="p">),</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">fc_v</span><span class="p">(</span><span class="n">x</span><span class="p">),</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">dot_scale</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">attention_matrix</span>

<span class="k">class</span> <span class="nc">MultiHeadAttention</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="s">"""
    In this class, we implement workflow of Multi-Head Self-Attention
    Args:
        dim_model: dimension of model's latent vector space, default 1024 from official paper
        num_heads: number of heads in MHSA, default 16 from official paper for ViT-Large
        dim_head: dimension of each attention head, default 64 from official paper (1024 / 16)
        dropout: dropout rate, default 0.1
    Math:
        MSA(z) = [SA1(z); SA2(z); · · · ; SAk(z)]•Umsa
    Reference:
        https://arxiv.org/abs/2010.11929
        https://arxiv.org/abs/1706.03762
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim_model</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1024</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span> <span class="n">dim_head</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">64</span><span class="p">,</span> <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MultiHeadAttention</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span> <span class="o">=</span> <span class="n">dim_model</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dim_head</span> <span class="o">=</span> <span class="n">dim_head</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">dropout</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">attention_heads</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">ModuleList</span><span class="p">(</span>
            <span class="p">[</span><span class="n">AttentionHead</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">dim_head</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">dropout</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">num_heads</span><span class="p">)]</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">fc_concat</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="s">""" x is already passed nn.Layernorm """</span>
        <span class="k">assert</span> <span class="n">x</span><span class="p">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">3</span><span class="p">,</span> <span class="sa">f</span><span class="s">'Expected (batch, seq, hidden) got </span><span class="si">{</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="s">'</span>
        <span class="n">attention_output</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">fc_concat</span><span class="p">(</span>
            <span class="n">torch</span><span class="p">.</span><span class="n">cat</span><span class="p">([</span><span class="n">head</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">head</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">attention_heads</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># concat all dim_head = num_heads * dim_head
</span>        <span class="p">)</span>
        <span class="k">return</span> <span class="n">attention_output</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">MultiHeadAttention</code>을 가장 최상위 객체로 두고, 하위에 <code class="language-plaintext highlighter-rouge">AttentionHead</code>객체를 따로 구현했다. 이렇게 구현하면, 어텐션 해드별로 쿼리, 키, 벨류 선영 투영 행렬(<code class="language-plaintext highlighter-rouge">nn.Linear</code>)을 따로 구현해줄 필요가 없어지며, <code class="language-plaintext highlighter-rouge">nn.ModuleList</code> 를 통해 개별 해드를 한 번에 그룹핑하고 <code class="language-plaintext highlighter-rouge">loop</code> 를 통해 출력 결과를 <code class="language-plaintext highlighter-rouge">concat</code> 해줄 수 있어 복잡하고 많은 에러를 유발하는 <strong><u>텐서 차원 조작을 피할 수 있으며</u></strong>, 코드의 가독성이 올라가는 효과가 있다.</p>

<h4 id="️-mlp"><code class="language-plaintext highlighter-rouge">🗳️ MLP</code></h4>

\[z_{t} = MLP(LN(z_{t}^{'}) + z_{t}^{'})\]

<p>이름만 <code class="language-plaintext highlighter-rouge">MLP</code>로 바뀌었을 뿐, 기존 트랜스포머의 피드 포워드 블럭과 동일한 역할을 한다. 역시 자세한 동작 방식은 여기 포스트에서 확인하자. 파이토치로 구현한 코드는 다음과 같다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">MLP</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="s">"""
    Class for MLP module in ViT-Large
    Args:
        dim_model: dimension of model's latent vector space, default 512
        dim_mlp: dimension of FFN's hidden layer, default 2048 from official paper
        dropout: dropout rate, default 0.1
    Math:
        MLP(x) = MLP(LN(x))+x
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim_model</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1024</span><span class="p">,</span> <span class="n">dim_mlp</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">4096</span><span class="p">,</span> <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MLP</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">mlp</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">dim_model</span><span class="p">,</span> <span class="n">dim_mlp</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">GELU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">dropout</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">dim_mlp</span><span class="p">,</span> <span class="n">dim_model</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">dropout</span><span class="p">),</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">mlp</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</code></pre></div></div>

<p>특이한 점은 <code class="language-plaintext highlighter-rouge">Activation Function</code>으로 <code class="language-plaintext highlighter-rouge">GELU</code>를 사용(기존 트랜스포머는 <code class="language-plaintext highlighter-rouge">RELU</code>)했다는 점이다.</p>

<h4 id="-vision-encoder-layer"><code class="language-plaintext highlighter-rouge">📘 Vision Encoder Layer</code></h4>

<p><code class="language-plaintext highlighter-rouge">ViT</code> 인코더 블럭 1개에 해당하는 하위 모듈과 동작을 구현한 객체이다. 구현한 코드는 아래와 같다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">VisionEncoderLayer</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="s">"""
    Class for encoder_model module in ViT-Large
    In this class, we stack each encoder_model module (Multi-Head Attention, Residual-Connection, Layer Normalization, MLP)
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim_model</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1024</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">16</span><span class="p">,</span> <span class="n">dim_mlp</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">4096</span><span class="p">,</span> <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">VisionEncoderLayer</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">self_attention</span> <span class="o">=</span> <span class="n">MultiHeadAttention</span><span class="p">(</span>
            <span class="n">dim_model</span><span class="p">,</span>
            <span class="n">num_heads</span><span class="p">,</span>
            <span class="nb">int</span><span class="p">(</span><span class="n">dim_model</span> <span class="o">/</span> <span class="n">num_heads</span><span class="p">),</span>
            <span class="n">dropout</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">layer_norm1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">dim_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">layer_norm2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">dim_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">dropout</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">mlp</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">(</span>
            <span class="n">dim_model</span><span class="p">,</span>
            <span class="n">dim_mlp</span><span class="p">,</span>
            <span class="n">dropout</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="n">ln_x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">layer_norm1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">residual_x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">dropout</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">self_attention</span><span class="p">(</span><span class="n">ln_x</span><span class="p">))</span> <span class="o">+</span> <span class="n">x</span>

        <span class="n">ln_x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">layer_norm2</span><span class="p">(</span><span class="n">residual_x</span><span class="p">)</span>
        <span class="n">fx</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">mlp</span><span class="p">(</span><span class="n">ln_x</span><span class="p">)</span> <span class="o">+</span> <span class="n">residual_x</span>  <span class="c1"># from official paper &amp; code by Google Research
</span>        <span class="k">return</span> <span class="n">fx</span>
</code></pre></div></div>

<p><del>특이점은 마지막 <code class="language-plaintext highlighter-rouge">MLP Layer</code>와 <code class="language-plaintext highlighter-rouge">Residual</code> 결과를 더한 뒤, 다음 인코더 블록에 전달하기 전에 층 정규화를 한 번 더 적용한다는 것이다. 모델 모식도에는 나와 있지 않지만, 본문에 해당 내용이 실려 있다.</del>
마지막 인코더의 출력값에만 한번 더 <code class="language-plaintext highlighter-rouge">layernorm</code>을 적용한다.</p>

<h4 id="-visionencoder"><code class="language-plaintext highlighter-rouge">📚 VisionEncoder</code></h4>

<p>입력 이미지를 <code class="language-plaintext highlighter-rouge">Patch Embedding</code>으로 인코딩 하고 N개의 <code class="language-plaintext highlighter-rouge">VisionEncoderLayer</code>를 쌓기 위해 구현된 객체이다. <code class="language-plaintext highlighter-rouge">Patch Embedding</code>을 만드는 부분은 이미 위에서 설명했기 때문에 넘어가고, 인코더 블럭을 N개 쌓는 방법은 역시나 <code class="language-plaintext highlighter-rouge">nn.ModuleList</code> 를 사용하면 간편하게 구현할 수 있다. 아래 코드를 살펴보자.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">VisionEncoder</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="s">"""
    In this class, encode input sequence(Image) and then we stack N VisionEncoderLayer
    This model is implemented by cls pooling method for classification
    First, we define "positional embedding" and then add to input embedding for making patch embedding
    Second, forward patch embedding to N EncoderLayer and then get output embedding
    Args:
        num_patches: number of patches in input image =&gt; (image_size / patch_size)**2
        N: number of EncoderLayer, default 24 for large model
    """</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_patches</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">N</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">24</span><span class="p">,</span> <span class="n">dim_model</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1024</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">16</span><span class="p">,</span> <span class="n">dim_mlp</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">4096</span><span class="p">,</span> <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">VisionEncoder</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">num_patches</span> <span class="o">=</span> <span class="n">num_patches</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">positional_embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Embedding</span><span class="p">((</span><span class="bp">self</span><span class="p">.</span><span class="n">num_patches</span> <span class="o">+</span> <span class="mi">1</span><span class="p">),</span> <span class="n">dim_model</span><span class="p">)</span>  <span class="c1"># add 1 for cls token
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">num_layers</span> <span class="o">=</span> <span class="n">N</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span> <span class="o">=</span> <span class="n">dim_model</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dim_mlp</span> <span class="o">=</span> <span class="n">dim_mlp</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">dropout</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">encoder_layers</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">ModuleList</span><span class="p">(</span>
            <span class="p">[</span><span class="n">VisionEncoderLayer</span><span class="p">(</span><span class="n">dim_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">dim_mlp</span><span class="p">,</span> <span class="n">dropout</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">num_layers</span><span class="p">)]</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">layer_norm</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">dim_model</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
        <span class="n">layer_output</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">pos_x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">num_patches</span> <span class="o">+</span> <span class="mi">1</span><span class="p">).</span><span class="n">repeat</span><span class="p">(</span><span class="n">inputs</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]).</span><span class="n">to</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">dropout</span><span class="p">(</span>
            <span class="n">inputs</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">positional_embedding</span><span class="p">(</span><span class="n">pos_x</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">encoder_layers</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            <span class="n">layer_output</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">encoded_x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">layer_norm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># from official paper &amp; code by Google Research
</span>        <span class="n">layer_output</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">stack</span><span class="p">(</span><span class="n">layer_output</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">).</span><span class="n">to</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>  <span class="c1"># For Weighted Layer Pool: [N, BS, SEQ_LEN, DIM]
</span>        <span class="k">return</span> <span class="n">encoded_x</span><span class="p">,</span> <span class="n">layer_output</span>
</code></pre></div></div>
<p>마지막 층의 인코더 출력값에는 <code class="language-plaintext highlighter-rouge">layernorm</code>을 적용해줘야 함을 잊지 말자. 한편, <code class="language-plaintext highlighter-rouge">layer_output</code>는 레이어 별 어텐션 결과를 시각화 하거나 나중에 <code class="language-plaintext highlighter-rouge">WeightedLayerPool</code>에 사용하려고 만들었다.</p>
<h4 id="-visiontransformer"><code class="language-plaintext highlighter-rouge">🤖 VisionTransformer</code></h4>

<p align="center">
<img src="/assets/images/vision_transformer/model_variant.png" alt="ViT Model Variant" class="align-center image-caption" width="75%&quot;, height=&quot;50%" />
<strong><em><a href="https://arxiv.org/abs/2010.11929">ViT Model Variant</a></em></strong>
</p>

<p><code class="language-plaintext highlighter-rouge">ViT</code> 모델의 가장 최상위 객체로, 앞에서 설명한 모든 모듈들의 동작이 이뤄지는 곳이다. 사용자로부터 하이퍼파라미터를 입력 받아 모델의 크기, 깊이, 패치 크기, 이미지 임베딩 추출 방식을 지정한다. 그리고 입력 이미지를 전달받아 임베딩을 만들고 인코더에 전달한 뒤, <code class="language-plaintext highlighter-rouge">MLP Head</code> 를 통해 최종 예측 결과를 반환하는 역할을 한다.</p>

<p>이미지 임베딩 추출 방식은 <code class="language-plaintext highlighter-rouge">Linear Projection</code>과 <code class="language-plaintext highlighter-rouge">Convolution</code>이 있다. 전자가 논문에서 말하는 일반적인 <code class="language-plaintext highlighter-rouge">ViT</code>를 말하며 후자는 저자가 <code class="language-plaintext highlighter-rouge">Hybrid ViT</code>라고 따로 명명하는 모델이다. 임베딩 추출 방식 이외에 다른 차이는 전혀 없다. <code class="language-plaintext highlighter-rouge">extractor</code> 매개변수를 통해 임베딩 추출 방식을 지정할 수 있으니 아래 코드를 확인해보자.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">VisionTransformer</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="s">"""
    Main class for ViT of cls pooling, Pytorch implementation
    We implement pure ViT, Not hybrid version which is using CNN for extracting patch embedding
    input must be [BS, CHANNEL, IMAGE_SIZE, IMAGE_SIZE]
    In NLP, input_sequence is always smaller than vocab size
    But in vision, input_sequence is always same as image size, not concept of vocab in vision
    So, ViT use nn.Linear instead of nn.Embedding for input_embedding
    Args:
        num_classes: number of classes for classification task
        image_size: size of input image, default 512
        patch_size: size of patch, default 16 from official paper for ViT-Large
        extractor: option for feature extractor, default 'base' which is crop &amp; just flatten
                   if you want to use Convolution for feature extractor, set extractor='cnn' named hybrid ver in paper
        classifier: option for pooling method, default token meaning that do cls pooling
                    if you want to use mean pooling, set classifier='mean'
        mode: option for train type, default fine-tune, if you want pretrain, set mode='pretrain'
              In official paper &amp; code by Google Research, they use different classifier head for pretrain, fine-tune
    Math:
        image2sequence: [batch, channel, image_size, image_size] -&gt; [batch, patch, patch_size^2*channel]
        input_embedding: R^(P^2 ·C)×D
    Reference:
        https://arxiv.org/abs/2010.11929
        https://arxiv.org/abs/1706.03762
        https://github.com/google-research/vision_transformer/blob/main/vit_jax/models_vit.py#L184
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span>
            <span class="n">num_classes</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
            <span class="n">channels</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span>
            <span class="n">image_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">512</span><span class="p">,</span>
            <span class="n">patch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">16</span><span class="p">,</span>
            <span class="n">num_layers</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">24</span><span class="p">,</span>
            <span class="n">dim_model</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1024</span><span class="p">,</span>
            <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">16</span><span class="p">,</span>
            <span class="n">dim_mlp</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">4096</span><span class="p">,</span>
            <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
            <span class="n">extractor</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s">'base'</span><span class="p">,</span>
            <span class="n">classifier</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s">'token'</span><span class="p">,</span>
            <span class="n">mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s">'fine_tune'</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">VisionTransformer</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">num_patches</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">image_size</span> <span class="o">/</span> <span class="n">patch_size</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">num_layers</span> <span class="o">=</span> <span class="n">num_layers</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">patch_size</span> <span class="o">=</span> <span class="n">patch_size</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span> <span class="o">=</span> <span class="n">dim_model</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dim_mlp</span> <span class="o">=</span> <span class="n">dim_mlp</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">dropout</span><span class="p">)</span>

        <span class="c1"># Input Embedding
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">extractor</span> <span class="o">=</span> <span class="n">extractor</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">input_embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">((</span><span class="n">channels</span> <span class="o">*</span> <span class="n">patch_size</span><span class="o">**</span><span class="mi">2</span><span class="p">),</span> <span class="n">dim_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">conv</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Conv2d</span><span class="p">(</span>
            <span class="n">in_channels</span><span class="o">=</span><span class="n">channels</span><span class="p">,</span>
            <span class="n">out_channels</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span><span class="p">,</span>
            <span class="n">kernel_size</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">patch_size</span><span class="p">,</span>
            <span class="n">stride</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">patch_size</span>
        <span class="p">)</span>

        <span class="c1"># Encoder Multi-Head Self-Attention
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">VisionEncoder</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">num_patches</span><span class="p">,</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">num_layers</span><span class="p">,</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span><span class="p">,</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">num_heads</span><span class="p">,</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">dim_mlp</span><span class="p">,</span>
            <span class="n">dropout</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">classifier</span> <span class="o">=</span> <span class="n">classifier</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">pretrain_classifier</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">Tanh</span><span class="p">(),</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">fine_tune_classifier</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">mode</span> <span class="o">=</span> <span class="n">mode</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">any</span><span class="p">:</span>
        <span class="s">""" For cls pooling """</span>
        <span class="k">assert</span> <span class="n">inputs</span><span class="p">.</span><span class="n">ndim</span> <span class="o">!=</span> <span class="mi">4</span><span class="p">,</span> <span class="sa">f</span><span class="s">"Input shape should be [BS, CHANNEL, IMAGE_SIZE, IMAGE_SIZE], but got </span><span class="si">{</span><span class="n">inputs</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="s">"</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">inputs</span>

        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">extractor</span> <span class="o">==</span> <span class="s">'cnn'</span><span class="p">:</span>
            <span class="c1"># self.conv(x).shape == [batch, dim, image_size/patch_size, image_size/patch_size]
</span>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">conv</span><span class="p">(</span><span class="n">x</span><span class="p">).</span><span class="n">reshape</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">num_patches</span><span class="o">**</span><span class="mi">2</span><span class="p">).</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># self.extractor == 'base':
</span>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">input_embedding</span><span class="p">(</span>
                <span class="n">x</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="p">.</span><span class="n">num_patches</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">patch_size</span><span class="o">**</span><span class="mi">2</span> <span class="o">*</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
            <span class="p">)</span>
        <span class="n">cls_token</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">,</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>  <span class="c1"># can change init method
</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cat</span><span class="p">([</span><span class="n">cls_token</span><span class="p">,</span> <span class="n">x</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="n">x</span><span class="p">,</span> <span class="n">layer_output</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># output
</span>
        <span class="c1"># classification
</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">,</span> <span class="p">:]</span>  <span class="c1"># select cls token, which is position 0 in sequence
</span>        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">mode</span> <span class="o">==</span> <span class="s">'fine_tune'</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">fine_tune_classifier</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">mode</span> <span class="o">==</span> <span class="s">'pretrain'</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">fine_tune_classifier</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">pretrain_classifier</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">x</span>
</code></pre></div></div>

<p>한편, 코드에서 눈여겨봐야 할 점은 <code class="language-plaintext highlighter-rouge">MLP Head</code>로, 저자는 <code class="language-plaintext highlighter-rouge">pre-train</code> 시점과 <code class="language-plaintext highlighter-rouge">fine-tune</code> 시점에 서로 다른 <code class="language-plaintext highlighter-rouge">Classifier Head</code>를 사용한다. 전자에는 <code class="language-plaintext highlighter-rouge">Activation Function</code> 1개와 두 개의 <code class="language-plaintext highlighter-rouge">MLP Layer</code>를 사용하고, 후자에는 1개의 <code class="language-plaintext highlighter-rouge">MLP Layer</code>를 사용한다.</p>

<p>다만, <code class="language-plaintext highlighter-rouge">pretrain_classifier</code>의 입출력 차원에 대한 정확한 수치를 논문이나 official repo code를 확인해도 찾을 수 없었다, 그래서 임시로 모델의 차원과 똑같이 세팅하게 되었다.</p>

<p>또한 저자는 <code class="language-plaintext highlighter-rouge">CLS Pooling</code>과 더불어 <code class="language-plaintext highlighter-rouge">GAP</code> 방식도 제시하는데, <code class="language-plaintext highlighter-rouge">GAP</code> 방식은 추후에 따로 추가가 필요하다. 그리고 사전 훈련과 파인 튜닝 모두 분류 테스크를 수행했는데 (심지어 같은 데이터 세트를 사용함) 왜 굳이 서로 다른 <code class="language-plaintext highlighter-rouge">Classifier Head</code>를 정의했는지 의도를 알 수 없어 논문을 다시 읽어봤지만, 이유에 대해서 상세히 언급하는 부분이 없었다.</p>

<p><code class="language-plaintext highlighter-rouge">ViT</code>는 입력 임베딩을 정의하는 부분을 제외하면 저자의 의도대로 기존 트랜스포머와 동일한 모델 구조를 가졌다. 완전히 다른 데이터인 이미지와 텍스트에 같은 구조의 모델을 적용한다는 것이 정말 쉽지 않아 보였는데, 패치 개념을 만들어 자연어의 토큰처럼 간주하고 사용한 것이 의도대로 구현하는데 직관적이면서도 정말 효과적이었다고 생각한다. 이제 이렇게 만들어진 모델을 통해 진행한 여러 실험 결과에 어떤 인사이트가 담겨 있는지 알아보자.</p>

<h3 id="insight-from-experiment"><code class="language-plaintext highlighter-rouge">🔬 Insight from Experiment</code></h3>

<h4 id="insight-1-vit의-scalability-증명"><code class="language-plaintext highlighter-rouge">💡 Insight 1. ViT의 Scalability 증명</code></h4>
<ul>
  <li><strong><code class="language-plaintext highlighter-rouge">Pre-Train</code>에 사용되는 이미지 데이터 세트의 크기가 커질수록 <code class="language-plaintext highlighter-rouge">Fine-Tune Stage</code>에서 <code class="language-plaintext highlighter-rouge">ViT</code>가 <code class="language-plaintext highlighter-rouge">CNN</code>보다 높은 성능</strong></li>
  <li><strong>같은 성능이라면 <code class="language-plaintext highlighter-rouge">ViT</code>가 상대적으로 적은 연산량을 기록</strong></li>
</ul>

<p align="center">
<img src="/assets/images/vision_transformer/insight1.png" alt="Performance per Dataset Scale" class="align-center image-caption" width="75%&quot;, height=&quot;50%" />
<strong><em><a href="https://arxiv.org/abs/2010.11929">Performance per Dataset Scale</a></em></strong>
</p>

<p>위 도표는 <code class="language-plaintext highlighter-rouge">Pre-Train Stage</code>에 사용된 이미지 데이터 세트에 따른 모델의 <code class="language-plaintext highlighter-rouge">Fine-Tune</code> 성능 추이를 나타낸 자료다. 사전 훈련 데이터 스케일이 크지 않을 때는 <code class="language-plaintext highlighter-rouge">Conv</code> 기반의 <code class="language-plaintext highlighter-rouge">ResNet</code> 시리즈가 <code class="language-plaintext highlighter-rouge">ViT</code> 시리즈를 압도하는 모습을 보여준다. 하지만 데이터 세트의 크기가 커질수록 점점 <code class="language-plaintext highlighter-rouge">ViT</code> 시리즈의 성능이 <code class="language-plaintext highlighter-rouge">ResNet</code>을 능가하는 결과를 볼 수 있다.</p>

<p>한편, ViT &amp; ResNet 성능 결과 모두 ImageNet과 JFT-Image로 사전 훈련 및 파인 튜닝을 거쳐 나왔다고 하니 참고하자. <strong><u>추가로 파인 튜닝 과정에서 사전 훈련 때보다 이미지 사이즈를 키워서 훈련을 시켰다고 논문에서 밝히고 있는데, 이는 저자의 실험 결과에 기인한 것이다</u></strong>. 논문에 따르면 파인 튜닝 때 사전 훈련 당시보다 더 높은 해상도의 이미지를 사용하면 성능이 향상 된다고 하니 기억했다가  써먹어보자.</p>

<p align="center">
<img src="/assets/images/vision_transformer/insight1_2.png" alt="Performance per FLOPs Scale" class="align-center image-caption" width="75%&quot;, height=&quot;50%" />
<strong><em><a href="https://arxiv.org/abs/2010.11929">Performance per FLOPs</a></em></strong>
</p>

<p>위 도표는 연산량 변화에 따른 모델의 성능 추이를 나타낸 그림이다. 두 지표 모두 같은 점수라면 <code class="language-plaintext highlighter-rouge">ViT</code> 시리즈의 연산량이 현저히 적음을 알 수 있다. 또한 정확도 95% 이하 구간에서 같은 성능이라면  <code class="language-plaintext highlighter-rouge">ViT</code>의 <code class="language-plaintext highlighter-rouge">Hybrid</code> 버전 모델의 연산량이 일반 <code class="language-plaintext highlighter-rouge">ViT</code> 버전보다 현저히 적음을 확인할 수 있다. 이러한 사실은 추후에 <code class="language-plaintext highlighter-rouge">Swin-Transformer</code> 설계에 영감을 준다.</p>

<p>두 개의 실험 결과를 종합했을 때, <code class="language-plaintext highlighter-rouge">ViT</code>가 <code class="language-plaintext highlighter-rouge">ResNet</code>보다 일반화 성능이 더 높으며(도표 1) 모델의 <code class="language-plaintext highlighter-rouge">Saturation</code> 현상이 두드러지지 않아 성능의 한계치(도표 2) 역시 더 높다고 볼 수 있다. 따라서 기존 트랜스포머의 연산•구조적 측면에서 <code class="language-plaintext highlighter-rouge">Scalability</code>를 성공적으로 이식했다고 평가할 수 있겠다.</p>

<h4 id="insight-2-pure-self-attention은-좋은-이미지-피처를-추출하기에-충분하다"><code class="language-plaintext highlighter-rouge">💡 Insight 2. Pure Self-Attention은 좋은 이미지 피처를 추출하기에 충분하다</code></h4>
<ul>
  <li><strong>Patch Embedding Layer의 PCA 결과, 패치의 기저가 되는 차원과 유사한 모양을 추출</strong>
    <ul>
      <li><strong><code class="language-plaintext highlighter-rouge">Convolution</code> 없이 <code class="language-plaintext highlighter-rouge">Self-Attention</code>만으로도 충분히 이미지의 좋은 피처를 추출하는 것이 가능</strong></li>
      <li><strong><code class="language-plaintext highlighter-rouge">Vision</code>에서 <code class="language-plaintext highlighter-rouge">Convolution</code>에 대한 <code class="language-plaintext highlighter-rouge">reliance</code> 탈피 가능</strong></li>
    </ul>
  </li>
</ul>

<p align="center">
<img src="/assets/images/vision_transformer/insight2.png" alt="Patch Embedding Layer’s Filter" class="align-center image-caption" width="75%&quot;, height=&quot;50%" />
<strong><em><a href="https://arxiv.org/abs/2010.11929">Patch Embedding Layer’s Filter</a></em></strong>
</p>

<p>위 자료는 충분한 학습을 거치고 난 <code class="language-plaintext highlighter-rouge">ViT</code>의 <code class="language-plaintext highlighter-rouge">Patch Embedding Layer</code>의 필터를 <code class="language-plaintext highlighter-rouge">PCA</code>한 결과 중에서 특잇값이 높은 상위 28개의 피처를 나열한 그림이다. 이미지의 기본 뼈대가 되기에 적합해 보이는 피처들이 추출된 모습을 볼 수 있다.</p>

<p>따라서 <code class="language-plaintext highlighter-rouge">Inductive Bias</code> 없이, 단일  <code class="language-plaintext highlighter-rouge">Self-Attention</code>만으로 이미지의 피처를 추출하는 것이 충분히 가능하다. 비전 분야에 만연한 <code class="language-plaintext highlighter-rouge">Convolution</code> 의존에서 벗어나 새로운 아키텍처의 도입이 가능함을 시사한 부분이라고 할 수 있겠다.</p>

<h4 id="insight-3-bottom2general-information-top2specific-information"><code class="language-plaintext highlighter-rouge">💡 Insight 3. Bottom2General Information, Top2Specific Information</code></h4>

<ul>
  <li><strong><code class="language-plaintext highlighter-rouge">입력</code>과 가까운 인코더일수록 <code class="language-plaintext highlighter-rouge">Global &amp; General</code>한 Information을 포착</strong></li>
  <li><strong><code class="language-plaintext highlighter-rouge">출력</code>과 가까운 인코더일수록 <code class="language-plaintext highlighter-rouge">Local &amp; Specific</code>한 Information을 포착</strong></li>
</ul>
<p align="center">
<img src="/assets/images/vision_transformer/insight3.png" alt="Multi-Head Attention Distance per Network Depth" class="align-center image-caption" width="75%&quot;, height=&quot;50%" />
<strong><em><a href="https://arxiv.org/abs/2010.11929">Multi-Head Attention Distance per Network Depth</a></em></strong>
</p>

<p>다음 자료는 인코더의 개수 변화에 따른 개별 어텐션 해드의 어텐션 거리 변화 추이를 나타낸 그림이다. 여기서 어텐션 거리란, 해드가 얼마나 멀리 떨어진 패치를 어텐션했는지 픽셀 단위로 표현한 지표다. 해당 값이 높을수록 거리상 멀리 떨어진 패치와 어텐션을, 작을수록 가까운 패치와 어텐션 했다는 것을 의미한다. 다시 도표를 살펴보자. 입력과 가까운 인코더일수록(Depth 0) 해드별 어텐션 거리의 분산이 커지고, 출력과 가까운 인코더일수록(Depth 23) 분산이 점자 줄어들다가 거의 한 점에 수렴하는듯한 양상을 보여준다. 다시 말해, 입력과 가까운 <code class="language-plaintext highlighter-rouge">Bottom Encoder</code>는 멀리 떨어진 패치부터 가까운 패치까지 모두 전역적(<code class="language-plaintext highlighter-rouge">Global</code>)으로 어텐션을 수행해 <code class="language-plaintext highlighter-rouge">General</code> 한 정보를 포착하게 되고 출력과 가까운 <code class="language-plaintext highlighter-rouge">Top Encoder</code>는 개별 해드들이 모두 비슷한 거리에 위치한 패치(<code class="language-plaintext highlighter-rouge">Local</code>)에 어텐션을 수행해 <code class="language-plaintext highlighter-rouge">Specific</code> 한 정보를 포착하게 된다.</p>

<p>이 때 <code class="language-plaintext highlighter-rouge">Global</code>과 <code class="language-plaintext highlighter-rouge">Local</code>이라는 용어 때문에 <code class="language-plaintext highlighter-rouge">Bottom Encoder</code> 는 멀리 떨어진 패치와 어텐션하고, <code class="language-plaintext highlighter-rouge">Top Encoder</code>는 가까운 패치와 어텐션한다고 착각하기 쉽다. <strong><u>그러나 개별 해드들의 어텐션 거리가 얼마나 분산되어 있는가가 바로 </u></strong><code class="language-plaintext highlighter-rouge">Global</code>, <code class="language-plaintext highlighter-rouge">Local</code><strong><u>을 구분하는 기준이 된다.</u></strong> 입력부에 가까운 레이어들은 헤드들의 어텐션 거리 분산이 매우 큰 편인데, 이것을 이패치 저패치 모두 어텐션 해보고 비교해본다고 해석해서 <code class="language-plaintext highlighter-rouge">Global</code>이라고 부르고, 출력부에 가까운 레이어는 헤드들의 어텐션 거리 분산이 매우 작은 편인데, 이게 바로 각각의 헤드들이 어떤 정보에 주목해야할지(분류 손실이 가장 작아지는 패치) 범위를 충분히 좁힌 상태에서 특정 부분에만 집중한다는 의미로 해석해 <code class="language-plaintext highlighter-rouge">Local</code> 이라고 부르게 되었다.</p>

<p>&lt;<strong><a href="https://arxiv.org/abs/2006.05987">Revisiting Few-sample BERT Fine-tuning</a></strong>&gt;도 위와 비슷한 맥락의 사실에 대해 언급하고 있으니 참고해보자. 이러한 사실은 트랜스포머 인코더 계열 모델을 튜닝할 때 <code class="language-plaintext highlighter-rouge">Depth</code> 별로 다른 <code class="language-plaintext highlighter-rouge">Learning Rate</code>을 적용하는 <code class="language-plaintext highlighter-rouge">Layerwise Learning Rate Decay</code> 의 초석이 되기도 한다. <code class="language-plaintext highlighter-rouge">Layerwise Learning Rate Decay</code> 에 대해서는 <strong><a href="https://towardsdatascience.com/advanced-techniques-for-fine-tuning-transformers-82e4e61e16e">여기 포스트</a></strong>를 참고하도록 하자.</p>

<p align="center">
<img src="/assets/images/vision_transformer/insight3_2.png" alt="Output from Last Encoder" class="align-center image-caption" width="40%&quot;, height=&quot;10%" />
<strong><em><a href="https://arxiv.org/abs/2010.11929">Output from Last Encoder</a></em></strong>
</p>

<p>한편 논문에는 언급되지 않은, 필자의 뇌피셜에 가깝지만, <strong><u>출력에 가까운 인코더들의 해드가 가진</u></strong> <code class="language-plaintext highlighter-rouge">Attention Distance</code><strong><u>이 모두 비슷하다는 사실로 이미지 분류에 결정적인 역할을 하는 피처가 이미지의 특정 구역에 모여 있으며, 그 스팟은 이미지의 중앙 부근일 가능성이 높다고 추측 해볼 수 있다.</u></strong> 모든 해드의 픽셀 거리가 서로 비슷하려면 일단 비슷한 위치의 패치에 어텐션을 해야하기 때문에 분류 손실값을 최소로 줄여주는 피처는 보통 한 구역(패치)에 몰려 있을 것이라고 유추가 가능하다. 또한 특정 스팟이 중앙에 위치할수록 어텐션 거리의 분산이 줄어들것이라고 생각 해볼 수도 있었다. 저자는 <code class="language-plaintext highlighter-rouge">Attention Rollout</code>이라는 개념을 통해 <code class="language-plaintext highlighter-rouge">Attention Distance</code>을 산출했다고 언급하는데, 자세한 내용은 옆에 두 링크를 참고해보자(<a href="https://hongl.tistory.com/234">한국어 설명 블로그</a>,  <a href="https://arxiv.org/abs/2005.00928">원논문</a>). 이러한 필자의 가설이 맞다면, <code class="language-plaintext highlighter-rouge">Convolution</code> 의 <code class="language-plaintext highlighter-rouge">Inductive Bias</code>  중 <code class="language-plaintext highlighter-rouge">Locality</code> 의 효과성을 <code class="language-plaintext highlighter-rouge">Self-Attention</code>을 통해 입증이 가능하며, 반대로 <code class="language-plaintext highlighter-rouge">Convolution</code>에 대한 의존에서 벗어나 단일 <code class="language-plaintext highlighter-rouge">Self-Attention</code> 으로도 같은 효과를 낼 수 있다는 증거 중 하나가 될 것이다.</p>

<h4 id="insight-4-vit는-cls-pooling-사용하는게-효율적"><code class="language-plaintext highlighter-rouge">💡 Insight 4. ViT는 CLS Pooling 사용하는게 효율적</code></h4>

<ul>
  <li><strong><code class="language-plaintext highlighter-rouge">CLS Pooling</code>은 <code class="language-plaintext highlighter-rouge">GAP</code> 보다 2배 이상 큰 학습률을 사용해도 비슷한 성능을 기록</strong>
    <ul>
      <li><strong><u>학습 속도는 더 빠르되 성능이 비슷하기 때문에</u></strong> <code class="language-plaintext highlighter-rouge">CLS Pooling</code> <strong><u>이 더 효율적</u></strong></li>
    </ul>
  </li>
</ul>

<p align="center">
<img src="/assets/images/vision_transformer/insight4.png" alt="Performance Trend by Pooling Method with LR" class="align-center image-caption" width="75%&quot;, height=&quot;50%" />
<strong><em><a href="https://arxiv.org/abs/2010.11929">Performance Trend by Pooling Method with LR</a></em></strong>
</p>

<p>다음 도표는 풀링 방식과 학습률의 변동에 따른 정확도 변화 추이를 나타낸 그림이다. 비슷한 성능이라면 <code class="language-plaintext highlighter-rouge">CLS Pooling</code>이 <code class="language-plaintext highlighter-rouge">GAP</code>보다 2배 이상 큰 학습률을 사용했다. 학습률이 크면 모델의 수렴 속도가 빨라져 학습 속도가 빨라지는 장점이 있다. 그런데 성능까지 비슷하다면 <code class="language-plaintext highlighter-rouge">ViT</code>는 <code class="language-plaintext highlighter-rouge">CLS Pooling</code>을 사용하는 것이 더 효율적이라고 할 수 있겠다.</p>

<p>나중에 시간이 된다면 다른 풀링 방식, 예를 들면 <code class="language-plaintext highlighter-rouge">Weighted Layer Pooling</code>, <code class="language-plaintext highlighter-rouge">GeM Pooling</code>, <code class="language-plaintext highlighter-rouge">Attention Pooling</code> 같은 것을 적용해 실험해보겠다.</p>

<h4 id="insight-5-vit는-absolute-1d-position-embedding-사용하는게-가장-효율적"><code class="language-plaintext highlighter-rouge">💡 Insight 5. ViT는 Absolute 1D-Position Embedding 사용하는게 가장 효율적</code></h4>

<ul>
  <li><strong>어떤 형태로든 위치 임베딩 값을 정의해준다면, 형태와 종류에 상관없이 거의 비슷한 성능을 보임</strong></li>
  <li><strong>성능이 비슷하면, 직관적이고 구현이 간편한 <code class="language-plaintext highlighter-rouge">Absolute 1D-Position Embedding</code> 방법을 사용하는 것이 가장 효율적</strong></li>
  <li><strong><code class="language-plaintext highlighter-rouge">ViT</code>는 <code class="language-plaintext highlighter-rouge">Patch-Level</code> 사용해, <code class="language-plaintext highlighter-rouge">Pixel-Level</code>보다 상대적으로 시퀀스 길이가 짧아 위치•공간 정보를 인코딩하는 방식에 영향을 덜 받음</strong></li>
</ul>

<p align="center">
<img src="/assets/images/vision_transformer/insight5.png" alt="Performance Table by making Position Embedding method" class="align-center image-caption" width="75%&quot;, height=&quot;50%" />
<strong><em><a href="https://arxiv.org/abs/2010.11929">Performance Table by making Position Embedding method</a></em></strong>
</p>

<p>위 실험 결과는 <code class="language-plaintext highlighter-rouge">Position Embedding</code> 인코딩 방식에 따른 <code class="language-plaintext highlighter-rouge">ViT</code> 모델의 성능 변화 추이를 나타낸 자료다. 인코딩 형태와 상관없이 위치 임베딩의 유무가 성능에 큰 영향을 미친다는 사실을 알려주고 있다. 한편, 인코딩 형태 변화에 따른 유의미한 성능 변화는 없었다. 하지만 <code class="language-plaintext highlighter-rouge">Absolute 1D-Position Embedding</code>의 컨셉이 가장 직관적이며 구현하기 편하고 연산량이 다른 인코딩보다 적다는 것을 감안하면 ViT에 가장 효율적인 위치 임베딩 방식이라고 판단할 수 있다.</p>

<p>논문은 결과에 대해 <code class="language-plaintext highlighter-rouge">ViT</code>가 사용하는 <code class="language-plaintext highlighter-rouge">Patch-Level Embedding</code>이 <code class="language-plaintext highlighter-rouge">Pixel-Level</code>보다 상대적으로 짧은 시퀀스 길이를 갖기 때문이라고 설명한다. 예를 들어 <code class="language-plaintext highlighter-rouge">224x224</code> 사이즈의 이미지를 <code class="language-plaintext highlighter-rouge">16x16</code> 사이즈의 패치 여러장으로 만든다고 생각해보자. 임베딩 차원에 들어가는 $N$ 은 $(224/16)^2$ , 즉 <code class="language-plaintext highlighter-rouge">196</code>이 된다. 한편 이것을 <code class="language-plaintext highlighter-rouge">Pixel-Level</code>로 임베딩 하게 되면 $224^2$, 즉 <code class="language-plaintext highlighter-rouge">50176</code> 개의 시퀀스가 생긴다. 따라서 <code class="language-plaintext highlighter-rouge">Pixel-Level</code> 에 비하면 훨씬 짧은 시퀀스 길이를 갖기 때문에 <code class="language-plaintext highlighter-rouge">Absolute 1D-Position Embedding</code> 만으로도 충분히 <code class="language-plaintext highlighter-rouge">Spatial Relation</code>을 학습할 수 있는 것이다.</p>

<p align="center">
<img src="/assets/images/vision_transformer/insight5_2.png" alt="Absolute 1D-Position Embedding" class="align-center image-caption" width="50%&quot;, height=&quot;50%" />
<strong><em><a href="https://arxiv.org/abs/2010.11929">Absolute 1D-Position Embedding</a></em></strong>
</p>

<p>하지만, 필자는 자연어 처리의 <code class="language-plaintext highlighter-rouge">Transformer-XL</code>, <code class="language-plaintext highlighter-rouge">XLNet</code>, <code class="language-plaintext highlighter-rouge">DeBERTa</code> 같은 모델들이 <code class="language-plaintext highlighter-rouge">Relative Position Embedding</code> 방식을 적용해 큰 성공을 거둔 바가 있다는 점을 생각하면 이런 결과가 납득이 가면서도 의아했다.</p>

<p>저자는 실험에 사용한 모든 데이터 세트를 <code class="language-plaintext highlighter-rouge">224x224</code>로 <code class="language-plaintext highlighter-rouge">resize</code> 했다고 밝히고 있는데, 만약 이미지 사이즈가 <code class="language-plaintext highlighter-rouge">512x512</code>정도만 되더라도 $N$ 값이 <code class="language-plaintext highlighter-rouge">1024</code> 이라서 위 결과와 상당히 다른 양상이 나타나지 않을까 하는 생각이 든다. 추후에 시간이 된다면 이 부분도 꼭 실험해봐야겠다. 예측컨데 이미자 사이즈가 커질수록 <code class="language-plaintext highlighter-rouge">2D Position Embedding</code> 혹은 <code class="language-plaintext highlighter-rouge">Relative Position Embedding</code>이 더 효율적일 것이라 예상한다.</p>

<h3 id="️conclusion"><code class="language-plaintext highlighter-rouge">🧑‍⚖️ Conclusion</code></h3>

<p>이렇게 <code class="language-plaintext highlighter-rouge">ViT</code> 모델을 제안한 <a href="https://arxiv.org/abs/2010.11929">&lt;An Image Is Worth 16x16 Words: Transformers For Image Recognition At Scale&gt;</a>에 실린 내용을 모두 살펴보았다. <code class="language-plaintext highlighter-rouge">Conv</code> 에 대한 의존을 탈피 했다는 점에서 매우 의미가 있는 시도였으며, Self-Attention &amp; Transformer 구조 채택만으로도 컴퓨터 비전 영역에 어느 정도  <code class="language-plaintext highlighter-rouge">scalability</code> 를  이식하는데 성공했다는 점에서 후대 연구에 중요한 시사점을 남겼다. 상대적으로 정체(??)되어 있던 비전 영역이 성능의 한계를 한단계 뛰어넘을 수 있는 초석을 마련해준 셈이다.</p>

<p>하지만, <code class="language-plaintext highlighter-rouge">ViT</code>의 <code class="language-plaintext highlighter-rouge">Pretrain Stage</code>에 적합한 <code class="language-plaintext highlighter-rouge">Self-Supervised Learning</code> 방법을 찾지 못해 여전히 <code class="language-plaintext highlighter-rouge">Supervised Learning</code> 방식을 채택한 점은 매우 아쉬웠다. <strong><u>이는 결국 데이터</u></strong> <code class="language-plaintext highlighter-rouge">Scale</code> <strong><u>확장에 한계를 의미하기 때문이다.</u></strong> 오늘날 BERT와 GPT의 성공 신화는 비단 <code class="language-plaintext highlighter-rouge">Self-Attention</code>와 <code class="language-plaintext highlighter-rouge">Transformer</code>의 구조적 탁월성에 의해서만 탄생한게 아니다. 이에 못지 않게(개인적으로 제일 중요하다 생각) 주요했던 것이 바로 데이터 <code class="language-plaintext highlighter-rouge">Scale</code> 확장이다.  <code class="language-plaintext highlighter-rouge">MLM</code>, <code class="language-plaintext highlighter-rouge">AR</code> 등의 <code class="language-plaintext highlighter-rouge">Self-Supervised Learning</code> 덕분에 데이터 <code class="language-plaintext highlighter-rouge">Scale</code>을 효율적으로 스케일 업 시킬 수 있었고, 사전 훈련 데이터의 증가는 모델 깊이, 너비, 차원까지 더욱 크케 키우는데 기여했다.</p>

<p>또한 <code class="language-plaintext highlighter-rouge">ViT</code>는 선천적으로 <code class="language-plaintext highlighter-rouge">Patch-Level Embedding</code>을 사용하기 때문에 다양한 이미지 테스크에 적용하는 것이 힘들다. <code class="language-plaintext highlighter-rouge">Segmentation</code>, <code class="language-plaintext highlighter-rouge">Object Detection</code> 같은 Task는 픽셀 단위로 예측을 수행해 객체를 탐지하거나 분할해야 한다. 하지만 <code class="language-plaintext highlighter-rouge">Patch</code> 단위로 훈련을 수행했던 <code class="language-plaintext highlighter-rouge">ViT</code>는 <code class="language-plaintext highlighter-rouge">Pixel</code> 단위의 예측을 수행하는데 어려움을 겪는다.</p>

<p>마지막으로 <code class="language-plaintext highlighter-rouge">Self-Attention</code> 자체의 <code class="language-plaintext highlighter-rouge">Computational Overhead</code>가 너무 심해 고해상도의 이미지를 적절히 다루기 힘들다. 위에서도 언급했지만 이미지의 사이즈가 <code class="language-plaintext highlighter-rouge">512x512</code>만 되어도 이미 패치의 개수가 <code class="language-plaintext highlighter-rouge">1024</code>가 된다. 사이즈가 커질수록 시퀀스 길이 역시 기하급수적으로 커지는데다가 <code class="language-plaintext highlighter-rouge">Self-Attention</code> 는 쿼리와 키 행렬을 내적 (자기 자신과 곱이라 볼 수 있음) 하기 때문에 <code class="language-plaintext highlighter-rouge">Computational Overhead</code>가 $N^2$이 된다.</p>

<p>필자는 <code class="language-plaintext highlighter-rouge">ViT</code>를 절반의 성공이라고 평하고 싶다. 본래 <code class="language-plaintext highlighter-rouge">ViT</code>의 설계 목적은 비전 분야의 <code class="language-plaintext highlighter-rouge">Conv</code>에 대한 의존을 탈피하면서, 퓨어한 <code class="language-plaintext highlighter-rouge">Self-Attention</code>을 도입해 <code class="language-plaintext highlighter-rouge">Scalabilty</code> 를 이식하는 것이었다. <code class="language-plaintext highlighter-rouge">Self-Attention</code>을 도입하는데는 성공했지만, 여전히 다룰 수 있는 이미지 사이즈나 Task에는 한계가 분명하며 결정적으로 <code class="language-plaintext highlighter-rouge">Self-Supervised Learning</code> 방식을 도입하지 못했다. <code class="language-plaintext highlighter-rouge">Scalabilty</code> 라는 단어의 의미를 생각하면, 방금 말한 부분에서까지 확장성이 있어야 설계 의도에 부합하는 결과라고 생각한다.</p>]]></content><author><name>qcqced</name><email>qcqced123@gmail.com</email></author><category term="Computer Vision" /><category term="Computer Vision" /><category term="Vision Transformer" /><category term="ViT" /><category term="Transformer" /><category term="Self-Attention" /><category term="Image Classification" /><summary type="html"><![CDATA[ViT Official Paper Review with Pytorch Implementation]]></summary></entry><entry><title type="html">🔢 Vector Space: Column Space, Basis, Rank, Null Space</title><link href="http://localhost:4000/linear-algebra/vector-subspace" rel="alternate" type="text/html" title="🔢 Vector Space: Column Space, Basis, Rank, Null Space" /><published>2023-07-19T00:00:00+09:00</published><updated>2023-07-10T13:00:00+09:00</updated><id>http://localhost:4000/linear-algebra/vector_space</id><content type="html" xml:base="http://localhost:4000/linear-algebra/vector-subspace"><![CDATA[<h3 id="-column-space"><code class="language-plaintext highlighter-rouge">🔢 Column Space</code></h3>

\[C(A) = Range(A)\]

<p>열벡터가 <code class="language-plaintext highlighter-rouge">span</code>하는 공간을 의미한다. <code class="language-plaintext highlighter-rouge">span</code> 이란, 벡터의 집합에 의해 생성된 모든 <code class="language-plaintext highlighter-rouge">linear combination</code>의 결과로 생성할 수 있는 부분 공간을 말한다. 따라서 <code class="language-plaintext highlighter-rouge">column space</code> 는 열벡터의 <code class="language-plaintext highlighter-rouge">linear combination</code> 결과로 생성할 수 있는 <code class="language-plaintext highlighter-rouge">vector space</code>의 부분 공간을 말한다.</p>
<h3 id="-basis"><code class="language-plaintext highlighter-rouge">🍖 Basis</code></h3>
<figure class="half">
  <a href="https://twlab.tistory.com/24"><img src="/assets/images/linear_independent.png" title="Linear Independent" /></a>
  <a href="https://twlab.tistory.com/24"><img src="/assets/images/linear_dependent.png" title="Linear Independent" /></a>
</figure>
<p>기저에 대해 알기 위해서는 먼저 <code class="language-plaintext highlighter-rouge">linear independent(선형 독립)</code>의 의미를 알아야 한다. 선형독립이란, 왼쪽 그림처럼 서로 다른 벡터들이 관련성 없이 독립적으로 존재하는 상태를 말한다. 따라서 서로 다른 두 벡터가 선형 독립이라면 한 벡터의 선형조합으로 다른 벡터를 표현할 수 없다. 반대로 선형 종속 상태면 오른쪽 그림처럼 벡터를 다른 벡터의 선형조합으로 표현 가능하다.</p>

<p>이제 기저에 대해 알아보자. 기저란 선형 독립이면서 벡터 공간을 <code class="language-plaintext highlighter-rouge">span</code> 하는 벡터 집합을 말한다. 다시 말해, 공간 또는 차원을 표현하는데 필요한 요소들의 집합이라고 볼 수 있다. 예를 들어 2차원 공간을 표현하고 싶다면 서로 선형 독립인 벡터 2개가 필요하다. 오른쪽 그림처럼 벡터 2개가 존재해도 서로 종속 관계라면 표현(span)할 수 있는 공간은 1차원의 직선이 되기 때문이다. 정리하면, $N$차원 공간의 기저란 선형 독립이면서 벡터 공간을 <code class="language-plaintext highlighter-rouge">span</code>하는 벡터가 $N$개 있는 상태다. 추가로, $N$차원 공간의 기저는<code class="language-plaintext highlighter-rouge">NxN</code> 크기의 <code class="language-plaintext highlighter-rouge">Invertable</code>한 행렬과 동치를 이룬다. 뒤에서 더 자세히 다루겠지만 역행렬은 좌표평면 상에서 <code class="language-plaintext highlighter-rouge">reverse linear combination</code> 의 역할을 하기 때문이다.<br />
한편 기저는 유일하지 않다. 위에서 언급한 $N$차원 기저의 필요충분조건을 만족하는 모든 벡터 집합은 모두 기저가 될 수 있다.</p>

<h3 id="-standard-basis"><code class="language-plaintext highlighter-rouge">🦴 Standard Basis</code></h3>

\[I= 
   \begin{pmatrix} 
   1 &amp; 0 &amp; 0  \\
   0 &amp; 1 &amp; 0  \\
   0 &amp; 0 &amp; 1  \\
   \end{pmatrix}\]

<p>표준 기저란, 기저가 표현하는 차원의 축이 우리가 흔히 아는 <code class="language-plaintext highlighter-rouge">x축, y축, z축</code> 이 되는 기저 벡터를 말한다. 수학적으로는 주대각성분의 값이 모두 1인 대각행렬 $D$, 즉 단위 행렬 $I$가 기저일 때 우리는 표준 기저라고 정의한다.</p>

<h3 id="-rank"><code class="language-plaintext highlighter-rouge">🧮 Rank</code></h3>

<p align="center">
<img src="/assets/images/column_space.png" alt="Column Space Image" class="align-center image-caption" width="100%&quot;, height=&quot;50%" />
<strong><em><a href="https://www.researchgate.net/figure/Example-of-a-projection-of-a-matrix-3-2-on-the-column-space_fig2_220103928">Column Space Image</a></em></strong>
</p>

<p>행렬에서 <code class="language-plaintext highlighter-rouge">independent</code>한 <code class="language-plaintext highlighter-rouge">column</code>의 개수를 의미하며, 기하학적으로는 <code class="language-plaintext highlighter-rouge">column space</code>가 실제 <code class="language-plaintext highlighter-rouge">span</code>하는 공간의 차원을 말한다. <code class="language-plaintext highlighter-rouge">Rank Theorem</code> 에 의해, 행렬 $A$ column vector는 행렬 $A^T$의 row vector와 같다. 따라서 column rank와 row rank 값 역시 항상 동일하다. 행렬 $A$의 랭크는 $rank(A)$로 표기한다.</p>

<p>행렬의 랭크는 행렬의 생김새에 따라 부르는 명칭이 조금씩 바뀐다. 예를 들어 열벡터가 모두 선형 독립이면서 크기가 <code class="language-plaintext highlighter-rouge">10x3</code> 인 행렬 $C$가 있다고 가정해보자. 모든 열벡터가 선형 독립이기 때문에 우리는 행렬 $C$의 랭크가 3이라는 것을 알 수 있다. 이 때 행렬 $C$를  <code class="language-plaintext highlighter-rouge">full-column rank</code> 라고 부른다. 그리고 행벡터의 랭크 역시 랭크 정리 이론에 의해 3이 될 것이다. 이번에는 행렬 $C$의 열벡터 랭크가 2라고 가정해보자. 우리는 이 때 행렬 $C$를 <code class="language-plaintext highlighter-rouge">rank-deficient</code>로 정의한다. 만약 행렬 $C$의 열벡터가 모두 선형독립이고 그 크기가 <code class="language-plaintext highlighter-rouge">10x10</code>이라면 뭐라고 부를까?? 이 때는 열벡터, 행벡터 모두 랭크가 10이 되기 때문에 <code class="language-plaintext highlighter-rouge">full-rank</code> 라고 부른다.</p>

<p>정리하면 행렬의 랭크란, 행렬의 행의 크기 M 그리고 열의 크기 N 중에서 더 작은값보다 같거나 작으면서 <code class="language-plaintext highlighter-rouge">independent</code>한 <code class="language-plaintext highlighter-rouge">column</code>의 개수라는 의미를 내포한 개념이라고 볼 수 있겠다.</p>

<p>추가로, column vector와 row vector를 순서대로 곱하면 항상 $Rank = 1$인 행렬 $A$가 만들어진다는 것이다. 그렇게 만들어진 행렬의 원소가 두 벡터의 <code class="language-plaintext highlighter-rouge">linear combination</code>  으로 구성된 것이라서 당연한 소리라고 생각할 수 있지만, 이것은 선형대수학에서 매우 중요한 성질이 된다. 뒤집어서 보면 어떤 행렬의 $Rank=1$이라는 것은 그 행렬이 어떤 다른 행렬의 기본 단위 요소가 된다는 의미이기 때문이다. 어떤 행렬의 랭크가 4라는 것은 랭크 1짜리 행렬 4개의 조합이라고 생각해볼 수 있다.</p>

<h3 id="-null-space"><code class="language-plaintext highlighter-rouge">👌 Null Space</code></h3>

\[Ax=0\]

<p>위 수식을 만족하는 벡터 $x$의 집합을 말한다. 다시 말해, 선형 변환 $A$(크기가 MxN인 행렬)를 통해 0이 되는 벡터 집합 $x$가 바로 <code class="language-plaintext highlighter-rouge">null space(영공간)</code>이다. 영공간은 선형변환 $A$의 랭크와 무관하며 선형변환 A의 열차원인 $R^N$상에 존재하는 공간이다. 그래서 $Ax=0$을 행렬과 벡터의 내적으로 해석하면 영공간은 선형변환 $A$의 row space와 수직이다라는 사실을 알 수 있다.</p>

\[N_A = dim(Null(A)) - rank(A)\]

<p>한편, 영공간이 <code class="language-plaintext highlighter-rouge">span</code> 하는 공간의 차원과 션형변환 $A$의 랭크를 더하면 션형변환 $A$의 열차원을 알 수 있다. 수식으로 표현하면 다음과 같다.</p>

<h3 id="-left-null-space"><code class="language-plaintext highlighter-rouge">🫲 Left Null Space</code></h3>

\[A^Tx=0\]

<p>선형변환 $A$의 크기가 MxN일 때, $A$의 좌 영공간은 $A$의 모든 열들에 대해 선형 조합으로 0 벡터(영벡터)가 되는 모든 벡터 집합 $x$의 공간을 <code class="language-plaintext highlighter-rouge">좌영공간</code>이라고 한다. $A$의 열벡터에 대한 영공간이라는 것이 포인트가 된다. 따라서 좌영공간은 선형변환 $A$의 전치행렬인 $A^T$의 영공간을 구하는 것과 같으며, 선형변환 $A$의 column space와 수직하게 된다.</p>]]></content><author><name>qcqced</name><email>qcqced123@gmail.com</email></author><category term="Linear Algebra" /><category term="Linear Algebra" /><category term="linear independent" /><category term="vector space" /><category term="rank" /><category term="column space" /><category term="null space" /><category term="basis" /><summary type="html"><![CDATA[💡 Concept of main sub-space]]></summary></entry><entry><title type="html">🎲 RuntimeError: CUDA error: device-side assert triggered</title><link href="http://localhost:4000/framework-library/mismatch-dimension" rel="alternate" type="text/html" title="🎲 RuntimeError: CUDA error: device-side assert triggered" /><published>2023-07-17T00:00:00+09:00</published><updated>2023-07-18T07:00:00+09:00</updated><id>http://localhost:4000/framework-library/dim_mismatch</id><content type="html" xml:base="http://localhost:4000/framework-library/mismatch-dimension"><![CDATA[<h3 id="-사전에-정의-입출력-차원--실제-입출력-차원"><code class="language-plaintext highlighter-rouge">😵 사전에 정의 입출력 차원 ≠ 실제 입출력 차원</code></h3>

<p>다양한 원인이 있다고 알려져 있는 에러지만, 필자의 경우 위 에러는 사전에 정의한 데이터의 입출력 차원과 실제 입출력 데이터 차원이 서로 상이할 때 발생했다. 하지만 원인을 확실히 특정하고 싶다면 아래 예시 코드를 먼저 추가한 뒤, 다시 한 번 에러 로그를 확인해보길 권장한다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">os</span>
<span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="s">'CUDA_LAUNCH_BLOCKING'</span><span class="p">]</span> <span class="o">=</span> <span class="s">"1"</span>
<span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="s">"CUDA_VISIBLE_DEVICES"</span><span class="p">]</span> <span class="o">=</span> <span class="s">"0"</span>
</code></pre></div></div>
<p>예시 코드처럼 환경변수를 추가하면 에러가 어느 부분에서 발생했는지 로그가 좀 더 구체적으로 나온다. 거의 대부분이 입출력 차원 문제일테니 귀찮으면 바로 차원을 수정하도록 하자.</p>]]></content><author><name>qcqced</name><email>qcqced123@gmail.com</email></author><category term="Framework &amp; Library" /><category term="Pytorch" /><category term="Dimension Mismatch" /><category term="CUDA" /><category term="Error Handling" /><summary type="html"><![CDATA[Pytorch Error: Mis-match between pre-defined dimension and input dimension]]></summary></entry><entry><title type="html">🎲 RuntimeError: CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling cublasCreate(hand≤)</title><link href="http://localhost:4000/framework-library/mismatch-embedding" rel="alternate" type="text/html" title="🎲 RuntimeError: CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling cublasCreate(hand≤)" /><published>2023-07-17T00:00:00+09:00</published><updated>2023-07-18T02:00:00+09:00</updated><id>http://localhost:4000/framework-library/embedding_mismatch</id><content type="html" xml:base="http://localhost:4000/framework-library/mismatch-embedding"><![CDATA[<h3 id="-nnembedding-차원--실제-데이터-입력-차원"><code class="language-plaintext highlighter-rouge">😵 nn.Embedding 차원 ≠ 실제 데이터 입력 차원</code></h3>
<p><code class="language-plaintext highlighter-rouge">torch.nn.Embedding</code>에서 정의한 입출력 차원과 실제 데이터의 차원이 다른 경우에 발생하는 에러다. 다양한 상황에서 마주할 수 있는 에러지만, 필자의 경우 <code class="language-plaintext highlighter-rouge">Huggingface</code>에서 불러온<code class="language-plaintext highlighter-rouge">pretrained tokenizer</code>에 <code class="language-plaintext highlighter-rouge">special token</code> 을 추가해 사용할 때, 토큰을 추가했다는 사실을 잊고 <code class="language-plaintext highlighter-rouge">nn.Embedding</code> 에 정의한 입출력 차원을 변경하지 않아서 발생하는 경우가 많았다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoConfig</span><span class="p">,</span> <span class="n">AutoModel</span>

<span class="k">class</span> <span class="nc">CFG</span><span class="p">:</span>
    <span class="n">model_name</span> <span class="o">=</span> <span class="s">'microsoft/deberta-v3-large'</span>
    <span class="n">config</span> <span class="o">=</span> <span class="n">AutoConfig</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>
    <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">AutoModel</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">add_markdown_token</span><span class="p">(</span><span class="n">cfg</span><span class="p">:</span> <span class="n">sCFG</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
    <span class="s">"""
    Add MarkDown token to pretrained tokenizer ('[MD]')
    Args:
        cfg: CFG, needed to load tokenizer from Huggingface AutoTokenizer
    """</span>
    <span class="n">markdown_token</span> <span class="o">=</span> <span class="s">'[MD]'</span>
    <span class="n">special_tokens_dict</span> <span class="o">=</span> <span class="p">{</span><span class="s">'additional_special_tokens'</span><span class="p">:</span> <span class="p">[</span><span class="sa">f</span><span class="s">'</span><span class="si">{</span><span class="n">markdown_token</span><span class="si">}</span><span class="s">'</span><span class="p">]}</span>
    <span class="n">cfg</span><span class="p">.</span><span class="n">tokenizer</span><span class="p">.</span><span class="n">add_special_tokens</span><span class="p">(</span><span class="n">special_tokens_dict</span><span class="p">)</span>
    <span class="n">markdown_token_id</span> <span class="o">=</span> <span class="n">cfg</span><span class="p">.</span><span class="n">tokenizer</span><span class="p">(</span><span class="sa">f</span><span class="s">'</span><span class="si">{</span><span class="n">markdown_token</span><span class="si">}</span><span class="s">'</span><span class="p">,</span> <span class="n">add_special_tokens</span><span class="o">=</span><span class="bp">False</span><span class="p">)[</span><span class="s">'input_ids'</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>

    <span class="nb">setattr</span><span class="p">(</span><span class="n">cfg</span><span class="p">.</span><span class="n">tokenizer</span><span class="p">,</span> <span class="s">'markdown_token'</span><span class="p">,</span> <span class="sa">f</span><span class="s">'</span><span class="si">{</span><span class="n">markdown_token</span><span class="si">}</span><span class="s">'</span><span class="p">)</span>
    <span class="nb">setattr</span><span class="p">(</span><span class="n">cfg</span><span class="p">.</span><span class="n">tokenizer</span><span class="p">,</span> <span class="s">'markdown_token_id'</span><span class="p">,</span> <span class="n">markdown_token_id</span><span class="p">)</span>
    <span class="n">cfg</span><span class="p">.</span><span class="n">tokenizer</span><span class="p">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="sa">f</span><span class="s">'</span><span class="si">{</span><span class="n">cfg</span><span class="p">.</span><span class="n">checkpoint_dir</span><span class="si">}</span><span class="s">/tokenizer/'</span><span class="p">)</span>


<span class="n">add_markdown_token</span><span class="p">(</span><span class="n">CFG</span><span class="p">)</span>
<span class="n">CFG</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">resize_token_embeddings</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">))</span>
</code></pre></div></div>
<p>구글링해보니 해결하는 방법은 다양한 것 같은데, <code class="language-plaintext highlighter-rouge">torch.nn.Embedding</code>에 정의된 입출력 차원을 실제 데이터 차원과 맞춰주면 간단하게 해결된다. 필자처럼 <code class="language-plaintext highlighter-rouge">special token</code> 을 추가해 사용하다 해당 에러가 발생하는 상황이라면 새로운 토큰이 추가된 토크나이저의 길이를 다시 측정한 뒤 값을 <code class="language-plaintext highlighter-rouge">resize_token_embeddings</code> 메서드에 전달해 <code class="language-plaintext highlighter-rouge">nn.Embedding</code>을 업데이트 해주면 된다.</p>]]></content><author><name>qcqced</name><email>qcqced123@gmail.com</email></author><category term="Framework &amp; Library" /><category term="Pytorch" /><category term="Dimension Mismatch" /><category term="nn.Embedding" /><category term="CUDA" /><category term="Error Handling" /><summary type="html"><![CDATA[Pytorch Error: Mis-match between pre-defined dimension and input dimension]]></summary></entry><entry><title type="html">🤔 RuntimeError: Function ‘LogSoftmaxBackward0’ returned nan values in its 0th output</title><link href="http://localhost:4000/framework-library/backward-nan/" rel="alternate" type="text/html" title="🤔 RuntimeError: Function ‘LogSoftmaxBackward0’ returned nan values in its 0th output" /><published>2023-07-11T00:00:00+09:00</published><updated>2023-07-11T13:00:00+09:00</updated><id>http://localhost:4000/framework-library/backward-nan</id><content type="html" xml:base="http://localhost:4000/framework-library/backward-nan/"><![CDATA[<h3 id="-pytorch-backward-과정에서-nan-발생하는-문제"><code class="language-plaintext highlighter-rouge">🔥 Pytorch Backward 과정에서 NaN 발생하는 문제</code></h3>

<p>커스텀으로 모델, 여러 풀링, 매트릭, 손실 함수들을 정의하면서부터 제일 많이 마주하게 되는 에러다. 진심으로 요즘 <code class="language-plaintext highlighter-rouge">CUDA OOM</code> 보다 훨씬 자주 보는 것 같다. 해당 에러는 <code class="language-plaintext highlighter-rouge">LogSoftmax</code> 레이어에 전달된 입력값 중에서 <code class="language-plaintext highlighter-rouge">nan</code>, <code class="language-plaintext highlighter-rouge">inf</code> 가 포함되어 연산을 진행할 수 없다는 것을 의미한다. 딥러닝 실험을 진행하면서 가장 해결하기 까다로운 녀석으로 원인을 특정하기 힘들기 때문이다. 원인을 잡기 어려운 이유는 바로 우리가 지금 하고 있는게 <code class="language-plaintext highlighter-rouge">‘딥러닝’</code> 이라서 그렇다. 위 에러는 대부분 연산자가 우리가 의도하지 않은 동작을 하는 케이스 때문인데, 하나 하나 디버깅하기에는 너무나도 연산자가 많다. 또한 딥러닝은 입출력으로 엄청나게 큰 사이즈의 행렬을 사용한다. 우리가 <code class="language-plaintext highlighter-rouge">nan</code>, <code class="language-plaintext highlighter-rouge">inf</code> 값 존재에 대해서 인지하기 쉽지 않다.</p>

<p><strong><u>위 에러는 필자의 경험상 대부분 커스텀으로 정의한 레이어에서 발생하는 경우가 많았으며 특히</u></strong> <code class="language-plaintext highlighter-rouge">분수</code>, <code class="language-plaintext highlighter-rouge">각도</code>, <code class="language-plaintext highlighter-rouge">제곱근</code>, <code class="language-plaintext highlighter-rouge">지수</code> <strong><u>개념을 사용하는 연산자가 대부분 원인이었다.</u></strong> 예를 들어 코사인 유사도를 구하는 과정에서 연산 대상 벡터값에  <code class="language-plaintext highlighter-rouge">zero-value</code> 가 포함된 경우 분모가 0이 되기 때문에 연산 정의가 되지 않아 <code class="language-plaintext highlighter-rouge">nan</code> 을 반환해 위와 같은 에러가 발생하는 경우가 있다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">check_nan</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
    <span class="s">""" Check if there is NaN in tensor """</span>
    <span class="n">checker</span> <span class="o">=</span> <span class="bp">False</span>
    <span class="k">if</span> <span class="bp">True</span> <span class="ow">in</span> <span class="n">torch</span><span class="p">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
        <span class="n">checker</span> <span class="o">=</span> <span class="bp">True</span>
    <span class="k">return</span> <span class="n">checker</span>

<span class="k">def</span> <span class="nf">zero_filtering</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="s">"""
    Add eps value for zero embedding, because competition metric is cosine similarity
    Cosine Similarity will be returned NaN, when input value has zero, like as torch.clamp()
    """</span>
    <span class="n">eps</span> <span class="o">=</span> <span class="mf">1e-4</span>
    <span class="n">x</span><span class="p">[</span><span class="n">x</span> <span class="o">&lt;=</span> <span class="n">eps</span><span class="p">]</span> <span class="o">=</span> <span class="n">eps</span>
    <span class="k">return</span> <span class="n">x</span>

<span class="k">def</span> <span class="nf">nan_filtering</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">eps</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-4</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="s">"""
    Change eps value for NaN Embedding, because competition metric is cosine similarity
    Cosine Similarity will be returned NaN
    """</span>
    <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">nan</span><span class="o">=</span><span class="n">eps</span><span class="p">)</span>

<span class="k">class</span> <span class="nc">CLIPGEMPooling</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="s">"""
    Generalized Mean Pooling for Natural Language Processing
    This class version of GEMPooling for CLIP, Transfer from NLP Task Code
    ViT don't use attention mask, because input image shape will be same

    Mean Pooling &lt;= GEMPooling &lt;= Max Pooling
    Because of doing exponent to each token embeddings, GEMPooling is like as weight to more activation token

    In original paper, they use p=3, but in this class, we use p=4 because torch doesn't support pow calculation
    for negative value tensor, only for non-negative value in odd number exponent
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">auto_cfg</span><span class="p">:</span> <span class="n">AutoConfig</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">CLIPGEMPooling</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>

    <span class="o">@</span><span class="nb">staticmethod</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">last_hidden_state</span><span class="p">,</span> <span class="n">p</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="s">"""
        last_hidden_state.size: [batch_size, patches_sequence, hidden_size]
        1) Pow last_hidden_state with p and then take a averaging
        2) pow sum_embeddings with 1/p
        """</span>
        <span class="n">p_embeddings</span> <span class="o">=</span> <span class="n">zero_filtering</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nb">pow</span><span class="p">(</span><span class="n">last_hidden_state</span><span class="p">,</span> <span class="n">p</span><span class="p">))</span>
        <span class="c1"># Check NaN value in Embedding after applying torch.pow
</span>        <span class="k">if</span> <span class="n">check_nan</span><span class="p">(</span><span class="n">p_embeddings</span><span class="p">):</span>
            <span class="n">p_embeddings</span> <span class="o">=</span> <span class="n">nan_filtering</span><span class="p">(</span><span class="n">p_embeddings</span><span class="p">)</span>
        <span class="n">sum_embeddings</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">p_embeddings</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

        <span class="n">gem_embeddings</span> <span class="o">=</span> <span class="n">zero_filtering</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nb">pow</span><span class="p">(</span><span class="n">sum_embeddings</span><span class="p">,</span> <span class="mf">1.</span> <span class="o">/</span> <span class="n">p</span><span class="p">))</span>
        <span class="c1"># Check NaN value in Embedding after applying torch.pow
</span>        <span class="k">if</span> <span class="n">check_nan</span><span class="p">(</span><span class="n">gem_embeddings</span><span class="p">):</span>
            <span class="n">gem_embeddings</span> <span class="o">=</span> <span class="n">nan_filtering</span><span class="p">(</span><span class="n">gem_embeddings</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">gem_embeddings</span>

<span class="k">class</span> <span class="nc">CLIPMultipleNegativeRankingLoss</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="s">"""
    Multiple Negative Ranking Loss for CLIP Model
    main concept is same as original one, but append suitable for other type of model (Not Sentence-Transformers)
    if you set more batch size, you can get more negative pairs for each anchor &amp; positive pair
    Args:
        scale: output of similarity function is multiplied by this value =&gt; I don't know why this is needed
        similarity_fct: standard of distance metrics, default cosine similarity
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">reduction</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">scale</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">20.0</span><span class="p">,</span> <span class="n">similarity_fct</span><span class="o">=</span><span class="n">cos_sim</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">reduction</span> <span class="o">=</span> <span class="n">reduction</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">scale</span> <span class="o">=</span> <span class="n">scale</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">similarity_fct</span> <span class="o">=</span> <span class="n">similarity_fct</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">reduction</span> <span class="o">=</span> <span class="n">reduction</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">cross_entropy_loss</span> <span class="o">=</span> <span class="n">CrossEntropyLoss</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">reduction</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">embeddings_a</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">embeddings_b</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="n">similarity_scores</span> <span class="o">=</span> <span class="n">zero_filtering</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">similarity_fct</span><span class="p">(</span><span class="n">embeddings_a</span><span class="p">,</span> <span class="n">embeddings_b</span><span class="p">))</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">scale</span>
        <span class="k">if</span> <span class="n">check_nan</span><span class="p">(</span><span class="n">similarity_scores</span><span class="p">):</span>
            <span class="s">""" Check NaN Value in similarity_scores """</span>
            <span class="n">similarity_scores</span> <span class="o">=</span> <span class="n">nan_filtering</span><span class="p">(</span><span class="n">similarity_scores</span><span class="p">)</span>

        <span class="n">labels</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">(</span>
            <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">similarity_scores</span><span class="p">)),</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">long</span><span class="p">,</span>
            <span class="n">device</span><span class="o">=</span><span class="n">similarity_scores</span><span class="p">.</span><span class="n">device</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">cross_entropy_loss</span><span class="p">(</span><span class="n">similarity_scores</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
</code></pre></div></div>

<p>필자의 경우, 두 개의 입력 행렬에 각각  <code class="language-plaintext highlighter-rouge">sqrt()</code> 를 적용하고 두 행렬의 개별 원소 사이의 코사인 유사도를 구해야 했던 적이 있다. <code class="language-plaintext highlighter-rouge">sqrt</code> <strong><u>과정에서 너무 작은 값들이 입력으로 들어가</u></strong> <code class="language-plaintext highlighter-rouge">underflow</code> <strong><u>가 발생해 행렬에</u></strong> <code class="language-plaintext highlighter-rouge">zero-value</code> <strong><u>가 생겼고, 이를 모른채 코사인 유사도를 구하다가 한참을 위 에러와 싸웠던 적이 있다.</u></strong> 심지어 연산속도 향상을 위해서 <strong><code class="language-plaintext highlighter-rouge">torch.autocast</code></strong> 클래스의 <code class="language-plaintext highlighter-rouge">grad_scaler(float32 to float16)</code> 까지 적용하고 있었다.</p>

<h3 id="️-내가-해결한-방법"><code class="language-plaintext highlighter-rouge">🖍️ 내가 해결한 방법</code></h3>
<p>이 글을 읽는 당신이 만약 <code class="language-plaintext highlighter-rouge">sqrt</code> 혹은 <code class="language-plaintext highlighter-rouge">pow</code>를 활용하는 경우, <code class="language-plaintext highlighter-rouge">underflow</code> 방지를 위해서 <del>위 예시 코드처럼 꼭 적당한 입실론 값을 연산 전후에 필요에 따라 더해줄 것을 권장한다.</del> 입실론 값의 설정은 현재 자신이 사용하고 있는 부동 소수점 정확도에 맞게 설정해주면 될 것 같다. <code class="language-plaintext highlighter-rouge">float32</code> 를 사용하는 경우에는 대부분 <code class="language-plaintext highlighter-rouge">1e-6</code> 을 많이 사용하는 것 같다. 필자도 정확히 어떤 값이 적당한지 아직 잘 모르겠다… 그리고 딥러닝 실험하면서 <code class="language-plaintext highlighter-rouge">overflow</code> 때문에 <code class="language-plaintext highlighter-rouge">inf</code> 이 발생했던 적은 없었다.</p>

<p>입실론 값을 문제가 되는 연산 전에 일괄적으로 더할 경우, 아무리 작은 값이라도 연산 종류에 따라서 결과가 크게 왜곡되는 경우가 발생한다. 따라서 연산을 먼저 적용한 뒤 결과에 <code class="language-plaintext highlighter-rouge">NaN</code>, <code class="language-plaintext highlighter-rouge">Inf</code>, <code class="language-plaintext highlighter-rouge">Zero</code>가 발생하는지 체크하고, 발생한 부분에 한해서 입실론 값을 더해주는 커스텀 <code class="language-plaintext highlighter-rouge">function</code>울 정의해 문제를 해결했다.<br />
(위의 코드 예제 <code class="language-plaintext highlighter-rouge">check_nan</code>, <code class="language-plaintext highlighter-rouge">zero_filtering</code>, <code class="language-plaintext highlighter-rouge">nan_filtering</code>)</p>

<p>한편 <code class="language-plaintext highlighter-rouge">torch.autograd.set_detect_anomaly(True)</code> 를 훈련 루프 초반에 정의해주면, <code class="language-plaintext highlighter-rouge">NaN</code>이 발생하는 즉시 실행이 멈추고 <code class="language-plaintext highlighter-rouge">NaN</code>을 유발한 라인을 출력해준다. 꼭 활용해보자.</p>]]></content><author><name>qcqced</name><email>qcqced123@gmail.com</email></author><category term="Framework &amp; Library" /><category term="Pytorch" /><category term="Logsoftmax" /><category term="NaN" /><category term="Error Handling" /><summary type="html"><![CDATA[Pytorch Error: Backward NaN values]]></summary></entry></feed>