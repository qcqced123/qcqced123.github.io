<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2024-03-11T11:28:55+09:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">AI/Business Study Log</title><subtitle>NLP, Marketing</subtitle><author><name>qcqced</name><email>qcqced123@gmail.com</email></author><entry><title type="html">ğŸŒ† [ELECTRA] Pre-training Text Encoders as Discriminators Rather Than Generators</title><link href="http://localhost:4000/nlp/electra" rel="alternate" type="text/html" title="ğŸŒ† [ELECTRA] Pre-training Text Encoders as Discriminators Rather Than Generators" /><published>2024-03-11T00:00:00+09:00</published><updated>2024-03-12T02:00:00+09:00</updated><id>http://localhost:4000/nlp/electra</id><content type="html" xml:base="http://localhost:4000/nlp/electra"><![CDATA[<h3 id="overview"><code class="language-plaintext highlighter-rouge">ğŸ”­Â Overview</code></h3>

<p><code class="language-plaintext highlighter-rouge">ELECTRA</code>ëŠ” 2020ë…„ Googleì—ì„œ ì²˜ìŒ ë°œí‘œí•œ ëª¨ë¸ë¡œ, GAN(Generative Adversarial Networks) Style ì•„í‚¤í…ì²˜ë¥¼ NLPì— ì ìš©í•œ ê²ƒì´ íŠ¹ì§•ì´ë‹¤. ìƒˆë¡œìš´ êµ¬ì¡° ì°¨ìš©ì— ë§ì¶°ì„œ <code class="language-plaintext highlighter-rouge">RTD(Replace Token Dection)</code> Taskë¥¼ ê³ ì•ˆì— ì‚¬ì „ í•™ìŠµìœ¼ë¡œ ì‚¬ìš©í–ˆë‹¤. ëª¨ë“  ì•„ì´ë””ì–´ëŠ” ê¸°ì¡´ MLM(Masked Language Model)ì„ ì‚¬ì „í•™ìŠµ ë°©ë²•ë¡ ìœ¼ë¡œ ì‚¬ìš©í•˜ëŠ” ì¸ì½”ë” ì–¸ì–´ ëª¨ë¸(BERT ê³„ì—´)ì˜ ë‹¨ì ìœ¼ë¡œë¶€í„° ì¶œë°œí•œë‹¤.</p>

<p><strong>[MLM ë‹¨ì ]</strong></p>
<ul>
  <li>1) ì‚¬ì „í•™ìŠµê³¼ íŒŒì¸íŠœë‹ ì‚¬ì´ ë¶ˆì¼ì¹˜
    <ul>
      <li>íŒŒì¸íŠœë‹ ë•Œ Masking Taskê°€ ì—†ìŒ</li>
    </ul>
  </li>
  <li>2) ì—°ì‚°ëŸ‰ ëŒ€ë¹„ í•™ìŠµëŸ‰ì€ ì ì€í¸
    <ul>
      <li>ì „ì²´ ì‹œí€€ìŠ¤ì˜ 15%ë§Œ ë§ˆìŠ¤í‚¹ í™œìš©(15%ë§Œ í•™ìŠµ)</li>
      <li>ì „ì—­ ì–´í…ì…˜ì˜ ì‹œê³µê°„ ë³µì¡ë„ ê³ ë ¤í•˜ë©´ ìƒë‹¹íˆ ë¹„íš¨ìœ¨ì ì¸ ìˆ˜ì¹˜
        <ul>
          <li>ì‹œí€€ìŠ¤ê¸¸ì´ ** 2ì˜ ë³µì¡ë„</li>
          <li>Vocab Sizeë§Œí¼ì˜ ì°¨ì›ì„ ê°–ëŠ” ì†Œí”„íŠ¸ë§¥ìŠ¤ ê³„ì‚° ë°˜ë³µ</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p>ê·¸ë˜ì„œ MLMì€ í™œìš©í•˜ë˜, íŒŒì¸íŠœë‹ê³¼ ê´´ë¦¬ëŠ” í¬ì§€ ì•Šì€ ëª©ì í•¨ìˆ˜ë¥¼ ì„¤ê³„í•¨ìœ¼ë¡œì„œ ì…ë ¥ëœ ì „ì²´ ì‹œí€€ìŠ¤ì— ëŒ€í•´ì„œ ëª¨ë¸ì´ í•™ìŠµí•˜ì—¬ ì—°ì‚°ëŸ‰ ëŒ€ë¹„ í•™ìŠµëŸ‰ì„ ëŠ˜ë¦¬ê³ ì í–ˆë˜ê²Œ ë°”ë¡œ ELECTRA ëª¨ë¸ì´ë‹¤.</p>

<p>ì •ë¦¬í•˜ìë©´, ELECTRA ëª¨ë¸ì€ ê¸°ì¡´ BERTì˜ êµ¬ì¡°ì  ì¸¡ë©´ ê°œì„ ì´ ì•„ë‹Œ, ì‚¬ì „í•™ìŠµ ë°©ë²•ì— ëŒ€í•œ ê°œì„  ì‹œë„ë¼ê³  ë³¼ ìˆ˜ ìˆë‹¤. ë”°ë¼ì„œ ì–´ë–¤ ëª¨ë¸ì´ë”ë¼ë„, ì¸ì½”ë” ì–¸ì–´ ëª¨ë¸ì´ë¼ë©´ ëª¨ë‘ ELECTRA êµ¬ì¡°ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìœ¼ë©°, ê¸°ì¡´ ë…¼ë¬¸ì—ì„œëŠ” ì›ë³¸ BERT êµ¬ì¡°ë¥¼ ì‚¬ìš©í–ˆë‹¤. ê·¸ë˜ì„œ ë³¸ í¬ìŠ¤íŒ…ì—ì„œë„ BERTì— ëŒ€í•œ ì„¤ëª… ì—†ì´ RTDì— ëŒ€í•´ì„œë§Œ ë‹¤ë£¨ë ¤ê³  í•œë‹¤.</p>

<h3 id="rtd-new-pre-train-task"><code class="language-plaintext highlighter-rouge">ğŸŒ†Â RTD: New Pre-train Task</code></h3>

<p align="center">
<img src="/assets/images/electra/electra.png" alt="RTD Task" class="align-center image-caption" width="100%&quot;, height=&quot;100%" />
<strong><em><a href="https://arxiv.org/abs/2003.10555">RTD Task</a></em></strong>
</p>

<p>RTDì˜ ì•„ì´ë””ì–´ëŠ” ê°„ë‹¨í•˜ë‹¤. ìƒì„±ì(Generator)ê°€ ì¶œë ¥ìœ¼ë¡œ ë‚´ë†“ì€ í† í° ì‹œí€€ìŠ¤ì— ëŒ€í•´ì„œ íŒë³„ì(Discriminator)ê°€ ê°œë³„ í† í°ë“¤ì´ ì›ë³¸ì¸ì§€ ì•„ë‹Œì§€ë¥¼ íŒì •(ì´ì§„ ë¶„ë¥˜)í•˜ë„ë¡ ë§Œë“ ë‹¤. ìƒì„±ìëŠ” ê¸°ì¡´ì˜ MLMì„ ê·¸ëŒ€ë¡œ ìˆ˜í–‰í•˜ê³ , íŒë³„ìëŠ” ìƒì„±ìì˜ ì˜ˆì¸¡ì— ëŒ€í•´ ì§„ì§œì¸ì§€ ê°€ì§œì¸ì§€ ë¶„ë¥˜í•˜ëŠ” ì‹ì´ë‹¤.</p>

<p>ìœ„ ê·¸ë¦¼ì„ ì˜ˆì‹œë¡œ ì‚´í´ë³´ì. ëª¨ë¸ì— ì…ë ¥ìœ¼ë¡œ <code class="language-plaintext highlighter-rouge">the chef cooked the meal</code>ë¼ëŠ” ì‹œí€€ìŠ¤ ì¤€ë‹¤. ê·¸ëŸ¬ë©´ MLM ê·œì¹™ì— ë”°ë¼ì„œ 15%ì˜ í† í°ì´ ë¬´ì‘ìœ„ë¡œ ì„ íƒëœë‹¤. ê·¸ë˜ì„œ <code class="language-plaintext highlighter-rouge">the</code>, <code class="language-plaintext highlighter-rouge">cooked</code>ê°€ ë§ˆìŠ¤í‚¹ ë˜ì—ˆë‹¤. ì´ì œ ìƒì„±ìëŠ” ë§ˆìŠ¤í‚¹ í† í°ì— ëŒ€í•´ <code class="language-plaintext highlighter-rouge">the</code>, <code class="language-plaintext highlighter-rouge">ate</code>ë¼ëŠ” ê²°ê³¼ë¥¼ ë‚´ë†“ëŠ”ë‹¤. ê·¸ë˜ì„œ ìµœì¢…ì ìœ¼ë¡œ ìƒì„±ìê°€ ë°˜í™˜í•˜ëŠ” ì‹œí€€ìŠ¤ëŠ” <code class="language-plaintext highlighter-rouge">the chef ate the meal</code>ì´ ëœë‹¤. ì´ì œ ìƒì„±ìê°€ ë°˜í™˜í•œ ì‹œí€€ìŠ¤ë¥¼ íŒë³„ìì— ì…ë ¥ìœ¼ë¡œ ëŒ€ì…í•œë‹¤. íŒë³„ìëŠ” ê°œë³„ í† í°ë“¤ì´ ì›ë³¸ì¸ì§€ ì•„ë‹Œì§€ë¥¼ íŒì •í•´ ê²°ê³¼ë¥¼ ì¶œë ¥í•œë‹¤.</p>

<p>ì´ëŸ¬í•œ êµ¬ì¡° ë° ì‚¬ì „í•™ìŠµ ë°©ì‹ì˜ ì¥ì ì€ íŒë³„ìê°€ MLM í•™ìŠµì— ë”°ë¥¸ ì§€ì‹ì„ ìƒì„±ìë¡œë¶€í„° ì „ìˆ˜ ë°›ëŠ” ë™ì‹œì— ì „ì²´ ì‹œí€€ìŠ¤ì— ëŒ€í•´ì„œ í•™ìŠµí•  ê¸°íšŒê°€ ìƒê¸´ë‹¤ëŠ” ê²ƒì´ë‹¤. ì‹œí€€ìŠ¤ ë‚´ë¶€ ëª¨ë“  í† í°ì— ëŒ€í•´ì„œ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•˜ê³  ì†ì‹¤ì„ ê³„ì‚°í•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì— ê°™ì€ í¬ê¸°ì˜ ì‹œí€€ìŠ¤ë¥¼ ì‚¬ìš©í•´ë„ ê¸°ì¡´ MLM ëŒ€ë¹„ ë” í’ë¶€í•œ ë¬¸ë§¥ ì •ë³´ë¥¼ ëª¨ë¸ì´ í¬ì°©í•  ìˆ˜ ìˆê²Œ ëœë‹¤. ë˜í•œ íŒë³„ìë¥¼ íŒŒì¸íŠœë‹ì˜ BackBoneìœ¼ë¡œ ì‚¬ìš©í•˜ë©´, íŒë³„ìì˜ ì‚¬ì „í•™ìŠµì€ ê²°êµ­ ë§ˆìŠ¤í‚¹ ì—†ì´ ëª¨ë“  ì‹œí€€ìŠ¤ë¥¼ í™œìš©í•œ ì´ì§„ ë¶„ë¥˜ë¼ê³  ë³¼ ìˆ˜ ìˆê¸° ë•Œë¬¸ì—, ì‚¬ì „í•™ìŠµê³¼ íŒŒì¸íŠœë‹ ì‚¬ì´ì˜ ê´´ë¦¬ë„ ìƒë‹¹íˆ ë§ì´ ì¤„ì–´ë“¤ê²Œ ëœë‹¤.</p>

<h3 id="architecture"><code class="language-plaintext highlighter-rouge">ğŸŒŸÂ Architecture</code></h3>

<p align="center">
<img src="/assets/images/electra/electra_experiment.png" alt="Model Architecture" class="align-center image-caption" width="100%&quot;, height=&quot;100%" />
<strong><em><a href="https://arxiv.org/abs/2003.10555">Model Architecture</a></em></strong>
</p>

<p>ì €ìëŠ” ìœ„ì™€ ê°™ì€ ì‹¤í—˜ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ìƒì„±ìì˜ width (ì€ë‹‰ì¸µ) í¬ê¸°ê°€ íŒë³„ìë³´ë‹¤ ì‘ë„ë¡ ëª¨ë¸ í¬ê¸°ë¥¼ ì„¸íŒ…í•˜ëŠ”ê²Œ ê°€ì¥ íš¨ìœ¨ì ì´ë¼ê³  ì£¼ì¥í•œë‹¤. ì œì‹œëœ ê·¸ë˜í”„ëŠ” ìƒì„±ìì™€ íŒë³„ìì˜ í¬ê¸° ë³€í™” ëŒ€ë¹„ íŒŒì¸íŠœë‹ ì„±ëŠ¥ì˜ ì¶”ì´ë¥¼ ë‚˜íƒ€ë‚¸ë‹¤. ìƒì„±ìì˜ width í¬ê¸°ê°€ 256, íŒë³„ìì˜ width í¬ê¸°ê°€ 768ì¼ ë•Œ ê°€ì¥ ì ìˆ˜ê°€ ë†’ë‹¤. depth(ë ˆì´ì–´ ê°œìˆ˜)ì— ëŒ€í•œ ì–¸ê¸‰ì€ ë”°ë¡œ ì—†ì§€ë§Œ, ì €ìì— ì˜í•´ ê³µê°œëœ Hyper-Param í…Œì´ë¸”ì„ ë³´ë©´ ì€ë‹‰ì¸µì˜ í¬ê¸°ë§Œ ì¤„ì´ê³ , ë ˆì´ì–´ ê°œìˆ˜ëŠ” ìƒì„±ìì™€ íŒë³„ìê°€ ê°™ì€ ê²ƒìœ¼ë¡œ ì¶”ì •ëœë‹¤.</p>

<p>ì¶”ê°€ë¡œ, ìƒì„±ìì™€ íŒë³„ìê°€ ì„ë² ë”© ì¸µì„ ì„œë¡œ ê³µìœ í•˜ëŠ”ê²Œ ê°€ì¥ ë†’ì€ ì„±ëŠ¥ì„ ë‚¸ë‹¤ê³  ì£¼ì¥í•œë‹¤. ì˜¤ë¥¸ìª½ ê·¸ë˜í”„ ì¶”ì´ë¥¼ ë³´ë©´ ê°™ì€ ì—°ì‚°ëŸ‰ì´ë¼ë©´, ì„ë² ë”© ê³µìœ (íŒŒë€ìƒ‰ ì‹¤ì„ ) ë°©ì‹ì´ ê°€ì¥ ë†’ì€ íŒŒì¸íŠœë‹ ì„±ëŠ¥ì„ ë³´ì—¬ì¤€ë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤. ë”°ë¼ì„œ ë‹¨ì–´ ì„ë² ë”©, ì ˆëŒ€ ìœ„ì¹˜ ì„ë² ë”©ì„ ì„œë¡œ ê³µìœ í•˜ë„ë¡ ì„¤ê³„í•œë‹¤. ëŒ€ì‹  ìƒì„±ì ì€ë‹‰ì¸µì˜ í¬ê¸°ê°€ ë” ì‘ì€ê²Œ ìœ ë¦¬í•˜ë‹¤ê³  ì–¸ê¸‰í–ˆê¸° ë•Œë¬¸ì—, ì´ê²ƒì„ ì‹¤ì œë¡œ êµ¬í˜„í•˜ë ¤ë©´ ì„ë² ë”© ì¸µìœ¼ë¡œë¶€í„° ë‚˜ì˜¨ ê²°ê³¼ê°’ì„ ìƒì„±ìì˜ ì€ë‹‰ì¸µ ì°¨ì›ìœ¼ë¡œ ì„ í˜• íˆ¬ì˜í•´ì¤˜ì•¼ í•œë‹¤. ê·¸ë˜ì„œ ìƒì„±ìì˜ ì„ë² ë”© ì¸µê³¼ ì¸ì½”ë” ì‚¬ì´ì— linear layerê°€ ì¶”ê°€ë˜ì–´ì•¼ í•œë‹¤.</p>

\[\min_{\theta_G, \theta_D}\sum_{x \in X} \mathcal{L}_{\text{MLM}}(x, \theta_G) + \lambda \mathcal{L}_{\text{Disc}}(x, \theta_D)\]

<p>ë”°ë¼ì„œ, ì§€ê¸ˆê¹Œì§€ ì‚´í´ë³¸ ëª¨ë“  ë‚´ìš©ì„ ì¢…í•©í•´ë³´ë©´ ELECTRAì˜ ëª©ì í•¨ìˆ˜ëŠ” ë‹¤ìŒ ìˆ˜ì‹ê³¼ ê°™ë‹¤. ìƒì„±ìì˜ MLM ì†ì‹¤ê³¼ íŒë³„ìì˜ ì´ì§„ ë¶„ë¥˜ ì†ì‹¤ì„ ë”í•´ì„œ ëª¨ë¸ì— ì˜¤ì°¨ ì—­ì „í•´ì£¼ë©´ ë˜ëŠ”ë°, íŠ¹ì´í•œ ì ì€ íŒë³„ìì˜ ì†ì‹¤ì— ìƒìˆ˜ê°’ì¸ ëŒë‹¤ê°€ ê³±í•´ì§„ë‹¤ëŠ” ê²ƒì´ë‹¤. ì‹¤ì œ ëª¨ë¸ì„ êµ¬í˜„í•˜ê³  ì‚¬ì „í•™ìŠµì„ í•´ë³´ë©´, ë°ì´í„°ì˜ ì–‘ì´ë‚˜ ëª¨ë¸ í¬ê¸° í˜¹ì€ ì¢…ë¥˜ì— ë”°ë¼ ë‹¬ë¼ì§€ê² ì§€ë§Œ ë‘ ì†ì‹¤ ì‚¬ì´ì˜ ìŠ¤ì¼€ì¼ì˜ ì°¨ì´ê°€ 10ë°°ì •ë„ ì°¨ì´ ë‚˜ê²Œ ëœë‹¤. ë‘ ì†ì‹¤ì˜ ìŠ¤ì¼€ì¼ì„ ë§ì¶°ì£¼ëŠ” ë™ì‹œì—, ì„ë² ë”©ì¸µì˜ í•™ìŠµì´ íŒë³„ìì˜ ì†ì‹¤ì„ ì¤„ì´ëŠ”ë° ë” ì§‘ì¤‘í•˜ë„ë¡ ë§Œë“¤ê¸° ìœ„í•´ ë„ì…í•œ ê²ƒìœ¼ë¡œ ì¶”ì •ëœë‹¤. ë…¼ë¬¸ê³¼ ì½”ë“œë¥¼ ë³´ë©´ ì €ìëŠ” $\lambda=50$ ìœ¼ë¡œ ë‘ê³  í•™ìŠµí•˜ê³  ìˆë‹¤.</p>

<h3 id="implementation-by-pytorch"><code class="language-plaintext highlighter-rouge">ğŸ‘©â€ğŸ’»Â Implementation by Pytorch</code></h3>

<p>ë…¼ë¬¸ì˜ ë‚´ìš©ê³¼ ì €ìê°€ ì§ì ‘ ê³µê°œí•œ ì½”ë“œë¥¼ ì¢…í•©í•˜ì—¬ íŒŒì´í† ì¹˜ë¡œ ELECTRAë¥¼ êµ¬í˜„í•´ë´¤ë‹¤. ë‘ ê°œì˜ ì„œë¡œ ë‹¤ë¥¸ ëª¨ë¸ì„ ê°™ì€ ìŠ¤íƒ­ì—ì„œ í•™ìŠµì‹œì¼œì•¼ í•˜ê¸° ë•Œë¬¸ì—, ì œì‹œëœ ë‚´ìš©ì— ë¹„í•´ ì‹¤ì œ êµ¬í˜„ì€ ë§¤ìš° ê¹Œë‹¤ë¡œìš´ í¸ì´ì—ˆë‹¤. ë³¸ í¬ìŠ¤íŒ…ì—ì„œëŠ” ELECTRA ëª¨ë¸ êµ¬ì¡°ë¥¼ ë¹„ë¡¯í•´ RTD í•™ìŠµ íŒŒì´í”„ë¼ì¸ êµ¬ì„±ì— í•„ìˆ˜ì ì¸ ìš”ì†Œ ëª‡ ê°€ì§€ì— ëŒ€í•´ì„œë§Œ ì„¤ëª…í•˜ë ¤ í•œë‹¤. ì „ì²´ êµ¬ì¡°ì— ëŒ€í•œ ì½”ë“œëŠ” <strong><a href="https://github.com/qcqced123/model_study">ì—¬ê¸° ë§í¬</a></strong>ë¥¼ í†µí•´ ì°¸ê³  ë¶€íƒë“œë¦°ë‹¤.</p>

<p>ELECTRAì˜ ì‚¬ì „ í•™ìŠµì¸ RTDì˜ í•™ìŠµ íŒŒì´í”„ë¼ì¸ì„ êµ¬í˜„í•œ ì½”ë“œë¥¼ ë³¸ ë’¤, ì„¸ë¶€ êµ¬ì„± ìš”ì†Œë“¤ì— ëŒ€í•´ì„œ ì‚´í´ë³´ì.</p>

<h4 id="-rtd-trainer-method"><strong><code class="language-plaintext highlighter-rouge">ğŸŒ† RTD trainer method</code></strong></h4>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">train_val_fn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">loader_train</span><span class="p">,</span> <span class="n">model</span><span class="p">:</span> <span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">criterion</span><span class="p">:</span> <span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">scheduler</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]:</span>
  <span class="n">scaler</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">amp</span><span class="p">.</span><span class="n">GradScaler</span><span class="p">(</span><span class="n">enabled</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">amp_scaler</span><span class="p">)</span>
  <span class="n">model</span><span class="p">.</span><span class="n">train</span><span class="p">()</span>
  <span class="k">for</span> <span class="n">step</span><span class="p">,</span> <span class="n">batch</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">tqdm</span><span class="p">(</span><span class="n">loader_train</span><span class="p">)):</span>
      <span class="n">optimizer</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">(</span><span class="n">set_to_none</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
      <span class="n">inputs</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="s">'input_ids'</span><span class="p">].</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">device</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
      <span class="n">labels</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="s">'labels'</span><span class="p">].</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">device</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>  
      <span class="n">padding_mask</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="s">'padding_mask'</span><span class="p">].</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">device</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>  

      <span class="n">mask_labels</span> <span class="o">=</span> <span class="bp">None</span>
      <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">rtd_masking</span> <span class="o">==</span> <span class="s">'SpanBoundaryObjective'</span><span class="p">:</span>
          <span class="n">mask_labels</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="s">'mask_labels'</span><span class="p">].</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">device</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

      <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">amp</span><span class="p">.</span><span class="n">autocast</span><span class="p">(</span><span class="n">enabled</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">amp_scaler</span><span class="p">):</span>
          <span class="n">g_logit</span><span class="p">,</span> <span class="n">d_inputs</span><span class="p">,</span> <span class="n">d_labels</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">generator_fw</span><span class="p">(</span>
              <span class="n">inputs</span><span class="p">,</span>
              <span class="n">labels</span><span class="p">,</span>
              <span class="n">padding_mask</span><span class="p">,</span>
              <span class="n">mask_labels</span>
          <span class="p">)</span>
          <span class="n">d_logit</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">discriminator_fw</span><span class="p">(</span>
              <span class="n">d_inputs</span><span class="p">,</span>
              <span class="n">padding_mask</span>
          <span class="p">)</span>
          <span class="n">g_loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">g_logit</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">vocab_size</span><span class="p">),</span> <span class="n">labels</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>
          <span class="n">d_loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">d_logit</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">d_labels</span><span class="p">)</span>
          <span class="n">loss</span> <span class="o">=</span> <span class="n">g_loss</span> <span class="o">+</span> <span class="n">d_loss</span><span class="o">*</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">discriminator_lambda</span>

      <span class="n">scaler</span><span class="p">.</span><span class="n">scale</span><span class="p">(</span><span class="n">loss</span><span class="p">).</span><span class="n">backward</span><span class="p">()</span>
      <span class="n">scaler</span><span class="p">.</span><span class="n">step</span><span class="p">(</span><span class="n">optimizer</span><span class="p">)</span>
      <span class="n">scaler</span><span class="p">.</span><span class="n">update</span><span class="p">()</span>
      <span class="n">scheduler</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>
</code></pre></div></div>
<p>ë°ì´í„°ë¡œë”ë¡œë¶€í„° ë°›ì€ ì…ë ¥ë“¤ì„ ìƒì„±ìì— ë„£ê³  MLM ì˜ˆì¸¡ ê²°ê³¼, RTD ìˆ˜í–‰ì„ ìœ„í•´ í•„ìš”í•œ ìƒˆë¡œìš´ ë¼ë²¨ê°’ì„ ë°˜í™˜ ë°›ëŠ”ë‹¤. ê·¸ë¦¬ê³  ì´ê²ƒì„ ë‹¤ì‹œ íŒë³„ìì˜ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©í•˜ê³ , íŒë³„ìì˜ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ë°˜í™˜ë°›ì•„ ì„œë¡œ ë‹¤ë¥¸ ë‘ ëª¨ë¸ì— ëŒ€í•œ ê°€ì¤‘ ì†ì‹¤í•©ì‚°ì„ êµ¬í•œ ë’¤, ì˜µí‹°ë§ˆì´ì €ì— ë³´ë‚´ê³  ìµœì í™”ë¥¼ ìˆ˜í–‰í•œë‹¤. ì´ ë•Œ, ì²˜ìŒì— ë°ì´í„°ë¡œë”ê°€ ë°˜í™˜í•˜ëŠ” ì…ë ¥ ì‹œí€€ìŠ¤ì™€ ë¼ë²¨ì€ MLMì˜ ê·¸ê²ƒê³¼ ë™ì¼í•˜ë‹¤,</p>

<p>êµ¬í˜„í•˜ë©´ì„œ ê°€ì¥ ì–´ë ¤ì› ë˜ê²Œ, ì˜µí‹°ë§ˆì´ì € ë° ìŠ¤ì¼€ì¤„ëŸ¬ì˜ êµ¬ì„±ì´ì—ˆë‹¤. ë‘ ê°œì˜ ëª¨ë¸ì„ ê°™ì€ ìŠ¤íƒ­ì—ì„œ í•™ìŠµì‹œí‚¤ëŠ” ê²½í—˜ì´ ì²˜ìŒì´ë¼ì„œ ì²˜ìŒì— ëª¨ë¸ ê°œìˆ˜ë§Œí¼ ì˜µí‹°ë§ˆì´ì €ì™€ ìŠ¤ì¼€ì¤„ëŸ¬ ê°ì²´ë¥¼ ë§Œë“¤ì–´ì¤˜ì•¼ í•œë‹¤ê³  ìƒê°í–ˆë‹¤. íŠ¹íˆ ë‘ ëª¨ë¸ì˜ ìŠ¤ì¼€ì¼ì´ ë‹¤ë¥´ê¸° ë•Œë¬¸ì— ì„œë¡œ ë‹¤ë¥¸ ì˜µí‹°ë§ˆì´ì €, ìŠ¤ì¼€ì¤„ëŸ¬ ë„ì…ìœ¼ë¡œ ê°ê¸° ë‹¤ë¥¸ í•™ìŠµë¥ ì„ ì ìš©í•˜ëŠ”ê²Œ ì •í™•í•  ê²ƒì´ë¼ ìƒê°í–ˆë‹¤.</p>

<p>í•˜ì§€ë§Œ, ì˜µí‹°ë§ˆì´ì €ë¥¼ ë‘ ê°œ ì‚¬ìš©í•˜ëŠ” ê²ƒì€ ë§¤ìš° ë§ì€ ë©”ëª¨ë¦¬ë¥¼ ì°¨ì§€í•  ë¿ë”ëŸ¬ ë…¼ë¬¸ì—ì„œ ê³µê°œí•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° í…Œì´ë¸”ì„ ë³´ë©´ ë‘ ëª¨ë¸ì— ê°™ì€ í•™ìŠµë¥ ì„ ì ìš©í•˜ê³  ìˆëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆì—ˆë‹¤. ë”°ë¼ì„œ ê·¸ì— ë§ê²Œ ê°™ì€ ì˜µí‹°ë§ˆì´ì €, ìŠ¤ì¼€ì¤„ëŸ¬ë¥¼ ì‚¬ìš©í•´ ë™ì‹œì— ë‘ ëª¨ë¸ì´ í•™ìŠµë˜ë„ë¡ íŒŒì´í”„ë¼ì¸ì„ ë§Œë“¤ê²Œ ë˜ì—ˆë‹¤.</p>

<p>ì¶”ê°€ë¡œ, ê³µê°œëœ ì˜¤í”¼ì…œ ì½”ë“œ ì—­ì‹œ ë‹¨ì¼ ì˜µí‹°ë§ˆì´ì € ë° ìŠ¤ì¼€ì¤„ëŸ¬ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì„ í™•ì¸í–ˆë‹¤.</p>

<h4 id="-electra-module"><strong><code class="language-plaintext highlighter-rouge">ğŸŒ† ELECTRA Module</code></strong></h4>
<p>ğŸŒŸÂ Architecture</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="nn">experiment.models.abstract_model</span> <span class="kn">import</span> <span class="n">AbstractModel</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">Tensor</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">Callable</span>
<span class="kn">from</span> <span class="nn">einops.layers.torch</span> <span class="kn">import</span> <span class="n">Rearrange</span>
<span class="kn">from</span> <span class="nn">experiment.tuner.mlm</span> <span class="kn">import</span> <span class="n">MLMHead</span>
<span class="kn">from</span> <span class="nn">experiment.tuner.sbo</span> <span class="kn">import</span> <span class="n">SBOHead</span>
<span class="kn">from</span> <span class="nn">experiment.tuner.rtd</span> <span class="kn">import</span> <span class="n">get_discriminator_input</span><span class="p">,</span> <span class="n">RTDHead</span>
<span class="kn">from</span> <span class="nn">configuration</span> <span class="kn">import</span> <span class="n">CFG</span>

<span class="k">class</span> <span class="nc">ELECTRA</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">AbstractModel</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">cfg</span><span class="p">:</span> <span class="n">CFG</span><span class="p">,</span> <span class="n">model_func</span><span class="p">:</span> <span class="n">Callable</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">ELECTRA</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span> <span class="o">=</span> <span class="n">cfg</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">generator</span> <span class="o">=</span> <span class="n">model_func</span><span class="p">(</span><span class="n">cfg</span><span class="p">.</span><span class="n">generator_num_layers</span><span class="p">)</span>  <span class="c1"># init generator
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">mlm_head</span> <span class="o">=</span> <span class="n">MLMHead</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">rtd_masking</span> <span class="o">==</span> <span class="s">'SpanBoundaryObjective'</span><span class="p">:</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">mlm_head</span> <span class="o">=</span> <span class="n">SBOHead</span><span class="p">(</span>
                <span class="n">cfg</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">,</span>
                <span class="n">is_concatenate</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">is_concatenate</span><span class="p">,</span>
                <span class="n">max_span_length</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">max_span_length</span>
            <span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">discriminator</span> <span class="o">=</span> <span class="n">model_func</span><span class="p">(</span><span class="n">cfg</span><span class="p">.</span><span class="n">discriminator_num_layers</span><span class="p">)</span>  <span class="c1"># init generator
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">rtd_head</span> <span class="o">=</span> <span class="n">RTDHead</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">share_embed_method</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">share_embed_method</span>  <span class="c1"># instance, es, gdes
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">share_embedding</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">share_embedding</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="k">def</span> <span class="nf">discriminator_hook</span><span class="p">(</span><span class="n">module</span><span class="p">:</span> <span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">,</span> <span class="o">*</span><span class="n">inputs</span><span class="p">):</span>
            <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">share_embed_method</span> <span class="o">==</span> <span class="s">'instance'</span><span class="p">:</span>  <span class="c1"># Instance Sharing
</span>                <span class="bp">self</span><span class="p">.</span><span class="n">discriminator</span><span class="p">.</span><span class="n">embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">generator</span><span class="p">.</span><span class="n">embeddings</span>

            <span class="k">elif</span> <span class="bp">self</span><span class="p">.</span><span class="n">share_embed_method</span> <span class="o">==</span> <span class="s">'ES'</span><span class="p">:</span>  <span class="c1"># ES (Embedding Sharing)
</span>                <span class="bp">self</span><span class="p">.</span><span class="n">discriminator</span><span class="p">.</span><span class="n">embeddings</span><span class="p">.</span><span class="n">word_embedding</span><span class="p">.</span><span class="n">weight</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">generator</span><span class="p">.</span><span class="n">embeddings</span><span class="p">.</span><span class="n">word_embedding</span><span class="p">.</span><span class="n">weight</span>
                <span class="bp">self</span><span class="p">.</span><span class="n">discriminator</span><span class="p">.</span><span class="n">embeddings</span><span class="p">.</span><span class="n">abs_pos_emb</span><span class="p">.</span><span class="n">weight</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">generator</span><span class="p">.</span><span class="n">embeddings</span><span class="p">.</span><span class="n">abs_pos_emb</span><span class="p">.</span><span class="n">weight</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">discriminator</span><span class="p">.</span><span class="n">register_forward_pre_hook</span><span class="p">(</span><span class="n">discriminator_hook</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">generator_fw</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span>
            <span class="n">inputs</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
            <span class="n">labels</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
            <span class="n">padding_mask</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
            <span class="n">mask_labels</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
            <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="bp">None</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
        <span class="n">g_last_hidden_states</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">generator</span><span class="p">(</span>
            <span class="n">inputs</span><span class="p">,</span>
            <span class="n">padding_mask</span><span class="p">,</span>
            <span class="n">attention_mask</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">rtd_masking</span> <span class="o">==</span> <span class="s">'MaskedLanguageModel'</span><span class="p">:</span>
            <span class="n">g_logit</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">mlm_head</span><span class="p">(</span>
                <span class="n">g_last_hidden_states</span>
            <span class="p">)</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">rtd_masking</span> <span class="o">==</span> <span class="s">'SpanBoundaryObjective'</span><span class="p">:</span>
            <span class="n">g_logit</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">mlm_head</span><span class="p">(</span>
                <span class="n">g_last_hidden_states</span><span class="p">,</span>
                <span class="n">mask_labels</span>
            <span class="p">)</span>
        <span class="n">pred</span> <span class="o">=</span> <span class="n">g_logit</span><span class="p">.</span><span class="n">clone</span><span class="p">().</span><span class="n">detach</span><span class="p">()</span>
        <span class="n">d_inputs</span><span class="p">,</span> <span class="n">d_labels</span> <span class="o">=</span> <span class="n">get_discriminator_input</span><span class="p">(</span>
            <span class="n">inputs</span><span class="p">,</span>
            <span class="n">labels</span><span class="p">,</span>
            <span class="n">pred</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">g_logit</span><span class="p">,</span> <span class="n">d_inputs</span><span class="p">,</span> <span class="n">d_labels</span>

    <span class="k">def</span> <span class="nf">discriminator_fw</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span>
            <span class="n">inputs</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
            <span class="n">padding_mask</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
            <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="bp">None</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="n">d_last_hidden_states</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">discriminator</span><span class="p">(</span>
            <span class="n">inputs</span><span class="p">,</span>
            <span class="n">padding_mask</span><span class="p">,</span>
            <span class="n">attention_mask</span>
        <span class="p">)</span>
        <span class="n">d_logit</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">rtd_head</span><span class="p">(</span>
            <span class="n">d_last_hidden_states</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">d_logit</span>

</code></pre></div></div>
<p>ELECTRA ëª¨ë¸ ê°ì²´ëŠ” í¬ê²Œ ì„ë°°ë”© ë ˆì´ì–´ ê³µìœ , ìƒì„±ì í¬ì›Œë“œ, íŒë³„ì í¬ì›Œë“œ íŒŒíŠ¸ë¡œ ë‚˜ë‰œë‹¤. ë¨¼ì € ì„ë² ë”© ë ˆì´ì–´ ê³µìœ ëŠ” í¬ê²Œ ë‘ ê°€ì§€ ë°©ì‹ìœ¼ë¡œ êµ¬í˜„ ê°€ëŠ¥í•˜ë‹¤. í•˜ë‚˜ëŠ” ì„ë² ë”© ë ˆì´ì–´ ì¸ìŠ¤í„´ìŠ¤ ìì²´ë¥¼ ê³µìœ í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ, ìƒì„±ìì™€ íŒë³„ìì˜ ìŠ¤ì¼€ì¼ì´ ë™ì¼í•  ë•Œ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤. ë‚˜ë¨¸ì§€ëŠ” ë‹¨ì–´ ì„ë² ë”©, í¬ì§€ì…˜ ì„ë² ë”©ì˜ ê°€ì¤‘ì¹˜ í–‰ë ¬ë§Œ ê³µìœ í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ, ì„œë¡œ ìŠ¤ì¼€ì¼ì´ ë‹¬ë¼ë„ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤. ë…¼ë¬¸ì—ì„œ ì œì‹œí•˜ëŠ” ê°€ì¥ íš¨ìœ¨ì ì¸ ë°©ë²•ì€ í›„ìì´ë©°, íŒë³„ìì˜ ì„ë² ë”© í–‰ë ¬ì´ ìƒì„±ìì˜ ì„ë² ë”© í–‰ë ¬ì˜ ì£¼ì†Œë¥¼ ê°€ë¦¬í‚¤ë„ë¡ í•¨ìœ¼ë¡œì„œ êµ¬í˜„ ê°€ëŠ¥í•˜ë‹¤.</p>

<h4 id="-rtd-input-making"><strong><code class="language-plaintext highlighter-rouge">ğŸŒ† RTD Input Making</code></strong></h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">Tensor</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Tuple</span>
<span class="kn">from</span> <span class="nn">configuration</span> <span class="kn">import</span> <span class="n">CFG</span>

<span class="k">def</span> <span class="nf">get_discriminator_input</span><span class="p">(</span><span class="n">inputs</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">labels</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">pred</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
    <span class="s">""" Post Processing for Replaced Token Detection Task
    1) get index of the highest probability of [MASK] token in pred tensor
    2) convert [MASK] token to prediction token
    3) make label for Discriminator

    Args:
        inputs: pure inputs from tokenizing by tokenizer
        labels: labels for masked language modeling
        pred: prediction tensor from Generator

    returns:
        d_inputs: torch.Tensor, shape of [Batch, Sequence], for Discriminator inputs
        d_labels: torch.Tensor, shape of [Sequence], for Discriminator labels
    """</span>
    <span class="c1"># 1) flatten pred to 2D Tensor
</span>    <span class="n">d_inputs</span><span class="p">,</span> <span class="n">d_labels</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">.</span><span class="n">clone</span><span class="p">().</span><span class="n">detach</span><span class="p">().</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="bp">None</span>  <span class="c1"># detach to prevent back-propagation
</span>    <span class="n">flat_pred</span><span class="p">,</span> <span class="n">flat_label</span> <span class="o">=</span> <span class="n">pred</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">pred</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)),</span> <span class="n">labels</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># (batch * sequence, vocab_size)
</span>
    <span class="c1"># 2) get index of the highest probability of [MASK] token
</span>    <span class="n">pred_token_idx</span><span class="p">,</span> <span class="n">mlm_mask_idx</span> <span class="o">=</span> <span class="n">flat_pred</span><span class="p">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">),</span> <span class="n">torch</span><span class="p">.</span><span class="n">where</span><span class="p">(</span><span class="n">flat_label</span> <span class="o">!=</span> <span class="o">-</span><span class="mi">100</span><span class="p">)</span>
    <span class="n">pred_tokens</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">index_select</span><span class="p">(</span><span class="n">pred_token_idx</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">mlm_mask_idx</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

    <span class="c1"># 3) convert [MASK] token to prediction token
</span>    <span class="n">d_inputs</span><span class="p">[</span><span class="n">mlm_mask_idx</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span> <span class="o">=</span> <span class="n">pred_tokens</span>

    <span class="c1"># 4) make label for Discriminator
</span>    <span class="n">original_tokens</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">.</span><span class="n">clone</span><span class="p">().</span><span class="n">detach</span><span class="p">().</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">original_tokens</span><span class="p">[</span><span class="n">mlm_mask_idx</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span> <span class="o">=</span> <span class="n">flat_label</span><span class="p">[</span><span class="n">mlm_mask_idx</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span>
    <span class="n">d_labels</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">eq</span><span class="p">(</span><span class="n">original_tokens</span><span class="p">,</span> <span class="n">d_inputs</span><span class="p">).</span><span class="nb">long</span><span class="p">()</span>
    <span class="n">d_inputs</span> <span class="o">=</span> <span class="n">d_inputs</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="n">pred</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># covert to [batch, sequence]
</span>    <span class="k">return</span> <span class="n">d_inputs</span><span class="p">,</span> <span class="n">d_labels</span>
</code></pre></div></div>
<p>ì´ì œ ë§ˆì§€ë§‰ìœ¼ë¡œ íŒë³„ìì˜ ì…ë ¥ì„ ë§Œë“œëŠ” ì•Œê³ ë¦¬ì¦˜ì— ëŒ€í•œ ì½”ë“œë¥¼ ë³´ì. ì•Œê³ ë¦¬ì¦˜ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.</p>
<ul>
  <li>1) ê°œë³„ ë§ˆìŠ¤í‚¹ í† í°ì— ëŒ€í•œ ì˜ˆì¸¡ í† í° êµ¬í•˜ê¸°</li>
  <li>2) ëª¨ë“  ë§ˆìŠ¤í‚¹ ë¶€ë¶„ì— ì˜ˆì¸¡ í† í°ë“¤ë¡œ ëŒ€ì²´</li>
  <li>3) ê¸°ì¡´ ì…ë ¥ê³¼ 2ë²ˆìœ¼ë¡œ ë§Œë“¤ì–´ì§„ ì‹œí€€ìŠ¤ ë¹„êµí•´ ë¼ë²¨ ìƒì„±
    <ul>
      <li>ì„œë¡œ ê°™ìœ¼ë©´ 0</li>
      <li>ì„œë¡œ ë‹¤ë¥´ë©´ 1
ì´ë ‡ê²Œ ë§Œë“¤ì–´ì§„ ìƒˆë¡œìš´ ì…ë ¥ ì‹œí€€ìŠ¤ì™€ ë¼ë²¨ì„ ELECTRA ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ì˜ íŒë³„ì í¬ì›Œë“œ ë©”ì„œë“œì— ì¸ìë¡œ ì „ë‹¬í•˜ë©´ ëœë‹¤.</li>
    </ul>
  </li>
</ul>

<h3 id="-future-work-ì½ê³ -êµ¬í˜„í•˜ë©´ì„œ-ëŠë‚€ì --ê°œì„ ë°©í–¥"><code class="language-plaintext highlighter-rouge">ğŸŒŸ Future Work (ì½ê³  êµ¬í˜„í•˜ë©´ì„œ ëŠë‚€ì  &amp; ê°œì„ ë°©í–¥)</code></h3>

<p>ì´ë ‡ê²Œ ELECTRA ëª¨ë¸ì— ëŒ€í•œ êµ¬í˜„ê¹Œì§€ ì‚´í´ë´¤ë‹¤. ë…¼ë¬¸ì„ ì½ê³  êµ¬í˜„í•˜ë©´ì„œ ê°€ì¥ ì˜ë¬¸ìŠ¤ëŸ¬ì› ë˜ ë¶€ë¶„ì€ ì„ë² ë”© ê³µìœ  ë°©ë²•ì´ì—ˆë‹¤. ìˆ˜í•™ì ìœ¼ë¡œ ì—„ë°€í•˜ê²Œ ê³„ì‚°í•˜ê³  ë”°ì ¸ë³´ì§€ ëª»í–ˆì§€ë§Œ, ì§ê´€ì ìœ¼ë¡œë„ ìƒì„±ìì˜ MLMê³¼ íŒë³„ìì˜ RTDëŠ” ì„œë¡œ ì„±ê²©ì´ ìƒë‹¹íˆ ë‹¤ë¥¸ ì‚¬ì „ í•™ìŠµ ë°©ë²•ë¡ ì´ë¼ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤. ê·¸ë ‡ë‹¤ë©´ ë‹¨ìˆœíˆ ë‹¨ì–´, í¬ì§€ì…˜ ì„ë² ë”©ì„ ê³µìœ í•˜ëŠ” ê²½ìš° í•™ìŠµ ë°©í–¥ì„±ì´ ë‹¬ë¼ì„œ ê°„ì„­ì´ ë°œìƒí•˜ê³  ëª¨ë¸ì´ ìˆ˜ë ´í•˜ì§€ ëª»í•  ì—¬ì§€ê°€ ìƒê¸´ë‹¤. ì´ëŸ¬í•œ <code class="language-plaintext highlighter-rouge">ì¤„ë‹¤ë¦¬ê¸° í˜„ìƒ(tag-of-war)</code>ì„ ì–´ë–»ê²Œ í•´ê²°í•  ìˆ˜ ìˆì„ê¹Œì— ëŒ€í•œ ê³ ë¯¼ì´ ë” í•„ìš”í•˜ë‹¤ê³  ìƒê°í•œë‹¤.</p>

<p>ê·¸ë˜ì„œ ë‹¤ìŒ í¬ìŠ¤íŒ…ì—ì„œëŠ” ì´ëŸ¬í•œ ì¤„ë‹¤ë¦¬ê¸° í˜„ìƒì„ í•´ê²°í•˜ê³ ìí•œ ë…¼ë¬¸ì¸ <strong><a href="https://arxiv.org/abs/2111.09543">&lt;DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing&gt;</a></strong>ì„ ë¦¬ë·°í•´ë³´ê³ ì í•œë‹¤.</p>]]></content><author><name>qcqced</name><email>qcqced123@gmail.com</email></author><category term="NLP" /><category term="Natural Language Process" /><category term="ELECTRA" /><category term="BERT" /><category term="GAN" /><category term="Transformer" /><category term="Self-Attention" /><category term="Pytorch" /><summary type="html"><![CDATA[ELECTRA Official Paper Review with Pytorch Implementation]]></summary></entry><entry><title type="html">ğŸ‘©â€ğŸ’»ğŸ„ [baekjoon] 1987ë²ˆ: ì•ŒíŒŒë²³</title><link href="http://localhost:4000/ps/baekjoon-1987" rel="alternate" type="text/html" title="ğŸ‘©â€ğŸ’»ğŸ„ [baekjoon] 1987ë²ˆ: ì•ŒíŒŒë²³" /><published>2024-01-30T00:00:00+09:00</published><updated>2024-01-31T02:00:00+09:00</updated><id>http://localhost:4000/ps/baekjoon_1987</id><content type="html" xml:base="http://localhost:4000/ps/baekjoon-1987"><![CDATA[<h3 id="ï¸solution"><strong><code class="language-plaintext highlighter-rouge">ğŸ–ï¸Â solution</code></strong></h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">sys</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">List</span>

<span class="k">def</span> <span class="nf">backtracking</span><span class="p">(</span><span class="n">y</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">count</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">visit</span><span class="p">:</span> <span class="n">List</span><span class="p">,</span> <span class="n">graph</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">]):</span>
    <span class="k">global</span> <span class="n">result</span>
    <span class="n">visit</span><span class="p">[</span><span class="nb">ord</span><span class="p">(</span><span class="n">graph</span><span class="p">[</span><span class="n">y</span><span class="p">][</span><span class="n">x</span><span class="p">])</span> <span class="o">-</span> <span class="mi">65</span><span class="p">]</span> <span class="o">=</span> <span class="bp">True</span>
    <span class="n">result</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">count</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">):</span>
        <span class="n">ny</span><span class="p">,</span> <span class="n">nx</span> <span class="o">=</span> <span class="n">dy</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">y</span><span class="p">,</span> <span class="n">dx</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">x</span>
        <span class="k">if</span> <span class="o">-</span><span class="mi">1</span> <span class="o">&lt;</span> <span class="n">ny</span> <span class="o">&lt;</span> <span class="n">r</span> <span class="ow">and</span> <span class="o">-</span><span class="mi">1</span> <span class="o">&lt;</span> <span class="n">nx</span> <span class="o">&lt;</span> <span class="n">c</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">visit</span><span class="p">[</span><span class="nb">ord</span><span class="p">(</span><span class="n">graph</span><span class="p">[</span><span class="n">ny</span><span class="p">][</span><span class="n">nx</span><span class="p">])</span> <span class="o">-</span> <span class="mi">65</span><span class="p">]:</span>
            <span class="n">backtracking</span><span class="p">(</span><span class="n">ny</span><span class="p">,</span> <span class="n">nx</span><span class="p">,</span> <span class="n">count</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">visit</span><span class="p">,</span> <span class="n">graph</span><span class="p">)</span>
            <span class="n">visit</span><span class="p">[</span><span class="nb">ord</span><span class="p">(</span><span class="n">graph</span><span class="p">[</span><span class="n">ny</span><span class="p">][</span><span class="n">nx</span><span class="p">])</span> <span class="o">-</span> <span class="mi">65</span><span class="p">]</span> <span class="o">=</span> <span class="bp">False</span>

<span class="n">r</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="nb">map</span><span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="n">sys</span><span class="p">.</span><span class="n">stdin</span><span class="p">.</span><span class="n">readline</span><span class="p">().</span><span class="n">split</span><span class="p">())</span>

<span class="n">result</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
<span class="n">dy</span><span class="p">,</span> <span class="n">dx</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">grid</span><span class="p">,</span> <span class="n">visited</span> <span class="o">=</span> <span class="p">[</span><span class="nb">list</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="nb">str</span><span class="p">,</span> <span class="n">sys</span><span class="p">.</span><span class="n">stdin</span><span class="p">.</span><span class="n">readline</span><span class="p">().</span><span class="n">rstrip</span><span class="p">()))</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">r</span><span class="p">)],</span> <span class="p">[</span><span class="bp">False</span><span class="p">]</span> <span class="o">*</span> <span class="mi">26</span>
<span class="n">backtracking</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">visited</span><span class="p">,</span> <span class="n">grid</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="nb">max</span><span class="p">(</span><span class="n">result</span><span class="p">))</span>
</code></pre></div></div>

<h3 id="idea"><strong><code class="language-plaintext highlighter-rouge">ğŸ’¡Â idea</code></strong></h3>

<ul>
  <li><strong>Back Tracking</strong></li>
  <li><strong>1) ë°©ë¬¸ ê¸°ë¡ ë°°ì—´ ë³€ê²½</strong>
    <ul>
      <li><strong>ì¡°ê±´ ì¤‘ì—ì„œ ê²½ë¡œì— ì•ŒíŒŒë²³ ì¤‘ë³µì´ ë¶ˆê°€ëŠ¥í•˜ë‹¤ëŠ” ì  ì´ìš©</strong></li>
      <li><strong>ì „ì²´ ê²©ì ì‚¬ì´ì¦ˆì™€ ë™ì¼í•œ ë°°ì—´ ëŒ€ì‹  ì•ŒíŒŒë²³ ì‚¬ì´ì¦ˆ(26)ë§Œ ì„ ì–¸</strong></li>
    </ul>
  </li>
</ul>

<p>ì¼ë°˜ì ì¸ ë°±íŠ¸ë˜í‚¹ ë¬¸ì œë¼ê³  ë³¼ ìˆ˜ ìˆë‹¤. í•˜ì§€ë§Œ íŒŒì´ì¬ìœ¼ë¡œ í•´ê²°í•˜ë ¤ëŠ” ê²½ìš° ì‹œê°„, ë©”ëª¨ë¦¬ ì œí•œ ë•Œë¬¸ì— ë¹¡ì„¼ ì½”ë“œ ìµœì í™”ê°€ í•„ìš”í•˜ë‹¤. ê²©ì ë¬¸ì œë¼ì„œ <code class="language-plaintext highlighter-rouge">bfs</code> ì„ íƒë„ ê°€ëŠ¥í•œë° ê·¸ë ‡ë‹¤ë©´ <code class="language-plaintext highlighter-rouge">python3</code>ë¡œë„ í•´ê²°ê°€ëŠ¥í•˜ë‹¤. í•œí¸, ì¼ë°˜ì ì¸ <code class="language-plaintext highlighter-rouge">dfs</code>ë¼ë©´ ë¹¡ì„¼ ìµœì í™”ë¥¼ í†µí•´ <code class="language-plaintext highlighter-rouge">pypy3</code>ìœ¼ë¡œë§Œ í†µê³¼ ê°€ëŠ¥í•˜ë‹¤.</p>

<p>ë¬¸ì œë¥¼ ë¦¬ë·°í•˜ë˜ ë„ì¤‘ ì¼ë°˜ì ì¸ <code class="language-plaintext highlighter-rouge">dfs</code> ë°±íŠ¸ë˜í‚¹ ë°©ì‹ì˜ ë¹„íš¨ìœ¨ì„±ì— ëŒ€í•´ ê³ ì°°í•´ë´¤ë‹¤. ì•„ë˜ì™€ ê°™ì€ ì…ë ¥ì´ ìˆë‹¤.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">IEFCJ</span>
<span class="n">FHFKC</span>
<span class="n">FFALF</span>
<span class="n">HFGCF</span>
<span class="n">HMCHH</span>
</code></pre></div></div>

<p>ì¼ë°˜ì ì¸ ë°±íŠ¸ë˜í‚¹ ì•Œê³ ë¦¬ì¦˜ì´ íƒìƒ‰í•˜ëŠ” ê³¼ì •ì„ ìƒê°í•´ë³´ì. ë¹¨ê°„ìƒ‰ìœ¼ë¡œ ì¹ í•´ì§„ ê¸€ìë¥¼ <code class="language-plaintext highlighter-rouge">IFHE</code> ìˆœì„œë¡œ íƒìƒ‰í–ˆë‹¤ë©´, ë‹¤ìŒì€ <code class="language-plaintext highlighter-rouge">F</code>ë¥¼ íƒìƒ‰í•´ ë°©ë¬¸í•´ë„ ë˜ëŠ”ì§€ ì—¬ë¶€ë¥¼ íŒì •í•  ê²ƒì´ë‹¤. ì´ë¯¸ <code class="language-plaintext highlighter-rouge">F</code>ëŠ” ë°©ë¬¸í–ˆê¸° ë•Œë¬¸ì— ì•„ë§ˆë„ ìŠ¤íƒ í”„ë ˆì„ í• ë‹¹ì„ ì·¨ì†Œí•˜ë©´ì„œ, ê²°êµ­ì—ëŠ” <code class="language-plaintext highlighter-rouge">I</code>ê¹Œì§€ ë˜ëŒì•„ ê°ˆ ê²ƒì´ë‹¤.</p>

<p>ê·¸ë¦¬ê³  ë‹¤ì‹œ ì˜¤ë¥¸ìª½ì— ìˆëŠ” <code class="language-plaintext highlighter-rouge">E</code>ë¥¼  ë°©ë¬¸í•œ ë’¤, <code class="language-plaintext highlighter-rouge">FCK</code> ìˆœì„œë¡œ ë°©ë¬¸í•˜ê²Œ ë  ê²ƒì´ë‹¤. ì´ ë•Œ ë“¤ê²Œ ë˜ëŠ” ì˜ë¬¸ì€ ë°”ë¡œ ì´ë ‡ë‹¤. êµ³ì´ <code class="language-plaintext highlighter-rouge">I</code>ê¹Œì§€ ë˜ëŒì•„ê°”ë‹¤ê°€ íƒìƒ‰í•´ì•¼ í• ê¹Œ?? ì´ë¯¸ <code class="language-plaintext highlighter-rouge">IE</code> ëŠ” íƒìƒ‰ì´ ê°€ëŠ¥í•œ ê²½ë¡œë¼ëŠ” ê²ƒì„ ìš°ë¦¬ëŠ” ì¶©ë¶„íˆ ì•Œ ìˆ˜ ìˆë‹¤. ë”°ë¼ì„œ <code class="language-plaintext highlighter-rouge">DP Tabulation</code> ê°œë…ì„ ì°¨ìš©í•œë‹¤ë©´ í›¨ì”¬ ë¹ ë¥´ê²Œ í’€ì´ê°€ ê°€ëŠ¥í•  ê²ƒì´ë‹¤.</p>

<p>ê²½ë¡œì˜ ìœ ì¼ì„±ì„ ë³´ì¥í•˜ë©´ì„œ ìˆ˜ì • ê°€ëŠ¥í•œ ìë£Œêµ¬ì¡°ê°€ í•„ìš”í•˜ê¸° ë•Œë¬¸ì— ë°°ì—´ ëŒ€ì‹  ì„¸íŠ¸ë¥¼ ì‚¬ìš©í•´ë³´ì. ì„¸íŠ¸ì—ëŠ” í˜„ì¬ê¹Œì§€ì˜ ê²½ë¡œ ê·¸ë¦¬ê³  í•´ë‹¹ ê²½ë¡œì˜ ë§ˆì§€ë§‰ ì¸ë±ìŠ¤ë¥¼ ì €ì¥í•´ì¤˜ì•¼ í•œë‹¤. ê°™ì€ ê²½ë¡œë¼ê³  í•  ì§€ë¼ë„ ì„œë¡œ ë‹¤ë¥¸ ì¸ë±ìŠ¤ì— ì˜í•´ ë§Œë“¤ì–´ì¡Œì„ ê°€ëŠ¥ì„±ì´ ìˆê¸° ë•Œë¬¸ì´ë‹¤. ì´ë ‡ê²Œ ì„¸íŠ¸ë¥¼ êµ¬ì„±í•œ ë’¤, í•˜ë‚˜ì”© popí•´ì„œ ê²½ë¡œë¥¼ ì–»ì–´ë‚¸ë‹¤. ê·¸ ë‹¤ìŒ í•´ë‹¹ ê²½ë¡œë¡œë¶€í„° íŒŒìƒë˜ëŠ” ì—¬ëŸ¬ ì ì¬ì  ê²½ë¡œë“¤ì„ ëª¨ë‘ ê²€ì‚¬í•´ ê²½ë¡œê°€ ë§Œë“¤ì–´ì§ˆ ìˆ˜ ìˆëŠ”ì§€ ì—¬ë¶€ë¥¼ íŒì •í•˜ë©´ ëœë‹¤. ì½”ë“œëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">sys</span>

<span class="k">def</span> <span class="nf">dfs</span><span class="p">(</span><span class="n">y</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
    <span class="n">dp</span><span class="p">,</span> <span class="n">result</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(),</span> <span class="mi">0</span>
    <span class="n">dp</span><span class="p">.</span><span class="n">add</span><span class="p">((</span><span class="n">y</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">grid</span><span class="p">[</span><span class="n">y</span><span class="p">][</span><span class="n">x</span><span class="p">]))</span>
    <span class="k">while</span> <span class="n">dp</span><span class="p">:</span>
        <span class="n">vy</span><span class="p">,</span> <span class="n">vx</span><span class="p">,</span> <span class="n">path</span> <span class="o">=</span> <span class="n">dp</span><span class="p">.</span><span class="n">pop</span><span class="p">()</span>
        <span class="n">result</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">result</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">path</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">result</span> <span class="o">==</span> <span class="mi">26</span><span class="p">:</span>
            <span class="k">return</span> <span class="mi">26</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">):</span>
            <span class="n">ny</span><span class="p">,</span> <span class="n">nx</span> <span class="o">=</span> <span class="n">dy</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">vy</span><span class="p">,</span> <span class="n">dx</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">vx</span>
            <span class="k">if</span> <span class="o">-</span><span class="mi">1</span> <span class="o">&lt;</span> <span class="n">ny</span> <span class="o">&lt;</span> <span class="n">r</span> <span class="ow">and</span> <span class="o">-</span><span class="mi">1</span> <span class="o">&lt;</span> <span class="n">nx</span> <span class="o">&lt;</span> <span class="n">c</span> <span class="ow">and</span> <span class="n">grid</span><span class="p">[</span><span class="n">ny</span><span class="p">][</span><span class="n">nx</span><span class="p">]</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">path</span><span class="p">:</span>
                <span class="n">dp</span><span class="p">.</span><span class="n">add</span><span class="p">((</span><span class="n">ny</span><span class="p">,</span> <span class="n">nx</span><span class="p">,</span> <span class="n">grid</span><span class="p">[</span><span class="n">ny</span><span class="p">][</span><span class="n">nx</span><span class="p">]</span> <span class="o">+</span> <span class="n">path</span><span class="p">))</span>
                
    <span class="k">return</span> <span class="n">result</span>

<span class="n">r</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="nb">map</span><span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="n">sys</span><span class="p">.</span><span class="n">stdin</span><span class="p">.</span><span class="n">readline</span><span class="p">().</span><span class="n">split</span><span class="p">())</span>
<span class="n">dy</span><span class="p">,</span> <span class="n">dx</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">grid</span> <span class="o">=</span> <span class="p">[</span><span class="nb">list</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="nb">str</span><span class="p">,</span> <span class="n">sys</span><span class="p">.</span><span class="n">stdin</span><span class="p">.</span><span class="n">readline</span><span class="p">().</span><span class="n">rstrip</span><span class="p">()))</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">r</span><span class="p">)]</span>
<span class="k">print</span><span class="p">(</span><span class="n">dfs</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
</code></pre></div></div>

<p align="center">
<img src="/assets/images/ps/after.png" alt="Common BackTracking" class="align-center image-caption" width="100%&quot;, height=&quot;100%" />
<strong><em>Common BackTracking</em></strong>
</p>

<p align="center">
<img src="/assets/images/ps/before.png" alt="DP Tabulation BackTracking" class="align-center image-caption" width="100%&quot;, height=&quot;100%" />
<strong><em>DP Tabulation BackTracking</em></strong>
</p>

<p>ìœ„ì—ëŠ” ê°œì„ ì´ì „ ê²°ê³¼ê³  ì•„ë˜ëŠ” ê°œì„  ì´í›„ ê²°ê³¼ë‹¤. ë¹„ì•½ì ì¸ ì†ë„ ìƒìŠ¹í•˜ëŠ” ë™ì‹œì— ë©”ëª¨ë¦¬ ì—­ì‹œ 3ë°°ë‚˜ ëœ ì‚¬ìš©í•˜ëŠ” ëª¨ìŠµì´ë‹¤. ì„¸íŠ¸ì— ìˆëŠ” ìœ ë‹ˆí¬í•œ ê²½ë¡œë“¤ì„ í•˜ë‚˜ì”© êº¼ë‚´ëŠ” ë°©ì‹ì„ ì„ íƒí–ˆê¸° ë•Œë¬¸ì— ì•Œê³ ë¦¬ì¦˜ ì„±ëŠ¥ì´ ì‹œë“œì— ì˜í–¥(<code class="language-plaintext highlighter-rouge">set.pop()</code>ì€ ëœë¤ìœ¼ë¡œ ì›ì†Œ ì„ íƒ)ì„ ë°›ëŠ”ë‹¤ëŠ” ì ë§Œ ê°ì•ˆí•œë‹¤ë©´ ë§¤ìš° ì¢‹ì€ í’€ì´ë¼ê³  ìƒê°í•œë‹¤.</p>]]></content><author><name>qcqced</name><email>qcqced123@gmail.com</email></author><category term="Problem Solving" /><category term="Python" /><category term="Codeing Test" /><category term="Algorithm" /><category term="Baekjoon" /><category term="Graph" /><category term="DFS" /><category term="BackTracking" /><summary type="html"><![CDATA[ë°±ì¤€ 1987ë²ˆ: ì•ŒíŒŒë²³]]></summary></entry><entry><title type="html">ğŸ”¥Â Pytorch Tensor Indexing ìì£¼ ì‚¬ìš©í•˜ëŠ” ë©”ì„œë“œ ëª¨ìŒì§‘</title><link href="http://localhost:4000/framework-library/torch-indexing-function" rel="alternate" type="text/html" title="ğŸ”¥Â Pytorch Tensor Indexing ìì£¼ ì‚¬ìš©í•˜ëŠ” ë©”ì„œë“œ ëª¨ìŒì§‘" /><published>2024-01-09T00:00:00+09:00</published><updated>2024-01-10T02:00:00+09:00</updated><id>http://localhost:4000/framework-library/Pytorch-Tensor-Indexing-Function</id><content type="html" xml:base="http://localhost:4000/framework-library/torch-indexing-function"><![CDATA[<p>íŒŒì´í† ì¹˜ì—ì„œ í•„ìê°€ ìì£¼ ì‚¬ìš©í•˜ëŠ” í…ì„œ ì¸ë±ì‹± ê´€ë ¨ ë©”ì„œë“œì˜ ì‚¬ìš©ë²• ë° ì‚¬ìš© ì˜ˆì‹œë¥¼ í•œë°©ì— ì •ë¦¬í•œ í¬ìŠ¤íŠ¸ë‹¤. ë©”ì„œë“œ í•˜ë‚˜ë‹¹ í•˜ë‚˜ì˜ í¬ìŠ¤íŠ¸ë¡œ ë§Œë“¤ê¸°ì—ëŠ” ë„ˆë¬´ ê¸¸ì´ê°€ ì§§ë‹¤ ìƒê°í•´ í•œ í˜ì´ì§€ì— ëª¨ë‘ ë„£ê²Œ ë˜ì—ˆë‹¤. ì§€ì†ì ìœ¼ë¡œ ì—…ë°ì´íŠ¸ ë  ì˜ˆì •ì´ë‹¤. ë˜í•œ í…ì„œ ì¸ë±ì‹± ë§ê³ ë„ ë‹¤ë¥¸ ì£¼ì œë¡œë„ ê´€ë ¨ ë©”ì„œë“œë¥¼ ì •ë¦¬í•´ ì˜¬ë¦´ ì˜ˆì •ì´ë‹ˆ ë§ì€ ê´€ì‹¬ ë¶€íƒë“œë¦°ë‹¤.</p>

<h3 id="torchargmax"><code class="language-plaintext highlighter-rouge">ğŸ”Â torch.argmax</code></h3>

<p>ì…ë ¥ í…ì„œì—ì„œ ê°€ì¥ í° ê°’ì„ ê°–ê³  ìˆëŠ” ì›ì†Œì˜ ì¸ë±ìŠ¤ë¥¼ ë°˜í™˜í•œë‹¤. ìµœëŒ€ê°’ì„ ì°¾ì„ ì°¨ì›ì„ ì§€ì •í•´ì¤„ ìˆ˜ ìˆë‹¤. ì•„ë˜ ì˜ˆì‹œ ì½”ë“œë¥¼ í™•ì¸í•´ë³´ì.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># torch.argmax params
</span><span class="n">torch</span><span class="p">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

<span class="c1"># torch.argmax example 1
</span><span class="n">test</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">29</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">45</span><span class="p">,</span><span class="mi">22</span><span class="p">,</span><span class="mi">3</span><span class="p">])</span>
<span class="n">torch</span><span class="p">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">test</span><span class="p">)</span>
<span class="n">torch</span><span class="p">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">test2</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="o">&lt;</span><span class="n">Result</span><span class="o">&gt;</span>
<span class="n">tensor</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>

<span class="c1"># torch.argmax example 2
</span><span class="n">test</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
                     <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">]])</span>

<span class="n">torch</span><span class="p">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">test</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="o">&lt;</span><span class="n">Result</span><span class="o">&gt;</span>
<span class="n">tensor</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>

<span class="c1"># torch.argmax example 3
</span><span class="n">test</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
                     <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">]])</span>

<span class="n">torch</span><span class="p">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">test</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">tensor</span><span class="p">([[</span><span class="mi">0</span><span class="p">],</span>
        <span class="p">[</span><span class="mi">2</span><span class="p">]])</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">dim</code> ë§¤ê°œë³€ìˆ˜ì— ì›í•˜ëŠ” ì°¨ì›ì„ ì…ë ¥í•˜ë©´ í•´ë‹¹ ì°¨ì› ë·°ì—ì„œ ê°€ì¥ í° ì›ì†Œë¥¼ ì°¾ì•„ ì¸ë±ìŠ¤ ê°’ì„ ë°˜í™˜í•´ì¤„ ê²ƒì´ë‹¤. ì´ ë•Œ <code class="language-plaintext highlighter-rouge">keepdim=True</code> ë¡œ ì„¤ì •í•œë‹¤ë©´ ì…ë ¥ ì°¨ì›ì—ì„œ ê°€ì¥ í° ì›ì†Œì˜ ì¸ë±ìŠ¤ë¥¼ ë°˜í™˜í•˜ë˜ ì›ë³¸ í…ì„œì˜ ì°¨ì›ê³¼ ë™ì¼í•œ í˜•íƒœë¡œ ì¶œë ¥í•´ì¤€ë‹¤. <code class="language-plaintext highlighter-rouge">example 2</code> ì˜ ê²½ìš° <code class="language-plaintext highlighter-rouge">dim=0</code> ë¼ì„œ í–‰ì´ ëˆ„ì ëœ ë°©í–¥ìœ¼ë¡œ í…ì„œë¥¼ ë°”ë¼ë´ì•¼ í•œë‹¤. í–‰ì´ ëˆ„ì ëœ ë°©í–¥ìœ¼ë¡œ í…ì„œë¥¼ ë³´ê²Œ ë˜ë©´ <code class="language-plaintext highlighter-rouge">tensor([[0, 1, 1]])</code>ì´ ëœë‹¤.</p>

<h3 id="torchstack"><code class="language-plaintext highlighter-rouge">ğŸ“šÂ torch.stack</code></h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="s">"""
torch.stack
Args:
	tensors(sequence of Tensors): í…ì„œê°€ ë‹´ê¸´ íŒŒì´ì¬ ì‹œí€€ìŠ¤ ê°ì²´
	dim(int): ì¶”ê°€í•  ì°¨ì› ë°©í–¥ì„ ì„¸íŒ…, ê¸°ë³¸ê°’ì€ 0
"""</span>
<span class="n">torch</span><span class="p">.</span><span class="n">stack</span><span class="p">(</span><span class="n">tensors</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</code></pre></div></div>

<p>ë§¤ê°œë³€ìˆ˜ë¡œ ì£¼ì–´ì§„ íŒŒì´ì¬ ì‹œí€€ìŠ¤ ê°ì²´(ë¦¬ìŠ¤íŠ¸, íŠœí”Œ)ë¥¼ ì‚¬ìš©ìê°€ ì§€ì •í•œ ìƒˆë¡œìš´ ì°¨ì›ì— ìŒ“ëŠ” ê¸°ëŠ¥ì„ í•œë‹¤. ë§¤ê°œë³€ìˆ˜ <code class="language-plaintext highlighter-rouge">tensors</code> ëŠ” í…ì„œê°€ ë‹´ê¸´ íŒŒì´ì¬ì˜ ì‹œí€€ìŠ¤ ê°ì²´ë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ëŠ”ë‹¤. <code class="language-plaintext highlighter-rouge">dim</code> ì€ ì‚¬ìš©ìê°€ í…ì„œ ì ì¬ë¥¼ í•˜ê³  ì‹¶ì€ ìƒˆë¡œìš´ ì°¨ì›ì„ ì§€ì •í•´ì£¼ë©´ ëœë‹¤. ê¸°ë³¸ê°’ì€ 0ì°¨ì›ìœ¼ë¡œ ì§€ì • ë˜ì–´ìˆìœ¼ë©°, í…ì„œì˜ ë§¨ ì•ì°¨ì›ì´ ìƒˆë¡­ê²Œ ìƒê¸°ê²Œ ëœë‹¤. <code class="language-plaintext highlighter-rouge">torch.stack</code> ì€ ê¸°ê³„í•™ìŠµ, íŠ¹íˆ ë”¥ëŸ¬ë‹ì—ì„œ ì •ë§ ìì£¼ ì‚¬ìš©ë˜ê¸° ë•Œë¬¸ì— ì‚¬ìš©ë²• ë° ì‚¬ìš©ìƒí™©ì„ ìµí˜€ë‘ë©´ ë„ì›€ì´ ëœë‹¤. ì˜ˆì‹œë¥¼ í†µí•´ í•´ë‹¹ ë©”ì„œë“œë¥¼ ì–´ë–¤ ìƒí™©ì—ì„œ ì–´ë–»ê²Œ ì‚¬ìš©í•˜ëŠ”ì§€ ì•Œì•„ë³´ì.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="s">""" torch.stack example """</span>

<span class="k">class</span> <span class="nc">Projector</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="s">"""
    Making projection matrix(Q, K, V) for each attention head
    When you call this class, it returns projection matrix of each attention head
    For example, if you call this class with 8 heads, it returns 8 set of projection matrices (Q, K, V)
    Args:
        num_heads: number of heads in MHA, default 8
        dim_head: dimension of each attention head, default 64
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span> <span class="n">dim_head</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">64</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Projector</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span> <span class="o">=</span> <span class="n">num_heads</span> <span class="o">*</span> <span class="n">dim_head</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dim_head</span> <span class="o">=</span> <span class="n">dim_head</span>

    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">fc_q</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">dim_head</span><span class="p">)</span>
        <span class="n">fc_k</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">dim_head</span><span class="p">)</span>
        <span class="n">fc_v</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">dim_head</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">fc_q</span><span class="p">,</span> <span class="n">fc_k</span><span class="p">,</span> <span class="n">fc_v</span>

<span class="n">num_heads</span> <span class="o">=</span> <span class="mi">8</span>
<span class="n">dim_head</span> <span class="o">=</span> <span class="mi">64</span>
<span class="n">projector</span> <span class="o">=</span> <span class="n">Projector</span><span class="p">(</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">dim_head</span><span class="p">)</span>  <span class="c1"># init instance
</span><span class="n">projector_list</span> <span class="o">=</span> <span class="p">[</span><span class="nb">list</span><span class="p">(</span><span class="n">projector</span><span class="p">())</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_heads</span><span class="p">)]</span>  <span class="c1"># call instance
</span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">512</span><span class="p">)</span> <span class="c1"># x.shape: [Batch_Size, Sequence_Length, Dim_model]
</span><span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">V</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[],</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_heads</span><span class="p">):</span>
    <span class="n">Q</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">projector_list</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">0</span><span class="p">](</span><span class="n">x</span><span class="p">))</span> <span class="c1"># [10, 512, 64]
</span>    <span class="n">K</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">projector_list</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">1</span><span class="p">](</span><span class="n">x</span><span class="p">))</span> <span class="c1"># [10, 512, 64]
</span>	  <span class="n">V</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">projector_list</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">2</span><span class="p">](</span><span class="n">x</span><span class="p">))</span> <span class="c1"># [10, 512, 64]
</span> 
<span class="n">Q</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">stack</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># Q.shape: [10, 8, 512, 64]
</span><span class="n">K</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">stack</span><span class="p">(</span><span class="n">K</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># K.shape: [10, 8, 512, 64]
</span><span class="n">V</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">stack</span><span class="p">(</span><span class="n">V</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># V.shape: [10, 8, 512, 64]
</span></code></pre></div></div>

<p>ìœ„ ì½”ë“œëŠ” <code class="language-plaintext highlighter-rouge">Transformer</code> ì˜ <code class="language-plaintext highlighter-rouge">Multi-Head Attention</code> êµ¬í˜„ì²´ ì¼ë¶€ë¥¼ ë°œì·Œí•´ì˜¨ ê²ƒì´ë‹¤. <code class="language-plaintext highlighter-rouge">Multi-Head Attention</code> ì€ ê°œë³„ ì–´í…ì…˜ í•´ë“œë³„ë¡œ í–‰ë ¬ $Q, K, V$ë¥¼ ê°€ì ¸ì•¼ í•œë‹¤. ë”°ë¼ì„œ ì…ë ¥ ì„ë² ë”©ì„ ê°œë³„ ì–´í…ì…˜ í—¤ë“œì— <code class="language-plaintext highlighter-rouge">Linear Combination</code> í•´ì¤˜ì•¼ í•˜ëŠ”ë° í—¤ë“œ ê°œìˆ˜ê°€ 8ê°œë‚˜ ë˜ê¸° ë•Œë¬¸ì— ê°œë³„ì ìœ¼ë¡œ <code class="language-plaintext highlighter-rouge">Projection Matrix</code> ë¥¼ ì„ ì–¸í•´ì£¼ëŠ” ê²ƒì€ ë§¤ìš° ë¹„íš¨ìœ¨ì ì´ë‹¤. ë”°ë¼ì„œ ê°ì²´  <code class="language-plaintext highlighter-rouge">Projector</code> ì— í–‰ë ¬ $Q, K, V$ì— ëŒ€í•œ <code class="language-plaintext highlighter-rouge">Projection Matrix</code> ë¥¼ ì •ì˜í•´ì¤¬ë‹¤. ì´í›„ í—¤ë“œ ê°œìˆ˜ë§Œí¼ ê°ì²´  <code class="language-plaintext highlighter-rouge">Projector</code> ë¥¼ í˜¸ì¶œí•´ ë¦¬ìŠ¤íŠ¸ì— í•´ë“œë³„ <code class="language-plaintext highlighter-rouge">Projection Matrix</code> ë¥¼ ë‹´ì•„ì¤€ë‹¤. ê·¸ ë‹¤ìŒ <code class="language-plaintext highlighter-rouge">torch.stack</code>ì„ ì‚¬ìš©í•´ <code class="language-plaintext highlighter-rouge">Attention Head</code> ë°©í–¥ì˜ ì°¨ì›ìœ¼ë¡œ ë¦¬ìŠ¤íŠ¸ ë‚´ë¶€ í…ì„œë“¤ì„ ìŒ“ì•„ì£¼ë©´ ëœë‹¤.</p>

<h3 id="torcharange"><code class="language-plaintext highlighter-rouge">ğŸ”¢Â torch.arange</code></h3>

<p>ì‚¬ìš©ìê°€ ì§€ì •í•œ ì‹œì‘ì ë¶€í„° ëì ê¹Œì§€ ì¼ì •í•œ ê°„ê²©ìœ¼ë¡œ í…ì„œë¥¼ ë‚˜ì—´í•œë‹¤. Pythonì˜ ë‚´ì¥ ë©”ì„œë“œ <code class="language-plaintext highlighter-rouge">range</code>ì™€ ë™ì¼í•œ ì—­í• ì„ í•˜ëŠ”ë°, ëŒ€ì‹  í…ì„œ ê·¸ ê²°ê³¼ë¥¼ í…ì„œ êµ¬ì¡°ì²´ë¡œ ë°˜í™˜í•œë‹¤ê³  ìƒê°í•˜ë©´ ë˜ê² ë‹¤.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># torch.arange usage
</span><span class="n">torch</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="n">start</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">end</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
<span class="n">tensor</span><span class="p">([</span> <span class="mi">0</span><span class="p">,</span>  <span class="mi">1</span><span class="p">,</span>  <span class="mi">2</span><span class="p">,</span>  <span class="mi">3</span><span class="p">,</span>  <span class="mi">4</span><span class="p">])</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">tensor</span><span class="p">([</span> <span class="mi">1</span><span class="p">,</span>  <span class="mi">2</span><span class="p">,</span>  <span class="mi">3</span><span class="p">])</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">)</span>
<span class="n">tensor</span><span class="p">([</span> <span class="mf">1.0000</span><span class="p">,</span>  <span class="mf">1.5000</span><span class="p">,</span>  <span class="mf">2.0000</span><span class="p">])</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">step</code> ë§¤ê°œë³€ìˆ˜ë¡œ ì›ì†Œê°„ ê°„ê²© ì¡°ì •ì„ í•  ìˆ˜ ìˆëŠ”ë°, ê¸°ë³¸ì€ 1ë¡œ ì§€ì • ë˜ì–´ ìˆìœ¼ë‹ˆ ì°¸ê³ í•˜ì. í•„ìì˜ ê²½ìš°ì—ëŠ” <code class="language-plaintext highlighter-rouge">nn.Embedding</code>ì˜ ì…ë ¥ í…ì„œë¥¼ ë§Œë“¤ ë•Œ ê°€ì¥ ë§ì´ ì‚¬ìš©í–ˆë‹¤. <code class="language-plaintext highlighter-rouge">nn.Embedding</code> ì˜ ê²½ìš° Inputìœ¼ë¡œ <code class="language-plaintext highlighter-rouge">IntTensor</code>, <code class="language-plaintext highlighter-rouge">LongTensor</code>ë¥¼ ë°›ê²Œ ë˜ì–´ ìˆìœ¼ë‹ˆ ì•Œì•„ë‘ì.</p>

<h3 id="torchrepeat"><code class="language-plaintext highlighter-rouge">ğŸ”Â torch.repeat</code></h3>

<p>ì…ë ¥ê°’ìœ¼ë¡œ ì£¼ì–´ì§„ í…ì„œë¥¼ ì‚¬ìš©ìê°€ ì§€ì •í•œ ë°˜ë³µ íšŸìˆ˜ë§Œí¼ íŠ¹ì • ì°¨ì› ë°©í–¥ìœ¼ë¡œ ëŠ˜ë¦°ë‹¤. ì˜ˆë¥¼ ë“¤ë©´ <code class="language-plaintext highlighter-rouge">[1,2,3] * 3</code>ì˜ ê²°ê³¼ëŠ” <code class="language-plaintext highlighter-rouge">[1, 2, 3, 1, 2, 3, 1, 2, 3]</code> ì¸ë°, ì´ê²ƒì„ ì‚¬ìš©ìê°€ ì§€ì •í•œ ë°˜ë³µ íšŸìˆ˜ë§Œí¼ íŠ¹ì • ì°¨ì›ìœ¼ë¡œ ìˆ˜í–‰í•˜ê² ë‹¤ëŠ” ê²ƒì´ë‹¤. ì•„ë˜ ì‚¬ìš© ì˜ˆì œë¥¼ í™•ì¸í•´ë³´ì.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># torch.repeat example
</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">x</span><span class="p">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">tensor</span><span class="p">([[</span> <span class="mi">1</span><span class="p">,</span>  <span class="mi">2</span><span class="p">,</span>  <span class="mi">3</span><span class="p">,</span>  <span class="mi">1</span><span class="p">,</span>  <span class="mi">2</span><span class="p">,</span>  <span class="mi">3</span><span class="p">],</span>
        <span class="p">[</span> <span class="mi">1</span><span class="p">,</span>  <span class="mi">2</span><span class="p">,</span>  <span class="mi">3</span><span class="p">,</span>  <span class="mi">1</span><span class="p">,</span>  <span class="mi">2</span><span class="p">,</span>  <span class="mi">3</span><span class="p">],</span>
        <span class="p">[</span> <span class="mi">1</span><span class="p">,</span>  <span class="mi">2</span><span class="p">,</span>  <span class="mi">3</span><span class="p">,</span>  <span class="mi">1</span><span class="p">,</span>  <span class="mi">2</span><span class="p">,</span>  <span class="mi">3</span><span class="p">],</span>
        <span class="p">[</span> <span class="mi">1</span><span class="p">,</span>  <span class="mi">2</span><span class="p">,</span>  <span class="mi">3</span><span class="p">,</span>  <span class="mi">1</span><span class="p">,</span>  <span class="mi">2</span><span class="p">,</span>  <span class="mi">3</span><span class="p">]])</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">x</span><span class="p">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">tensor</span><span class="p">([[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
         <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]],</span>

        <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
         <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]],</span>

        <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
         <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]],</span>

        <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
         <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]]])</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">x</span><span class="p">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">).</span><span class="n">size</span>
<span class="n">torch</span><span class="p">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">x</span><span class="p">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">tensor</span><span class="p">([[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
         <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]],</span>

        <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
         <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]],</span>

        <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
         <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]],</span>

        <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
         <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]]])</span>
</code></pre></div></div>

<p>$t$ë¥¼ ì–´ë–¤ í…ì„œ êµ¬ì¡°ì²´ $x$ì˜ ìµœëŒ€ ì°¨ì›ì´ë¼ê³  í–ˆì„ , $x_t$ë¥¼ ê°€ì¥ ì™¼ìª½ì— ë„£ê³  ê°€ì¥ ë‚®ì€ ì°¨ì›ì¸ 0ì°¨ì›ì— ëŒ€í•œ ë°˜ë³µ íšŸìˆ˜ë¥¼ ì˜¤ë¥¸ìª½ ëì— ëŒ€ì…í•´ì„œ ì‚¬ìš©í•˜ë©´ ëœë‹¤. (<code class="language-plaintext highlighter-rouge">torch.repeat(</code>$x_t, x_{t-1}, â€¦ x_2, x_1, x_0$<code class="language-plaintext highlighter-rouge">))</code>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># torch.arange &amp; torch.repeate usage example
</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">pos_x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">num_patches</span> <span class="o">+</span> <span class="mi">1</span><span class="p">).</span><span class="n">repeat</span><span class="p">(</span><span class="n">inputs</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]).</span><span class="n">to</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">pos_x</span><span class="p">.</span><span class="n">shape</span>
<span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">16</span><span class="p">,</span> <span class="mi">1025</span><span class="p">])</span>
</code></pre></div></div>

<p>í•„ìì˜ ê²½ìš°, <code class="language-plaintext highlighter-rouge">position embedding</code>ì˜ ì…ë ¥ì„ ë§Œë“¤ê³  ì‹¶ì„ ë•Œ <code class="language-plaintext highlighter-rouge">torch.arange</code> ì™€ ì—°ê³„í•´ ìì£¼ ì‚¬ìš© í–ˆë˜ ê²ƒ ê°™ë‹¤. ìœ„ ì½”ë“œë¥¼ ì°¸ê³ í•˜ì.</p>

<h3 id="torchclamp"><code class="language-plaintext highlighter-rouge">ğŸ”¬Â torch.clamp</code></h3>

<p>ì…ë ¥ í…ì„œì˜ ì›ì†Œê°’ì„ ì‚¬ìš©ìê°€ ì§€ì •í•œ ìµœëŒ€â€¢ìµœì†Œê°’ ë²”ìœ„ ì´ë‚´ë¡œ ì œí•œí•˜ëŠ” ë©”ì„œë“œë‹¤.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># torch.clamp params
</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">clamp</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="nb">min</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span> <span class="err">â†’</span> <span class="n">Tensor</span>

<span class="c1"># torch.clamp usage example
</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">a</span>
<span class="n">tensor</span><span class="p">([</span><span class="o">-</span><span class="mf">1.7120</span><span class="p">,</span>  <span class="mf">0.1734</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0478</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0922</span><span class="p">])</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="nb">min</span><span class="o">=-</span><span class="mf">0.5</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">tensor</span><span class="p">([</span><span class="o">-</span><span class="mf">0.5000</span><span class="p">,</span>  <span class="mf">0.1734</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0478</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0922</span><span class="p">])</span>
</code></pre></div></div>

<p>ì…ë ¥ëœ í…ì„œì˜ ì›ì†Œë¥¼ ì§€ì • ìµœëŒ€â€¢ìµœì†Œ ì„¤ì •ê°’ê³¼ í•˜ë‚˜ í•˜ë‚˜ ëŒ€ì¡°í•´ì„œ í…ì„œ ë‚´ë¶€ì˜ ëª¨ë“  ì›ì†Œê°€ ì§€ì • ë²”ìœ„ ì•ˆì— ë“¤ë„ë¡ ë§Œë“¤ì–´ì¤€ë‹¤. <code class="language-plaintext highlighter-rouge">torch.clamp</code> ì—­ì‹œ ë‹¤ì–‘í•œ ìƒí™©ì—ì„œ ì‚¬ìš©ë˜ëŠ”ë°, í•„ìì˜ ê²½ìš° ëª¨ë¸ ë ˆì´ì–´ ì¤‘ê°„ì— ì œê³±ê·¼ì´ë‚˜ ì§€ìˆ˜, ë¶„ìˆ˜ í˜¹ì€ ê°ë„ ê´€ë ¨ ì—°ì‚°ì´ ë“¤ì–´ê°€ <code class="language-plaintext highlighter-rouge">Backward Pass</code>ì—ì„œ <code class="language-plaintext highlighter-rouge">NaN</code>ì´ ë°œìƒí•  ìˆ˜ ìˆëŠ” ê²½ìš°ì— ì•ˆì „ì¥ì¹˜ë¡œ ë§ì´ ì‚¬ìš©í•˜ê³  ìˆë‹¤. (<a href="https://qcqced123.github.io/framework-library/backward-nan/">ìì„¸íˆ ì•Œê³  ì‹¶ë‹¤ë©´ í´ë¦­</a>)</p>

<h3 id="torchgather"><code class="language-plaintext highlighter-rouge">ğŸ‘©â€ğŸ‘©â€ğŸ‘§â€ğŸ‘¦Â torch.gather</code></h3>

<p>í…ì„œ ê°ì²´ ë‚´ë¶€ì—ì„œ ì›í•˜ëŠ” ì¸ë±ìŠ¤ì— ìœ„ì¹˜í•œ ì›ì†Œë§Œ ì¶”ì¶œí•˜ê³  ì‹¶ì„ ë•Œ ì‚¬ìš©í•˜ë©´ ë§¤ìš° ìœ ìš©í•œ ë©”ì„œë“œë‹¤. í…ì„œ ì—­ì‹œ <code class="language-plaintext highlighter-rouge">iterable</code> ê°ì²´ë¼ì„œ <code class="language-plaintext highlighter-rouge">loop</code> ë¥¼ ì‚¬ìš©í•´ ì ‘ê·¼í•˜ëŠ” ê²ƒì´ ì§ê´€ì ìœ¼ë¡œ ë³´ì¼ ìˆ˜ ìˆìœ¼ë‚˜, í†µìƒì ìœ¼ë¡œ í…ì„œë¥¼ ì‚¬ìš©í•˜ëŠ” ìƒí™©ì´ë¼ë©´ ê°ì²´ì˜ ì°¨ì›ì´ ì–´ë§ˆë¬´ì‹œ í•˜ê¸° ë•Œë¬¸ì— ë£¨í”„ë¡œ ì ‘ê·¼í•´ ê´€ë¦¬í•˜ëŠ” ê²ƒì€ ë§¤ìš° ë¹„íš¨ìœ¨ì ì´ë‹¤. ë£¨í”„ë¥¼ í†µí•´ ì ‘ê·¼í•˜ë©´ íŒŒì´ì¬ì˜ ë‚´ì¥ ë¦¬ìŠ¤íŠ¸ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒê³¼ ë³„ë°˜ ë‹¤ë¥¼ê²Œ ì—†ì–´ì§€ê¸° ë•Œë¬¸ì—, í…ì„œë¥¼ ì‚¬ìš©í•˜ëŠ” ë©”ë¦¬íŠ¸ê°€ ì‚¬ë¼ì§„ë‹¤. ë¹„êµì  í¬ì§€ ì•Šì€ 2~3ì°¨ì›ì˜ í…ì„œ ì •ë„ë¼ë©´ ì‚¬ìš©í•´ë„ í¬ê²Œ ë¬¸ì œëŠ” ì—†ì„ê±°ë¼ ìƒê°í•˜ì§€ë§Œ ê·¸ë˜ë„ ì½”ë“œì˜ ì¼ê´€ì„±ì„ ìœ„í•´ <code class="language-plaintext highlighter-rouge">torch.gather</code> ì‚¬ìš©ì„ ê¶Œì¥í•œë‹¤. ì´ì œ <code class="language-plaintext highlighter-rouge">torch.gather</code>ì˜ ì‚¬ìš©ë²•ì— ëŒ€í•´ ì•Œì•„ë³´ì.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># torch.gather params
</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">gather</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">sparse_grad</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">dim</code>ê³¼ <code class="language-plaintext highlighter-rouge">index</code>ì— ì£¼ëª©í•´ë³´ì. ë¨¼ì € <code class="language-plaintext highlighter-rouge">dim</code>ì€ ì‚¬ìš©ìê°€ ì¸ë±ì‹±ì„ ì ìš©í•˜ê³  ì‹¶ì€ ì°¨ì›ì„ ì§€ì •í•´ì£¼ëŠ” ì—­í• ì„ í•œë‹¤. <code class="language-plaintext highlighter-rouge">index</code> ë§¤ê°œë³€ìˆ˜ë¡œ ì „ë‹¬í•˜ëŠ” í…ì„œ ì•ˆì—ëŠ” ì›ì†Œì˜ <code class="language-plaintext highlighter-rouge">â€˜ì¸ë±ìŠ¤â€™</code>ë¥¼ ì˜ë¯¸í•˜ëŠ” ìˆ«ìë“¤ì´ ë§ˆêµ¬ì¡ì´ë¡œ ë‹´ê²¨ìˆëŠ”ë°, í•´ë‹¹ ì¸ë±ìŠ¤ê°€ ëŒ€ìƒ í…ì„œì˜ ì–´ëŠ ì°¨ì›ì„ ê°€ë¦¬í‚¬ ê²ƒì¸ì§€ë¥¼ ì»´í“¨í„°ì—ê²Œ ì•Œë ¤ì¤€ë‹¤ê³  ìƒê°í•˜ë©´ ëœë‹¤. <code class="language-plaintext highlighter-rouge">index</code> ëŠ” ì•ì—ì„œ ì„¤ëª…í–ˆë“¯ì´ ì›ì†Œì˜ <code class="language-plaintext highlighter-rouge">â€˜ì¸ë±ìŠ¤â€™</code>ë¥¼ ì˜ë¯¸í•˜ëŠ” ìˆ«ìë“¤ì´ ë‹´ê¸´ í…ì„œë¥¼ ì…ë ¥ìœ¼ë¡œ í•˜ëŠ” ë§¤ê°œë³€ìˆ˜ë‹¤. ì´ ë•Œ ì£¼ì˜í•  ì ì€ ëŒ€ìƒ í…ì„œ(<code class="language-plaintext highlighter-rouge">input</code>)ì™€ ì¸ë±ìŠ¤ í…ì„œì˜ ì°¨ì› í˜•íƒœê°€ ë°˜ë“œì‹œ ë™ì¼í•´ì•¼ í•œë‹¤ëŠ” ê²ƒì´ë‹¤. ì—­ì‹œ ë§ë¡œë§Œ ë“¤ìœ¼ë©´ ì´í•´í•˜ê¸° í˜ë“œë‹ˆ ì‚¬ìš© ì˜ˆì‹œë¥¼ í•¨ê¼ ì‚´í´ë³´ì.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># torch.gather usage example
</span><span class="o">&gt;&gt;&gt;</span> <span class="n">q</span><span class="p">,</span> <span class="n">kr</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1024</span><span class="p">,</span> <span class="mi">64</span><span class="p">),</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1024</span><span class="p">,</span> <span class="mi">64</span><span class="p">)</span> <span class="c1"># [batch, sequence, dim_head], [batch, 2*sequence, dim_head]
</span><span class="o">&gt;&gt;&gt;</span> <span class="n">tmp_c2p</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">kr</span><span class="p">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">))</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">tmp_c2p</span><span class="p">,</span> <span class="n">tmp_c2p</span><span class="p">.</span><span class="n">shape</span>
<span class="p">(</span><span class="n">tensor</span><span class="p">([[</span><span class="o">-</span><span class="mf">2.6477</span><span class="p">,</span> <span class="o">-</span><span class="mf">4.7478</span><span class="p">,</span> <span class="o">-</span><span class="mf">5.3250</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mf">1.6062</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.9717</span><span class="p">,</span>  <span class="mf">3.8004</span><span class="p">],</span>
         <span class="p">[</span> <span class="mf">0.0662</span><span class="p">,</span>  <span class="mf">1.5240</span><span class="p">,</span>  <span class="mf">0.1182</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mf">0.1653</span><span class="p">,</span>  <span class="mf">2.8476</span><span class="p">,</span>  <span class="mf">1.6337</span><span class="p">],</span>
         <span class="p">[</span><span class="o">-</span><span class="mf">0.5010</span><span class="p">,</span> <span class="o">-</span><span class="mf">4.2267</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.1179</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mf">1.1447</span><span class="p">,</span>  <span class="mf">1.7845</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1493</span><span class="p">],</span>
         <span class="p">...,</span>
         <span class="p">[</span><span class="o">-</span><span class="mf">2.1073</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2149</span><span class="p">,</span> <span class="o">-</span><span class="mf">4.8630</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mf">0.8238</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5833</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2066</span><span class="p">],</span>
         <span class="p">[</span> <span class="mf">2.1747</span><span class="p">,</span>  <span class="mf">3.2924</span><span class="p">,</span>  <span class="mf">6.5808</span><span class="p">,</span>  <span class="p">...,</span> <span class="o">-</span><span class="mf">0.2926</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2511</span><span class="p">,</span>  <span class="mf">2.6996</span><span class="p">],</span>
         <span class="p">[</span><span class="o">-</span><span class="mf">2.8362</span><span class="p">,</span>  <span class="mf">2.8700</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.9729</span><span class="p">,</span>  <span class="p">...,</span> <span class="o">-</span><span class="mf">4.9913</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3616</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1708</span><span class="p">]],</span>
        <span class="n">grad_fn</span><span class="o">=&lt;</span><span class="n">MmBackward0</span><span class="o">&gt;</span><span class="p">)</span>
<span class="n">torch</span><span class="p">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">1024</span><span class="p">,</span> <span class="mi">1024</span><span class="p">]))</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">max_seq</span><span class="p">,</span> <span class="n">max_relative_position</span> <span class="o">=</span> <span class="mi">1024</span><span class="p">,</span> <span class="mi">512</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">q_index</span><span class="p">,</span> <span class="n">k_index</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="n">max_seq</span><span class="p">),</span> <span class="n">torch</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">max_relative_position</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">q_index</span><span class="p">,</span> <span class="n">k_index</span>
<span class="p">(</span><span class="n">tensor</span><span class="p">([</span>   <span class="mi">0</span><span class="p">,</span>    <span class="mi">1</span><span class="p">,</span>    <span class="mi">2</span><span class="p">,</span>  <span class="p">...,</span> <span class="mi">1021</span><span class="p">,</span> <span class="mi">1022</span><span class="p">,</span> <span class="mi">1023</span><span class="p">]),</span>
 <span class="n">tensor</span><span class="p">([</span>   <span class="mi">0</span><span class="p">,</span>    <span class="mi">1</span><span class="p">,</span>    <span class="mi">2</span><span class="p">,</span>  <span class="p">...,</span> <span class="mi">1021</span><span class="p">,</span> <span class="mi">1022</span><span class="p">,</span> <span class="mi">1023</span><span class="p">]))</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">tmp_pos</span> <span class="o">=</span> <span class="n">q_index</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="o">-</span> <span class="n">k_index</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">rel_pos_matrix</span> <span class="o">=</span> <span class="n">tmp_pos</span> <span class="o">+</span> <span class="n">max_relative_position</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">rel_pos_matrix</span>
<span class="n">tensor</span><span class="p">([[</span> <span class="mi">512</span><span class="p">,</span>  <span class="mi">511</span><span class="p">,</span>  <span class="mi">510</span><span class="p">,</span>  <span class="p">...,</span> <span class="o">-</span><span class="mi">509</span><span class="p">,</span> <span class="o">-</span><span class="mi">510</span><span class="p">,</span> <span class="o">-</span><span class="mi">511</span><span class="p">],</span>
        <span class="p">[</span> <span class="mi">513</span><span class="p">,</span>  <span class="mi">512</span><span class="p">,</span>  <span class="mi">511</span><span class="p">,</span>  <span class="p">...,</span> <span class="o">-</span><span class="mi">508</span><span class="p">,</span> <span class="o">-</span><span class="mi">509</span><span class="p">,</span> <span class="o">-</span><span class="mi">510</span><span class="p">],</span>
        <span class="p">[</span> <span class="mi">514</span><span class="p">,</span>  <span class="mi">513</span><span class="p">,</span>  <span class="mi">512</span><span class="p">,</span>  <span class="p">...,</span> <span class="o">-</span><span class="mi">507</span><span class="p">,</span> <span class="o">-</span><span class="mi">508</span><span class="p">,</span> <span class="o">-</span><span class="mi">509</span><span class="p">],</span>
        <span class="p">...,</span>
        <span class="p">[</span><span class="mi">1533</span><span class="p">,</span> <span class="mi">1532</span><span class="p">,</span> <span class="mi">1531</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mi">512</span><span class="p">,</span>  <span class="mi">511</span><span class="p">,</span>  <span class="mi">510</span><span class="p">],</span>
        <span class="p">[</span><span class="mi">1534</span><span class="p">,</span> <span class="mi">1533</span><span class="p">,</span> <span class="mi">1532</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mi">513</span><span class="p">,</span>  <span class="mi">512</span><span class="p">,</span>  <span class="mi">511</span><span class="p">],</span>
        <span class="p">[</span><span class="mi">1535</span><span class="p">,</span> <span class="mi">1534</span><span class="p">,</span> <span class="mi">1533</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mi">514</span><span class="p">,</span>  <span class="mi">513</span><span class="p">,</span>  <span class="mi">512</span><span class="p">]])</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">rel_pos_matrix</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">rel_pos_matrix</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="o">*</span><span class="n">max_relative_position</span> <span class="o">-</span> <span class="mi">1</span><span class="p">).</span><span class="n">repeat</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">tmp_c2p</span> <span class="o">=</span> <span class="n">tmp_c2p</span><span class="p">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">rel_pos_matrix</span><span class="p">,</span> <span class="n">rel_pos_matrix</span><span class="p">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">tmp_c2p</span><span class="p">.</span><span class="n">shape</span> 
<span class="p">(</span><span class="n">tensor</span><span class="p">([[[</span> <span class="mi">512</span><span class="p">,</span>  <span class="mi">511</span><span class="p">,</span>  <span class="mi">510</span><span class="p">,</span>  <span class="p">...,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">],</span>
          <span class="p">[</span> <span class="mi">513</span><span class="p">,</span>  <span class="mi">512</span><span class="p">,</span>  <span class="mi">511</span><span class="p">,</span>  <span class="p">...,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">],</span>
          <span class="p">[</span> <span class="mi">514</span><span class="p">,</span>  <span class="mi">513</span><span class="p">,</span>  <span class="mi">512</span><span class="p">,</span>  <span class="p">...,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">],</span>
          <span class="p">...,</span>
          <span class="p">[</span><span class="mi">1023</span><span class="p">,</span> <span class="mi">1023</span><span class="p">,</span> <span class="mi">1023</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mi">512</span><span class="p">,</span>  <span class="mi">511</span><span class="p">,</span>  <span class="mi">510</span><span class="p">],</span>
          <span class="p">[</span><span class="mi">1023</span><span class="p">,</span> <span class="mi">1023</span><span class="p">,</span> <span class="mi">1023</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mi">513</span><span class="p">,</span>  <span class="mi">512</span><span class="p">,</span>  <span class="mi">511</span><span class="p">],</span>
          <span class="p">[</span><span class="mi">1023</span><span class="p">,</span> <span class="mi">1023</span><span class="p">,</span> <span class="mi">1023</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mi">514</span><span class="p">,</span>  <span class="mi">513</span><span class="p">,</span>  <span class="mi">512</span><span class="p">]],</span>
 
         <span class="p">[[</span> <span class="mi">512</span><span class="p">,</span>  <span class="mi">511</span><span class="p">,</span>  <span class="mi">510</span><span class="p">,</span>  <span class="p">...,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">],</span>
          <span class="p">[</span> <span class="mi">513</span><span class="p">,</span>  <span class="mi">512</span><span class="p">,</span>  <span class="mi">511</span><span class="p">,</span>  <span class="p">...,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">],</span>
          <span class="p">[</span> <span class="mi">514</span><span class="p">,</span>  <span class="mi">513</span><span class="p">,</span>  <span class="mi">512</span><span class="p">,</span>  <span class="p">...,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">],</span>
          <span class="p">...,</span>
          <span class="p">[</span><span class="mi">1023</span><span class="p">,</span> <span class="mi">1023</span><span class="p">,</span> <span class="mi">1023</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mi">512</span><span class="p">,</span>  <span class="mi">511</span><span class="p">,</span>  <span class="mi">510</span><span class="p">],</span>
          <span class="p">[</span><span class="mi">1023</span><span class="p">,</span> <span class="mi">1023</span><span class="p">,</span> <span class="mi">1023</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mi">513</span><span class="p">,</span>  <span class="mi">512</span><span class="p">,</span>  <span class="mi">511</span><span class="p">],</span>
          <span class="p">[</span><span class="mi">1023</span><span class="p">,</span> <span class="mi">1023</span><span class="p">,</span> <span class="mi">1023</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mi">514</span><span class="p">,</span>  <span class="mi">513</span><span class="p">,</span>  <span class="mi">512</span><span class="p">]],</span>
<span class="n">torch</span><span class="p">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1024</span><span class="p">,</span> <span class="mi">1024</span><span class="p">]),</span>
<span class="n">torch</span><span class="p">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1024</span><span class="p">,</span> <span class="mi">1024</span><span class="p">]))</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">gather</span><span class="p">(</span><span class="n">tmp_c2p</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">rel_pos_matrix</span><span class="p">)</span>
<span class="n">tensor</span><span class="p">([[[</span><span class="o">-</span><span class="mf">0.8579</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2178</span><span class="p">,</span>  <span class="mf">1.6323</span><span class="p">,</span>  <span class="p">...,</span> <span class="o">-</span><span class="mf">2.6477</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.6477</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.6477</span><span class="p">],</span>
         <span class="p">[</span> <span class="mf">1.1601</span><span class="p">,</span>  <span class="mf">2.1752</span><span class="p">,</span>  <span class="mf">0.7187</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mf">0.0662</span><span class="p">,</span>  <span class="mf">0.0662</span><span class="p">,</span>  <span class="mf">0.0662</span><span class="p">],</span>
         <span class="p">[</span> <span class="mf">3.4379</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2573</span><span class="p">,</span>  <span class="mf">0.1375</span><span class="p">,</span>  <span class="p">...,</span> <span class="o">-</span><span class="mf">0.5010</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5010</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5010</span><span class="p">],</span>
         <span class="p">...,</span>
         <span class="p">[</span><span class="o">-</span><span class="mf">1.2066</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2066</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2066</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mf">0.5943</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5169</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.0820</span><span class="p">],</span>
         <span class="p">[</span> <span class="mf">2.6996</span><span class="p">,</span>  <span class="mf">2.6996</span><span class="p">,</span>  <span class="mf">2.6996</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mf">0.2014</span><span class="p">,</span>  <span class="mf">1.1458</span><span class="p">,</span>  <span class="mf">3.2626</span><span class="p">],</span>
         <span class="p">[</span><span class="o">-</span><span class="mf">0.1708</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1708</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1708</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mf">1.9955</span><span class="p">,</span>  <span class="mf">4.1549</span><span class="p">,</span>  <span class="mf">2.6356</span><span class="p">]],</span>
</code></pre></div></div>

<p>ìœ„ ì½”ë“œëŠ” <code class="language-plaintext highlighter-rouge">DeBERTa</code> ì˜ <code class="language-plaintext highlighter-rouge">Disentangled Self-Attention</code>ì„ êµ¬í˜„í•œ ì½”ë“œì˜ ì¼ë¶€ë¶„ì´ë‹¤. ìì„¸í•œ ì›ë¦¬ëŠ” <code class="language-plaintext highlighter-rouge">DeBERTa</code> ë…¼ë¬¸ ë¦¬ë·° í¬ìŠ¤íŒ…ì—ì„œ í™•ì¸í•˜ë©´ ë˜ê³ , ìš°ë¦¬ê°€ ì§€ê¸ˆ ì£¼ëª©í•  ë¶€ë¶„ì€ ë°”ë¡œ <code class="language-plaintext highlighter-rouge">tmp_c2p</code>, <code class="language-plaintext highlighter-rouge">rel_pos_matrix</code> ê·¸ë¦¬ê³  ë§ˆì§€ë§‰ ì¤„ì— ìœ„ì¹˜í•œ <code class="language-plaintext highlighter-rouge">torch.gather</code> ë‹¤. <code class="language-plaintext highlighter-rouge">[10, 1024, 1024]</code> ëª¨ì–‘ì„ ê°€ì§„ ëŒ€ìƒ í…ì„œ <code class="language-plaintext highlighter-rouge">tmp_c2p</code> ì—ì„œ ë‚´ê°€ ì›í•˜ëŠ” ì›ì†Œë§Œ ì¶”ì¶œí•˜ë ¤ëŠ” ìƒí™©ì¸ë°, ì¶”ì¶œí•´ì•¼í•  ì›ì†Œì˜ ì¸ë±ìŠ¤ ê°’ì´ ë‹´ê¸´ í…ì„œë¥¼ <code class="language-plaintext highlighter-rouge">rel_pos_matrix</code> ë¡œ ì •ì˜í–ˆë‹¤. <code class="language-plaintext highlighter-rouge">rel_pos_matrix</code> ì˜ ì°¨ì›ì€ <code class="language-plaintext highlighter-rouge">[10, 1024, 1024]</code>ë¡œ <code class="language-plaintext highlighter-rouge">tmp_c2p</code>ì™€ ë™ì¼í•˜ë‹¤. ì°¸ê³ ë¡œ ì¶”ì¶œí•´ì•¼ í•˜ëŠ” ì°¨ì› ë°©í–¥ì€ ê°€ë¡œ ë°©í–¥(ë‘ ë²ˆì§¸ 1024)ì´ë‹¤.</p>

<p>ì´ì œ <code class="language-plaintext highlighter-rouge">torch.gather</code>ì˜ ë™ì‘ì„ ì‚´í´ë³´ì. ìš°ë¦¬ê°€ í˜„ì¬ ì¶”ì¶œí•˜ê³  ì‹¶ì€ ëŒ€ìƒì€ 3ì°¨ì› í…ì„œì˜ ê°€ë¡œ ë°©í–¥(ë‘ ë²ˆì§¸ 1024, í…ì„œì˜ í–‰ ë²¡í„°), ì¦‰ <code class="language-plaintext highlighter-rouge">2 * max_sequence_length</code> ë¥¼ ì˜ë¯¸í•˜ëŠ” ì°¨ì› ë°©í–¥ì˜ ì›ì†Œë‹¤. ë”°ë¼ì„œ <code class="language-plaintext highlighter-rouge">dim=-1</code>ìœ¼ë¡œ ì„¤ì •í•´ì¤€ë‹¤. ì´ì œ ë©”ì„œë“œê°€ ì˜ë„ëŒ€ë¡œ ì ìš©ë˜ì—ˆëŠ”ì§€ í™•ì¸í•´ë³´ì. <code class="language-plaintext highlighter-rouge">rel_pos_matrix</code> ì˜ 0ë²ˆ ë°°ì¹˜, 0ë²ˆì§¸ ì‹œí€€ìŠ¤ì˜ ê°€ì¥ ë§ˆì§€ë§‰ ì°¨ì›ì˜ ê°’ì€ <code class="language-plaintext highlighter-rouge">0</code>ìœ¼ë¡œ ì´ˆê¸°í™” ë˜ì–´ ìˆë‹¤. ë‹¤ì‹œ ë§í•´, ëŒ€ìƒ í…ì„œì˜ ëŒ€ìƒ ì°¨ì›ì—ì„œ 0ë²ˆì§¸ ì¸ë±ìŠ¤ì— í•´ë‹¹í•˜ëŠ” ê°’ì„ ê°€ì ¸ì˜¤ë¼ëŠ” ì˜ë¯¸ë¥¼ ë‹´ê³  ìˆë‹¤. ê·¸ë ‡ë‹¤ë©´ <code class="language-plaintext highlighter-rouge">torch.gather</code> ì‹¤í–‰ ê²°ê³¼ê°€ <code class="language-plaintext highlighter-rouge">tmp_c2p</code>ì˜ 0ë²ˆ ë°°ì¹˜, 0ë²ˆì§¸ ì‹œí€€ìŠ¤ì˜ 0ë²ˆì§¸ ì°¨ì› ê°’ê³¼ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸í•´ë³´ì. ë‘˜ ë‹¤ <code class="language-plaintext highlighter-rouge">-2.6477</code>, <code class="language-plaintext highlighter-rouge">-2.6477</code> ìœ¼ë¡œ ê°™ì€ ê°’ì„ ë‚˜íƒ€ë‚´ê³  ìˆë‹¤. ë”°ë¼ì„œ ìš°ë¦¬ ì˜ë„ëŒ€ë¡œ ì˜ ì‹¤í–‰ë˜ì—ˆë‹¤ëŠ” ì‚¬ì‹¤ì„ ì•Œ ìˆ˜ ìˆë‹¤.</p>

<h3 id="torchtriu-torchtril"><code class="language-plaintext highlighter-rouge">ğŸ‘©â€ğŸ‘©â€ğŸ‘§â€ğŸ‘¦Â torch.triu, torch.tril</code></h3>

<p>ê°ê° ì…ë ¥ í…ì„œë¥¼ <code class="language-plaintext highlighter-rouge">ìƒì‚¼ê°í–‰ë ¬</code>, <code class="language-plaintext highlighter-rouge">í•˜ì‚¼ê°í–‰ë ¬</code>ë¡œ ë§Œë“ ë‹¤. <code class="language-plaintext highlighter-rouge">triu</code>ë‚˜ <code class="language-plaintext highlighter-rouge">tril</code>ì€ ì‚¬ì‹¤ ë’¤ì§‘ìœ¼ë©´ ê°™ì€ ê²°ê³¼ë¥¼ ë°˜í™˜í•˜ê¸° ë•Œë¬¸ì— <code class="language-plaintext highlighter-rouge">tril</code>ì„ ê¸°ì¤€ìœ¼ë¡œ ì„¤ëª…ì„ í•˜ê² ë‹¤. ë©”ì„œë“œì˜ ë§¤ê°œë³€ìˆ˜ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># torch.triu, tril params
</span><span class="n">upper_tri_matrix</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">triu</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span> <span class="n">diagonal</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
<span class="n">lower_tri_matrix</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tril</span><span class="p">(</span><span class="n">input_tensors</span><span class="p">,</span> <span class="n">diagonal</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">diagonal</code> ì— ì£¼ëª©í•´ë³´ì. ì–‘ìˆ˜ë¥¼ ì „ë‹¬í•˜ë©´ ì£¼ëŒ€ê°ì„±ë¶„ì—ì„œ í•´ë‹¹í•˜ëŠ” ê°’ë§Œí¼ ë–¨ì–´ì§„ ê³³ì˜ ëŒ€ê°ì„±ë¶„ê¹Œì§€ ê·¸ ê°’ì„ ì‚´ë ¤ë‘”ë‹¤. í•œí¸ ìŒìˆ˜ë¥¼ ì „ë‹¬í•˜ë©´ ì£¼ëŒ€ê°ì„±ë¶„ì„ í¬í•¨í•´ ì£¼ì–´ì§„ ê°’ë§Œí¼ ë–¨ì–´ì§„ ê³³ê¹Œì§€ì˜ ëŒ€ê°ì„±ë¶„ì„ ëª¨ë‘ 0ìœ¼ë¡œ ë§Œë“¤ì–´ë²„ë¦°ë‹¤. ê¸°ë³¸ì€ 0ìœ¼ë¡œ ì„¤ì •ë˜ì–´ ìˆìœ¼ë©°, ì´ëŠ” ì£¼ëŒ€ê°ì„±ë¶„ë¶€í„° ì™¼ìª½ í•˜ë‹¨ì˜ ì›ì†Œë¥¼ ëª¨ë‘ ì‚´ë ¤ë‘ê² ë‹¤ëŠ” ì˜ë¯¸ê°€ ëœë‹¤.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># torch.tril usage example
</span><span class="o">&gt;&gt;&gt;</span> <span class="n">lm_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tril</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">ones</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">lm_mask</span>
<span class="mi">1</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span>
<span class="mi">1</span> <span class="mi">1</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span>
<span class="mi">1</span> <span class="mi">1</span> <span class="mi">1</span> <span class="mi">0</span> <span class="mi">0</span>
<span class="mi">1</span> <span class="mi">1</span> <span class="mi">1</span> <span class="mi">1</span> <span class="mi">0</span>
</code></pre></div></div>

<p>ë‘ ë©”ì„œë“œëŠ” ì„ í˜•ëŒ€ìˆ˜í•™ì´ í•„ìš”í•œ ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ ì‚¬ìš©ë˜ëŠ”ë°, í•„ìì˜ ê²½ìš°, <code class="language-plaintext highlighter-rouge">GPT</code>ì²˜ëŸ¼ <code class="language-plaintext highlighter-rouge">Transformer</code>ì˜ <code class="language-plaintext highlighter-rouge">Decoder</code> ë¥¼ ì‚¬ìš©í•˜ëŠ” ëª¨ë¸ì„ ë¹Œë“œí•  ë•Œ ê°€ì¥ ë§ì´ ì‚¬ìš©í–ˆë˜ ê²ƒ ê°™ë‹¤. <code class="language-plaintext highlighter-rouge">Decoder</code>ë¥¼ ì‚¬ìš©í•˜ëŠ” ëª¨ë¸ì€ ëŒ€ë¶€ë¶„ êµ¬ì¡°ìƒ <code class="language-plaintext highlighter-rouge">Language Modeling</code>ì„ ìœ„í•´ì„œ <code class="language-plaintext highlighter-rouge">Masked Multi-Head Self-Attention Block</code>ì„ ì‚¬ìš©í•˜ëŠ”ë° ì´ ë•Œ ë¯¸ë˜ ì‹œì ì˜ í† í° ì„ë² ë”© ê°’ì— ë§ˆìŠ¤í‚¹ì„ í•´ì£¼ê¸° ìœ„í•´ <code class="language-plaintext highlighter-rouge">torch.tril</code> ì„ ì‚¬ìš©í•˜ê²Œ ë˜ë‹ˆ ì°¸ê³ í•˜ì.</p>

<h3 id="torchtensormasked_fill"><code class="language-plaintext highlighter-rouge">ğŸ‘©â€ğŸ‘©â€ğŸ‘§â€ğŸ‘¦Â torch.Tensor.masked_fill</code></h3>

<p>ì‚¬ìš©ìê°€ ì§€ì •í•œ ê°’ì— í•´ë‹¹ë˜ëŠ” ì›ì†Œë¥¼ ëª¨ë‘ ë§ˆìŠ¤í‚¹ ì²˜ë¦¬í•´ì£¼ëŠ” ë©”ì„œë“œë‹¤. ë¨¼ì € ë§¤ê°œë³€ìˆ˜ë¥¼ í™•ì¸í•´ë³´ì.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># torch.Tensor.masked_fill params
</span><span class="n">input_tensors</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">6</span><span class="p">]])</span>
<span class="n">input_tensors</span><span class="p">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">mask</span><span class="p">:</span> <span class="n">BoolTensor</span><span class="p">,</span> <span class="n">value</span><span class="p">:</span> <span class="nb">float</span><span class="p">)</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">masked_fill</code> ì€ í…ì„œ ê°ì²´ì˜ ë‚´ë¶€ <code class="language-plaintext highlighter-rouge">attribute</code> ë¡œ ì •ì˜ë˜ê¸° ë•Œë¬¸ì— í•´ë‹¹ ë©”ì„œë“œë¥¼ ì‚¬ìš©í•˜ê³  ì‹¶ë‹¤ë©´ ë¨¼ì € ë§ˆìŠ¤í‚¹ ëŒ€ìƒ í…ì„œë¥¼ ë§Œë“¤ì–´ì•¼ í•œë‹¤. í…ì„œë¥¼ ì •ì˜í–ˆë‹¤ë©´ í…ì„œ ê°ì²´ì˜ <code class="language-plaintext highlighter-rouge">attributes</code> ì ‘ê·¼ì„ í†µí•´ <code class="language-plaintext highlighter-rouge">masked_fill()</code> ì„ í˜¸ì¶œí•œ ë’¤, í•„ìš”í•œ ë§¤ê°œë³€ìˆ˜ë¥¼ ì „ë‹¬í•´ì£¼ëŠ” ë°©ì‹ìœ¼ë¡œ ì‚¬ìš©í•˜ë©´ ëœë‹¤.</p>

<p><code class="language-plaintext highlighter-rouge">mask</code> ë§¤ê°œë³€ìˆ˜ì—ëŠ” ë§ˆìŠ¤í‚¹ í…ì„œë¥¼ ì „ë‹¬í•´ì•¼ í•˜ëŠ”ë°, ì´ ë•Œ ë‚´ë¶€ ì›ì†ŒëŠ” ëª¨ë‘ <code class="language-plaintext highlighter-rouge">boolean</code>ì´ì–´ì•¼ í•˜ê³  í…ì„œì˜ í˜•íƒœëŠ” ëŒ€ìƒ í…ì„œì™€ ë™ì¼í•´ì•¼ í•œë‹¤(ì™„ì „íˆ ê°™ì„ í•„ìš”ëŠ” ì—†ê³ , ë¸Œë¡œë“œ ìºìŠ¤íŒ…ë§Œ ê°€ëŠ¥í•˜ë©´ ìƒê´€ ì—†ìŒ).</p>

<p><code class="language-plaintext highlighter-rouge">value</code> ë§¤ê°œë³€ìˆ˜ì—ëŠ” ë§ˆìŠ¤í‚¹ ëŒ€ìƒ ì›ì†Œë“¤ì— ì¼ê´„ì ìœ¼ë¡œ ì ìš©í•´ì£¼ê³  ì‹¶ì€ ê°’ì„ ì „ë‹¬í•œë‹¤. ì´ê²Œ ë§ë¡œë§Œ ë“¤ìœ¼ë©´ ì´í•´í•˜ê¸° ì‰½ì§€ ì•Šë‹¤. ì•„ë˜ ì‚¬ìš© ì˜ˆì‹œë¥¼ í•¨ê»˜ ì²¨ë¶€í–ˆìœ¼ë‹ˆ ì°¸ê³  ë°”ë€ë‹¤.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># torch.masked_fill usage
</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">lm_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tril</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">ones</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">lm_mask</span>
<span class="mi">1</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span>
<span class="mi">1</span> <span class="mi">1</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span>
<span class="mi">1</span> <span class="mi">1</span> <span class="mi">1</span> <span class="mi">0</span> <span class="mi">0</span>
<span class="mi">1</span> <span class="mi">1</span> <span class="mi">1</span> <span class="mi">1</span> <span class="mi">0</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">attention_matrix</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">))</span> <span class="o">/</span> <span class="n">dot_scale</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">attention_matrix</span>
<span class="mf">1.22</span> <span class="mf">2.1</span> <span class="mf">3.4</span> <span class="mf">1.2</span> <span class="mf">1.1</span>
<span class="mf">1.22</span> <span class="mf">2.1</span> <span class="mf">3.4</span> <span class="mf">9.9</span> <span class="mf">9.9</span>
<span class="mf">1.22</span> <span class="mf">2.1</span> <span class="mf">3.4</span> <span class="mf">9.9</span> <span class="mf">9.9</span>
<span class="mf">1.22</span> <span class="mf">2.1</span> <span class="mf">3.4</span> <span class="mf">9.9</span> <span class="mf">9.9</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">attention_matrix</span> <span class="o">=</span> <span class="n">attention_matrix</span><span class="p">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">lm_mask</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="s">'-inf'</span><span class="p">))</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">attention_matrix</span>
<span class="mf">1.22</span> <span class="o">-</span><span class="n">inf</span> <span class="o">-</span><span class="n">inf</span> <span class="o">-</span><span class="n">inf</span> <span class="o">-</span><span class="n">inf</span>
<span class="mf">1.22</span> <span class="mf">2.1</span> <span class="o">-</span><span class="n">inf</span> <span class="o">-</span><span class="n">inf</span> <span class="o">-</span><span class="n">inf</span>
<span class="mf">1.22</span> <span class="mf">2.1</span> <span class="mf">3.4</span> <span class="o">-</span><span class="n">inf</span> <span class="o">-</span><span class="n">inf</span>
<span class="mf">1.22</span> <span class="mf">2.1</span> <span class="mf">3.4</span> <span class="mf">9.9</span> <span class="o">-</span><span class="n">inf</span>
</code></pre></div></div>

<h3 id="ï¸torchclone"><code class="language-plaintext highlighter-rouge">ğŸ—‚ï¸Â torch.clone</code></h3>

<p><code class="language-plaintext highlighter-rouge">inputs</code> ì¸ìë¡œ ì „ë‹¬í•œ í…ì„œë¥¼ ë³µì‚¬í•˜ëŠ” íŒŒì´í† ì¹˜ ë‚´ì¥ ë©”ì„œë“œë‹¤.  ì‚¬ìš©ë²•ì€ ì•„ë˜ì™€ ê°™ë‹¤.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="s">""" torch.clone """</span>
<span class="n">torch</span><span class="p">.</span><span class="n">clone</span><span class="p">(</span>
    <span class="nb">input</span><span class="p">,</span><span class="err">Â </span>
    <span class="o">*</span><span class="p">,</span>
   <span class="err">Â </span><span class="n">memory_format</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">preserve_format</span>
<span class="p">)</span><span class="err">Â â†’Â </span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span>
</code></pre></div></div>

<p>ë”¥ëŸ¬ë‹ íŒŒì´í”„ë¼ì¸ì„ ë§Œë“¤ë‹¤ ë³´ë©´ ë§ì´ ì‚¬ìš©í•˜ê²Œ ë˜ëŠ” ê¸°ë³¸ì ì¸ ë©”ì„œë“œì¸ë°, ì´ë ‡ê²Œ ë”°ë¡œ ì •ë¦¬í•˜ê²Œ ëœ ì´ìœ ê°€ ìˆë‹¤. ì…ë ¥ëœ í…ì„œë¥¼ ê·¸ëŒ€ë¡œ ë³µì‚¬í•œë‹¤ëŠ” íŠ¹ì„± ë•Œë¬¸ì— ì‚¬ìš©ì‹œ ì£¼ì˜í•´ì•¼ í•  ì ì´ ìˆê¸° ë•Œë¬¸ì´ë‹¤. í•´ë‹¹ ë©”ì„œë“œë¥¼ ì‚¬ìš©í•˜ê¸° ì „ì— ë°˜ë“œì‹œ ì…ë ¥í•  í…ì„œê°€ í˜„ì¬ ì–´ëŠ ë””ë°”ì´ìŠ¤(CPU, GPU) ìœ„ì— ìˆëŠ”ì§€, ê·¸ë¦¬ê³  í•´ë‹¹ í…ì„œê°€ ê³„ì‚° ê·¸ë˜í”„ë¥¼ ê°€ì§€ê³  ìˆëŠ”ì§€ë¥¼ <strong>ë°˜ë“œì‹œ</strong> íŒŒì•…í•´ì•¼ í•œë‹¤.</p>

<p>í•„ìëŠ” ELECTRA ëª¨ë¸ì„ ì§ì ‘ êµ¬í˜„í•˜ëŠ” ê³¼ì •ì—ì„œ <code class="language-plaintext highlighter-rouge">clone()</code> ë©”ì„œë“œë¥¼ ì‚¬ìš©í–ˆëŠ”ë°, Generator ëª¨ë¸ì˜ ê²°ê³¼ ë¡œì§“ì„ Discriminatorì˜ ì…ë ¥ìœ¼ë¡œ ë³€í™˜í•´ì£¼ê¸° ìœ„í•¨ì´ì—ˆë‹¤. ê·¸ ê³¼ì •ì—ì„œ Generatorê°€ ë°˜í™˜í•œ ë¡œì§“ì„ ê·¸ëŒ€ë¡œ <code class="language-plaintext highlighter-rouge">clone</code>í•œ ë’¤, ì…ë ¥ì„ ë§Œë“¤ì–´ ì£¼ì—ˆê³  ê·¸ ê²°ê³¼ ì•„ë˜ì™€ ê°™ì€ ì—ëŸ¬ë¥¼ ë§ˆì£¼í–ˆë‹¤.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">RuntimeError</span><span class="p">:</span> <span class="n">one</span> <span class="n">of</span> <span class="n">the</span> <span class="n">variables</span> <span class="n">needed</span> <span class="k">for</span> <span class="n">gradient</span> <span class="n">computation</span> <span class="n">has</span> <span class="n">been</span> <span class="n">modified</span> <span class="n">by</span> <span class="n">an</span> <span class="n">inplace</span> <span class="n">operation</span><span class="p">:</span> <span class="p">[</span>
<span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">LongTensor</span> <span class="p">[</span><span class="mi">8</span><span class="p">,</span> <span class="mi">511</span><span class="p">]]</span> <span class="ow">is</span> <span class="n">at</span> <span class="n">version</span> <span class="mi">1</span><span class="p">;</span> <span class="n">expected</span> <span class="n">version</span> <span class="mi">0</span> 
<span class="n">instead</span><span class="p">.</span> <span class="n">Hint</span><span class="p">:</span> <span class="n">the</span> <span class="n">backtrace</span> <span class="n">further</span> <span class="n">above</span> <span class="n">shows</span> <span class="n">the</span> <span class="n">operation</span> <span class="n">that</span> <span class="n">failed</span> <span class="n">to</span> <span class="n">compute</span> <span class="n">its</span> <span class="n">gradient</span><span class="p">.</span> 
<span class="n">The</span> <span class="n">variable</span> <span class="ow">in</span> <span class="n">question</span> <span class="n">was</span> <span class="n">changed</span> <span class="ow">in</span> <span class="n">there</span> <span class="ow">or</span> <span class="n">anywhere</span> <span class="n">later</span><span class="p">.</span> <span class="n">Good</span> <span class="n">luck</span><span class="err">!</span>
</code></pre></div></div>

<p>ì—ëŸ¬ ë¡œê·¸ë¥¼ ìì„¸íˆ ì½ì–´ë³´ë©´ í…ì„œ ë²„ì „ì˜ ë³€ê²½ìœ¼ë¡œ ì¸í•´ ê·¸ë¼ë””ì–¸íŠ¸ ê³„ì‚°ì´ ë¶ˆê°€í•˜ë‹¤ëŠ” ë‚´ìš©ì´ ë‹´ê²¨ìˆë‹¤. êµ¬ê¸€ë§í•´ë´ë„ ì˜ ì•ˆë‚˜ì™€ì„œ í¬ê¸°í•˜ë ¤ë˜ ì°°ë¼ì— ìš°ì—°íˆ <code class="language-plaintext highlighter-rouge">torch.clone()</code> ë©”ì„œë“œì˜ ì •í™•í•œ ì‚¬ìš©ë²•ì´ ê¶ê¸ˆí•´ ê³µì‹ Docsë¥¼ ì½ê²Œ ë˜ì—ˆê³ , ê±°ê¸°ì„œ ì—„ì²­ë‚œ ì‚¬ì‹¤ì„ ë°œê²¬í–ˆë‹¤. <code class="language-plaintext highlighter-rouge">clone()</code> ë©”ì„œë“œê°€ ì…ë ¥ëœ í…ì„œì˜ í˜„ì¬ ë””ë°”ì´ìŠ¤ ìœ„ì¹˜ì— ë˜‘ê°™ì´ ë³µì‚¬ë  ê²ƒì´ë€ ì˜ˆìƒì€ í–ˆì§€ë§Œ, ì…ë ¥ í…ì„œì˜ ê³„ì‚°ê·¸ë˜í”„ê¹Œì§€ ë³µì‚¬ë  ê²ƒì´ë€ ìƒê°ì€ ì „í˜€ í•˜ì§€ ëª»í–ˆê¸° ë•Œë¬¸ì´ë‹¤. ê·¸ë˜ì„œ ìœ„ì™€ ê°™ì€ ì—ëŸ¬ë¥¼ ë§ˆì£¼í•˜ì§€ ì•Šìœ¼ë ¤ë©´, <code class="language-plaintext highlighter-rouge">clone()</code>ì„ í˜¸ì¶œí•  ë•Œ ë’¤ì— ë°˜ë“œì‹œ <code class="language-plaintext highlighter-rouge">detach()</code>ë¥¼ í•¨ê»˜ í˜¸ì¶œí•´ì¤˜ì•¼ í•œë‹¤.</p>

<p><code class="language-plaintext highlighter-rouge">clone()</code> ë©”ì„œë“œëŠ” ì…ë ¥ëœ í…ì„œì˜ ëª¨ë“  ê²ƒì„ ë³µì‚¬í•œë‹¤ëŠ” ì ì„ ë°˜ë“œì‹œ ê¸°ì–µí•˜ì.</p>]]></content><author><name>qcqced</name><email>qcqced123@gmail.com</email></author><category term="Framework &amp; Library" /><category term="Pytorch" /><category term="Tensor" /><category term="Linear Algebra" /><summary type="html"><![CDATA[íŒŒì´í† ì¹˜ì—ì„œ ìì£¼ ì‚¬ìš©í•˜ëŠ” í…ì„œ ì¸ë±ì‹± ê´€ë ¨ ë©”ì„œë“œ ëª¨ìŒ]]></summary></entry><entry><title type="html">ğŸ“ˆÂ Chain Rule: í•©ì„±í•¨ìˆ˜ ë¯¸ë¶„ë²•</title><link href="http://localhost:4000/optimization-theory/chain-rule" rel="alternate" type="text/html" title="ğŸ“ˆÂ Chain Rule: í•©ì„±í•¨ìˆ˜ ë¯¸ë¶„ë²•" /><published>2024-01-08T00:00:00+09:00</published><updated>2024-01-08T23:00:00+09:00</updated><id>http://localhost:4000/optimization-theory/chain_rule</id><content type="html" xml:base="http://localhost:4000/optimization-theory/chain-rule"><![CDATA[<p><code class="language-plaintext highlighter-rouge">Chain Rule</code> ì´ë¼ê³  ë¶ˆë¦¬ê¸°ë„ í•˜ëŠ” í•©ì„±í•¨ìˆ˜ ë¯¸ë¶„ë²•ì€ ë¯¸ì ë¶„í•™ì—ì„œ íŠ¹íˆë‚˜ ì¤‘ìš”í•œ ê°œë… ì¤‘ í•˜ë‚˜ë‹¤. ê·¼ë˜ì—ëŠ” ì‹ ê²½ë§ì„ í™œìš©í•œ ë”¥ëŸ¬ë‹ì´ ì£¼ëª©ë°›ìœ¼ë©´ì„œ ê·¸ ì¤‘ìš”ì„±ì´ ë”ìš± ë¶€ê°ë˜ê³  ìˆë‹¤. ì‹ ê²½ë§ ëª¨ë¸ì€ ì‰½ê²Œ ìƒê°í•˜ë©´ ì •ë§ ë§ì€ 1ì°¨í•¨ìˆ˜ì™€ ì—¬ëŸ¬ í™œì„±í•¨ìˆ˜ë¥¼ í•©ì„±í•œ ê²ƒê³¼ ê°™ê¸° ë•Œë¬¸ì´ë‹¤. ë”°ë¼ì„œ ì˜¤ì°¨ ì—­ì „ì„ í†µí•´ ê°€ì¤‘ì¹˜ë¥¼ ìµœì í™” í•˜ëŠ” ê³¼ì •ì„ ì •í™•íˆ ì´í•´í•˜ë ¤ë©´ í•©ì„±í•¨ìˆ˜ ë¯¸ë¶„ë²•ì— ëŒ€í•œ ì´í•´ëŠ” í•„ìˆ˜ì ì´ë‹¤.</p>

<h3 id="concept"><code class="language-plaintext highlighter-rouge">ğŸ’¡Â Concept</code></h3>

<p>í•©ì„±í•¨ìˆ˜ ë¯¸ë¶„ì„ ì´í•´í•˜ê¸° ì „ì— ë¨¼ì € ë„í•¨ìˆ˜ í‘œê¸°ë²•ì— ëŒ€í•´ì„œ ìµìˆ™í•´ì§ˆ í•„ìš”ê°€ ìˆë‹¤. ë„í•¨ìˆ˜ í‘œê¸°ë²•ì€ í¬ê²Œ ë‰´í„´ í‘œê¸°ë²•ê³¼ ë¼ì´í”„ë‹ˆì¸  í‘œê¸°ë²•ìœ¼ë¡œ ë‚˜ë‰œë‹¤. ì•„ë˜ í‘œê¸°ëœ ìˆ˜ì‹ì„ ë³´ì. (1)ë²ˆ ìˆ˜ì‹ì´ ë‰´í„´ í‘œê¸°ë²•, (2)ë²ˆì´ ë¼ì´í”„ë‹ˆì¸ ì˜ í‘œê¸°ë²•ì´ë‹¤.</p>

\[y'= 2x\ \ \ \ (1) \\
f'(x) = 2x\ \ \ (2)\]

<p>ì´ì œ ì•„ë˜ì™€ ê°™ì€ í•©ì„±í•¨ìˆ˜ê°€ ìˆë‹¤ê³  ê°€ì •í•´ë³´ì. ë„í•¨ìˆ˜ì˜ ì •ì˜ì— ë”°ë¼ì„œ í•´ë‹¹ í•¨ìˆ˜ì— ëŒ€í•œ ë„í•¨ìˆ˜ëŠ” ë‹¤ìŒê³¼ ê°™ì´ í‘œí˜„í•  ìˆ˜ ìˆì„ ê²ƒì´ë‹¤.</p>

\[y = g(f(x))\ \ \ (3) \\
y' = {g(f(x))}' = \frac{dy}{dx}\ \ \ (4)\]

<p>ìœ„ì— ì‘ì„±í•œ ìˆ˜ì‹ë“¤ì€ ì ì‹œ ìŠê³  ì´ì œ $fâ€™(x)$ ë¥¼ ë¨¼ì € ìƒê°í•´ë³´ì. $f(x)$ì— ëŒ€í•œ ë„í•¨ìˆ˜ $fâ€™(x)$ëŠ” ë‹¤ìŒê³¼ ê°™ì´ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆë‹¤.</p>

\[u = f(x) \\
u' = f'(x) = \frac{du}{dx} \\\]

<p>ê·¸ë ‡ë‹¤ë©´ í•¨ìˆ˜ $g$ì— ëŒ€í•œ ë„í•¨ìˆ˜ëŠ” ì–´ë–»ê²Œ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆì„ê¹Œ?? ë‹µì€ ë°”ë¡œ ì¹˜í™˜ì„ ì´ìš©í•˜ëŠ” ê²ƒì´ë‹¤. ìœ„ì—ì„œ ìš°ë¦¬ëŠ” $f(x)$ê°€ $u$ì™€ ê°™ë‹¤ê³  ì •ì˜í–ˆë‹¤. ì´ê²ƒì„ ì´ìš©í•´ ë„í•¨ìˆ˜ $gâ€™$ëŠ” ë‹¤ìŒê³¼ ê°™ì´ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆê² ë‹¤.</p>

\[y' = g'(u) = \frac{dy}{du} \\\]

<p>ëˆˆì¹˜ê°€ ë¹ ë¥´ë‹¤ë©´ ë²Œì¨ ì™œ í•©ì„±í•¨ìˆ˜ ë¯¸ë¶„ë²•ì„ <code class="language-plaintext highlighter-rouge">Chain Rule</code>ì´ë¼ê³  ë¶€ë¥´ëŠ”ì§€ ê¹¨ë‹«ê²Œ ë˜ì—ˆì„ ê²ƒì´ë‹¤. (4)ë²ˆ ìˆ˜ì‹ì˜ ìš°ë³€ì€ ì‚¬ì‹¤ ë¶„ìì™€ ë¶„ëª¨ì— ìœ„ì¹˜í•œ $du$ê°€ ì•½ë¶„ëœ ê¼´ì´ë¼ê³  ë³¼ ìˆ˜ ìˆë‹¤. ì´ê²ƒì„ ë‰´í„´ í‘œê¸°ë²•ìœ¼ë¡œ ë‚˜íƒ€ë‚´ë©´ ì•„ë˜ì™€ ê°™ë‹¤.</p>

\[y' = \frac{dy}{du}â€¢\frac{du}{dx}\]

<p>ë‰´í„´ í‘œê¸°ë²•ì€ ì§ê´€ì ì´ì§€ ì•Šê¸° ë•Œë¬¸ì— ë¼ì´í”„ë‹ˆì¸  í‘œê¸°ë²•ìœ¼ë¡œ ë‹¤ì‹œ ì •ë¦¬í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.</p>

\[y' = g'(f(x))â€¢f'(x)\]

<p>ì•„ë§ˆ ê³ ë“±í•™êµ ë•ŒëŠ” í•©ì„±í•¨ìˆ˜ ë¯¸ë¶„ ê³µì‹ì„ ê²‰ë¯¸ë¶„â€¢ì†ë¯¸ë¶„ì´ë¼ëŠ” ëª…ì¹­ìœ¼ë¡œ ì²˜ìŒ ì ‘í–ˆì„ ê²ƒì´ë‹¤. ì•ˆì— ê°ì‹¸ì ¸ ìˆëŠ” í•¨ìˆ˜ë¥¼ ë¯¸ë¶„í•´ì„œ ë°–ìœ¼ë¡œ ë¹¼ë‚¸ë‹¤ í•´ì„œ ì†ë¯¸ë¶„ì´ë¼ ë¶€ë¥´ê³ , ë‹¤ì‹œ ì†ì€ ëƒ…ë‘ê³  ë°–ì˜ ë‘˜ëŸ¬ì ¸ ìˆëŠ” í•¨ìˆ˜ë§Œ ë¯¸ë¶„í•œë‹¤í•´ì„œ ê²‰ë¯¸ë¶„ì´ë¼ ì¹­í•˜ëŠ”ë°, ì´ë ‡ê²Œ ë‹¨ìˆœí•˜ê²Œ ì™¸ìš°ê¸°ë³´ë‹¤ëŠ” ê³µì‹ì´ ë„ì¶œë˜ëŠ” íë¦„ì„ ì´í•´í•˜ëŠ” ê²ƒì´ í›¨ì”¬ ì˜¤ë˜ ê¸°ì–µì— ë‚¨ëŠ”ë‹¤. ë¶€ë„ëŸ½ì§€ë§Œ í•„ìê°€ ë°”ë¡œ ê·¸ëŸ¬í–ˆë‹¤. ëŒ€í•™êµì— ì…í•™í•˜ê³  ìˆ˜ë„ ì—†ì´ í•©ì„±í•¨ìˆ˜ ë¯¸ë¶„ì„ í•´ì•¼ í–ˆì§€ë§Œ ì‚¬ìš©í•  ë•Œë§ˆë‹¤ ê¹Œë¨¹ì–´ì„œ ì¸í„°ë„·ì„ ì°¾ì•„ë³´ê±°ë‚˜ êµê³¼ì„œë¥¼ ë’¤ì ë’¤ì  í–ˆë˜ ê¸°ì–µì´ ìˆë‹¤. ì´ê¸€ì„ ì½ëŠ” ë…ìë“¤ì€ ë‚˜ì™€ ê°™ì€ ì‹¤ìˆ˜ë¥¼ ë°˜ë³µí•˜ì§€ ì•Šê¸°ë¥¼ ë°”ë¼ë©° í¬ìŠ¤íŒ…ì„ ë§ˆì¹œë‹¤.</p>]]></content><author><name>qcqced</name><email>qcqced123@gmail.com</email></author><category term="Optimization Theory" /><category term="Calculus" /><summary type="html"><![CDATA[í•©ì„±í•¨ìˆ˜ ë¯¸ë¶„ë²• ê³µì‹ ìœ ë„]]></summary></entry><entry><title type="html">ğŸ”¢Â Product &amp;amp; Quotient Rule: ê³±ì˜ ë¯¸ë¶„, ëª«ì˜ ë¯¸ë¶„</title><link href="http://localhost:4000/optimization-theory/product_quotient_rule" rel="alternate" type="text/html" title="ğŸ”¢Â Product &amp;amp; Quotient Rule: ê³±ì˜ ë¯¸ë¶„, ëª«ì˜ ë¯¸ë¶„" /><published>2024-01-08T00:00:00+09:00</published><updated>2024-01-08T23:00:00+09:00</updated><id>http://localhost:4000/optimization-theory/product_quotient_rule</id><content type="html" xml:base="http://localhost:4000/optimization-theory/product_quotient_rule"><![CDATA[<p>ê³±ì˜ ë¯¸ë¶„, ëª«ì˜ ë¯¸ë¶„ì€ í•¨ìˆ˜ê°€ ê³±ì˜ ê¼´ í˜•íƒœ $f(x)g(x)$ í˜¹ì€ ë¶„ìˆ˜ ê¼´ í˜•íƒœ $\frac{f(x)}{g(x)}$ë¥¼ ê°€ì§€ê³  ìˆì„ ë•Œ ë„í•¨ìˆ˜ë¥¼ êµ¬í•˜ëŠ” ë°©ë²•ì´ë‹¤. ê³ ë“±í•™êµ ë¯¸ì ë¶„ ì‹œê°„(17~18í•™ë²ˆ ê¸°ì¤€)ì— ë°°ìš´ì ì´ ìˆì§€ë§Œ, í•©ì„±í•¨ìˆ˜ ë¯¸ë¶„ë²•ê³¼ ë”ë¶ˆì–´ ë‹¨ìˆœ ì•”ê¸°ì˜ íí•´ë¡œ ê¹Œë¨¹ê¸° ì¢‹ì€ ë¯¸ë¶„ë²•ë“¤ì´ë‹¤. í¬ë¡œìŠ¤ ì—”íŠ¸ë¡œí”¼, ì†Œí”„íŠ¸ë§¥ìŠ¤ ë¯¸ë¶„ì— ì“°ì´ë¯€ë¡œ í•©ì„±í•¨ìˆ˜ ë¯¸ë¶„ë²•ê³¼ ë§ˆì°¬ê°€ì§€ë¡œ ë”¥ëŸ¬ë‹, ë¨¸ì‹ ëŸ¬ë‹ì—ì„œ ë§¤ìš° ì¤‘ìš”í•˜ë‹¤.</p>

<h3 id="ï¸product-rule"><code class="language-plaintext highlighter-rouge">âœ–ï¸Â Product Rule</code></h3>

<p>ëª«ì˜ ë¯¸ë¶„ì€ ê³±ì˜ ë¯¸ë¶„ì˜ ì›ë¦¬ë¥¼ ì´í•´í•˜ë©´ ìë™ìœ¼ë¡œ ì•Œ ìˆ˜ ìˆê¸° ë•Œë¬¸ì— ê³±ì˜ ë¯¸ë¶„ë¶€í„° ì‚´í´ë³´ê² ë‹¤. ë¨¼ì € ì•„ë˜ì™€ ê°™ì´ ê³±ì˜ í˜•íƒœë¥¼ ê°€ì§€ëŠ” í•¨ìˆ˜ $p(x)$ê°€ ìˆë‹¤ê³  ê°€ì •í•´ë³´ì.</p>

\[p(x) = f(x)g(x)\ \ \ (0)\]

<p>ìš°ë³€ì˜ ë‘ í•­ì„ ë¶„ë¦¬í•˜ê¸° ì „ì— ë„í•¨ìˆ˜ì˜ ì •ì˜ë¥¼ ì´ìš©í•´ ë„í•¨ìˆ˜ $pâ€™(x)$ë¥¼ êµ¬í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.</p>

\[p'(x) = \lim_{h -&gt; 0} \frac{p(x+h) - p(x)}{h}\ \ \ (1)\]

<p>ì´ì œ ë‹¤ì‹œ $p(x+h),\  p(x)$ì— $f(x)g(x)$ë¥¼ ëŒ€ì…í•´ë³´ì.</p>

\[p'(x) = \lim_{h -&gt; 0} \frac{p(x+h) - p(x)}{h} = \lim_{h-&gt;0} \frac{f(x+h)g(x+h) - f(x)(g(x)}{h}\ \ \ (2)\]

<p>ì´ ì§€ì ì—ì„œ ìš°ë¦¬ê°€ ë­˜í•˜ë ¤ê³  ì§€ê¸ˆ ì´ë ‡ê²Œ ìˆ˜ì‹ì„ ì „ê°œí•˜ê³  ìˆëŠ”ì§€ ìƒê¸°í•  í•„ìš”ê°€ ìˆë‹¤. ìš°ë¦¬ëŠ” ê³±ì˜ í˜•íƒœë¥¼ ê°–ëŠ” í•¨ìˆ˜ì˜ <code class="language-plaintext highlighter-rouge">ë„í•¨ìˆ˜</code>ë¥¼ êµ¬í•˜ê³  ì‹¶ì€ ê²ƒì´ë‹¤. (1)ë²ˆì²˜ëŸ¼ í•¨ìˆ˜ì— ëŒ€ì…í•˜ëŠ” ì…ë ¥ê°’ì„ ëº€ ê²°ê³¼ê°€ $h$ê°€ ë˜ë„ë¡ ë§ì´ë‹¤. (1)ê³¼ ê°™ì€ ê¼´ì„ ë§Œë“¤ì–´ì£¼ê¸° ìœ„í•´ ì•½ê°„ì˜ íŠ¸ë¦­ì„ ì“¸ í•„ìš”ê°€ ìˆë‹¤. ì‚¬ìš©í•  íŠ¸ë¦­ì€ ëŒ€ìˆ˜í•™ì—ì„œ ì •ë§ ë¹ˆë²ˆí•˜ê²Œ ì‚¬ìš©ë˜ë¯€ë¡œ ì˜ ê¸°ì–µí•˜ê³  ìˆëŠ”ê²Œ ì¢‹ë‹¤. ë°”ë¡œ $A-A = 0$ì´ë¼ëŠ” ê²ƒì„ ì´ìš©í•˜ëŠ” ê²ƒì´ë‹¤. ì´ê²Œ ë¬´ìŠ¨ ë§ì¸ê°€ëŠ” ì•„ë˜ ìˆ˜ì‹ì„ ë³´ë©´ ì•Œ ìˆ˜ ìˆë‹¤.</p>

\[p'(x) = \lim_{h-&gt;0} \frac{f(x+h)g(x+h) - f(x)g(x+h) + f(x)g(x+h) - f(x)g(x)}{h}\ \ \ (3)\]

<p>(3)ë²ˆ ìˆ˜ì‹ì€ (2)ë²ˆ ìˆ˜ì‹ì˜ ë¶„ìì— $- f(x)g(x+h) + f(x)g(x+h)$ë§Œ ì¶”ê°€ëœ í˜•íƒœë‹¤. ë‘í•­ì„ ë”í•˜ë©´ 0ì´ ë˜ê¸° ë•Œë¬¸ì— ì‚¬ì‹¤ (2)ë²ˆê³¼ (3)ë²ˆì€ ê°™ì€ ìˆ˜ì‹ì´ë¼ê³  ë³¼ ìˆ˜ ìˆëŠ” ê²ƒì´ë‹¤. ê·¸ë˜ì„œ ë‘í•­ì„ ì¶”ê°€í•´ë„ ì „í˜€ ë¬¸ì œê°€ ì—†ë‹¤. ì´ì œ ìš°ë¦¬ê°€ ìµìˆ™í•œ ë„í•¨ìˆ˜ ì •ì˜ë¥¼ ë§Œì¡±í•˜ëŠ” í•­ë“¤ì´ ì§ê´€ì ìœ¼ë¡œ ë³´ì¸ë‹¤.</p>

\[p'(x) = f'(x)g(x) + f(x)g'(x)\ \ \ (4)\]

<p>ë”°ë¼ì„œ ìˆ˜ì‹ì„ ì •ë¦¬í•˜ë©´ ê²°êµ­ ê³±ì˜ í˜•íƒœë¥¼ ê°–ëŠ” í•¨ìˆ˜ì˜ ë„í•¨ìˆ˜ëŠ” (4)ë²ˆê³¼ ê°™ì€ ê³µì‹ì„ ê°–ê²Œ ë˜ëŠ” ê²ƒì´ë‹¤. í•œí¸, ê³±ì˜ ë¯¸ë¶„ë²•ì€ (0)ë²ˆ ìˆ˜ì‹ì„ ì§ì‚¬ê°í˜•ì˜ ë„“ì´ë¼ê³  ê°„ì£¼í•˜ë©´ ì¢€ ë” ì§ê´€ì ìœ¼ë¡œ ì´í•´í•  ìˆ˜ ìˆë‹¤. ë”°ë¼ì„œ ì§ì‚¬ê°í˜• ë„“ì´ $p(x)$ì— ëŒ€í•œ ë„í•¨ìˆ˜ $pâ€™(x)$ëŠ” ë„“ì´ì˜ ìˆœê°„ ë³€í™”ìœ¨ë¡œ í•´ì„í•  ìˆ˜ ìˆë‹¤.</p>

<p align="center">
<img src="/assets/images/optimization/product_rule_rectangle.jpeg" alt="ê³±ì˜ ë¯¸ë¶„ì˜ ì§ê´€ì  í•´ì„" class="align-center image-caption" width="50%&quot;, height=&quot;30%" />
<strong><em><a href="https://blog.naver.com/sodong212/220924875183">ê³±ì˜ ë¯¸ë¶„ì˜ ì§ê´€ì  í•´ì„</a></em></strong>
</p>

<p>ìš°ë³€ì˜ ì™¼ìª½í•­ì´ ì§ì‚¬ê°í˜•ì˜ ì•„ë˜ ë¶€ë¶„ì˜ ì¦ê°€í•˜ëŠ” ì˜ì—­ì´ ë˜ê³ , ìš°ì¸¡í•­ì´ íšŒìƒ‰ì´ ì¹ í•´ì§„ ì˜ì—­ì´ ë˜ëŠ” ê²ƒì´ë‹¤. ê·¸ë ‡ë‹¤ë©´ ê·¸ë¦¼ì˜ ìš°ì¸¡ í•˜ë‹¨ì— ìœ„ì¹˜í•œ ì‘ì€ ì‚¬ê°í˜•ì˜ ë„“ì´ëŠ” ì–´ë–»ê²Œ ì²˜ë¦¬í•´ì¤˜ì•¼ í• ê¹Œ?? ê³±ì˜ ë¯¸ë¶„ ê³µì‹ì—ëŠ” í•´ë‹¹ ì˜ì—­ì„ ë°˜ì˜í•˜ëŠ” í•­ì´ ì „í˜€ ì—†ë‹¤. ê·¸ ì´ìœ ëŠ” ì˜ì—­ì˜ ë„“ì´ê°€ ë„ˆë¬´ë‚˜ ì‘ì•„ì„œ ê·¼ì‚¬ì¹˜ë¡œ ê°„ì£¼í•˜ê³  ë¬´ì‹œí•´ë„ ë  ì •ë„ë¼ì„œ ê·¸ë ‡ë‹¤. í•´ë‹¹ ì‚¬ê°í˜•ì˜ ê°€ë¡œ ê¸¸ì´ëŠ” $gâ€™(h)$, ì„¸ë¡œ ê¸¸ì´ëŠ” $fâ€™(h)$ê°€ ëœë‹¤. ë„í•¨ìˆ˜ì˜ ì •ì˜ë¥¼ ë‹¤ì‹œ ë– ì˜¬ë ¤ë³´ë©´ $h$ëŠ” 0ì˜ ê·¹í•œìœ¼ë¡œ ê·¼ì‚¬í•˜ê¸° ë•Œë¬¸ì— ë‘ í•­ì˜ ê³±ì¸ ì˜ì—­ì˜ ë„“ì´ ì—­ì‹œ 0ì— ë§¤ìš° ê·¼ì ‘í•˜ê²Œ ëœë‹¤. ë”°ë¼ì„œ ê³ ë ¤í•  í•„ìš” ì—†ì´ ë¬´ì‹œí•´ë„ ëœë‹¤.</p>

<h3 id="quotient-rule"><code class="language-plaintext highlighter-rouge">â—Â Quotient Rule</code></h3>

<p>ëª«ì˜ ë¯¸ë¶„ì€ ê³±ì˜ ë¯¸ë¶„ ê³µì‹ì„ ì´ìš©í•˜ê³  ë‚˜ì„œ ë‚¨ì€ ì§€ì €ë¶„í•œ ìˆ˜ì‹ë§Œ ì˜ ì •ë¦¬í•˜ë©´ ëœë‹¤. ë‹¤ìŒê³¼ ê°™ì´ ë¶„ìˆ˜ ê¼´ í˜•íƒœì˜ í•¨ìˆ˜ $q(x)$ê°€ ìˆë‹¤ê³  ê°€ì •í•´ë³´ì.</p>

\[q(x) = \frac{f(x)}{g(x)} \ \ \ (0)\]

<p>ê³±ì˜ ë¯¸ë¶„ ê³µì‹ì„ ì´ìš©í•˜ê¸° ìœ„í•´ ë¶„ìˆ˜ ê¼´ì˜ í•¨ìˆ˜ë¥¼ ë‹¤ì‹œ ê³±ì˜ í˜•íƒœë¡œ ë°”ê¿”ë³´ì.</p>

\[f(x) = q(x)g(x) \ \ \ (1)\]

<p>ì´ì œ ê³µì‹ì„ ì´ìš©í•´ ì¢Œë³€ì— ëŒ€í•œ ë„í•¨ìˆ˜ë¥¼ êµ¬í•´ë³´ì.</p>

\[f'(x) = q'(x)g(x) + q(x)g'(x)\ \ \  (2)\]

<p>ìš°ë¦¬ê°€ êµ¬í•˜ê³  ì‹¶ì€ ê²ƒì€ ë¶„ìˆ˜ ê¼´ì„ ê°–ëŠ” í•¨ìˆ˜ $q(x)$ì˜ ë„í•¨ìˆ˜ $qâ€™(x)$ì´ë‹¤. ë”°ë¼ì„œ (2)ë²ˆ ìˆ˜ì‹ì„ $qâ€™(x)$ì— ëŒ€í•´ì„œ ì •ë¦¬í•´ì•¼ í•œë‹¤.</p>

\[q'(x) = \frac{f'(x) - q(x)g'(x)}{g(x)}\ \ \ (3)\]

<p>(3)ë²ˆ ìˆ˜ì‹ì„ ì˜ˆì˜ê²Œ ì •ë¦¬í•˜ê¸° ìœ„í•´ ê³µí†µ ë¶„ëª¨ $\frac{1}{g(x)}$ë¥¼ ì•ìœ¼ë¡œ ë¹¼ì£¼ê³ , $q(x)$ì— (0)ë²ˆ ìˆ˜ì‹ì„ ëŒ€ì…í•´ ì •ë¦¬í•˜ë©´ ëª«ì˜ ë¯¸ë¶„ë²• ê³µì‹, (5)ë²ˆì´ ë„ì¶œëœë‹¤.</p>

\[q'(x) = \frac{1}{g(x)}(\frac{f'(x)g(x)-f(x)g'(x)}{g(x)})\ \ \ (4) \\
q'(x) = \frac{f'(x)g(x)-f(x)g'(x)}{g(x)^2}\ \ \ (5)\]

<p>ì´ë ‡ê²Œ ê³±ì˜ ë¯¸ë¶„ë²•, ëª«ì˜ ë¯¸ë¶„ë²•ì˜ ê³µì‹ì´ ìœ ë„ë˜ëŠ” ê³¼ì •ì„ ëª¨ë‘ ì‚´í´ë³´ì•˜ë‹¤.</p>]]></content><author><name>qcqced</name><email>qcqced123@gmail.com</email></author><category term="Optimization Theory" /><category term="Calculus" /><category term="Product Rule" /><category term="Quotient Rule" /><summary type="html"><![CDATA[ê³±ì˜ ë¯¸ë¶„, ëª«ì˜ ë¯¸ë¶„ ê³µì‹ ìœ ë„]]></summary></entry><entry><title type="html">ğŸ—„ï¸Â SVD: Singular Value Decomposition</title><link href="http://localhost:4000/linear-algebra/svd" rel="alternate" type="text/html" title="ğŸ—„ï¸Â SVD: Singular Value Decomposition" /><published>2023-11-26T00:00:00+09:00</published><updated>2023-11-27T13:00:00+09:00</updated><id>http://localhost:4000/linear-algebra/svd</id><content type="html" xml:base="http://localhost:4000/linear-algebra/svd"><![CDATA[<p>íŠ¹ì´ê°’ ë¶„í•´ëŠ” ê³ ìœ ê°’ ë¶„í•´ë¥¼ ì¼ë°˜ì ì¸ ìƒí™©ìœ¼ë¡œ í™•ì¥ì‹œí‚¨ ê°œë…ìœ¼ë¡œ LSA(Latent Semantic Anaylsis), Collaborative Filteringê³¼ ê°™ì€ ë¨¸ì‹ ëŸ¬ë‹ ê¸°ë²•ì— ì‚¬ìš©ë˜ê¸° ë•Œë¬¸ì— ìì—°ì–´ì²˜ë¦¬, ì¶”ì²œì‹œìŠ¤í…œì— ê´€ì‹¬ì´ ìˆë‹¤ë©´ ë°˜ë“œì‹œ ì´í•´í•˜ê³  ë„˜ì–´ê°€ì•¼ í•˜ëŠ” ì¤‘ìš”í•œ ë°©ë²•ë¡ ì´ë‹¤. <a href="https://www.youtube.com/watch?v=PP9VQXKvSCY&amp;t=108s&amp;ab_channel=%ED%98%81%ED%8E%9C%ED%95%98%EC%9E%84%7CAI%26%EB%94%A5%EB%9F%AC%EB%8B%9D%EA%B0%95%EC%9D%98"><strong><u>í˜íœí•˜ì„ë‹˜ì˜ ì„ í˜•ëŒ€ìˆ˜í•™ ê°•ì˜</u></strong></a>ì™€ <a href="https://www.youtube.com/watch?v=7dmV3p3Iy90&amp;ab_channel=%EA%B3%B5%EB%8F%8C%EC%9D%B4%EC%9D%98%EC%88%98%ED%95%99%EC%A0%95%EB%A6%AC%EB%85%B8%ED%8A%B8"><strong><u>ê³µëŒì´ì˜ ìˆ˜í•™ì •ë¦¬ë‹˜ì˜ ê°•ì˜ ë° í¬ìŠ¤íŠ¸</u></strong></a> ê·¸ë¦¬ê³  <a href="https://product.kyobobook.co.kr/detail/S000001743773"><strong><u>ë”¥ëŸ¬ë‹ì„ ìœ„í•œ ì„ í˜•ëŒ€ìˆ˜í•™ êµì¬</u></strong></a>ì„ ì°¸ê³ í•˜ê³  ê°œì¸ì ì¸ í•´ì„ì„ ë”í•´ ì •ë¦¬í–ˆë‹¤.</p>

<h3 id="concept-of-svd"><code class="language-plaintext highlighter-rouge">ğŸŒŸÂ Concept of SVD</code></h3>

\[A = UÎ£V^T\]

<p>í¬ê¸°ê°€ <code class="language-plaintext highlighter-rouge">mxn</code>ì¸ ì„ì˜ì˜ í–‰ë ¬ $A$ë¥¼ ìœ„ ìˆ˜ì‹ì˜ ìš°ë³€ì²˜ëŸ¼ ì—¬ëŸ¬ ë‹¤ë¥¸ í–‰ë ¬ë¡œ ë¶„í•´í•˜ëŠ” ë°©ë²•ì„ ë§í•œë‹¤. í–‰ë ¬ ë¶„í•´ ê¸°ë²•ì´ë¼ëŠ” ì ì—ì„œ ê³ ìœ ê°’ ë¶„í•´ì™€ ê¶¤ë¥¼ ê°™ì´í•˜ì§€ë§Œ ì¢€ ë” ì‹¤ìš©ì ì¸ í˜•íƒœë¡œ ë³€í™”í–ˆë‹¤. ê³ ìœ ê°’ ë¶„í•´ëŠ” í–‰ë ¬ $A$ê°€ ì •ì‚¬ê°í–‰ë ¬ ì¼ ë•Œë§Œ ì ìš©í•  ìˆ˜ ìˆë‹¤ëŠ” í•œê³„ë¥¼ ê°€ì§€ê³  ìˆë‹¤. ì‹¤ìƒí™œì—ì„œ ë‹¤ë£¨ëŠ” í–‰ë ¬ ëª¨í˜•ì˜ í…Œì´ë¸” ë°ì´í„°ëŠ” 99.9999999%ì˜ í™•ë¥ ë¡œ ì§ì‚¬ê°í–‰ë ¬ì´ë‹¤. í•„ìëŠ” ì‚´ë©´ì„œ í•œ ë²ˆë„ ì •ì‚¬ê°í–‰ë ¬ í˜•íƒœì˜ í…Œì´ë¸” ë°ì´í„°ë¥¼ ë³¸ì ì´ ì—†ë‹¤. ì¦‰ ì‹¤ìƒí™œì˜ ë°ì´í„°ì— ê³ ìœ ê°’ ë¶„í•´ë¥¼ ì ìš©í•˜ê¸° ìœ„í•´ ì§ì‚¬ê°í–‰ë ¬ í˜•íƒœë¡œ í™•ì¥í•œ ê²ƒì´ ë°”ë¡œ íŠ¹ì´ê°’ ë¶„í•´ë‹¤.</p>

<p>ì„ í˜•ë³€í™˜ $A$
ê°€ ë‹¤ìŒê³¼ ê°™ë‹¤ê³  ê°€ì •í•˜ê³  êµ¬ì²´ì ì¸ ì˜ˆì‹œì™€ í•¨ê»˜ ìš°ë³€ì˜ í•­ë“¤ì— ëŒ€í•´ ìì„¸íˆ ì‚´í´ë³´ì.</p>

\[A = \begin{pmatrix}
1 &amp; 2 \\
3 &amp; 4 \\
5 &amp; 6
\end{pmatrix}\]

<p>ë¨¼ì € í–‰ë ¬ $U$ëŠ” í¬ê¸°ê°€ <code class="language-plaintext highlighter-rouge">mxm</code>ì¸ ì •ì‚¬ê°í–‰ë ¬ì´ë‹¤.</p>

\[U = \begin{pmatrix}-0.2298 &amp; 0.8835 &amp; 0.4082 \\-0.5247 &amp; 0.2408 &amp; -0.8165 \\-0.8196 &amp; -0.4019 &amp; 0.4082\end{pmatrix}\]

<p>ê³ ìœ ê°’ ë¶„í•´ì—ì„œ ê³ ìœ ë²¡í„°ë¥¼ ìŒ“ì•„ ë§Œë“  í–‰ë ¬ $V$ì™€ ë¹„ìŠ·í•œ ì—­í• ì„ í•˜ë©´ì„œ íŠ¹ì´ê°’ í–‰ë ¬ì˜ ì™¼ì¡±ì— ìˆë‹¤ëŠ” ì˜ë¯¸ì—ì„œ <code class="language-plaintext highlighter-rouge">Left Singular Vector</code>ë¼ê³ ë„ ë¶€ë¥¸ë‹¤. í–‰ë ¬ì˜ ê°œë³„ ì—´ì°¨ì›ì˜ ë²¡í„°ëŠ” ê³ ìœ í•œ íŠ¹ì´ë²¡í„°ê°€ ëœë‹¤. ë”°ë¼ì„œ ê°œë³„ ê³ ìœ ë²¡í„°ëŠ” ì„œë¡œì—ê²Œ ë…ë¦½ì´ë©° ì§êµí•œë‹¤. ë”°ë¼ì„œ ì „ì²´ í–‰ë ¬ $U$ëŠ” <code class="language-plaintext highlighter-rouge">Orthogonal Matrix</code>ê°€ ëœë‹¤.</p>

<p>í•œí¸,  $Î£$ëŠ” ê°œë³„ íŠ¹ì´ë²¡í„°ì— í•´ë‹¹ë˜ëŠ” íŠ¹ì´ê°’ë“¤ì„ ì €ì¥í•œ í–‰ë ¬ë¡œ í¬ê¸°ëŠ” <code class="language-plaintext highlighter-rouge">mxn</code>ì´ë‹¤.</p>

\[\Sigma = \begin{pmatrix}
\sigma_1 &amp; 0 \\
0 &amp; \sigma_2 \\
0 &amp; 0
\end{pmatrix}\]

<p>í˜„ì¬ ì œì‹œëœ ì˜ˆì‹œ í–‰ë ¬ì˜ í¬ê¸°ê°€ <code class="language-plaintext highlighter-rouge">m&gt;n</code>ì´ê¸° ë•Œë¬¸ì— $Î£$ì˜ ëª¨ì–‘ ì—­ì‹œ í–‰ ë°©í–¥ìœ¼ë¡œ ë” ê¸¸ê²Œ ë†“ì¸ ì§ì‚¬ê°í–‰ë ¬ì´ ëœë‹¤. ë°˜ëŒ€ì˜ ìƒí™©ì´ë¼ë©´ ì—´ë°©í–¥ìœ¼ë¡œ ê¸¸ê²Œ ëˆ„ì ëœ ì§ì‚¬ê°í–‰ë ¬ì´ ë  ê²ƒì´ë‹¤. ë˜í•œ $Î£$ì€ ì§ì‚¬ê°í–‰ë ¬ì´ì§€ë§Œ ëŒ€ê°í–‰ë ¬ì´ë¼ì„œ, ì£¼ëŒ€ê°ì„±ë¶„ ì›ì†Œë¥¼ ì œì™¸í•œ ë‚˜ë¨¸ì§€ ì›ì†ŒëŠ” ëª¨ë‘ ë°˜ë“œì‹œ 0ì´ ëœë‹¤.</p>

<p>ë§ˆì§€ë§‰ìœ¼ë¡œ í–‰ë ¬ $V^T$ëŠ” í¬ê¸°ê°€ <code class="language-plaintext highlighter-rouge">nxn</code>ì¸ ì •ì‚¬ê°í–‰ë ¬ì´ë‹¤.</p>

\[V^T = \begin{pmatrix}-0.6196 &amp; -0.7849 \\-0.7849 &amp; 0.6196\end{pmatrix}\]

<p>í–‰ë ¬ $U$ë•Œì™€ ê°™ì€ ë…¼ë¦¬ì™€ ë”ë¶ˆì–´ íŠ¹ì´ê°’ í–‰ë ¬ì˜ ì˜¤ë¥¸ì¡±ì— ë°°ì¹˜ë˜ì–´ ìˆê¸° ë•Œë¬¸ì— <code class="language-plaintext highlighter-rouge">Right singular Vector</code> ë¼ê³  ë¶€ë¥¸ë‹¤. ë§ˆì°¬ê°€ì§€ë¡œ ê°œë³„ ê³ ìœ ë²¡í„°ëŠ” ì„œë¡œì—ê²Œ ë…ë¦½ì´ë©° ì§êµí•œë‹¤. ë”°ë¼ì„œ ì „ì²´ í–‰ë ¬ $V^T$ì—­ì‹œ <code class="language-plaintext highlighter-rouge">Orthogonal Matrix</code>ê°€ ëœë‹¤.</p>

<p>ê·¸ë ‡ë‹¤ë©´ íŠ¹ì´ê°’ ë¶„í•´ëŠ” ì–´ë–»ê²Œ ê³ ìœ ê°’ ë¶„í•´ì˜ ì œì•½ì¡°ê±´ì„ íƒˆí”¼í–ˆëŠ”ì§€ ì‚´í´ë³´ì. ë°”ë¡œ í¬ê¸°ê°€ <code class="language-plaintext highlighter-rouge">mxn</code>ì¸ í–‰ë ¬ $A$ì™€ ê·¸ê²ƒì˜ ì „ì¹˜ í–‰ë ¬ $A^T$ ì‚¬ì´ì˜ ê³±ì€ í•­ìƒ ëŒ€ì¹­ í–‰ë ¬ì´ ë˜ê³ , ëŒ€ì¹­í–‰ë ¬ì€ ëŒ€ê°í™”-ê°€ëŠ¥ í–‰ë ¬ì´ë¼ëŠ” ì ì„ ì´ìš©í•œë‹¤.</p>

\[Aâ€¢A^T = UÎ£V^Tâ€¢VÎ£^TU^T = UÎ£Î£^TU^T \\
A^Tâ€¢A = VÎ£^TU^Tâ€¢UÎ£V^T = VÎ£^TÎ£V^T\]

<p>ìƒˆë¡­ê²Œ ì •ì˜ëœ í–‰ë ¬ $Aâ€¢A^T$ì€ í¬ê¸°ê°€ mxmì¸ ì •ì‚¬ê°â€¢ëŒ€ì¹­í–‰ë ¬ì´ ë˜ê³ , í–‰ë ¬ $A^Tâ€¢A$ì€ í¬ê¸°ê°€ nxnì¸ ì •ì‚¬ê°â€¢ëŒ€ì¹­í–‰ë ¬ì´ ëœë‹¤. í–‰ë ¬ $U,V$ ëª¨ë‘ <code class="language-plaintext highlighter-rouge">Orthogonal Matrix</code> ë¼ëŠ” ì ì„ ì´ìš©í•˜ë©´ ìš°ë¦¬ëŠ” ìƒˆë¡­ê²Œ ì •ì˜ëœ ë‘ í–‰ë ¬ì„ ê³ ìœ ê°’ë¶„í•´ ê²°ê³¼ì™€ ìœ ì‚¬í•˜ê²Œ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆë‹¤.</p>

\[Aâ€¢A^T = Q_m \Lambda_m Q^T_m \\
A^Tâ€¢A = Q_n \Lambda_n Q^T_n\]

<p>ìˆ˜ì‹ ìš°ë³€ì— ë†“ì¸ $\Lambda_m, \Lambda_n$ ì˜ êµ¬ì²´ì ì¸ ì˜ˆì‹œëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.</p>

\[\Lambda_m = \Sigma\Sigma^T = \begin{pmatrix}
\sigma_1^2 &amp; 0 &amp; 0 \\
0 &amp; \sigma_2^2 &amp; 0 \\
0 &amp; 0 &amp; 0 \\
\end{pmatrix} \\

\Lambda_n = \Sigma^T\Sigma = \begin{pmatrix}
\sigma_1^2 &amp; 0 \\
0 &amp; \sigma_2^2 \\
\end{pmatrix}\]

<p>ì´ì œ ë‘ í–‰ë ¬ì— ê°ê° ì–‘ì˜ ì œê³±ê·¼ì„ ì‚¬ìš©í•´ $\sigma_1, \sigma_2$(íŠ¹ì´ê°’)ë¥¼ êµ¬í•˜ê³  ì›ë˜ ìˆ˜ì‹($A = UÎ£V^T$)ì˜ $Î£$ì— ëŒ€ì…í•œë‹¤. ì´ë ‡ê²Œ í•˜ë©´ ì„ í˜•ë³€í™˜ $A$ì— ëŒ€í•œ íŠ¹ì´ê°’ ë¶„í•´ë¥¼ ëª¨ë‘ ëë§ˆì¹œ ì…ˆì´ë‹¤. ì§€ê¸ˆê¹Œì§€ ì‚´í´ë³¸ ë‚´ìš©ì„ ìš”ì•½ ì •ë¦¬í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.</p>

<ul>
  <li>1) $AA^T, A^TA$ ê³„ì‚°</li>
  <li>2) $AA^T, A^TA$ì˜ ê³ ìœ ê°’, ê³ ìœ ë²¡í„° ê³„ì‚°</li>
  <li>3) $A^TA$ë¡œ Right Singular Vector $V$ë¥¼,  $AA^T$ë¡œ Left Singular Vector $U$ë¥¼ ë„ì¶œ</li>
  <li>4) ê³ ìœ ê°’($\Sigma\Sigma^T$)ì˜ ì œê³±ê·¼ì—ì„œ ë‚˜ì˜¨ ê°’ë“¤ì„ $\Sigma$ì˜ ëŒ€ê°ì„ ì— ë°°ì¹˜í•©ë‹ˆë‹¤. ì´ ê°’ë“¤ì´ íŠ¹ì´ê°’ì— í•´ë‹¹</li>
</ul>

<h3 id="insight-of-svd"><code class="language-plaintext highlighter-rouge">ğŸ’¡Â Insight of SVD</code></h3>

\[A\vec x = \sigma_1u_1v_1^T\vec x + \sigma_2u_2v_2^T\vec x + ... +\sigma_n u_nv_n^T\vec x\]

<p>ê³ ìœ ê°’ ë¶„í•´ì˜ í™œìš©ê³¼ ì‚¬ì‹¤ìƒ ë‹¤ë¥¸ê²Œ ì—†ë‹¤. ì—­ì‹œ ë°ì´í„° ì••ì¶•â€¢ë³µì› í˜¹ì€ PCA ê°™ì€ ì°¨ì› ì¶•ì†Œ ê¸°ë²•ì— ì‚¬ìš©ëœë‹¤. ê³ ìœ ê°’ ë¶„í•´ì™€ ë‹¤ë¥¸ê²Œ ìˆë‹¤ë©´ ì„ í˜•ë³€í™˜ $A$ê°€ ë°˜ë“œì‹œ ì •ì‚¬ê°í–‰ë ¬ì¼ í•„ìš”ê°€ ì—†ë‹¤ëŠ” ê²ƒì´ë‹¤. ê·¸ë¦¬ê³  ê³ ìœ ê°’ ë¶„í•´ë¥¼ ë°ì´í„° ì••ì¶•ì´ë‚˜ ì°¨ì› ì¶•ì†Œì— ê°„í¸íˆ ì‚¬ìš©í•˜ë ¤ë©´, ì„ í˜•ë³€í™˜ $A$ê°€ ëŒ€ì¹­í–‰ë ¬ì´ì—¬ì•¼ í•œë‹¤ëŠ” ì¡°ê±´ë„ ë¶™ê³  ìƒë‹¹íˆ ì‚¬ìš©í•˜ê²Œ ê¹Œë‹¤ë¡œì› ëŠ”ë° íŠ¹ì´ê°’ ë¶„í•´ì—ì„œëŠ” ê·¸ëŸ° ë²ˆê±°ë¡œìš´ ì œì•½ ì¡°ê±´ì´ ëª¨ë‘ ì‚¬ë¼ì§„ë‹¤ëŠ” ì¥ì ì´ ìˆë‹¤.</p>

<p>íŠ¹íˆ íŠ¹ì´ê°’ ë¶„í•´ëŠ” ë¶„í•´ë˜ëŠ” ê³¼ì • ìì²´ë³´ë‹¤ ë¶„í•´ë˜ëŠ” í–‰ë ¬ì„ ê°œë³„ íŠ¹ì´ë²¡í„°ì— ëŒ€í•œ íŠ¹ì´ê°’ì˜ ê°€ì¤‘í•© ë°©ì‹ìœ¼ë¡œ ì¡°í•©í•˜ëŠ” ê³¼ì •ì—ì„œ ê·¸ ë¹›ì„ ë°œí•œë‹¤. íŠ¹ì´ê°’ì— $argmax$ë¥¼ ì ìš©í•´ ìƒìœ„ pê°œì˜ íŠ¹ì´ë²¡í„°ë§Œ ë°˜ì˜í•œ ë¶€ë¶„ í–‰ë ¬ $A$ë¥¼ ì´ìš©í•  ìˆ˜ë„ ìˆê³ , í•„ìš”ì— ë”°ë¼ì„œ ë‹¤ì‹œ pê°œì˜ ê°œìˆ˜ë¥¼ ëŠ˜ë ¤ ì›ë³¸ìœ¼ë¡œ ë³µì›í•˜ëŠ” ê²ƒë„ ê°€ëŠ¥í•´ì§„ë‹¤. ì´ëŸ¬í•œ ê¸°ëŠ¥ì´ ê°€ì¥ ë¹›ì„ ë°œí•˜ëŠ” ë¶„ì•¼ê°€ ì´ë¯¸ì§€ í•´ìƒë„ ì••ì¶•â€¢ë³µì› ë¶„ì•¼ë‹¤. í˜„ì¬ ë‚´ê°€ ì‚¬ìš©í•˜ë ¤ë©´ ì´ë¯¸ì§€ì˜ í•´ìƒë„ê°€ ë„ˆë¬´ ë†’ì•„ì„œ ìš©ëŸ‰ì´ ì»¤ì ¸ ì‚¬ìš©í•˜ê¸° ì–´ë µë‹¤ë©´ SVDë¥¼ ì´ìš©í•´ í•„ìˆ˜ì ì¸ íŠ¹ì´ë²¡í„° ëª‡ê°œë§Œ ë‚¨ê²¨ë†“ëŠ” ë°©ì‹ìœ¼ë¡œ í•´ìƒë„ ì••ì¶•ì„ ìˆ˜í–‰í•´ ìš©ëŸ‰ì„ ì¤„ì¼ ìˆ˜ ë„ ìˆë‹¤. <a href="https://angeloyeo.github.io/2019/08/01/SVD.html#%ED%8A%B9%EC%9D%B4%EA%B0%92-%EB%B6%84%ED%95%B4%EC%9D%98-%ED%99%9C%EC%9A%A9"><strong><u>ê³µëŒì´ì˜ ìˆ˜í•™ì •ë¦¬ë‹˜ì˜ í¬ìŠ¤íŠ¸ í•˜ë‹¨ë¶€ì—</u></strong></a> ì •ë§ ì§ê´€ì ìœ¼ë¡œ ì˜ ë§Œë“¤ì–´ì§„ ì‚¬ë¡€ê°€ ìˆìœ¼ë‹ˆ ê¼­ í•œ ë²ˆì”© ë³´ê³  ì˜¤ì‹œê¸¸ ê¶Œì¥í•œë‹¤.</p>]]></content><author><name>qcqced</name><email>qcqced123@gmail.com</email></author><category term="Linear Algebra" /><category term="Linear Algebra" /><category term="Singular Value Decomposition" /><category term="Singular Vector" /><category term="Singular Value" /><category term="SVD" /><category term="PCA" /><summary type="html"><![CDATA[ğŸ’¡ Concept of Singular Value Decomposition]]></summary></entry><entry><title type="html">ğŸ”¢Â Eigen Decomposition</title><link href="http://localhost:4000/linear-algebra/eigen-decomposition" rel="alternate" type="text/html" title="ğŸ”¢Â Eigen Decomposition" /><published>2023-11-25T00:00:00+09:00</published><updated>2023-11-26T13:00:00+09:00</updated><id>http://localhost:4000/linear-algebra/eigen-decomposition</id><content type="html" xml:base="http://localhost:4000/linear-algebra/eigen-decomposition"><![CDATA[<p>ê³ ìœ ê°’, ê³ ìœ ë²¡í„°, ê³ ìœ ê°’ ë¶„í•´ëŠ” ë¹„ë‹¨ ì„ í˜•ëŒ€ìˆ˜í•™ë¿ë§Œ ì•„ë‹ˆë¼ í•´ì„ê¸°í•˜í•™ ë‚˜ì•„ê°€ ë°ì´í„° ì‚¬ì´ì–¸ìŠ¤ ì „ë°˜ì—ì„œ ê°€ì¥ ì¤‘ìš”í•œ ê°œë… ì¤‘ í•˜ë‚˜ë¼ê³  ìƒê°í•œë‹¤. ë¨¸ì‹ ëŸ¬ë‹ì—ì„œ ìì£¼ ì‚¬ìš©í•˜ëŠ” ì—¬ëŸ¬ í–‰ë ¬ ë¶„í•´(Matrix Factorization) ê¸°ë²•(ex: <code class="language-plaintext highlighter-rouge">SVD</code>)ê³¼ <code class="language-plaintext highlighter-rouge">PCA</code>ì˜ ì´ë¡ ì  í† ëŒ€ê°€ ë˜ë¯€ë¡œ ë°˜ë“œì‹œ ì™„ë²½í•˜ê²Œ ìˆ™ì§€í•˜ê³  ë„˜ì–´ê°€ì•¼ í•˜ëŠ” íŒŒíŠ¸ë‹¤. ì´ë²ˆ í¬ìŠ¤íŒ… ì—­ì‹œ <a href="https://www.youtube.com/watch?v=PP9VQXKvSCY&amp;t=108s&amp;ab_channel=%ED%98%81%ED%8E%9C%ED%95%98%EC%9E%84%7CAI%26%EB%94%A5%EB%9F%AC%EB%8B%9D%EA%B0%95%EC%9D%98"><strong><u>í˜íœí•˜ì„ë‹˜ì˜ ì„ í˜•ëŒ€ìˆ˜í•™ ê°•ì˜</u></strong></a>ì™€ <a href="https://www.youtube.com/watch?v=7dmV3p3Iy90&amp;ab_channel=%EA%B3%B5%EB%8F%8C%EC%9D%B4%EC%9D%98%EC%88%98%ED%95%99%EC%A0%95%EB%A6%AC%EB%85%B8%ED%8A%B8"><strong><u>ê³µëŒì´ì˜ ìˆ˜í•™ì •ë¦¬ë‹˜ì˜ ê°•ì˜ ë° í¬ìŠ¤íŠ¸</u></strong></a> ê·¸ë¦¬ê³  <a href="https://product.kyobobook.co.kr/detail/S000001743773"><strong><u>ë”¥ëŸ¬ë‹ì„ ìœ„í•œ ì„ í˜•ëŒ€ìˆ˜í•™ êµì¬</u></strong></a>ì„ ì°¸ê³ í•˜ê³  ê°œì¸ì ì¸ í•´ì„ì„ ë”í•´ ì •ë¦¬í–ˆë‹¤.</p>

<h3 id="concept-of-eigen-value--vector"><code class="language-plaintext highlighter-rouge">ğŸŒŸÂ Concept of Eigen Value &amp; Vector</code></h3>

\[Av = \lambda v\]

<p>ë“±ì‹ì„ ë§Œì¡±ì‹œí‚¤ëŠ” ë²¡í„° $v$ë¥¼ <code class="language-plaintext highlighter-rouge">ê³ ìœ  ë²¡í„°(Eigen Vector)</code>, ëŒë‹¤ $\lambda$ë¥¼ <code class="language-plaintext highlighter-rouge">ê³ ìœ ê°’(Eigen Value)</code>ì´ë¼ê³  ì •ì˜í•œë‹¤. ì¢Œë³€ì˜ $A$ëŠ” <code class="language-plaintext highlighter-rouge">ì„ í˜• ë³€í™˜(í–‰ë ¬)</code>ì„ ì˜ë¯¸í•œë‹¤. ì´ëŸ¬í•œ ì •ë³´ë¥¼ í™œìš©í•´ ìœ„ ë“±ì‹ì˜ ì˜ë¯¸ë¥¼ ì‚´í´ë³´ì. ì–´ë–¤ ì„ í˜•ë³€í™˜ $A$ì™€ ë²¡í„° $v$ë¥¼ ê³±í–ˆë”ë‹ˆ, ì–´ë–¤ ìŠ¤ì¹¼ë¼ì™€ ë²¡í„°ë¥¼ ê³±í•œ ê²°ê³¼ì™€ ê°™ì•˜ë‹¤ëŠ” ê²ƒì¸ë°, ë²¡í„°ì— ìŠ¤ì¹¼ë¼ë¥¼ ê³±í•˜ë©´ ê·¸ í¬ê¸°ë§Œ ë³€í™”í•  ë¿ ë°©í–¥ì€ ì´ì „ê³¼ ë™ì¼í•˜ë‹¤. ë”°ë¼ì„œ ì„ í˜•ë³€í™˜ $A$ë¥¼ ê°€í•´ë„ ê·¸ í¬ê¸°ë§Œ ìŠ¤ì¹¼ë¼ ë°°(ê³ ìœ ê°’ ë°°)ë§Œí¼ ë³€í• ë¿ ë°©í–¥ì€ ë™ì¼í•œ ë²¡í„°ë¥¼ ì°¾ê³ ì í•˜ëŠ”ê²Œ ìœ„ ìˆ˜ì‹ì˜ ëª©ì ì´ë©° ì´ê²Œ ë°”ë¡œ ê³ ìœ ë²¡í„°ì˜ ì •ì˜ê°€ ëœë‹¤.</p>

<p>ê·¸ë ‡ë‹¤ë©´ ìˆ˜ì‹ì„ í’€ì–´ì„œ ê³ ìœ ê°’ê³¼ ê³ ìœ ë²¡í„°ë¥¼ ì§ì ‘ êµ¬í•´ë³´ì. ë¨¼ì € ì¢Œë³€ìœ¼ë¡œ ëª¨ë“  í•­ì„ ë„˜ê¸´ ë’¤, ê³ ìœ  ë²¡í„° $v$ë¡œ ì¢Œë³€ì˜ ëª¨ë“  í•­ì„ ë¬¶ì–´ì¤€ë‹¤.</p>

\[(A - \lambda I) v = 0\]

<p>ê³ ìœ  ë²¡í„° $v$ë¡œ ë¬¶ì–´ì¤€ë‹¤ê³ ë§Œ í–ˆëŠ”ë° ì™œ ê°‘ìê¸° ëŒë‹¤ ë’¤ì— í•­ë“±í–‰ë ¬ì´ ë¶™ê²Œ ë˜ì—ˆì„ê¹Œ?? ëŒë‹¤ëŠ” ê³ ìœ ê°’, ë‹¤ì‹œ ë§í•´ ìŠ¤ì¹¼ë¼ë‹¤. <code class="language-plaintext highlighter-rouge">í–‰ë ¬ - ìŠ¤ì¹¼ë¼</code>ëŠ” ë¶ˆê°€ëŠ¥í•˜ê¸° ë•Œë¬¸ì— ì„ í˜•ë³€í™˜ $A$ì™€ í¬ê¸°ë¥¼ ë§ì¶°ì£¼ê¸° ìœ„í•´ ê³±í•œ ê²ƒì´ë‹¤. ë‹¤ì‹œ ë“±ì‹ì„ ì „ì²´ì ì¸ ê´€ì ì—ì„œ ì‚´í´ë³´ì. ì§€ê¸ˆ <code class="language-plaintext highlighter-rouge">í–‰ë ¬â€¢ë²¡í„° = 0</code> ì˜ í˜•íƒœë¥¼ ì·¨í•˜ê³  ìˆë‹¤. ì–´ë””ì„œ ë§ì´ ë³¸ ë“¯í•œ ê¼´ì´ ì•„ë‹Œê°€?? ë°”ë¡œ í–‰ë ¬ì˜ ì˜ê³µê°„ì„ êµ¬í•  ë•Œ ì‚¬ìš©í•˜ë˜ ìˆ˜ì‹ì´ë‹¤. ë”°ë¼ì„œ ìš°ë¦¬ëŠ” ê³ ìœ ë²¡í„° $v$ê°€ ì¢Œì¸¡ ê´„í˜¸ ì•ˆì˜ í–‰ë ¬ $A-\lambda I$ì˜ ì˜ê³µê°„ì´ spaní•˜ëŠ” ê³µê°„ ì–´ë”˜ê°€ì— ìœ„ì¹˜í–ˆë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤.</p>

<p>í•œí¸, ìš°ë¦¬ê°€ ì´ ë“±ì‹ì„ í’€ì–´í—¤ì¹œ ëª©ì ì€ ê³ ìœ ê°’ ê·¸ë¦¬ê³  ê³ ìœ ë²¡í„°ë¥¼ êµ¬í•˜ê¸° ìœ„í•¨ì´ë‹¤. ë“±ì‹ì„ ë§Œì¡±ì‹œí‚¤ë ¤ë©´ ê³ ìœ ë²¡í„°ê°€ 0ì´ê¸°ë§Œ í•˜ë©´ ë˜ê² ì§€ë§Œ, $v=0$ì¸ ê²½ìš°ë¥¼ ì°¾ìê³  ìš°ë¦¬ê°€ ì´ë ‡ê²Œ ê³ ìƒí•˜ëŠ” ê²ƒì€ ë‹¹ì—°íˆ ì•„ë‹ ê²ƒì´ë‹¤. ë”°ë¼ì„œ $v=0$ì´ ì•„ë‹ˆë¼ ì¢Œì¸¡ ê´„í˜¸ ì•ˆì˜ í•­ $A-\lambda I=0$ì´ ë˜ì–´ì•¼ í•œë‹¤. ì´ ë•Œ $det(A-\lambda I) =0$ë¥¼ ë§Œì¡±í•´ì•¼ í•œë‹¤. ê·¸ ì´ìœ ëŠ” ë§Œì•½ í–‰ë ¬ì‹ì´ 0ì´ ì•„ë‹ˆë¼ë©´ ì—­í–‰ë ¬ì´ ì¡´ì¬í•œë‹¤ëŠ” ê²ƒì´ ë˜ê³ , ì „ì²´ ë“±ì‹ì—ì„œ ì¢Œì¸¡ í•­ì— ëŒ€í•œ ì—­í–‰ë ¬ì„ ì–‘ë³€ì— ê³±í•´ì£¼ë©´ ë‹¤ì‹œ $v=0$ì´ë¼ëŠ” ê²°ê³¼ë¥¼ ì–»ê²Œ ëœë‹¤. ë”°ë¼ì„œ ë°˜ë“œì‹œ $det(A-\lambda I) =0$ì„ ì¶©ì¡±í•´ ì—­í–‰ë ¬ì´ ì—†ë„ë¡ ë§Œë“¤ì–´ì•¼ í•œë‹¤.</p>

<p>ë”°ë¼ì„œ ê²°ë¡ ì ìœ¼ë¡œ ìš°ë¦¬ëŠ” ë‘ ê°€ì§€ ìˆ˜ì‹ì„ í’€ì–´ë‚´ë©´ ê³ ìœ ê°’ê³¼ ê³ ìœ ë²¡í„°ë¥¼ êµ¬í•  ìˆ˜ ìˆë‹¤.</p>

\[N(A-\lambda I) = V \\
det(A-\lambda I) = 0\]

<p>ì´ ë•Œ, ì˜ê³µê°„ì— <code class="language-plaintext highlighter-rouge">span</code>í•˜ëŠ” ë²¡í„°ëŠ” ë¬´ìˆ˜íˆ ë§ê¸° ë•Œë¬¸ì— ì¼ë°˜ì ìœ¼ë¡œ <code class="language-plaintext highlighter-rouge">Basis</code>ë¥¼ ê³ ìœ ê°’ìœ¼ë¡œ ê°„ì£¼í•œë‹¤.</p>

<h3 id="-eigen-decomposition"><code class="language-plaintext highlighter-rouge">ğŸ”¢ Eigen Decomposition</code></h3>

\[A = V\Lambda V^{-1} \\
\Lambda = V^{-1}AV\]

<p>ìœ„ ìˆ˜ì‹ê³¼ ê°™ì€ í˜•íƒœë¡œ ì„ì˜ì˜ ì •ì‚¬ê°í–‰ë ¬ $A$ë¥¼ í‘œí˜„ ê°€ëŠ¥í•˜ë‹¤ë©´, ìš°ë¦¬ëŠ” ì´ëŸ¬í•œ í–‰ë ¬ $A$ë¥¼ <code class="language-plaintext highlighter-rouge">Diagonalizable Matrix</code>ë¼ê³  ë¶€ë¥´ë©°, <code class="language-plaintext highlighter-rouge">Diagonalizable Matrix</code>ë¥¼ ê³ ìœ ë²¡í„°ì™€ ê³ ìœ ê°’ í–‰ë ¬ë¡œ ë¶„í•´í•˜ëŠ” ê²ƒì„ <code class="language-plaintext highlighter-rouge">ê³ ìœ ê°’ ë¶„í•´(Eidgen Decomposition)</code>ë¼ê³  í•œë‹¤.</p>

<p>ì—¬ê¸°ì„œ <code class="language-plaintext highlighter-rouge">Diagonalizable Matrix</code> ì´ë€, ê³ ìœ ê°’ í–‰ë ¬ì„ ì´ìš©í•´ ëŒ€ê°í–‰ë ¬ë¡œ ë³€í™˜ì´ ê°€ëŠ¥í•œ ì •ì‚¬ê°í–‰ë ¬ì„ ë§í•œë‹¤. ë‘ë²ˆì§¸ ìˆ˜ì‹ì´ ë°”ë¡œ <code class="language-plaintext highlighter-rouge">Diagonalizable Matrix</code> ë¥¼ í‘œí˜„í•œ ê²ƒì´ë‹¤. ì–´ë–¤ í–‰ë ¬ì´ <code class="language-plaintext highlighter-rouge">Diagonalizable Matrix</code> í•˜ë‹¤ëŠ” ê²ƒì€ ë‹¤ì‹œ ë§í•´, í–‰ë ¬ì— <code class="language-plaintext highlighter-rouge">Independent</code>í•œ ê³ ìœ ë²¡í„°ê°€ Nê°œ ìˆë‹¤ëŠ” ê²ƒê³¼ ë™ì¹˜ë‹¤. ë°©ê¸ˆ ì„œìˆ í•œ ì‚¬ì‹¤ì„ ìœ ë„í•´ë³´ì.</p>

<p>3X3 í¬ê¸°ì˜ í–‰ë ¬ $A$ì™€ ì„œë¡œ ë…ë¦½ì¸ ê³ ìœ  ë²¡í„° $v_1, v_2, v_3$ê³¼ ì´ì— ëŒ€ì‘ë˜ëŠ” ê³ ìœ ê°’ $\lambda_1, \lambda_2, \lambda_3$ì´ ìˆë‹¤ê³  ê°€ì •í•´ë³´ì. ì—¬ëŸ¬ê°œì˜ ê³ ìœ  ë²¡í„°ì™€ ê³ ìœ ê°’ì„ ìˆ˜ì‹ í•˜ë‚˜ë¡œ í‘œí˜„í•˜ê¸° ìœ„í•´ ë²¡í„°í™”ë¥¼ ì´ìš©í•˜ê³ ì í•œë‹¤.</p>

\[A[v_1, v_2, v_3] = [v_1, v_2, v_3]â€¢   \begin{bmatrix} 
   \lambda_1 &amp; 0 &amp; 0 \\
   0 &amp; \lambda_2 &amp; 0 \\
   0 &amp; 0 &amp; \lambda_3 \\
   \end{bmatrix} \\\]

<p>ìš°ë¦¬ëŠ” ì§€ê¸ˆ ì–´ë–¤ í–‰ë ¬ì´ <code class="language-plaintext highlighter-rouge">Diagonalizable Matrix</code> ì¼ ë•Œ ë²Œì–´ì§€ëŠ” í˜„ìƒì— ëŒ€í•´ ì¦ëª…í•˜ëŠ”ê²Œ ëª©í‘œë¼ì„œ ì¢Œë³€ì— í–‰ë ¬ $A$ë§Œ ë‚¨ê¸°ë ¤ê³  í•œë‹¤. $[v_1, v_2, v_3]$ ì€ ì„œë¡œ ë…ë¦½ì¸ ê³ ìœ  ë²¡í„°ë‹¤. ê·¸ë¦¬ê³  ì‚¬ì´ì¦ˆëŠ” 3x3ìœ¼ë¡œ ì •ì‚¬ê°í–‰ë ¬ì— í•´ë‹¹ëœë‹¤. ì—´ë²¡í„°ê°€ ì„œë¡œ ë…ë¦½ì´ë©´ì„œ ì •ì‚¬ê°í–‰ë ¬ì— í•´ë‹¹ë˜ê¸° ë•Œë¬¸ì— $[v_1, v_2, v_3]$ ì€ ê°€ì—­í–‰ë ¬ì˜ ì¡°ê±´ì„ ëª¨ë‘ ì¶©ì¡±í•œë‹¤. ë”°ë¼ì„œ ì–‘ë³€ì— $[v_1, v_2, v_3]$ ì˜ ì—­í–‰ë ¬ì„ ê³±í•´ì£¼ì. ì´ì œë¶€í„° í¸ì˜ìƒ $[v_1, v_2, v_3]$ ì€ $V$,  ê³ ìœ ê°’-ëŒ€ê°í–‰ë ¬(ìš°ë³€ ì˜¤ë¥¸ìª½ í•­) $\Lambda$ë¡œ í‘œê¸°í•˜ê² ë‹¤.</p>

\[A = V\Lambda V^{-1} \\\]

<p>$V$ê°€ <code class="language-plaintext highlighter-rouge">Independent</code>í•œ ê³ ìœ ë²¡í„°ê°€ Nê°œë¥¼ ê°–ê³  ìˆê¸° ë•Œë¬¸ì— $V$ë¥¼ ì¼ë¶€ë¶„ìœ¼ë¡œ ê°–ê³  ìˆëŠ” í–‰ë ¬ $A$ëŠ” ë‹¹ì—°íˆ <code class="language-plaintext highlighter-rouge">Independent</code>í•œ ê³ ìœ ë²¡í„°ê°€ Nê°œ ìˆë‹¤ê³  ë§í•  ìˆ˜ ìˆë‹¤.</p>

<h3 id="ï¸-property-of-eigen-decomposition"><code class="language-plaintext highlighter-rouge">â­ï¸ Property of Eigen Decomposition</code></h3>

<p>ê³ ìœ ê°’ ë¶„í•´ê°€ ê°€ëŠ¥í•œ <code class="language-plaintext highlighter-rouge">Diagonalizable Matrix</code> $A$ì˜ ì†ì„±ì— ëŒ€í•´ ì•Œì•„ë³´ì. ì´ëŸ° ì†ì„±ë“¤ì€ ì´í›„ <code class="language-plaintext highlighter-rouge">PCA</code>, <code class="language-plaintext highlighter-rouge">SVD</code>ì—ì„œ ì‚¬ìš©ë˜ë‹ˆ ìˆ™ì§€í•˜ê³  ìˆëŠ”ê²Œ ì¢‹ë‹¤.</p>

<ul>
  <li><strong>1) $A^k = V \Lambda V^{-1}â€¢V \Lambda V^{-1} â€¦ = V \Lambda^k V^{-1}$</strong></li>
  <li><strong>2) $A^{-1} = (V \Lambda V^{-1})^{-1}$= $(V \Lambda^{-1} V^{-1})$</strong>
    <ul>
      <li>$AA^{-1}=I$</li>
    </ul>
  </li>
  <li><strong>3) $det(A) = det(V \Lambda V^{-1}) = det(V)det(\Lambda)det(V ^{-1}) = \prod_{i=1}^{N} {\lambda_i}$</strong>
    <ul>
      <li><strong>í–‰ë ¬ì‹ì€ ê³±ìœ¼ë¡œ ìª¼ê°œëŠ”ê²Œ ì„±ë¦½</strong></li>
    </ul>
  </li>
  <li><strong>4) $tr(A)=tr(V \Lambda V^{-1})=tr( \Lambda V^{-1}V)=tr(\Lambda)=\sum_i^{N}\lambda_i$</strong>
    <ul>
      <li><strong><code class="language-plaintext highlighter-rouge">trace</code> ëŠ” ì›ì†Œì˜ ìˆœì„œë¥¼ ë°”ê¾¸ëŠ”ê±° í—ˆìš©</strong></li>
    </ul>
  </li>
  <li><strong>5) rank-difficient == $det(A)=0$ â‡’ í–‰ë ¬ $A$ì—ëŠ” ê°’ì´ 0ì¸ ê³ ìœ ê°’ì´ ì ì–´ë„ í•˜ë‚˜ ì´ìƒ ì¡´ì¬</strong>
    <ul>
      <li><strong>3)ë²ˆ ì†ì„± ì´ìš©</strong></li>
    </ul>
  </li>
  <li><strong>6) Diagonalizable Matrixì˜ non-zero eidgen value ê°œìˆ˜ == rank(A)</strong>
    <ul>
      <li>$rank(A) = rank(V \Lambda V^{-1}) = rank(\Lambda)$</li>
      <li>$V, V^{-1}$ <strong>ì€ ì„œë¡œ ë…ë¦½ì¸ ì—´ë²¡í„°ë¥¼ ìŒ“ì•„ ë§Œë“  í–‰ë ¬ì´ë¼ì„œ ë°˜ë“œì‹œ Full Rank</strong></li>
      <li><strong>ë­í¬ì˜ ì„±ì§ˆì— ì˜í•´ ê°€ì¥ ì‘ì€ ê°’ì´ í–‰ë ¬ì˜ ë­í¬ê°€ ëœë‹¤</strong></li>
      <li><strong><code class="language-plaintext highlighter-rouge">non-zero eigen value ê°œìˆ˜</code> ==</strong> $rank(\Lambda)$</li>
    </ul>
  </li>
</ul>

<h3 id="insight-of-eigen-decomposition"><code class="language-plaintext highlighter-rouge">ğŸ’¡Â Insight of Eigen Decomposition</code></h3>

<p>ì´ë ‡ê²Œ ê³ ìœ ê°’, ê³ ìœ ë²¡í„°, ê³ ìœ ê°’ ë¶„í•´ì— ëŒ€í•´ì„œ ì „ë°˜ì ìœ¼ë¡œ ì‚´í´ë³´ì•˜ë‹¤. í•˜ì§€ë§Œ ì•„ì§ë„ ì™œ ê³ ìœ ê°’ ë¶„í•´ê°€ ê·¸ë¦¬ë„ ì¤‘ìš”í•˜ë‹¤ëŠ” ê²ƒì¸ì§€ ì•„ì§ ê°ì´ ì˜¤ì§€ ì•Šì„ ê²ƒì´ë‹¤. ê³ ìœ ê°’ ë¶„í•´ì˜ ì¤‘ìš”ì„±ì— ëŒ€í•´ ì•Œì•„ë³´ê¸° ìœ„í•´ ë¨¼ì € ë‹¤ìŒê³¼ ê°™ì€ ëª…ì œì— ëŒ€í•´ì„œ ì¦ëª…í•´ë³´ì.</p>

<p><strong><em>â€œëŒ€ì¹­í–‰ë ¬(Symmetric Matrix)ì€ ëŒ€ê°í™” ê°€ëŠ¥í•œ í–‰ë ¬(Diagonalizable Matrix)ì´ë‹¤â€</em></strong></p>

\[V^T = V^{-1} = Q\]

<p>ëŒ€ì¹­í–‰ë ¬ì€ ì •ì‚¬ê°í–‰ë ¬ ì¤‘ì—ì„œ ì›ë³¸ê³¼ ì „ì¹˜í–‰ë ¬ì´ ë™ì¼í•œ($A=A^{T}$) íŠ¹ìˆ˜ í–‰ë ¬ì„ ë§í•œë‹¤. ë”°ë¼ì„œ ì–´ë–¤ í–‰ë ¬ $A$ê°€ ëŒ€ì¹­í–‰ë ¬ì´ë¼ë©´, $V \Lambda V^{-1} = V^{-T} \Lambda V^{T}$ê°€ ëœë‹¤. ê°ë³€ì˜ ê°€ì¥ ë§ˆì§€ë§‰ í•­ì— ì£¼ëª©í•´ë³´ì. ë“±ì‹ ì¡°ê±´ì— ì˜í•´ $V^{-1} = V^T$ê°€ ì„±ë¦½í•˜ê¸° ë•Œë¬¸ì— í–‰ë ¬ $V$ëŠ” ì „ì¹˜í–‰ë ¬ê³¼ ì—­í–‰ë ¬ì´ ê°™ì€ í–‰ë ¬ì´ ëœë‹¤. ë‹¤ì‹œ ë§í•´, $V$ëŠ” ì§êµí–‰ë ¬ $Q$ê°€ ëœë‹¤. ë”°ë¼ì„œ í–‰ë ¬ $A$ì— ëŒ€í•œ ê³ ìœ ê°’ ë¶„í•´ì‹ì„ ì•„ë˜ì²˜ëŸ¼ ì§êµí–‰ë ¬ë¡œ í‘œí˜„í•  ìˆ˜ ìˆë‹¤.</p>

\[A = Q \Lambda Q^{-1} \\
A = [q_1, q_2, q_3]â€¢\begin{bmatrix} 
   \lambda_1 &amp; 0 &amp; 0 \\
   0 &amp; \lambda_2 &amp; 0 \\
   0 &amp; 0 &amp; \lambda_3 \\
   \end{bmatrix}â€¢\begin{bmatrix} 
   q_1^T \\
   q_2^T \\
   q_3^T \\
   \end{bmatrix} \\\]

<p>ì´ì œ ìš°ë³€ì„ ìˆ˜ì‹ì„ ì „ê°œí•´ì„œ ê·¸ ì˜ë¯¸ë¥¼ ì•Œì•„ë³´ì. ì „ê°œí•˜ë©´ ì•„ë˜ì™€ ê°™ë‹¤.</p>

\[A = \lambda_1q_1q_1^T + \lambda_2q_2q_2^T + \lambda_3q_3q_3^T\]

<p>ìš°ë³€ì˜ í•­ì„ í•˜ë‚˜ í•˜ë‚˜ ì‚´í´ë³´ì. ì„¸ê°œì˜ í•­ì€ ëª¨ë‘ ê°œë³„ ê³ ìœ ë²¡í„°ì— ëŒ€í•œ <code class="language-plaintext highlighter-rouge">ê³ ìœ ê°’</code>, <code class="language-plaintext highlighter-rouge">ê³ ìœ ë²¡í„°</code> ê·¸ë¦¬ê³  <code class="language-plaintext highlighter-rouge">ê³ ìœ ë²¡í„°ì˜ ì „ì¹˜</code>ì— ëŒ€í•œ ê³±ìœ¼ë¡œ êµ¬ì„±ë˜ì–´ ìˆë‹¤. ê³ ìœ ë²¡í„°ì™€ ê·¸ê²ƒì˜ ì „ì¹˜ë²¡í„°ì˜ ê³±ì€ í¬ê¸°ëŠ” <code class="language-plaintext highlighter-rouge">nxn</code>ì´ì§€ë§Œ, ì‚¬ì‹¤ ê°™ì€ ë²¡í„°ë¥¼ ë‘ë²ˆ ê³±í•œ ê²ƒê³¼ ê°™ê¸°ì— ë­í¬ëŠ” 1ì´ëœë‹¤. ë‹¤ì‹œ ë§í•´, 3ì°¨ì› ê³µê°„ì—ì„œ 1ì°¨ì› ì§ì„  ê³µê°„ìœ¼ë¡œ <code class="language-plaintext highlighter-rouge">span</code>í•˜ëŠ” ë¶€ë¶„ ê³µê°„ì´ 3ê°œê°€ ë§Œë“¤ì–´ì§€ë©° 3ê°œì˜ ë¶€ë¶„ ê³µê°„ì€ ì„œë¡œ ë…ë¦½ì´ë©´ì„œ, ëª¨ë‘ ê·¼ë³¸ì´ ì§êµ í–‰ë ¬ì˜ ì—´ë²¡í„°ë¼ëŠ” ì  ë•Œë¬¸ì— ì„œë¡œ ì§êµí•œë‹¤. ë”°ë¼ì„œ ìš°ë¦¬ëŠ” ëŒ€ì¹­í–‰ë ¬ $A$ë¥¼ í¬ê¸°ëŠ” <code class="language-plaintext highlighter-rouge">NxN</code>ì´ë©´ì„œ ë­í¬ëŠ” <code class="language-plaintext highlighter-rouge">1</code>ì¸ í–‰ë ¬ 3ê°œë¥¼ ê³ ìœ ê°’ì„ ì´ìš©í•´ <code class="language-plaintext highlighter-rouge">ê°€ì¤‘í•© ë°©ì‹</code>ìœ¼ë¡œ ë”í•œ ê²ƒì´ë¼ê³  í•´ì„í•  ìˆ˜ ìˆë‹¤. ë’¤ì§‘ì–´ ì„œìˆ í•˜ë©´ ëŒ€ì¹­í–‰ë ¬ $A$ë¥¼ í¬ê¸°ëŠ” <code class="language-plaintext highlighter-rouge">NxN</code>ì´ë©´ì„œ ë­í¬ëŠ” <code class="language-plaintext highlighter-rouge">1</code>ì¸ í–‰ë ¬ <code class="language-plaintext highlighter-rouge">3</code>ê°œë¡œ ìª¼ê°œëŠ” ë°©ì‹ì´ ë°”ë¡œ <code class="language-plaintext highlighter-rouge">ê³ ìœ ê°’ ë¶„í•´</code>ì´ë‹¤.</p>

<p>ê³ ìœ ê°’ì— ë”°ë¼, ë¶€ë¶„ ê³µê°„ì˜ í¬ê¸°ë¥¼ ì¡°ì ˆí•  ìˆ˜ ìˆë‹¤ëŠ” ì ì—ì„œ ì°¨ì› ì¶•ì†Œë‚˜ ì œê±°ì²˜ëŸ¼ ì¤‘ìš”ë„ê°€ ë†’ì€ ë°ì´í„°â€¢íŠ¹ì§•ë§Œ ì¶”ì¶œí•˜ëŠ”ê²Œ ê°€ëŠ¥í•´ì§„ë‹¤. ì´ëŸ¬í•œ ê³ ìœ ê°’ ë¶„í•´ë¥¼ ì •ì‚¬ê°í–‰ë ¬(ëŒ€ì¹­í–‰ë ¬)ì´ ì•„ë‹Œ ì¼ë°˜ì ì¸ ì§ì‚¬ê°í–‰ë ¬ì—ë„ ì ìš©í•  ìˆ˜ ìˆë„ë¡ ê°œë…ì„ í™•ì¥í•œê²Œ ë°”ë¡œ <code class="language-plaintext highlighter-rouge">SVD(Singular Vector Decomposition)</code>ì´ê³ , ì¤‘ìš”ë„(ê³ ìœ ê°’ì˜ í¬ê¸°)ì— ë”°ë¼ì„œ ì¤‘ìš”í•œ íŠ¹ì§•â€¢ë°ì´í„°ë§Œ ë‚¨ê¸°ëŠ” ë°©ë²•ë¡ ì€ <code class="language-plaintext highlighter-rouge">PCA(Princlpal Component Analysis)</code>ì˜ ì´ë¡ ì  í† ëŒ€ê°€ ëœë‹¤.</p>

<p>ì´ë ‡ê²Œ ëŒ€ì¹­í–‰ë ¬ì€ ëŒ€ê°í™” ê°€ëŠ¥í•œ í–‰ë ¬ì´ë¼ëŠ” ì ì„ í†µí•´ <code class="language-plaintext highlighter-rouge">ê³ ìœ ê°’ ë¶„í•´</code>ì˜ ì˜ë¯¸ì— ëŒ€í•´ì„œ ì•Œì•„ë³´ì•˜ë‹¤. ì´ì œ ë§ˆì§€ë§‰ìœ¼ë¡œ ì„ í˜•ë³€í™˜ìœ¼ë¡œì„œ í–‰ë ¬ $A$ê°€ ê°–ëŠ” ì˜ë¯¸ë¥¼ ì‚´í´ë³´ì. ê³ ìœ ë²¡í„°ê°€ ì•„ë‹Œ ì„ì˜ì˜ ë²¡í„° $\vec x$ë¥¼ ì„ í˜•ë³€í™˜ $A$ì— í†µê³¼ì‹œì¼œë³´ì. ê·¸ëŸ¼ ìš°ë¦¬ëŠ” ì•„ë˜ì™€ ê°™ì€ ì‹ì„ ì–»ì„ ìˆ˜ ìˆë‹¤.</p>

\[A\vec x = \lambda_1q_1q_1^Tâ€¢\vec x + \lambda_2q_2q_2^Tâ€¢\vec x + \lambda_3q_3q_3^Tâ€¢\vec x\]

<p>ìš°ë³€ì„ í•´ì„í•´ë³´ì. ì•„ê¹Œ ê³ ìœ ê°’ ë¶„í•´ì˜ ì˜ë¯¸ë¥¼ ì‚´í´ë³´ë©´ì„œ $q_1q_1^T$ëŠ” ì „ì²´ ê³µê°„ì—ì„œ 1ì°¨ì› ì§ì„  ê³µê°„ìœ¼ë¡œ spaní•˜ëŠ” ë¶€ë¶„ ê³µê°„ì„ ì˜ë¯¸í•œë‹¤ê³  í–ˆì—ˆë‹¤. ë”°ë¼ì„œ ìš°ë³€ì—ëŠ” ì„œë¡œ ë‹¤ë¥¸ í•­ì´ 3ê°œ ìˆê¸° ë•Œë¬¸ì— ë¶€ë¶„ ê³µê°„ì´ 3ê°œ ìˆëŠ” 3ì°¨ì› ê³µê°„ì´ í˜•ì„±ëœë‹¤. ì´ì œ ë¶€ë¶„ ê³µê°„ê³¼ ë²¡í„° $\vec x$ë¥¼ ë‚´ì í•œ í˜•íƒœë¡œ ë°”ë¼ë³¼ ìˆ˜ ìˆë‹¤. ë‚´ì ì€ ì •ì‚¬ì˜ì´ë‹¤. ë”°ë¼ì„œ $q_nq_n^Tâ€¢\vec x$ì€ ë²¡í„° $\vec x$ë¥¼ ë¶€ë¶„ ê³µê°„ì— ì •ì‚¬ì˜ ë‚´ë ¤ì¤€ ë²¡í„°ê°€ ëœë‹¤. ê·¸ë¦¬ê³  ê³ ìœ ê°’ì„ ê³±í•´ ì •ì‚¬ì˜ ë‚´ë¦° ë²¡í„°ë“¤ì˜ í¬ê¸°ë¥¼ ì¡°ì ˆí•´ì£¼ëŠ”ê²Œ ìš°ë³€ì˜ ì˜ë¯¸ê°€ ëœë‹¤.</p>]]></content><author><name>qcqced</name><email>qcqced123@gmail.com</email></author><category term="Linear Algebra" /><category term="Linear Algebra" /><category term="Eigen Decomposition" /><category term="Eigen Vector" /><category term="Eigen Value" /><category term="SVD" /><category term="PCA" /><summary type="html"><![CDATA[ğŸ’¡ Concept of Eigen Decomposition]]></summary></entry><entry><title type="html">ğŸ“ˆÂ Gradient: Directional Derivative</title><link href="http://localhost:4000/optimization-theory/gradient" rel="alternate" type="text/html" title="ğŸ“ˆÂ Gradient: Directional Derivative" /><published>2023-11-25T00:00:00+09:00</published><updated>2023-11-25T23:00:00+09:00</updated><id>http://localhost:4000/optimization-theory/gradient</id><content type="html" xml:base="http://localhost:4000/optimization-theory/gradient"><![CDATA[<h3 id="concept-of-gradient"><code class="language-plaintext highlighter-rouge">ğŸ¤”Â Concept of Gradient</code></h3>

<p>ê·¸ë¼ë””ì–¸íŠ¸ëŠ” ë‹¤ë³€ìˆ˜ í•¨ìˆ˜ì˜ ê¸°ìš¸ê¸°ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ë²¡í„°ë¥¼ ë§í•œë‹¤. ê·¸ë¼ë””ì–¸íŠ¸ì˜ ì›ì†ŒëŠ” í•¨ìˆ˜ì— ì¡´ì¬í•˜ëŠ” ëª¨ë“  ë³€ìˆ˜ë¥¼ ëŒ€ìƒìœ¼ë¡œ í¸ë¯¸ë¶„í•œ ê²°ê³¼ë¡œ êµ¬ì„±ë˜ëŠ”ë°, ì˜ˆë¥¼ ë“¤ì–´ ë³€ìˆ˜ê°€ $x_1, x_2$ 2ê°œì¸ ë‹¤ë³€ìˆ˜ í•¨ìˆ˜ $f(x_1, x_2)$ê°€ ìˆë‹¤ê³  ê°€ì •í•´ë³´ì. ë‹¤ë³€ìˆ˜ í•¨ìˆ˜ $f$ì˜ ê·¸ë¼ë””ì–¸íŠ¸ëŠ” ì•„ë˜ ìˆ˜ì‹ì²˜ëŸ¼ í‘œí˜„í•  ìˆ˜ ìˆë‹¤.</p>

\[f'(x_1, x_2) = \begin{vmatrix}
  \frac{âˆ‚f}{âˆ‚x_1} \\
  \frac{âˆ‚f}{âˆ‚x_2}
\end{vmatrix}\]

<p>ì´ëŸ¬í•œ ê·¸ë¼ë””ì–¸íŠ¸ëŠ” ë¨¸ì‹  ëŸ¬ë‹, ìˆ˜ì¹˜ ìµœì í™” í•™ë¬¸ì—ì„œ ë§¤ìš° ì¤‘ìš”í•œ ê°œë…ìœ¼ë¡œ ê¼½íŒë‹¤. ê·¸ë¼ë””ì–¸íŠ¸ ë²¡í„°ê°€ ê°€ë¦¬í‚¤ëŠ” ë°©í–¥ì´ ë°”ë¡œ ë‹¤ë³€ìˆ˜ í•¨ìˆ˜ê°€ íŠ¹ì • ì§€ì ì—ì„œ ê°€ì¥ ê°€íŒŒë¥´ê²Œ ì¦ê°€í•˜ëŠ” ë°©í–¥ì„ ê°€ë¦¬í‚¤ê¸° ë•Œë¬¸ì´ë‹¤. ì´ì²˜ëŸ¼ ê·¸ë¼ë””ì–¸íŠ¸ëŠ” í•¨ìˆ˜ì˜ ì…ë ¥ ê³µê°„ì„ ë”°ë¼ í•¨ìˆ˜ê°€ ì–´ë–»ê²Œ ë³€í™”í•˜ëŠ”ì§€ë¥¼ ì•Œë ¤ì£¼ëŠ” ê¸¸ì¡ì´ ì—­í• ì„ í•˜ê¸° ë•Œë¬¸ì—, ê·¸ë¼ë””ì–¸íŠ¸ ë°©í–¥ì„ ë”°ë¼ ë³€ìˆ˜ê°’ì„ íŠœë‹í•˜ë‹¤ ë³´ë©´ í•¨ìˆ˜ì˜ ìµœëŒ€ê°’â€¢ìµœì†Œê°’ì— ë„ë‹¬í•˜ì—¬ ìµœì í™” ë¬¸ì œë¥¼ í•´ê²°í•  ìˆ˜ ìˆê²Œ ëœë‹¤. ê·¸ë ‡ë‹¤ë©´ ì™œ ê·¸ë¼ë””ì–¸íŠ¸ ë²¡í„°ì˜ ë°©í–¥ì´ íŠ¹ì • ì§€ì ì—ì„œ í•¨ìˆ˜ê°€ ê°€ì¥ ê°€íŒŒë¥´ê²Œ ì¦ê°€í•˜ëŠ” ë°©í–¥ì„ ë‚˜íƒ€ë‚´ëŠ” ê²ƒì¼ê¹Œ?? í¸ë¯¸ë¶„, ë„í•¨ìˆ˜ ì •ì˜ ê·¸ë¦¬ê³  ë‚´ì ì„ í™œìš©í•´ ì¦ëª…í•  ìˆ˜ ìˆë‹¤.</p>

<h3 id="-proof-of-gradient"><code class="language-plaintext highlighter-rouge">ğŸªª Proof of Gradient</code></h3>

<p align="center">
<img src="/assets/images/gradient/gradient.jpg" alt="Example of multivariate function" class="align-center image-caption" width="60%&quot;, height=&quot;25%" />
<strong><em><a href="https://m.blog.naver.com/PostView.naver?isHttpsRedirect=true&amp;blogId=galaxyenergy&amp;logNo=221431325545">Example of multivariate function</a></em></strong>
</p>

<p>ê·¸ë¼ë””ì–¸íŠ¸ ë²¡í„°ì˜ ë°©í–¥ì´ í•¨ìˆ˜ê°€ ê°€ì¥ ê°€íŒŒë¥´ê²Œ ì¦ê°€í•˜ëŠ” ë°©í–¥ê³¼ ì¼ì¹˜í•œë‹¤ëŠ” ëª…ì œë¥¼ ì¦ëª…í•˜ê¸° ìœ„í•´ ìµœë‹¨ ê²½ë¡œë¡œ ì‚° ì •ìƒì— ì˜¤ë¥´ëŠ” ê³¼ì •ì„ ë– ì˜¬ë ¤ë³´ë ¤ í•œë‹¤. ìš°ë¦¬ëŠ” í˜„ì¬ ì´ë³€ìˆ˜ í•¨ìˆ˜ë¡œ ì •ì˜ë˜ëŠ” ì‚° ì¤‘í„± ì–´ë”˜ê°€, ì  $(x_1^0, x_2^0)$ë¥¼ ì§€ë‚˜ê³  ìˆë‹¤. ì‚° ì •ìƒì„ ìµœë‹¨ ê²½ë¡œë¡œ ì˜¤ë¥´ë ¤ë©´ ì–´ë–»ê²Œ í•´ì•¼í• ê¹Œ?? ê°€ì¥ ê²½ì‚¬ê°€ ê°€íŒŒë¥¸ ê¸‰ê²½ì‚¬ ì§€ëŒ€ë¥¼ í–¥í•´ ë‚˜ì•„ê°€ë©´ ë  ê²ƒì´ë‹¤. í•˜ì§€ë§Œ ì‚° ì¤‘í„±ì— ìˆëŠ” ìš°ë¦¬ê°€ ì–´ëŠ ë°©í–¥ì´ ê°€ì¥ ê°€íŒŒë¥¸ ê¸‰ê²½ì‚¬ ì§€ëŒ€ì¸ì§€ ì§ê´€ì ìœ¼ë¡œ ì•Œ ê¸¸ì´ ì—†ë‹¤. ê·¸ë˜ì„œ ë°©í–¥ ë„í•¨ìˆ˜ë¥¼ ë„ì…í•´ ê¸‰ê²½ì‚¬ ì§€ëŒ€ë¡œ í–¥í•  ìˆ˜ ìˆëŠ” ë°©í–¥ì„ êµ¬í•´ ë³´ê¸°ë¡œ í–ˆë‹¤. ì•„ë˜ ìˆ˜ì‹ì„ ë³´ì.</p>

\[\lim_{\Delta{x}-&gt;0}\frac{f(x+\Delta{x}) - f(x)}{\Delta{x}} =    \frac{df}{dx}= f'(x) \\
df = f'(x)dx\]

<p>ë„ˆë¬´ë‚˜ë„ ìµìˆ™í•œ í˜•íƒœ ì•„ë‹Œê°€?? ìš°ë¦¬ê°€ ì¼ë°˜ì ìœ¼ë¡œ ì•Œê³  ìˆëŠ” ì¼ë³€ìˆ˜ í•¨ìˆ˜ì˜ ë¯¸ë¶„ ì •ì˜ ê·¸ë¦¬ê³  ì¢Œë³€ì˜ $dx$ë¥¼ ìš°ë³€ìœ¼ë¡œ ë„˜ê²¨ ì‚´ì§ ë³€í˜•í•œ ì‹ì´ë‹¤. ì´ê²ƒì„ ì´ì œ ë‹¤ë³€ìˆ˜ í•¨ìˆ˜ì— ì ìš©í•˜ë©´ ë°”ë¡œ ë°©í–¥ ë„í•¨ìˆ˜ê°€ ëœë‹¤. ë‹¤ì‹œ ìš°ë¦¬ê°€ ì˜¤ë¥´ë ¤ëŠ” ì‚°(ì´ë³€ìˆ˜ í•¨ìˆ˜)ìœ¼ë¡œ ëŒì•„ì™€ ë³´ì.</p>

\[f(x_1 + dx_1, x_2) = f(x_1, x_2) + f'(x_1, x_2)dx_1 \\
f(x_1, x_2 + dx_2) = f(x_1, x_2) + f'(x_1, x_2)dx_2 \\\]

<p>ìœ„ì—ì„œ ì„œìˆ í•œ ë„í•¨ìˆ˜ ì •ì˜ë¥¼ í™œìš©í•´ ìš°ë¦¬ê°€ ë‹¤ìŒì— ë°œê±¸ìŒì„ ì˜®ê¸¸ ìœ„ì¹˜ë¥¼ ì   $A$ë¥¼ $(x_1^0 + dx_1, x_2^0+dx_2)$ ì´ë¼ê³  í‘œí˜„í•  ìˆ˜ ìˆë‹¤. ì´ í‘œí˜„ì„ í™œìš©í•´ ë‹¤ë³€ìˆ˜ í•¨ìˆ˜ì˜ ë¯¸ë¶„ì„ ì •ì˜í•´ë³´ì. ìš°ë¦¬ëŠ” ì´ë¯¸ ë‹¤ë³€ìˆ˜ í•¨ìˆ˜ì˜ ê°œë³„ ë³€ìˆ˜ì— í¸ë¯¸ë¶„ì„ ì·¨í•˜ê³  í–‰ë²¡í„°ë¡œ ìŒ“ì€ ê²°ê³¼ê°€ ë°”ë¡œ ì „ë¯¸ë¶„ì´ë¼ëŠ” ê²ƒì„ ì•Œê³  ìˆë‹¤.</p>

\[f(x_1 + dx_1, x_2 + dx_2) - f(x_1, x_2) = f'(x_1)dx_1 + f'(x_2)dx_2\]

<p>ë‹¤ì‹œ í¸ë¯¸ë¶„ì˜ ì •ì˜ë¥¼ í™œìš©í•´ ìˆ˜ì‹ì„ ì •ë¦¬í•˜ë©´ ë°©í–¥ ë²¡í„°ì™€ í¸ë¯¸ë¶„ ê²°ê³¼ì˜ ë‚´ì ìœ¼ë¡œ í‘œí˜„í•  ìˆ˜ ìˆë‹¤.</p>

\[dL = \frac{âˆ‚L}{âˆ‚{x_1}}dx_1 + \frac{âˆ‚L}{âˆ‚{x_2}}dx_2 \\
dL = [dx_1, dx_2]\ â€¢\ \begin{vmatrix}
  \frac{âˆ‚L}{âˆ‚x_1} \\
  \frac{âˆ‚L}{âˆ‚x_2}
\end{vmatrix}\]

<p>ìŸì•„ì§€ëŠ” ìˆ˜ì‹ ì†ì— ìš°ë¦¬ì˜ ë³¸ë˜ ëª©ì ì„ ìŠì–´ì„œëŠ” ì•ˆëœë‹¤. ìš°ë¦¬ëŠ” ì§€ê¸ˆ ê°€ì¥ ë¹ ë¥´ê²Œ ì‚° ì •ìƒì— ë„ë‹¬í•  ìˆ˜ ìˆëŠ” ë°©ë²•ì„ ì°¾ê¸° ìœ„í•´ ì§€ê¸ˆê¹Œì§€ ë‹¬ë ¤ì™”ë‹¤. ì‚° ì •ìƒì— ê°€ì¥ ë¹ ë¥´ê²Œ ë„ë‹¬í•˜ê¸° ìœ„í•´ ê°€ì¥ ê°€íŒŒë¥¸ ê¸‰ê²½ì‚¬ ì§€ëŒ€ë§Œ ì°¾ì•„ì„œ ì˜¬ë¼ê°€ëŠ” ì „ëµì„ ì„¸ì› ì—ˆë‹¤. ë‹¤ì‹œ ë§í•´, ë‹¤ë³€ìˆ˜ í•¨ìˆ˜ $f(x)$ì˜ ê·¹ì†Œ ë³€í™”ëŸ‰ $dL$ì´ ìµœëŒ€ê°€ ë˜ëŠ” ë°©í–¥ìœ¼ë¡œ ë°œê±¸ìŒì„ ì˜®ê¸°ë©´ ëœë‹¤ëŠ” ê²ƒì´ë‹¤. ê·¸ë ‡ë‹¤ë©´ ê·¹ì†Œ ë³€í™”ëŸ‰ $dL$ì€ ì–¸ì œ ìµœëŒ€ê°€ ë ê¹Œ??</p>

<p>ì´ì œ ê¹Œë¨¹ê³  ìˆì—ˆë˜ ë‚´ì ì˜ ê°œë…ì„ ë‹¤ì‹œ í•œ ë²ˆ ìƒê¸°ì‹œì¼œë³´ì. ë‚´ì ì€ ë‹¤ì–‘í•˜ê²Œ í•´ì„ë˜ì§€ë§Œ, ë³¸ë”” ì„œë¡œ ë‹¤ë¥¸ ë‘ ë²¡í„°ì˜ <code class="language-plaintext highlighter-rouge">ë‹®ì€ ì •ë„</code>ë¥¼ ë‚˜íƒ€ë‚¸ë‹¤. ê·¹ì†Œ ë³€í™”ëŸ‰ $dL$ì´ ìµœëŒ€ê°€ ë˜ë ¤ë©´ ìš°ë³€ì˜ ë‚´ì  ê²°ê³¼ê°€ ìµœëŒ€ê°€ ë˜ì–´ì•¼ í•œë‹¤. ë‚´ì ì˜ ìµœëŒ€ê°’ì€ ì„œë¡œ ë‹¤ë¥¸ ë‘ ë²¡í„° ì‚¬ì´ì˜ ë¼ì¸ê°ë„ê°€ 0Ëšì¼ ë•Œ ì¦‰, ë‘ ë²¡í„°ê°€ ë™ì¼í•œ ë°©í–¥ì„ ë‚˜íƒ€ë‚¼ ë•Œ ì •ì˜ëœë‹¤. <strong><u>ë”°ë¼ì„œ ë°©í–¥ ë²¡í„°ê°€ ê·¸ë¼ë””ì–¸íŠ¸(í¸ë¯¸ë¶„ì˜ í–‰ë²¡í„°) ë°©í–¥ì¼ ë•Œ</u></strong> <code class="language-plaintext highlighter-rouge">ë‚´ì  ê²°ê³¼</code>(ê·¹ì†Œ ë³€í™”ëŸ‰ $dL$)<strong><u>ê°€ ìµœëŒ€ê°€ ëœë‹¤.</u></strong></p>

<p><strong><u>í•œí¸, ì‹¤ì œ ê¸°ê³„í•™ìŠµì—ì„œëŠ” ì†ì‹¤í•¨ìˆ˜ì˜ ìµœì í™”ë¥¼ ëª©ì  í•¨ìˆ˜ë¡œ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì— ê·¸ë¼ë””ì–¸íŠ¸(ì†ì‹¤í•¨ìˆ˜ì˜ ì „ë¯¸ë¶„) ë°©í–¥ì— ìŒìˆ˜ë¥¼ ì·¨í•´ì¤€ ê°’ì„ ì‚¬ìš©í•˜ê²Œ ëœë‹¤.</u></strong></p>]]></content><author><name>qcqced</name><email>qcqced123@gmail.com</email></author><category term="Optimization Theory" /><category term="Optimization Theory" /><category term="Calculus" /><category term="Partial Derivative" /><category term="Total Derivative" /><category term="loss function" /><category term="Gradient" /><category term="Gradient Descent" /><category term="Machine Learning" /><summary type="html"><![CDATA[Proof of gradient direction with Total Derivative]]></summary></entry><entry><title type="html">ğŸ Newton-Raphson Method for Optimization</title><link href="http://localhost:4000/optimization-theory/newton-raphson" rel="alternate" type="text/html" title="ğŸ Newton-Raphson Method for Optimization" /><published>2023-11-15T00:00:00+09:00</published><updated>2023-11-16T02:00:00+09:00</updated><id>http://localhost:4000/optimization-theory/newton_raphson</id><content type="html" xml:base="http://localhost:4000/optimization-theory/newton-raphson"><![CDATA[<h3 id="zero-find-ver"><code class="language-plaintext highlighter-rouge">ğŸ¤”Â Zero-Find Ver</code></h3>

<p>ë¹„ì„ í˜• ë°©ì •ì‹ì˜ ê·¼ì‚¬í•´ë¥¼ ì°¾ê±°ë‚˜ ìµœì í™” ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ, ê°™ì€ ê³¼ì •ì„ ë°˜ë³µí•´ ìµœì ê°’ì— ìˆ˜ë ´í•œë‹¤ëŠ” ì ì—ì„œ ê²½ì‚¬í•˜ê°•ë²•ì´ë‘ ê·¼ë³¸ì´ ê°™ë‹¤. ë°˜ë©´, ê²½ì‚¬í•˜ê°•ì— ë¹„í•´ ë¹ ë¥¸ ìˆ˜ë ´ ì†ë„ë¥¼ ìë‘í•˜ê³  í’€ì´ ë°©ì‹ì´ ë§¤ìš° ê°„ë‹¨í•˜ë‹¤ëŠ” ì¥ì ì´ ìˆë‹¤. í•˜ì§€ë§Œ ì—¬ëŸ¬ ì œì•½ ì¡°ê±´ê³¼ ë”ë¶ˆì–´ í•´ë‹¹ ì•Œê³ ë¦¬ì¦˜ì´ ì˜ ì‘ë™í•˜ëŠ” ìƒí™©ì´ ë¹„í˜„ì‹¤ì ì¸ ë¶€ë¶„ì´ ë§ì•„ ê²½ì‚¬í•˜ê°•ì— ë¹„í•´ ìì£¼ ì‚¬ìš©ë˜ì§€ëŠ” ì•Šê³  ìˆë‹¤. ë‰´í„´-ë©ìŠ¨ ë°©ì‹ì€ ê·¼ì‚¬í•´ë¥¼ ì°¾ê±°ë‚˜, ìµœì í™” ë¬¸ì œë¥¼ í‘¸ëŠ” ë‘ ê°€ì§€ ë²„ì ¼ì´ ìˆëŠ”ë° ë¨¼ì € í•´ë¥¼ ì°¾ëŠ” ë²„ì „ë¶€í„° ì‚´í´ë³´ì. ì•Œê³ ë¦¬ì¦˜ì˜ ìˆ˜ì‹ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.ã…‚</p>

\[x_{n+1}:= x_n - \frac{f(x_n)}{f'(x_n)}\]

<p>ë°˜ë³µë²•ì„ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì— ìˆ˜ì‹ì˜ ìƒê¹€ìƒˆê°€ ìƒë‹¹íˆ ê²½ì‚¬í•˜ê°•ë²•ê³¼ ë¹„ìŠ·í•˜ë‹¤. ì™œ ì´ëŸ° ìˆ˜ì‹ì´ ë“±ì¥í•˜ê²Œ ë˜ì—ˆì„ê¹Œ?? ì¼ë‹¨ ë‰´í„´-ë©ìŠ¨ ë°©ì‹ì˜ í’€ì´ ê³¼ì •ì„ ì‚´í´ë³´ì. ë¨¼ì € ì´ˆê¸°ê°’ì„ ì„¤ì •í•œë‹¤. ê·¸ ë‹¤ìŒ í•´ë‹¹ ì ì„ ì§€ë‚˜ëŠ” ì ‘ì„ ì˜ ë°©ì •ì‹ì„ ì„¸ìš´ë‹¤. ì´ì œ ì ‘ì„ ì˜ ë°©ì •ì‹ì˜ $x$ì ˆí¸ì„ êµ¬í•˜ê³  ì´ê²ƒì„ ë‹¤ìŒ ì´ˆê¸°ê°’ìœ¼ë¡œ ì‚¬ìš©í•œë‹¤. ì´ì œ $f(x_n) \approx 0$ì´ ë  ë•Œê¹Œì§€ ìœ„ ê³¼ì •ì„ ì§€ì†ì ìœ¼ë¡œ ë°˜ë³µí•˜ë©´ ëœë‹¤. ì•„ë˜ ê·¸ë˜í”„ì™€ í•¨ê»˜ ë‹¤ì‹œ ì‚´í´ë³´ì.</p>

<p align="center">
<img src="/assets/images/optimization/zero_find.png" alt="Newton-Raphson for Zero Find" class="align-center image-caption" width="60%&quot;, height=&quot;50%" />
<strong><em><a href="">Newton-Raphson for Zero Find</a></em></strong>
</p>

<p>ì´ˆê¸°ê°’ì€ $x_0=3$ì´ë‹¤. ì‹œì‘ì  $(x_0, f(x_0))$ì„ ì§€ë‚˜ëŠ” ì ‘ì„ ì˜ ë°©ì •ì‹ì„ ì„¸ìš°ê³  í•´ë‹¹ ë°©ì •ì‹ì˜ $x$ì ˆí¸ì„ êµ¬í•˜ëŠ” ìˆ˜ì‹ì„ ì‘ì„±í•˜ë©´ ì•„ë˜ì™€ ê°™ë‹¤.</p>

\[f'(x_0)(x-x_n) + f(x_0) = 0\]

<p>ì´ì œ ì´ê²ƒì„ ì˜ˆì˜ê²Œ ì˜ ì •ë¦¬í•´ì„œ ë‹¤ìŒ ì´ˆê¸°ê°’ $x_1$ì„ êµ¬í•´ë³´ì.</p>

\[x = x_0 - \frac{f(x_0)}{f'(x_0)}\]

<p>ì´ë²ˆ í¬ìŠ¤íŠ¸ ë§¨ ì²˜ìŒì— ë´¤ë˜ ë‰´í„´-ë©ìŠ¨ ë°©ë²•ì˜ ìˆ˜ì‹ê³¼ ë˜‘ê°™ë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤. ë‹¤ì‹œ ë§í•´ ë‰´í„´-ë©ìŠ¨ì˜ Zero-Find ë²„ì „ì€ ì ‘ì„ ì˜ ë°©ì •ì‹ì˜ $x$ì ˆí¸ì„ í™œìš©í•´ ëª©ì  í•¨ìˆ˜ì˜ í•´ë¥¼ ì°¾ì•„ê°€ëŠ” ë°©ì‹ì¸ ê²ƒì´ë‹¤.</p>

<p>ì§€ê¸ˆê¹Œì§€ ê·¼ì‚¬í•´ë¥¼ ì°¾ì•„ì£¼ëŠ” ë‰´í„´-ë©ìŠ¨ ë©”ì„œë“œë¥¼ ì‚´í´ë³´ì•˜ë‹¤. í•˜ì§€ë§Œ ë¨¸ì‹ ëŸ¬ë‹ì²˜ëŸ¼ í˜„ì‹¤ì˜ ìµœì í™” ë¬¸ì œë¥¼ í’€ì–´ì•¼ í•˜ëŠ” ìš°ë¦¬ ì…ì¥ì—ì„œëŠ” ë‹¨ìˆœ ëª©ì  í•¨ìˆ˜ì˜ ê·¼ì„ ì°¾ëŠ” ê²ƒë§Œìœ¼ë¡œëŠ” ì£¼ì–´ì§„ ë¬¸ì œë¥¼ í•´ê²°í•  ìˆ˜ ì—†ë‹¤. ë¨¸ì‹ ëŸ¬ë‹ì˜ ìµœì í™” ëŒ€ìƒì¸ ë¹„ìš© í•¨ìˆ˜ëŠ” ê±°ì˜ ëª¨ë“  ê²½ìš°ì— ê·¼ì´ ì—†ê¸°(ë² ì´ì§€ì•ˆ ì˜¤ì°¨ê¹Œì§€ ê³ ë ¤í•˜ë©´ ì‚¬ì‹¤ìƒ ë¶ˆê°€ëŠ¥) ë•Œë¬¸ì— ì¼ë‹¨ ì•Œê³ ë¦¬ì¦˜ì˜ ê°€ì • ìì²´ê°€ ì„±ë¦½í•˜ì§€ ì•ŠëŠ”ë‹¤. ì´ëŸ¬í•œ í•œê³„ì ì„ ê·¹ë³µí•˜ê³ ì ìµœì í™” ë²„ì „ì˜ ë‰´í„´-ë©ìŠ¨ ë©”ì„œë“œê°€ ë“±ì¥í•˜ê²Œ ëœë‹¤.</p>

<h3 id="optimization-ver"><code class="language-plaintext highlighter-rouge">ğŸ“‰Â Optimization Ver</code></h3>

\[x_{n+1}:= x_n - \frac{f'(x_n)}{f''(x_n)}\]

<p>ìµœì í™” ë²„ì „ì˜ ë‰´í„´ ë©ìŠ¨ ë©”ì„œë“œëŠ” ì´ê³„ë„í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•œë‹¤. ì›í•¨ìˆ˜(ë¹„ìš©í•¨ìˆ˜)ê°€ ê·¼ì´ ì—†ì„ì§€ë¼ë„, í•¨ìˆ˜ì˜ ê·¹ì ì´ ì¡´ì¬í•˜ëŠ”í•œ ë„í•¨ìˆ˜ì˜ ê·¼ì€ í•­ìƒ ì¡´ì¬í•œë‹¤ëŠ” ê°€ì •ì—ì„œ ì¶œë°œí•œë‹¤. ê·¼ì„ ì°¾ëŠ” í–‰ìœ„ëŠ” ë™ì¼í•˜ê²Œ í•˜ë˜, ì´ë²ˆì—ëŠ” ì›í•¨ìˆ˜ì˜ ê·¼ì´ ì•„ë‹ˆë¼ ë„í•¨ìˆ˜ì˜ ê·¼ì„ ì°¾ëŠ”ë‹¤. ë„í•¨ìˆ˜ì˜ ê·¼ì‚¬í•´ë¥¼ ì°¾ìœ¼ë©´, í•´ë‹¹ ìœ„ì¹˜ëŠ” êµ­ì†Œ/ì „ì—­ ìµœì ê°’ì— ê·¼ì ‘í•œ ìˆ˜ì¹˜ì¼ ê²ƒì´ë¼ê³  ê¸°ëŒ€í•´ë³¼ ìˆ˜ ìˆë‹¤.</p>

<p>í•˜ì§€ë§Œ ìµœì í™” ë²„ì „ì˜ ë‰´í„´ ë©ìŠ¨ ë©”ì„œë“œ ì—­ì‹œ ì—¬ì „íˆ ë§ì€ ë‹¨ì ì„ ê°–ê³  ìˆë‹¤. ì¼ë‹¨ ë¨¼ì € ê³„ì‚°ëŸ‰ì´ ì§€ë‚˜ì¹˜ê²Œ ë§ì•„ì§„ë‹¤. ì˜ˆì‹œë¥¼ ëª¨ë‘ ìŠ¤ì¹¼ë¼ í˜•íƒœë¡œ ë“¤ì–´ì„œ ê°„ë‹¨í•´ ë³´ì´ì§€ë§Œ, ë‹¤ë³€ìˆ˜í•¨ìˆ˜ì— ì ìš©í•˜ë©´ ê³¼ì •ì´ ë§¤ìš° ë§¤ìš° ë³µì¡í•´ì§„ë‹¤. ëª¨ë“  <code class="language-plaintext highlighter-rouge">iteration</code> ë§ˆë‹¤ ìì½”ë¹„ì•ˆ, í—¤ì‹œì•ˆ í–‰ë ¬ì„ êµ¬í•´ì¤˜ì•¼ í•œë‹¤. ë„í•¨ìˆ˜ë§Œ ì´ìš©í•˜ëŠ” ê²½ì‚¬ í•˜ê°•ì— ë¹„í•´ ì—°ì‚° ë¶€ë‹´ì´ ìƒë‹¹íˆ ì»¤ì§ˆ ìˆ˜ ë°–ì— ì—†ëŠ” ê²ƒì´ë‹¤. ê·¸ë¦¬ê³  ê²°ì •ì ìœ¼ë¡œ í—¤ì‹œì•ˆ í–‰ë ¬ì´ <code class="language-plaintext highlighter-rouge">invertible</code> í•´ì•¼í•œë‹¤. ì´ê²Œ ê°œì¸ì ìœ¼ë¡œ <code class="language-plaintext highlighter-rouge">ë‰´í„´-ë©ìŠ¨</code> ë°©ì‹ì˜ ê°€ì¥ í° ë‹¨ì ì´ë¼ê³  ìƒê°í•œë‹¤. í—¤ì‹œì•ˆ í–‰ë ¬ì˜ ì—­í–‰ë ¬ì´ ì¡´ì¬í•˜ë ¤ë©´ ë°˜ë“œì‹œ ì›í•¨ìˆ˜ëŠ” <code class="language-plaintext highlighter-rouge">Convex Function</code>ì´ì–´ì•¼ í•˜ê¸° ë•Œë¬¸ì´ë‹¤. ë”°ë¼ì„œ ìƒë‹¹íˆ ë¹„í˜„ì‹¤ì ì¸ í’€ì´ ë°©ì‹ì´ë¼ê³  ë³¼ ìˆ˜ ìˆë‹¤.</p>

<p>í•œí¸, ìœ„ ëª¨ë“  ì œì•½ ì¡°ê±´ì„ ë§Œì¡±í•œë‹¤ë©´ ìµœì í™” ë²„ì „ì˜ ë‰´í„´-ë©ìŠ¨ ë°©ì‹ì€ ê²½ì‚¬í•˜ê°•ì— ë¹„í•´ ìƒë‹¹íˆ ë¹ ë¥¸ ìˆ˜ë ´ ì†ë„ë¥¼ ê°–ëŠ”ë° ê·¸ ì´ìœ ë¥¼ ê°„ë‹¨íˆ ì‚´í´ë³´ì. ê²°ê³¼ë¶€í„° ì„¤ëª…í•˜ë©´ ë‰´í„´-ë©ìŠ¨ ë°©ì‹ì´ ì‚¬ì‹¤ìƒ <code class="language-plaintext highlighter-rouge">Least Square Method(ìµœì†Œ ììŠ¹ë²•)</code> ì™€ ë™ì¹˜ë¼ì„œ ê·¸ë ‡ë‹¤. ëª©ì í•¨ìˆ˜ $f(x)$ë¥¼ <code class="language-plaintext highlighter-rouge">MSE</code> ë¡œ ë‘ê³  ì„ í˜• íšŒê·€ ë¬¸ì œë¥¼ í‘¸ëŠ” ìƒí™©ì„ ê°€ì •í•´ë³´ì.</p>

\[Z = Ax+n \\
f(x) = (Z-Ax)^T(Z-Ax)\]

<p>ëª©ì í•¨ìˆ˜ë¥¼ ì •ì˜í–ˆê¸° ë•Œë¬¸ì— ìš°ë¦¬ëŠ” ì´ì œ ëª©ì í•¨ìˆ˜ì˜ ë„í•¨ìˆ˜ì™€ ì´ê³„ë„í•¨ìˆ˜ë¥¼ êµ¬í•  ìˆ˜ ìˆë‹¤.</p>

\[f'(x) = -2A^T(Z-Ax)\\
f''(x) = 2A^tA\]

<p>ë„í•¨ìˆ˜ì™€ ì´ê³„ë„ í•¨ìˆ˜ë¥¼ ë‰´í„´â€”ë©ìŠ¨ ìˆ˜ì‹ì— ëŒ€ì…í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.</p>

\[x_{n+1} := x_n + \frac{A^T(Z-Ax)} {A^TA} = x_n + (A^TA)^{-1}A^T(Z-Ax)\]

<p>ë¶„ëª¨ëŠ” í—¤ì‹œì•ˆ í–‰ë ¬ê³¼ ë™ì¹˜ë‹¤. í–‰ë ¬ë¡œ ì–´ë–¤ ìˆ˜ë¥¼ ë‚˜ëˆŒ ìˆ˜ëŠ” ì—†ê¸° ë•Œë¬¸ì— ë‚˜ëˆ—ì…ˆ í‘œí˜„ ëŒ€ì‹  ì—­í–‰ë ¬ë¡œ í‘œê¸°í–ˆë‹¤. ê·¸ë¦¬ê³  ìˆ˜ì‹ì´ ìƒë‹¹íˆ ë”ëŸ½ê¸° ë•Œë¬¸ì— ì •ë¦¬ë¥¼ ìœ„í•´ ì „ê°œë¥¼ í•´ë³´ë ¤ í•œë‹¤. ì „ê°œ ê²°ê³¼ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.</p>

\[x_{n+1} := x_n + (A^TA)^{-1}A^TZ - (A^TA)^{-1}A^TAx_n = (A^TA)^{-1}A^TZ\]

<p>í—¤ì‹œì•ˆ í–‰ë ¬ì´ <code class="language-plaintext highlighter-rouge">invertible</code> í•´ì•¼í•œë‹¤ë¼ëŠ” ì œì•½ ì¡°ê±´ì´ ì—¬ê¸°ì„œ ë“±ì¥í•œë‹¤. ë§Œì•½ í—¤ì‹œì•ˆ í–‰ë ¬ì´ <code class="language-plaintext highlighter-rouge">invertible</code> ì´ë¼ë©´, ë‹¤ ë‚ ë¼ê°€ê³  ìš°ë³€ì˜ í•­ë§Œ ë‚¨ê²Œ ëœë‹¤. ìš°ë³€ì˜ í•­ì„ ìì„¸íˆ ì‚´í´ë³´ë©´, <code class="language-plaintext highlighter-rouge">Least Square Method(ìµœì†Œ ììŠ¹ë²•)</code> ì˜ ìˆ˜ì‹ê³¼ ë™ì¼í•˜ë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤. ê²½ì‚¬í•˜ê°•ê³¼ëŠ” ë‹¤ë¥´ê²Œ $x_n$ê³¼ ê´€ë ¨ëœ í•­ì´ ìˆ˜ì‹ì— ì „í˜€ ë‚¨ì•„ìˆì§€ ì•Šê¸° ë•Œë¬¸ì—, ìµœì†Œ ììŠ¹ë²• ìˆ˜ì‹ì„ í•œ ë²ˆ í’€ì–´ë‚´ëŠ” ê²ƒë§Œìœ¼ë¡œ ê·¹ì ì— ë„ë‹¬í•˜ì—¬ ìˆ˜ë ´ì†ë„ê°€ í›¨ì”¬ ë¹ ë¥´ê²Œ ë˜ëŠ” ê²ƒì´ë‹¤.</p>

<p align="center">
<img src="/assets/images/optimization/gd_nr_1.png" alt="Newton-Raphson for Optimization" class="align-center image-caption" width="60%&quot;, height=&quot;50%" />
<strong><em><a href="">Newton-Raphson for Optimization</a></em></strong>
</p>

<p>ë‘ ë°©ì‹ì´ ìµœì í™” ë¬¸ì œë¥¼ í’€ì–´ë‚˜ê°€ëŠ” ê³¼ì •ì„ ë¹„êµí•˜ê¸° ìœ„í•´ ì‹œê°í™”ë¥¼ ì‹œë„í•´ë´¤ë‹¤. í•„ìì˜ ì‹œê°í™” ì‹¤ë ¥ì´ ë§¤ìš° ì¢‹ì§€ ëª»í•´ ê·¸ ì°¨ì´ê°€ ì§ê´€ì ìœ¼ë¡œ ì˜ ì•ˆë³´ì¸ë‹¤â€¦ ë¹¨ê°„ ì§ì„ ì€ ë‰´í„´-ë©ìŠ¨ ë°©ì‹ì´ê³  íŒŒë€ ì§ì„ ì€ ê²½ì‚¬ í•˜ê°• ë°©ë²•ì´ë‹¤. ì „ìëŠ” ìœ„ì—ì„œ ì‚´í´ë³¸ ê²ƒì²˜ëŸ¼ í•œë²ˆì— ê·¹ì†Œì ìœ¼ë¡œ ì´ë™í•˜ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤. í•œí¸ í›„ìëŠ” ìˆ˜ë§ì€ <code class="language-plaintext highlighter-rouge">Iteration</code> ì„ ê±°ì³ ê·¹ì†Œì ì— ë„ë‹¬í•œë‹¤. í•„ìì˜ ì‹œê°í™” ìë£Œê°€ ìƒë‹¹íˆ ì¢‹ì§€ ëª»í•˜ë‹¤ê³  ìƒê°í•´ í•˜ë‹¨ì— <a href="https://www.youtube.com/watch?v=MlZoafOnMS0&amp;list=PL_iJu012NOxeMJ5TPPW1JZKec7rhjKXUy&amp;index=6&amp;ab_channel=%ED%98%81%ED%8E%9C%ED%95%98%EC%9E%84%7CAI%26%EB%94%A5%EB%9F%AC%EB%8B%9D%EA%B0%95%EC%9D%98">í˜íœí•˜ì„</a>ë‹˜ì˜ ìë£Œë„ í•¨ê»˜ ì²¨ë¶€í–ˆìœ¼ë‹ˆ ì°¸ê³ í•˜ì. í›¨ì”¬ ì§ê´€ì ìœ¼ë¡œ ì˜ ë³´ì¸ë‹¤.</p>

<p align="center">
<img src="/assets/images/optimization/gradient_vs_newton.png" alt="Newton-Raphson vs Gradient-Descent" class="align-center image-caption" width="60%&quot;, height=&quot;50%" />
<strong><em><a href="https://ibb.co/VjvkYL7">Newton-Raphson vs Gradient-Descent</a></em></strong>
</p>]]></content><author><name>qcqced</name><email>qcqced123@gmail.com</email></author><category term="Optimization Theory" /><category term="Optimization Theory" /><category term="Newton-Raphson" /><summary type="html"><![CDATA[ìµœì í™” ë¬¸ì œë¥¼ ìœ„í•œ ë‰´í„´-ë©ìŠ¨ ë©”ì„œë“œ ì„¤ëª…]]></summary></entry><entry><title type="html">ğŸ—‚ï¸ Convex Optimization Problem</title><link href="http://localhost:4000/optimization-theory/convex" rel="alternate" type="text/html" title="ğŸ—‚ï¸ Convex Optimization Problem" /><published>2023-11-13T00:00:00+09:00</published><updated>2023-11-14T02:00:00+09:00</updated><id>http://localhost:4000/optimization-theory/convex_problem</id><content type="html" xml:base="http://localhost:4000/optimization-theory/convex"><![CDATA[<h3 id="convex-optimization-problem"><code class="language-plaintext highlighter-rouge">â“Â Convex Optimization Problem</code></h3>

\[f(wx_1 + (1-w)x_2)â‰¤ wf(x_1) + (1-w)f(x_2),\ \ w \in [0,1] \\
f''(x) â‰¥ 0\]

<p><code class="language-plaintext highlighter-rouge">Convex Problem</code> ì´ë€, ëª©ì  í•¨ìˆ˜ $f(x)$ê°€ <code class="language-plaintext highlighter-rouge">Convex Function</code> ì´ë©´ì„œ <code class="language-plaintext highlighter-rouge">Feasible Set</code> ì—­ì‹œ <code class="language-plaintext highlighter-rouge">Convex Set</code> ì´ ë˜ëŠ” ë¬¸ì œ ìƒí™©ì„ ì¼ì»«ëŠ”ë‹¤. <code class="language-plaintext highlighter-rouge">Convex Problem</code> ëŠ” ìˆ˜í•™ì  ìµœì í™”ì—ì„œ ë§¤ìš° ì¤‘ìš”í•œ ê°œë…ì¸ë°, ê·¸ ì´ìœ ëŠ” í•´ë‹¹ ì¡°ê±´ì„ ë§Œì¡±í•˜ë©´ <code class="language-plaintext highlighter-rouge">êµ­ì†Œ ìµœì í•´</code>ê°€ <code class="language-plaintext highlighter-rouge">ì „ì—­ ìµœì í•´</code>ì™€ ë™ì¹˜ê°€ ë˜ì–´ ìµœì í™” ë‚œì´ë„ê°€ ê¸‰ê²©íˆ ë‚®ì•„ì§€ê¸° ë•Œë¬¸ì´ë‹¤. ë˜í•œ <code class="language-plaintext highlighter-rouge">Convex Problem</code>ì„ í•´ê²°í•´ êµ­ì†Œ ìµœì í•´ë¥¼ êµ¬í•˜ëŠ” ì•Œê³ ë¦¬ì¦˜ì€ ì´ë¯¸ ë§ì´ ê°œë°œ ë˜ì–´ ìˆê¸° ë•Œë¬¸ì— ì£¼ì–´ì§„ ìµœì í™” ë¬¸ì œë¥¼ <code class="language-plaintext highlighter-rouge">Convex Problem</code>ìœ¼ë¡œ ì¹˜í™˜í•´ í•´ê²°í•˜ëŠ”ê²Œ ê°€ì¥ íš¨ìœ¨ì ì´ë‹¤. í•œí¸, ì—¬ê¸°ì„œ <code class="language-plaintext highlighter-rouge">Feasible Set</code> ì´ë€, í•¨ìˆ˜ì˜ <code class="language-plaintext highlighter-rouge">ì‹¤í–‰ ê°€ëŠ¥ ì˜ì—­â€¢ì •ì˜ì—­</code>ì´ë¼ê³  ìƒê°í•˜ë©´ ëœë‹¤. ì•„ë˜ ê·¸ë¦¼, ë¹¨ê°„ ì§ì„ ì˜ ì˜ì—­ì— í•´ë‹¹í•œë‹¤. ì„¸íŠ¸ë¼ëŠ” ëª…ì¹­ì€ ë¬´í•œí•œ ì§ì„ ì´ ì•„ë‹Œ ìœ í•œí•œ ì„ ë¶„ì„ í‘œí˜„í•˜ëŠ” ìš©ì–´ë¼ê³  ìƒê°í•˜ë©´ ëœë‹¤.</p>

<p>ìœ„ì— ì„œìˆ í•œ ë‘ê°œì˜ ìˆ˜ì‹ì€ ì–´ë–¤ ë¬¸ì œ ìƒí™©ì´ <code class="language-plaintext highlighter-rouge">Convex Problem</code> ì¸ì§€ ì•„ë‹Œì§€ êµ¬ë¶„í•´ì£¼ëŠ” íŒë³„ì‹ì˜ ì—­í• ì„ í•œë‹¤. ì™œ ë‘ ìˆ˜ì‹ì„ ë§Œì¡±í•˜ë©´ <code class="language-plaintext highlighter-rouge">Convex Problem</code>ì´ ë˜ëŠ”ì§€ ì‚´í´ë³´ê³  ë§ˆì§€ë§‰ì—ëŠ” <code class="language-plaintext highlighter-rouge">Convex Problem</code> ì—ì„œ ì™œ <code class="language-plaintext highlighter-rouge">êµ­ì†Œ ìµœì í•´</code>ê°€ <code class="language-plaintext highlighter-rouge">ì „ì—­ ìµœì í•´</code>ì™€ ë™ì¹˜ê°€ ë˜ëŠ”ì§€ ê·¸ ì¦ëª…ì„ í•´ë³´ë ¤ í•œë‹¤.</p>

<p align="center">
<img src="/assets/images/optimization/convex_function.png" alt="Convex Function" class="align-center image-caption" width="60%&quot;, height=&quot;50%" />
<strong><em><a href="">Convex Function</a></em></strong>
</p>

<h3 id="-jensens-inequality"><code class="language-plaintext highlighter-rouge">ï¹¤ Jensenâ€™s Inequality</code></h3>

<p>ì²«ë²ˆì§¸ ìˆ˜ì‹ì„ ë³´ì. ìš°ë¦¬ëŠ” ì´ê²ƒì„ <code class="language-plaintext highlighter-rouge">ì–€ì„¼ ë¶€ë“±ì‹</code>ì´ë¼ê³  ë¶€ë¥¸ë‹¤. <code class="language-plaintext highlighter-rouge">ì–€ì„¼ ë¶€ë“±ì‹</code> ì€ ì–´ë–¤ í•¨ìˆ˜ $f(x)$ì˜ <code class="language-plaintext highlighter-rouge">Convex Function</code> ì—¬ë¶€ë¥¼ íŒì • í•˜ëŠ”ë° ì‚¬ìš©ëœë‹¤. ì¢Œë³€ì€ <code class="language-plaintext highlighter-rouge">Feasible Set</code> ì— í•´ë‹¹ë˜ëŠ” ë°ì´í„° í¬ì¸íŠ¸ì˜ í•¨ìˆ˜ê°’ì„ ì˜ë¯¸í•˜ë©° ê·¸ë¦¼ ìƒì—ì„œ ì´ˆë¡ìƒ‰ ê³¡ì„ ìœ¼ë¡œ í‘œí˜„ëœë‹¤. í•œí¸, ìš°ë³€ì€ <code class="language-plaintext highlighter-rouge">Feasible Set</code>ì˜ í‰ê· ë³€í™”ìœ¨ì„ ê¸°ìš¸ê¸°ë¡œ í•˜ë©´ì„œ êµ¬ê°„ ì–‘ìª½ ëì„ ì§€ë‚˜ëŠ” ì„ ë¶„ì„ ì¼ì»«ëŠ”ë‹¤. ê·¸ë¦¼ì—ì„œ íŒŒë€ìƒ‰ ì§ì„ ì´ ë°”ë¡œ ë¶€ë“±ì‹ì˜ ìš°ë³€ì´ë‹¤.</p>

<p>ê·¸ë ‡ë‹¤ë©´ ìœ„ ë¶€ë“±ì‹ì„ í•­ìƒ ë§Œì¡±í•˜ë ¤ë©´ í•¨ìˆ˜ $f(x)$ëŠ” ì–´ë–¤ í˜•íƒœë¥¼ ê°€ì ¸ì•¼ í• ê¹Œ?? ë¨¼ì € ì˜¤ëª© í•¨ìˆ˜ì¸ <code class="language-plaintext highlighter-rouge">Concave Function</code>ë¶€í„° ìƒê°í•´ë³´ì. ìœ„ ê·¸ë¦¼ì„ ë’¤ì§‘ì–´ì„œ ìƒê°í•´ë³´ë©´ ë˜ëŠ”ë°, í•¨ìˆ˜ê°€ ì •ì˜ë˜ëŠ” ì „ì²´ ì •ì˜ì—­ì—ì„œ ìœ„ ë¶€ë“±ì‹ì„ ë§Œì¡±í•˜ëŠ” êµ¬ê°„(íŒŒë€ìƒ‰ ì§ì„ ì´ ì´ˆë¡ìƒ‰ ê³¡ì„ ë³´ë‹¤ ìœ„ì— ìˆëŠ”)ì„ ì „í˜€ ì°¾ì•„ë³¼ ìˆ˜ ì—†ë‹¤. ë”°ë¼ì„œ ì–€ì„¼ ë¶€ë“±ì‹ì„ ë§Œì¡±í•˜ë ¤ë©´ <code class="language-plaintext highlighter-rouge">Feasible Set</code> ì˜ êµ¬ê°„ì´ ë°˜ë“œì‹œ <code class="language-plaintext highlighter-rouge">Convex Set</code> ì´ì–´ì•¼ í•˜ê³ , í•´ë‹¹ êµ¬ê°„ì—ì„œ ëª©ì í•¨ìˆ˜ëŠ” ë°˜ë“œì‹œ <code class="language-plaintext highlighter-rouge">Convex Function</code>ì˜ í˜•íƒœë¥¼ ë„ê³  ìˆì–´ì•¼ í•œë‹¤.</p>

<p>ê·¸ë ‡ë‹¤ë©´ ëª©ì í•¨ìˆ˜ê°€ <code class="language-plaintext highlighter-rouge">Convex</code>ì¸ì§€ëŠ” ì–´ë–»ê²Œ íŒë³„í•  ìˆ˜ ìˆì„ê¹Œ?? ì£¼ì–´ì§„ ëª¨ë“  ìƒí™©ì—ì„œ ìœ„ ê·¸ë¦¼ì²˜ëŸ¼ ì‰½ê²Œ í•¨ìˆ˜ì˜ ê·¸ë˜í”„ë¥¼ ê·¸ë¦´ ìˆ˜ëŠ” ì—†ì„ ê²ƒì´ë‹¤. ê·¸ë˜ì„œ ìˆ˜ì‹ìœ¼ë¡œ ì–´ë–¤ ëª©ì í•¨ìˆ˜ê°€ <code class="language-plaintext highlighter-rouge">Convex</code>ì¸ì§€ íŒë³„í•  ìˆ˜ ìˆì–´ì•¼ í•œë‹¤. ë“œë””ì–´ ì œì‹œëœ ë‘ë²ˆì§¸ ìˆ˜ì‹ì„ í™œìš©í•  ì°¨ë¡€ë‹¤.</p>

<h3 id="second-derivative"><code class="language-plaintext highlighter-rouge">ğŸ“ˆÂ Second Derivative</code></h3>

<p>ë‘ë²ˆì§¸ ìˆ˜ì‹ì„ í”íˆ ì´ê³„ë„í•¨ìˆ˜ë¼ê³  ë¶€ë¥¸ë‹¤. ì•„ë§ˆ ìˆ˜ëŠ¥ ìˆ˜í•™ì—ì„œ 21, 30ë²ˆê³¼ ê°™ì€ í‚¬ëŸ¬ ë¬¸ì œë¥¼ í’€ ë•Œ ê°€ë”ì”© ì‚¬ìš©í•˜ë˜ ê¸°ì–µì´ ë‚  ê²ƒì´ë‹¤. ì´ê³„ë„í•¨ìˆ˜ëŠ” ë„í•¨ìˆ˜ë¥¼ í•œ ë²ˆ ë” ë¯¸ë¶„í•œ ê²ƒìœ¼ë¡œ ì›í•¨ìˆ˜ì˜ ê³¡ì„ ì´ ì–¼ë§ˆë‚˜ ë¹ ë¥´ê²Œ ë³€í•˜ëŠ”ì§€ í˜¹ì€ ê³¡ì„ ì˜ ê³¡ë¥ ì— ëŒ€í•œ ì •ë³´ë¥¼ ì•Œë ¤ì¤€ë‹¤. ì´ë¥¼ í†µí•´ ì›í•¨ìˆ˜ì˜ ê·¹ëŒ€, ê·¹ì†ŒëŠ” ë¬¼ë¡  ë³€ê³¡ì ì˜ ìœ„ì¹˜ë¥¼ ì•Œì•„ë‚¼ ìˆ˜ ìˆë‹¤. ê·¸ë˜ì„œ ì–´ë–¤ ì´ì°¨í•¨ìˆ˜ë¥¼ ì˜ˆì‹œë¡œ ë“¤ì–´ë³´ì. 2ì°¨í•­ì˜ ë¶€í˜¸ê°€ ì–‘ìˆ˜ë¼ë©´ ì´ê³„ë„í•¨ìˆ˜ì˜ ê°’ì€ í•­ìƒ ì–‘ìˆ˜ê°€ ë  ê²ƒì´ê³ , ìŒìˆ˜ë¼ë©´ í•­ìƒ ìŒìˆ˜ê°€ ë  ê²ƒì´ë‹¤. ê·¸ëŸ°ë° ìš°ë¦¬ëŠ” ì´ë¯¸ ì§ê´€ì ìœ¼ë¡œ 2ì°¨í•¨ìˆ˜ì—ì„œ ìµœê³ ì°¨í•­ì˜ ë¶€í˜¸ê°€ ì–‘ìˆ˜ë©´ ì•„ë˜ë¡œ ë³¼ë¡í•œ í•¨ìˆ˜, ë°˜ëŒ€ì˜ ê²½ìš° ìœ„ë¡œ ì˜¤ëª©í•œ ì˜¤ëª©í•¨ìˆ˜ê°€ ëœë‹¤ëŠ” ê²ƒì„ ì•Œê³  ìˆë‹¤. ë”°ë¼ì„œ ì´ê³„ë„ í•¨ìˆ˜ì˜ ê°’ì´ í•­ìƒ ì–‘ìˆ˜ë¼ë©´ í•´ë‹¹ í•¨ìˆ˜ëŠ” <code class="language-plaintext highlighter-rouge">Convex Function</code>ì´ ëœë‹¤.</p>

<p>ì§€ê¸ˆê¹Œì§€ëŠ” ë‹¨ë³€ìˆ˜ í•¨ìˆ˜ì— ëŒ€í•œ ì¼€ì´ìŠ¤ë§Œ ì‚´í´ë³´ì•˜ë‹¤. ê·¸ë ‡ë‹¤ë©´ ì´ê²ƒì„ ë‹¤ë³€ìˆ˜ë¡œ í™•ì¥í•  ìˆ˜ëŠ” ì—†ì„ê¹Œ?? ë¬¼ë¡  ê°€ëŠ¥í•˜ë‹¤. ì–´ë–¤ ë‹¤ë³€ìˆ˜ í•¨ìˆ˜ê°€ <code class="language-plaintext highlighter-rouge">Convex</code>ì¸ì§€ íŒì •í•˜ëŠ” ê²ƒë„ ìœ„ì™€ ë™ì¼í•œ ì¡°ê±´ì„ í†µí•´ íŒë³„í•œë‹¤. ì´ ë•Œ ë“±ì¥í•˜ëŠ”ê²Œ ë°”ë¡œ í—¤ì‹œì•ˆ í–‰ë ¬ì´ë‹¤. í—¤ì‹œì•ˆ í–‰ë ¬ì´ë€, ì–´ë–¤ ë‹¤ë³€ìˆ˜ í•¨ìˆ˜ì˜ ì´ê³„ë„í•¨ìˆ˜ê°’ì„ í–‰ë ¬ë¡œ ë‚˜íƒ€ë‚¸ ê²ƒì´ë‹¤. ë‹¨ë³€ìˆ˜ í•¨ìˆ˜ì˜ ì´ê³„ë„í•¨ìˆ˜ ì—­í• ê³¼ ë™ì¼í•˜ë‹¤. ê·¸ë˜ì„œ ì–´ë–¤ ë‹¤ë³€ìˆ˜ í•¨ìˆ˜ê°€ ì»¨ë°±ìŠ¤ í•¨ìˆ˜ì´ë ¤ë©´, í—¤ì‹œì•ˆ í–‰ë ¬ì´ <code class="language-plaintext highlighter-rouge">Positive Semi-Define</code> ì¡°ê±´ì„ ë§Œì¡±í•´ì•¼ í•œë‹¤. í—¤ì‹œì•ˆ í–‰ë ¬ê³¼ <code class="language-plaintext highlighter-rouge">Positive Semi-Define</code>ì— ëŒ€í•´ì„œëŠ” ë‹¤ë¥¸ í¬ìŠ¤íŠ¸ì—ì„œ ìì„¸íˆ ë‹¤ë£¨ë„ë¡ í•˜ê² ë‹¤.</p>

<h3 id="-proof-local-minimum-same-as-global-minimum-in-convex"><code class="language-plaintext highlighter-rouge">ğŸªª Proof: local minimum same as global minimum in Convex</code></h3>

<p>ì´ì œ ë“œë””ì–´ ëª©ì í•¨ìˆ˜ê°€ ì»¨ë°±ìŠ¤ í•¨ìˆ˜ì¼ ë•Œ, êµ­ì†Œ ìµœì í•´ê°€ ì „ì—­ ìµœì í•´ì™€ ë™ì¹˜ê°€ ë˜ëŠ”ì§€ë¥¼ ì¦ëª…í•´ë³´ë ¤ í•œë‹¤. ë¨¼ì € ì¦ëª…ì€ ê·€ë¥˜ë²•ì„ ì‚¬ìš©í•œë‹¤. ê·€ë¥˜ë²•ì´ë€, ìˆ˜í•™, ë…¼ë¦¬í•™, ì² í•™ ë“±ì—ì„œ ì£¼ì¥ì´ ë¶€ì •ë¨ì„ ë³´ì´ê¸° ìœ„í•´ ëª¨ìˆœ ë˜ëŠ” ë¶€ì •ëœ ê°€ì •ì„ ìœ ë„í•˜ëŠ” ë…¼ì¦ ê¸°ë²•ìœ¼ë¡œ, íŠ¹ì • ì£¼ì¥ì´ ì°¸ì„ì„ ì¦ëª…í•˜ê¸°ë³´ë‹¤ëŠ” ê·¸ ë°˜ëŒ€ì¸ ë¶€ì •ëœ ì£¼ì¥ì´ ê±°ì§“ì„ì„ ë³´ì´ëŠ” ë° ì‚¬ìš©í•œë‹¤. ê·¸ë˜ì„œ ê·€ë¥˜ë²•ì„ í™œìš©í•´ ë‹¤ìŒê³¼ ê°™ì€ ëª…ì œê°€ ê±°ì§“ì„ì„ ì¦ëª…í•´ë³´ë ¤ í•œë‹¤.</p>

<p>ì–´ë–¤ í•¨ìˆ˜ $f(x)$ëŠ” Convexë©´ì„œ Feasible Setì´ Convex Setì„ì„ ë§Œì¡±í•˜ëŠ” ë™ì‹œì— ì•„ë˜ ìˆ˜ì‹ì„ ë§Œì¡±í•œë‹¤ê³  í•œë‹¤.</p>

\[f(x^@) &lt; f(x^!)\]

<p>ë¶€ë“±ì‹ì˜ ìš°ë³€ì€ êµ­ì†Œ ìµœì í•´ë¥¼ ì˜ë¯¸í•˜ê³ , ì¢Œë³€ì€ êµ­ì†Œ ìµœì í•´ë³´ë‹¤ ì‘ì€ í•¨ìˆ˜ê°’ì´ ì¡´ì¬í•œë‹¤ê³  ê°€ì •í•œ ê²ƒì´ë‹¤. ì´ì œ ì´ ëª…ì œê°€ í‹€ë¦¼ì„ ì¦ëª…í•˜ë©´ ìš°ë¦¬ëŠ” ìì—°ìŠ¤ëŸ½ê²Œ êµ­ì†Œ ìµœì í•´ê°€ ì „ì—­ ìµœì í•´ì™€ ë™ì¹˜ê°€ ëœë‹¤ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆê²Œ ëœë‹¤.</p>

<p>ìš°ë¦¬ëŠ” í•¨ìˆ˜ $f(x)$ê°€ <code class="language-plaintext highlighter-rouge">Convex</code>ë©´ì„œ <code class="language-plaintext highlighter-rouge">Feasible Set</code>ì´ <code class="language-plaintext highlighter-rouge">Convex Set</code>ì´ë¼ê³  ê°€ì •í–ˆê¸° ë•Œë¬¸ì—, $\alpha x^! + (1-\alpha)x^@$ ì—­ì‹œ <code class="language-plaintext highlighter-rouge">Feasible Set</code>ì´ ë  ê²ƒì´ë‹¤. ì´ëŸ¬í•œ ì‚¬ì‹¤ì„ í™œìš©í•´ ë‘ ì ($x^!, x^@$)ì„ ì–€ì„¼ ë¶€ë“±ì‹ì— ë„£ì–´ë³´ì.</p>

\[f(wx^! + (1-w)x^@)â‰¤ wf(x^!) + f(x^!) - f(x^!) + (1-w)f(x^@) \\
f(wx^! + (1-w)x^@)â‰¤ f(x^!) +(1-w)(f(x^@) -f(x^!))\]

<p>ìš°ë³€ë¶€í„° í•¨ê»˜ ì‚´í´ë³´ì. $(1-w)$ëŠ” ë°˜ë“œì‹œ ì–‘ìˆ˜ê°€ ëœë‹¤. ì–€ì„¼ ë¶€ë“±ì‹ ì •ì˜ìƒ $w \in [0,1]$ì„ ë§Œì¡±í•˜ê¸° ë•Œë¬¸ì´ë‹¤. í•œí¸, $f(x^@) -f(x^!)$ëŠ” ìŒìˆ˜ê°€ ëœë‹¤. ì¦ëª…ì„ ì‹œì‘í•˜ë©´ì„œ $f(x^@)$ê°€ $f(x^!)$ë³´ë‹¤ ì‘ë‹¤ê³  ê°€ì •í–ˆê¸° ë•Œë¬¸ì´ë‹¤. ë”°ë¼ì„œ ìš°ë³€ì€ $f(x^!)$ë³´ë‹¤ ì•„ì£¼ ì¡°ê¸ˆ ì‘ì€ ê°’(wâ†’1)ì´ ë  ê²ƒì´ë‹¤. ì´ì œ ì¢Œë³€ì„ ì‚´í´ë³´ì. ë§ˆì°¬ê°€ì§€ë¡œ wâ†’1ì„ í•´ì£¼ë©´ ì¢Œë³€ì€ $f(x^!)$ì˜ ë§¤ìš° ê·¼ì ‘í•œ ìœ„ì¹˜ì˜ í•¨ìˆ˜ê°’ì„ ë³´ìëŠ” ì˜ë¯¸ê°€ ëœë‹¤. ì¢Œë³€ì„ $\alpha$, ìš°ë³€ì„ $\beta$ë¡œ ì¹˜í™˜í•´ ì§€ê¸ˆê¹Œì§€ ì¦ëª… ê³¼ì •ì„ ë‹¤ì‹œ ë¶€ë“±ì‹ìœ¼ë¡œ í‘œí˜„í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.</p>

\[\alpha â‰¤ \beta &lt; f(x^!)\]

<p>ìš°ë¦¬ëŠ” $f(x^!)$ê°€ ì§€ì—­ ìµœì†Œê°’ì´ë¼ê³  ì •ì˜í•œ ë°” ìˆë‹¤. ì§€ì—­ ìµœì†Œê°’ì´ë¼ëŠ” ê²ƒì€ ê·¸ ì£¼ë³€ì—ì„œ ê°€ì¥ ì‘ì€ê°’ì´ë¼ëŠ” ì˜ë¯¸ë¥¼ ê°–ëŠ”ë‹¤. ê·¸ëŸ°ë° $f(x^!)$ì˜ ì£¼ë³€ê°’ì¸ $\alpha$ê°€ ì§€ì—­ ìµœì†Œê°’ë³´ë‹¤ ì‘ë‹¤ê³  ë¶€ë“±ì‹ì€ ë§í•˜ê³  ìˆê¸° ë•Œë¬¸ì—, ì§€ì—­ ìµœì†Œê°’ ì •ì˜ì— ìœ„ë°°ë˜ì–´ ìœ„ ëª…ì œëŠ” ê±°ì§“ì´ ëœë‹¤.</p>

<p>ì •ë¦¬í•˜ìë©´, ì–´ë–¤ í•¨ìˆ˜ $f(x)$ê°€ ì»¨ë°±ìŠ¤ ì„±ì§ˆì„ ë§Œì¡±í•˜ë©´ì„œ, Feasible Set ì—­ì‹œ Convex Setì¸ ê²½ìš°ì— êµ­ì†Œ ìµœì†Œê°’ì´ ì „ì—­ ìµœì†Œê°’ì´ ì•„ë‹ ê²½ìš°, êµ­ì†Œ ìµœì ì´ êµ­ì†Œ ìµœì ì´ ì•„ë‹Œ ê²½ìš°ê°€ ë°œìƒí•˜ê¸° ë–„ë¬¸ì— ë°˜ë“œì‹œ ìœ„ ì¡°ê±´ì—ì„œ êµ­ì†Œ ìµœì ê°’ì€ ì „ì—­ ìµœì ê°’ì´ ë˜ì–´ì•¼ í•œë‹¤.</p>]]></content><author><name>qcqced</name><email>qcqced123@gmail.com</email></author><category term="Optimization Theory" /><category term="Optimization Theory" /><category term="Convex Optimization" /><summary type="html"><![CDATA[ì»¨ë°±ìŠ¤ ìµœì í™” ë¬¸ì œ ì¦ëª…]]></summary></entry></feed>