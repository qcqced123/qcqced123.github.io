<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2023-07-12T12:00:48+09:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">AI/Business Study Log</title><subtitle>NLP, Marketing</subtitle><author><name>qcqced</name><email>qcqced123@gmail.com</email></author><entry><title type="html">🤔 RuntimeError: Function ‘LogSoftmaxBackward0’ returned nan values in its 0th output</title><link href="http://localhost:4000/cs-ai/framework-library/backward-nan" rel="alternate" type="text/html" title="🤔 RuntimeError: Function ‘LogSoftmaxBackward0’ returned nan values in its 0th output" /><published>2023-07-11T00:00:00+09:00</published><updated>2023-07-11T13:00:00+09:00</updated><id>http://localhost:4000/cs-ai/framework-library/pytorch-backward-nan</id><content type="html" xml:base="http://localhost:4000/cs-ai/framework-library/backward-nan"><![CDATA[<h3 id="-pytorch-backward-과정에서-nan-발생하는-문제"><code class="language-plaintext highlighter-rouge">🔥 Pytorch Backward 과정에서 NaN 발생하는 문제</code></h3>

<p>커스텀으로 모델, 여러 풀링, 매트릭, 손실 함수들을 정의하면서부터 제일 많이 마주하게 되는 에러다. 진심으로 요즘 <code class="language-plaintext highlighter-rouge">CUDA OOM</code> 보다 훨씬 자주 보는 것 같다. 해당 에러는 <code class="language-plaintext highlighter-rouge">LogSoftmax</code> 레이어에 전달된 입력값 중에서 <code class="language-plaintext highlighter-rouge">nan</code>, <code class="language-plaintext highlighter-rouge">inf</code> 가 포함되어 연산을 진행할 수 없다는 것을 의미한다. 딥러닝 실험을 진행하면서 가장 해결하기 까다로운 녀석으로 원인을 특정하기 힘들기 때문이다. 원인을 잡기 어려운 이유는 바로 우리가 지금 하고 있는게 <code class="language-plaintext highlighter-rouge">‘딥러닝’</code> 이라서 그렇다. 위 에러는 대부분 연산자가 우리가 의도하지 않은 동작을 하는 케이스 때문인데, 하나 하나 디버깅하기에는 너무나도 연산자가 많다. 또한 딥러닝은 입출력으로 엄청나게 큰 사이즈의 행렬을 사용한다. 우리가 <code class="language-plaintext highlighter-rouge">nan</code>, <code class="language-plaintext highlighter-rouge">inf</code> 값 존재에 대해서 인지하기 쉽지 않다.</p>

<p><strong><u>위 에러는 필자의 경험상 대부분 커스텀으로 정의한 레이어에서 발생하는 경우가 많았으며 특히</u></strong> <code class="language-plaintext highlighter-rouge">분수</code>, <code class="language-plaintext highlighter-rouge">각도</code>, <code class="language-plaintext highlighter-rouge">제곱근</code>, <code class="language-plaintext highlighter-rouge">지수</code> <strong><u>개념을 사용하는 연산자가 대부분 원인이었다.</u></strong> 예를 들어 코사인 유사도를 구하는 과정에서 연산 대상 벡터값에  <code class="language-plaintext highlighter-rouge">zero-value</code> 가 포함된 경우 분모가 0이 되기 때문에 연산 정의가 되지 않아 <code class="language-plaintext highlighter-rouge">nan</code> 을 반환해 위와 같은 에러가 발생하는 경우가 있다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">check_nan</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
    <span class="s">""" Check if there is NaN in tensor """</span>
    <span class="n">checker</span> <span class="o">=</span> <span class="bp">False</span>
    <span class="k">if</span> <span class="bp">True</span> <span class="ow">in</span> <span class="n">torch</span><span class="p">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
        <span class="n">checker</span> <span class="o">=</span> <span class="bp">True</span>
    <span class="k">return</span> <span class="n">checker</span>

<span class="k">def</span> <span class="nf">zero_filtering</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="s">"""
    Add eps value for zero embedding, because competition metric is cosine similarity
    Cosine Similarity will be returned NaN, when input value has zero, like as torch.clamp()
    """</span>
    <span class="n">eps</span> <span class="o">=</span> <span class="mf">1e-4</span>
    <span class="n">x</span><span class="p">[</span><span class="n">x</span> <span class="o">&lt;=</span> <span class="n">eps</span><span class="p">]</span> <span class="o">=</span> <span class="n">eps</span>
    <span class="k">return</span> <span class="n">x</span>

<span class="k">def</span> <span class="nf">nan_filtering</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">eps</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-4</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="s">"""
    Change eps value for NaN Embedding, because competition metric is cosine similarity
    Cosine Similarity will be returned NaN
    """</span>
    <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">nan</span><span class="o">=</span><span class="n">eps</span><span class="p">)</span>

<span class="k">class</span> <span class="nc">CLIPGEMPooling</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="s">"""
    Generalized Mean Pooling for Natural Language Processing
    This class version of GEMPooling for CLIP, Transfer from NLP Task Code
    ViT don't use attention mask, because input image shape will be same

    Mean Pooling &lt;= GEMPooling &lt;= Max Pooling
    Because of doing exponent to each token embeddings, GEMPooling is like as weight to more activation token

    In original paper, they use p=3, but in this class, we use p=4 because torch doesn't support pow calculation
    for negative value tensor, only for non-negative value in odd number exponent
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">auto_cfg</span><span class="p">:</span> <span class="n">AutoConfig</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">CLIPGEMPooling</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>

    <span class="o">@</span><span class="nb">staticmethod</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">last_hidden_state</span><span class="p">,</span> <span class="n">p</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="s">"""
        last_hidden_state.size: [batch_size, patches_sequence, hidden_size]
        1) Pow last_hidden_state with p and then take a averaging
        2) pow sum_embeddings with 1/p
        """</span>
        <span class="n">p_embeddings</span> <span class="o">=</span> <span class="n">zero_filtering</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nb">pow</span><span class="p">(</span><span class="n">last_hidden_state</span><span class="p">,</span> <span class="n">p</span><span class="p">))</span>
        <span class="c1"># Check NaN value in Embedding after applying torch.pow
</span>        <span class="k">if</span> <span class="n">check_nan</span><span class="p">(</span><span class="n">p_embeddings</span><span class="p">):</span>
            <span class="n">p_embeddings</span> <span class="o">=</span> <span class="n">nan_filtering</span><span class="p">(</span><span class="n">p_embeddings</span><span class="p">)</span>
        <span class="n">sum_embeddings</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">p_embeddings</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

        <span class="n">gem_embeddings</span> <span class="o">=</span> <span class="n">zero_filtering</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nb">pow</span><span class="p">(</span><span class="n">sum_embeddings</span><span class="p">,</span> <span class="mf">1.</span> <span class="o">/</span> <span class="n">p</span><span class="p">))</span>
        <span class="c1"># Check NaN value in Embedding after applying torch.pow
</span>        <span class="k">if</span> <span class="n">check_nan</span><span class="p">(</span><span class="n">gem_embeddings</span><span class="p">):</span>
            <span class="n">gem_embeddings</span> <span class="o">=</span> <span class="n">nan_filtering</span><span class="p">(</span><span class="n">gem_embeddings</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">gem_embeddings</span>

<span class="k">class</span> <span class="nc">CLIPMultipleNegativeRankingLoss</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="s">"""
    Multiple Negative Ranking Loss for CLIP Model
    main concept is same as original one, but append suitable for other type of model (Not Sentence-Transformers)
    if you set more batch size, you can get more negative pairs for each anchor &amp; positive pair
    Args:
        scale: output of similarity function is multiplied by this value =&gt; I don't know why this is needed
        similarity_fct: standard of distance metrics, default cosine similarity
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">reduction</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">scale</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">20.0</span><span class="p">,</span> <span class="n">similarity_fct</span><span class="o">=</span><span class="n">cos_sim</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">reduction</span> <span class="o">=</span> <span class="n">reduction</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">scale</span> <span class="o">=</span> <span class="n">scale</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">similarity_fct</span> <span class="o">=</span> <span class="n">similarity_fct</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">reduction</span> <span class="o">=</span> <span class="n">reduction</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">cross_entropy_loss</span> <span class="o">=</span> <span class="n">CrossEntropyLoss</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">reduction</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">embeddings_a</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">embeddings_b</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="n">similarity_scores</span> <span class="o">=</span> <span class="n">zero_filtering</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">similarity_fct</span><span class="p">(</span><span class="n">embeddings_a</span><span class="p">,</span> <span class="n">embeddings_b</span><span class="p">))</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">scale</span>
        <span class="k">if</span> <span class="n">check_nan</span><span class="p">(</span><span class="n">similarity_scores</span><span class="p">):</span>
            <span class="s">""" Check NaN Value in similarity_scores """</span>
            <span class="n">similarity_scores</span> <span class="o">=</span> <span class="n">nan_filtering</span><span class="p">(</span><span class="n">similarity_scores</span><span class="p">)</span>

        <span class="n">labels</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">(</span>
            <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">similarity_scores</span><span class="p">)),</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">long</span><span class="p">,</span>
            <span class="n">device</span><span class="o">=</span><span class="n">similarity_scores</span><span class="p">.</span><span class="n">device</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">cross_entropy_loss</span><span class="p">(</span><span class="n">similarity_scores</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
</code></pre></div></div>

<p>필자의 경우, 두 개의 입력 행렬에 각각  <code class="language-plaintext highlighter-rouge">sqrt()</code> 를 적용하고 두 행렬의 개별 원소 사이의 코사인 유사도를 구해야 했던 적이 있다. <code class="language-plaintext highlighter-rouge">sqrt</code> <strong><u>과정에서 너무 작은 값들이 입력으로 들어가</u></strong> <code class="language-plaintext highlighter-rouge">underflow</code> <strong><u>가 발생해 행렬에</u></strong> <code class="language-plaintext highlighter-rouge">zero-value</code> <strong><u>가 생겼고, 이를 모른채 코사인 유사도를 구하다가 한참을 위 에러와 싸웠던 적이 있다.</u></strong> 심지어 연산속도 향상을 위해서 <strong><code class="language-plaintext highlighter-rouge">torch.autocast</code></strong> 클래스의 <code class="language-plaintext highlighter-rouge">grad_scaler(float32 to float16)</code> 까지 적용하고 있었다.</p>

<h3 id="️-내가-해결한-방법"><code class="language-plaintext highlighter-rouge">🖍️ 내가 해결한 방법</code></h3>
<p>이 글을 읽는 당신이 만약 <code class="language-plaintext highlighter-rouge">sqrt</code> 혹은 <code class="language-plaintext highlighter-rouge">pow</code>를 활용하는 경우, <code class="language-plaintext highlighter-rouge">underflow</code> 방지를 위해서 <del>위 예시 코드처럼 꼭 적당한 입실론 값을 연산 전후에 필요에 따라 더해줄 것을 권장한다.</del> 입실론 값의 설정은 현재 자신이 사용하고 있는 부동 소수점 정확도에 맞게 설정해주면 될 것 같다. <code class="language-plaintext highlighter-rouge">float32</code> 를 사용하는 경우에는 대부분 <code class="language-plaintext highlighter-rouge">1e-6</code> 을 많이 사용하는 것 같다. 필자도 정확히 어떤 값이 적당한지 아직 잘 모르겠다… 그리고 딥러닝 실험하면서 <code class="language-plaintext highlighter-rouge">overflow</code> 때문에 <code class="language-plaintext highlighter-rouge">inf</code> 이 발생했던 적은 없었다.</p>

<p>입실론 값을 문제가 되는 연산 전에 일괄적으로 더할 경우, 아무리 작은 값이라도 연산 종류에 따라서 결과가 크게 왜곡되는 경우가 발생한다. 따라서 연산을 먼저 적용한 뒤 결과에 <code class="language-plaintext highlighter-rouge">NaN</code>, <code class="language-plaintext highlighter-rouge">Inf</code>, <code class="language-plaintext highlighter-rouge">Zero</code>가 발생하는지 체크하고, 발생한 부분에 한해서 입실론 값을 더해주는 커스텀 <code class="language-plaintext highlighter-rouge">function</code>울 정의해 문제를 해결했다.<br />
(위의 코드 예제 <code class="language-plaintext highlighter-rouge">check_nan</code>, <code class="language-plaintext highlighter-rouge">zero_filtering</code>, <code class="language-plaintext highlighter-rouge">nan_filtering</code>)</p>

<p>한편 <code class="language-plaintext highlighter-rouge">torch.autograd.set_detect_anomaly(True)</code> 를 훈련 루프 초반에 정의해주면, <code class="language-plaintext highlighter-rouge">NaN</code>이 발생하는 즉시 실행이 멈추고 <code class="language-plaintext highlighter-rouge">NaN</code>을 유발한 라인을 출력해준다. 꼭 활용해보자.</p>]]></content><author><name>qcqced</name><email>qcqced123@gmail.com</email></author><category term="Pytorch Error Handling" /><category term="Pytorch" /><category term="Logsoftmax" /><category term="NaN" /><category term="Error Handling" /><summary type="html"><![CDATA[Pytorch Error: Backward NaN values]]></summary></entry><entry><title type="html">🖥️ RuntimeError: Attempting to deserialize object on CUDA device 2 but torch.cuda.device_count() is 1. Please use torch.load with map_location to map your storages to an existing device</title><link href="http://localhost:4000/cs-ai/framework-library/cuda-num" rel="alternate" type="text/html" title="🖥️ RuntimeError: Attempting to deserialize object on CUDA device 2 but torch.cuda.device_count() is 1. Please use torch.load with map_location to map your storages to an existing device" /><published>2023-07-11T00:00:00+09:00</published><updated>2023-07-11T13:00:00+09:00</updated><id>http://localhost:4000/cs-ai/framework-library/pytorch-cuda-device-num</id><content type="html" xml:base="http://localhost:4000/cs-ai/framework-library/cuda-num"><![CDATA[<h3 id="-pytorch-잘못된-cuda-장치-번호-사용-문제"><code class="language-plaintext highlighter-rouge">🔢 Pytorch 잘못된 CUDA 장치 번호 사용 문제</code></h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span><span class="p">.</span><span class="n">load_state_dict</span><span class="p">(</span>
    <span class="n">torch</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">map_location</span><span class="o">=</span><span class="s">'cuda:0'</span><span class="p">)</span> 
<span class="p">)</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">pretrained model</code>, <code class="language-plaintext highlighter-rouge">weight</code>를 <code class="language-plaintext highlighter-rouge">load</code>하거나 혹은 훈련 루프를 <code class="language-plaintext highlighter-rouge">resume</code> 을 위해 <code class="language-plaintext highlighter-rouge">torch.load()</code> 를 사용할 때 마주할 수 있는 에러 로그다. 발생하는 이유는 현재 <code class="language-plaintext highlighter-rouge">GPU</code> 에 할당하려는 모델이 사전 훈련때 할당 되었던 <code class="language-plaintext highlighter-rouge">GPU</code> 번호와 현재 할당하려는 <code class="language-plaintext highlighter-rouge">GPU</code> 번호가 서로 상이하기 때문이다. 따라서 <code class="language-plaintext highlighter-rouge">torch.load</code>의 <code class="language-plaintext highlighter-rouge">map_location</code>인자에 현재 자신이 사용하려는 <code class="language-plaintext highlighter-rouge">GPU</code> 번호를 입력해주자.</p>]]></content><author><name>qcqced</name><email>qcqced123@gmail.com</email></author><category term="Pytorch Error Handling" /><category term="Pytorch" /><category term="CUDA" /><category term="Error Handling" /><summary type="html"><![CDATA[Pytorch Error: Wrong CUDA Device Number]]></summary></entry><entry><title type="html">🪢 assert len(optimizer_state[“found_inf_per_device”]) &amp;gt; 0, “No inf checks were recorded for this optimizer.” AssertionError: No inf checks were recorded for this optimizer.</title><link href="http://localhost:4000/cs-ai/framework-library/inf-per-device" rel="alternate" type="text/html" title="🪢 assert len(optimizer_state[“found_inf_per_device”]) &amp;gt; 0, “No inf checks were recorded for this optimizer.” AssertionError: No inf checks were recorded for this optimizer." /><published>2023-07-11T00:00:00+09:00</published><updated>2023-07-11T13:00:00+09:00</updated><id>http://localhost:4000/cs-ai/framework-library/pytorch-found_inf_per_device</id><content type="html" xml:base="http://localhost:4000/cs-ai/framework-library/inf-per-device"><![CDATA[<h3 id="-optimizer가-손실값을-제대로-backward-할-수-없는-문제"><code class="language-plaintext highlighter-rouge">🤔 Optimizer가 손실값을 제대로 Backward 할 수 없는 문제</code></h3>

<p>텐서의 계산 그래프가 중간에 끊어져 옵티마이저가 그라디언트를 제대로 <code class="language-plaintext highlighter-rouge">Backward</code> 하지 못해 발생하는 에러다. 공부를 시작하고 정말 처음 마주하는 에러라서 정말 많이 당황했다. 래퍼런스 자료 역시 거의 없어서 해결하는데 애를 먹었던  쓰라린 사연이 있는 에러다. 이 글을 읽는 독자라면 대부분 텐서의 계산 그래프가 중간에 끊어진다는 것이 무슨 의미일지 이해하시지 못할거라 생각한다. 그게 정상이다. 필자 역시 알고 싶지 않았으나 욕심만 많고 멍청한 탓에… 알게 되었다. 아래 예시 코드를 먼저 살펴보자.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Before Append
</span><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">:</span> <span class="nb">dict</span><span class="p">,</span> <span class="n">position_list</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">feature</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
        <span class="n">feature</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">.</span><span class="n">last_hidden_state</span>
        <span class="n">pred</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">batch_size</span><span class="p">):</span>
            <span class="s">""" Apply Pooling &amp; Fully Connected Layer for each unique cell in batch (one notebook_id) """</span>
            <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">position_list</span><span class="p">[</span><span class="n">i</span><span class="p">])):</span>
                <span class="n">src</span><span class="p">,</span> <span class="n">end</span> <span class="o">=</span> <span class="n">position_list</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">idx</span><span class="p">]</span>
                <span class="n">embedding</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">pooling</span><span class="p">(</span><span class="n">feature</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">src</span><span class="p">:</span><span class="n">end</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="p">:].</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>  <span class="c1"># maybe don't need mask
</span>                <span class="n">logit</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">fc</span><span class="p">(</span><span class="n">embedding</span><span class="p">)</span>
                <span class="n">pred</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">logit</span><span class="p">)</span>  
            <span class="n">pred</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">pred</span>

<span class="c1"># After Append
</span><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">:</span> <span class="nb">dict</span><span class="p">,</span> <span class="n">position_list</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">feature</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
        <span class="n">feature</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">.</span><span class="n">last_hidden_state</span>
        <span class="n">pred</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([],</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">batch_size</span><span class="p">):</span>
            <span class="s">""" Apply Pooling &amp; Fully Connected Layer for each unique cell in batch (one notebook_id) """</span>
            <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">position_list</span><span class="p">[</span><span class="n">i</span><span class="p">])):</span>
                <span class="n">src</span><span class="p">,</span> <span class="n">end</span> <span class="o">=</span> <span class="n">position_list</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">idx</span><span class="p">]</span>
                <span class="n">embedding</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">pooling</span><span class="p">(</span><span class="n">feature</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">src</span><span class="p">:</span><span class="n">end</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="p">:].</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>  <span class="c1"># maybe don't need mask
</span>                <span class="n">logit</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">fc</span><span class="p">(</span><span class="n">embedding</span><span class="p">)</span>
                <span class="n">pred</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cat</span><span class="p">([</span><span class="n">pred</span><span class="p">,</span> <span class="n">logit</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">pred</span>
</code></pre></div></div>

<p>다음 코드는 필자가 공부를 위해 만든 모델 클래스의 <code class="language-plaintext highlighter-rouge">forward</code> 메서드이다. 전자는 이번 포스팅의 주제인 에러를 일으킨 주인공이고, 후자는 에러를 수정한 이후 정상적으로 작동하는 코드다. 독자 여러분들도 두 코드에 어떤 차이가 있는지 스스로 질문을 던지면서 읽어주시길 바란다.</p>

<p>위의 코드들은 <code class="language-plaintext highlighter-rouge">DeBERTa-V3-Large</code> 의 마지막 인코더 레이어가 반환하는 <code class="language-plaintext highlighter-rouge">last_hidden_state</code> 를 미리 설정한 서브 구간별로 나누고 개별적으로 <code class="language-plaintext highlighter-rouge">pooling &amp; fully connected layer</code> 에 통과시켜 로짓값으로 변환하기 위해 만들었다. 쉽게 말해 입력으로 토큰(단어) 384개 짜리 문장을 하나 넣었고, 모델은 384개의 개별 토큰에 대한 임베딩 값을 반환했는데 그것을 전부 이용하는 것이 아니라 예를 들어 <code class="language-plaintext highlighter-rouge">2번~4번</code> 토큰을 1번 구간, <code class="language-plaintext highlighter-rouge">6번~20번</code> 토큰을 2번 구간, <code class="language-plaintext highlighter-rouge">30번~50번</code> 토큰을 3번 구간 … <code class="language-plaintext highlighter-rouge">370번~380번</code> 토큰을 30번 구간으로 설정하고 구간 별로 따로 <code class="language-plaintext highlighter-rouge">pooling &amp; fully connected layer</code> 에 통과시켜 로짓을 구하는 것이다. 일반적이라면 1개의 문장에서 1개의 최종 로짓값이 도출되는 것이라면, 위 코드는 30개의 로짓값이 도출된다.</p>

<h3 id="️-내가-해결한-방법"><code class="language-plaintext highlighter-rouge">🖍️ 내가 해결한 방법</code></h3>

<p>코드 이해를 위한 설명은 마쳤으니 본격적으로 본 에러와 어떤 연관이 있는지 살펴보자. <code class="language-plaintext highlighter-rouge">Before</code> 코드는 <code class="language-plaintext highlighter-rouge">pred</code> 라는 리스트에 개별 구간에 대한 로짓값을 <code class="language-plaintext highlighter-rouge">append</code> 하고 마지막에 <code class="language-plaintext highlighter-rouge">torch.as_tensor</code>를 활용해 텐서로 변환하고 있다. 한편 후자는 <code class="language-plaintext highlighter-rouge">pred</code> 를 깡통 텐서로 선언한 뒤, <code class="language-plaintext highlighter-rouge">torch.cat</code>으로 모든 구간에 대한 로짓값을 하나의 텐서 구조체에 담고 있다.</p>

<p>얼핏보면 크게 다른점이 없어 보인다. 하지만 전자는 텐서 구조체를 새로 정의 하면서 <code class="language-plaintext highlighter-rouge">torch.Tensor[[logit1], [logit2], ….]</code> 형태를 갖고 후자는 <code class="language-plaintext highlighter-rouge">torch.Tensor[logit1, logit2, …]</code> 형태를 갖는다. 서로 다른 텐서 구조체를 그대로 모델 객체의 <code class="language-plaintext highlighter-rouge">forward</code> 메서드 및 <code class="language-plaintext highlighter-rouge">loss function</code>에 통과시키고 오차 역전을 하면 어떤 일이 생기는지 지금부터 알아보자.</p>

<p>전자의 경우는 도출된 손실함수의 미분값이 정의된 계산 그래프를 타고 역전될 수 없다. 이유는 전자의 <code class="language-plaintext highlighter-rouge">pred</code> 가 forward 메서드 내부에서 새로이 정의 되었기 때문이다. 후자 역시 마찬가지 아닌가 싶을 것이다. 후자의 <code class="language-plaintext highlighter-rouge">pred</code> 역시 <code class="language-plaintext highlighter-rouge">forward</code> 메서드 내부에서 정의된 것은 맞지만 <code class="language-plaintext highlighter-rouge">torch.cat</code>을 사용하면서 구간의 로짓값들 위에 새로이 차원을 덮어쓰는것이 아니게 된다. 이것이 매우 중요한 차이가 되는데, 후자와 같은 형태가 되는 경우, 손실값으로 부터 <code class="language-plaintext highlighter-rouge">Backward</code> 되는 미분값들이 곧바로 <code class="language-plaintext highlighter-rouge">forward</code> 과정에서 기록된 자신의 계산 그래프로 찾아 갈 수 있다. 한편 전자의 경우 새롭게 덮어 쓰여진 차원 때문에 미분값들이 자신의 계산 그래프로 찾아갈 수 없게 된다. 따라서 옵티마이저가 더 이상 <code class="language-plaintext highlighter-rouge">Backward</code> 를 수행할 수 없어 제목과 같은 에러를 반환하게 되는 것이다.</p>

<p>처음 이 에러를 마주했을 때는  <code class="language-plaintext highlighter-rouge">found_inf_per_device</code>, <code class="language-plaintext highlighter-rouge">No inf checks</code> 라는 키워드에 꽂혀 (특히 <code class="language-plaintext highlighter-rouge">inf</code>)  <code class="language-plaintext highlighter-rouge">&lt;RuntimeError: Function 'LogSoftmaxBackward0' returned nan values in its 0th output&gt;</code> 이것과 유사한 종류의 에러라 생각하고 열심히 연산 과정에 문제가 없는지, 어디서 NaN이 발생하는지, 학습률을 너무 크게 설정했는지 등을 검토하며 하루를 날렸었던 기억이 있다.</p>]]></content><author><name>qcqced</name><email>qcqced123@gmail.com</email></author><category term="Pytorch Error Handling" /><category term="Pytorch" /><category term="CUDA" /><category term="Error Handling" /><summary type="html"><![CDATA[Pytorch Error: Optimizer can't backward loss]]></summary></entry><entry><title type="html">📐 Inner Product: Projection Matrix</title><link href="http://localhost:4000/math/linear-algebra/inner-product" rel="alternate" type="text/html" title="📐 Inner Product: Projection Matrix" /><published>2023-07-10T00:00:00+09:00</published><updated>2023-07-11T13:00:00+09:00</updated><id>http://localhost:4000/math/linear-algebra/Inner_Product</id><content type="html" xml:base="http://localhost:4000/math/linear-algebra/inner-product"><![CDATA[<h3 id="concept-of-inner-product"><code class="language-plaintext highlighter-rouge">💡 Concept of Inner Product</code></h3>

\[a^Tb = ||a||•||b||cos\theta\]

<p>내적은 <code class="language-plaintext highlighter-rouge">Inner Product</code>, <code class="language-plaintext highlighter-rouge">Dot Product</code>, <code class="language-plaintext highlighter-rouge">Scalar Product</code>로 불리며 두 벡터의 유사도, 즉 닮은 정도를 구하는데 사용되는 벡터•행렬 연산의 한 종류다. 두 벡터의 정사영과도 동일한 개념으로 사용된다. 위 수식의 우변에 주목해보자. 
$||a||cos\theta$ 는 벡터 $a$를 벡터 $b$에 정사영 내린 크기로 해석할 수 있다. 한편 $||b||$ 는 벡터 $b$의 길이이므로, 결국 내적이란 한 벡터를 다른 벡터에 정사영 해준 결과와 벡터 크기의 곱이 된다.</p>

<p align="center">
<img src="/assets/images/inner_product.png" alt="Inner Product Image" class="align-center image-caption" width="50%&quot;, height=&quot;50%" />
<strong><em><a href="https://wikidocs.net/22384">Inner Product Image</a></em></strong>
</p>

<p>내적을 기하학적으로 생각해보자. $\theta$는 두 벡터 사이의 끼인각이다. 그렇다면 끼인각과 내적의 크기 사이의 상관관계는 어떻게 될까?? 내적의 의미는 서로 같은 정도가 아니라 <code class="language-plaintext highlighter-rouge">“서로 닮은 정도”</code>라고 했다. 중학교 때 배웠던 닮음 개념을 떠올려보자. 닮음이란 유클리드 공간에서 모든 각을 보존하며 모든 거리를 일정한 비율로 확대 또는 축소시키는 아핀 변환이다. 다시 말해, 어떤 두 도형을 닮았다고 말하는데 절대적인 거리 혹은 길이가 같을 필요가 없다. 그래서 두 벡터의 닮은 정도를 파악하려면 우리는 벡터의 길이 대신 방향이라는 물리량에 주목해야 한다. 벡터는 직선으로 표현되기 때문에 두 벡터가 완전히 닮았다고 말하려면, 끼인각의 크기가 0이 되어야 한다. 따라서 끼인각의 크기가 작아질수록 두 벡터의 닮은 정도는 커지게 되고, $\theta=0$ 에서 내적값은 최대가 된다. 만약 $\theta=90$ 이라면 내적값은 어떻게 될까?? 삼각비 정의에 의해 $cos \theta = 0$ 이 될 것이다. 따라서 내적값은 0이 되고, 두 벡터는 서로 전혀 닮지 않았다고 판단할 수 있다. 한편 $\theta=180$ 일 때 내적값은 최소가 되고, 두 벡터는 음의 방향으로 닮은 상태를 갖는다.</p>

\[N_a=\frac{a}{\sqrt{a^Ta}} = \frac{a}{||a||}\]

<p>한편 내적을 벡터 정규화에도 사용할 수 있는데, 방법은 위 수식과 같다. 일반적으로 벡터를 정규화하는 방법은 벡터를 벡터의 크기로 나누면 된다고만 알고 있을 것이다. 하지만 벡터의 전치와 벡터와의 내적으로도 벡터의 크기를 구할 수 있기 때문에 (벡터의 전치와 벡터는 $cos\theta=0$이 되기 때문) 위 등식이 성립한다. 한편, 벡터 정규화 결과는 벡터의 길이가 1이 되기 때문에 <code class="language-plaintext highlighter-rouge">‘단위 벡터’</code> 라고 정의한다. 벡터의 길이가 1이라는 점을 이용하면, 단위 벡터에는 방향에 대한 물리량만 남아 있다는 사실을 알 수 있다. 그래서 우리가 어떤 벡터의 방향 정보를 얻고 싶을 때, 벡터의 정규화를 사용하면 간단하게 구할 수 있다.</p>

\[\frac{b^Ta}{||b||} * \frac{b}{||b||} = \frac{b^Ta}{\sqrt{b^Tb}} * \frac{b}{\sqrt{b^Tb}} = \frac{b^Ta}{b^Tb}*b\]

<p>내적 공식과 벡터 정규화 공식을 함께 사용하면 벡터 $a$를 벡터 $b$에 정사영 내렸을 때 도출되는 벡터 또한 직접 구할 수 있다. 위 수식을 살펴보자. 각 변의 좌측 항은 내적 공식에 의해 
$||a||\cos\theta$ 가 된다. 지금 우리가 구하고 싶은 것은 정사영 내린 벡터 자체인데, $||a||\cos\theta$ 은 크기에 대한 물리량만 담고 있을뿐 방향에 대한 정보가 담겨 있지 않다. 앞에서 언급했듯이, 벡터 정규화 결과는 해당 벡터의 방향에 대한 물리량을 의미한다. 따라서 우변에 벡터 $b$ 의 방향(정규화 결과)를 곱하게 되는 것이다.</p>

\[(a - \hat{x}•b)^T•\ b\hat{x} = 0 \\
\hat{x} = \frac{b^Ta}{b^Tb} \\
\hat{x}•b = \frac{b^Ta}{b^Tb}*b\]

<p>한편, 정사영 내린 벡터(벡터 
$\frac{a^Tb}{b^Tb}*b$)를 기준 벡터(벡터 $b$)의 스칼라배 해준 벡터로 생각해도 쉽게 구할 수 있다. 스칼라를 미지수로 두고 정사영 내린 벡터를 $b•\hat{x}$라고 정의하면 우리는 벡터 $a$를 빗변, 정사영 내린 벡터를 밑변으로 하는 직각삼각형의 높이를 구할 수 있게 된다. 정사영 내린 벡터 $b•\hat{x}$에 마이너스 부호를 취해 반대 방향으로 뒤집어주면 우리는 직각삼각형의 높이를 밑변과 빗변의 합($a - b•\hat{x}$)으로 나타낼 수 있기 때문이다. 따라서 밑변과 높이의 끼인각이 수직이라는 점을 내적 공식에 적용해 우리는 다음 수식을 풀어내면 정사영 내린 벡터를 얻을 수 있다. 정사영 내린 벡터를 실제로는 <code class="language-plaintext highlighter-rouge">투영 벡터</code>라고 정의한다.</p>

<h3 id="-what-is-projection-matrix"><code class="language-plaintext highlighter-rouge">🔢 What is Projection Matrix</code></h3>

<p>벡터 $a$와 벡터 $b$의 변화에 따른 투영 벡터 크기의 추이를 살펴보자. 벡터 $a$가 만약 2배 커진다면 $a$가 분자에만 있기 때문에 투영 벡터 역시 그대로 2배 증가할 것이다. 반면 벡터 $b$는 2배가 커져도 분자•분모 모두 동일하게 2개씩  $b$가 있기 때문에 이전과 변화가 없다. 따라서 투영 벡터의 크기는 전적으로 벡터 $a$에 의존적이다. 다시 말해, 벡터 $a$는 무언가 매개체에 의해 벡터 $b$로 정사영 되고 있으며 그 매개체를 우리는 <code class="language-plaintext highlighter-rouge">projection matrix</code> 라고 한다.</p>

\[P = \frac{bb^T}{b^Tb}\]

<p>분모는 행벡터와 열벡터의 내적이라서 스칼라(상수), 분자는 열벡터와 행벡터의 곱이라서 행렬 형태가 될 것이다. 따라서 끝에 <code class="language-plaintext highlighter-rouge">Matrix</code> 라는 단어를 붙이게 되었다. 이러한 <code class="language-plaintext highlighter-rouge">Projection Matrix</code>는 $n$차원 벡터에도 동일하게 적용할 수 있기 때문에 훗날 차원축소 혹은 선형변환 같은 테크닉으로 머신러닝, 딥러닝에서 유용하게 사용된다.</p>

<p>마지막으로 <code class="language-plaintext highlighter-rouge">Dot Product</code>, <code class="language-plaintext highlighter-rouge">Scalar Product</code> , <code class="language-plaintext highlighter-rouge">Inner Product</code>는 넓은 의미에서 모두 내적에 포함된다. 하지만 미세한 의미 차이는 있다. 하나 하나 살펴보자. 먼저 <code class="language-plaintext highlighter-rouge">Dot Product</code>란, 내적을 유클리드 좌표계에서 정의할 때 사용되는 명칭이며, 연산 기호가 점곱이라는 점을 강조하기 위해 <code class="language-plaintext highlighter-rouge">Dot</code> 이라는 단어를 사용하게 되었다. 한편, <code class="language-plaintext highlighter-rouge">Scalar Product</code> 역시 내적을 유클리드 좌표계에서 정의할 때 사용하는 명칭이지만, 그 결과가 스칼라 값이라는 점을 강조하기 위해 <code class="language-plaintext highlighter-rouge">Scalar Product</code>라고 명명했다. 마지막으로 <code class="language-plaintext highlighter-rouge">Inner Product</code>는 벡터 공간에서 정의 되어 행렬 같은 다른 개체들에 대해서 확장 적용이 가능하다는 점에서 <code class="language-plaintext highlighter-rouge">Dot Product</code>  , <code class="language-plaintext highlighter-rouge">Scalar Product</code> 보다 더 일반적인 개념으로 볼 수 있겠다.</p>]]></content><author><name>qcqced</name><email>qcqced123@gmail.com</email></author><category term="Linear Algebra" /><category term="Linear Algebra" /><category term="Inner Product" /><category term="Projection Matrix" /><category term="내적" /><category term="정사영" /><summary type="html"><![CDATA[💡 Concept & Insight of Inner Product]]></summary></entry><entry><title type="html">📏 Lp-Norm: Concept &amp;amp; Insight</title><link href="http://localhost:4000/math/linear-algebra/lp-norm" rel="alternate" type="text/html" title="📏 Lp-Norm: Concept &amp;amp; Insight" /><published>2023-07-04T00:00:00+09:00</published><updated>2023-07-05T13:00:00+09:00</updated><id>http://localhost:4000/math/linear-algebra/Lp_Norm</id><content type="html" xml:base="http://localhost:4000/math/linear-algebra/lp-norm"><![CDATA[\[||x||_p = (∑_{i=1}^n |x_i|^p)^{1/p}\]

<p><strong><code class="language-plaintext highlighter-rouge">Lp-Norm</code></strong>은 <code class="language-plaintext highlighter-rouge">Lebesgue</code>라는 프랑스 수학자에 의해 고안된 개념으로<strong>,</strong> 기계학습을 공부하는 사람이라면 지겹도록 듣는 <code class="language-plaintext highlighter-rouge">L2-Norm</code>, <code class="language-plaintext highlighter-rouge">L1-Norm</code>을 일반화 버전이라고 생각하면 된다. <strong><u>다시 말해, 벡터의 크기를 나타내는 표현식을 일반화한</u></strong> 것이 바로 <code class="language-plaintext highlighter-rouge">Lp-Norm</code> 이며 수식은 위와 같다.</p>

<p><code class="language-plaintext highlighter-rouge">p=1</code>이라고 가정하고 수식을 전개해보자. 
$||x||_1 = (|x_1|^1 + |x_2|^1+ … + |x_n|^1)^{1/1}$이 된다. 우리가 아는 <code class="language-plaintext highlighter-rouge">L1-Norm</code> 의 수식과 동일하다.</p>

<p>그렇다면 <code class="language-plaintext highlighter-rouge">p=2</code>일 때 수식을 살펴보자. 
$||x||_2 = (|x_1|^2 + |x_2|^2+ … + |x_n|^2)^{1/2}$으로 전개 된다는 것을 알 수 있다. 역시 우리가 맨날 보는 <code class="language-plaintext highlighter-rouge">L2-Norm</code> 과 동일하다.</p>

<p><code class="language-plaintext highlighter-rouge">L1-Norm</code>은 맨허튼 거리, <code class="language-plaintext highlighter-rouge">L2-Norm</code> 은 유클리드 거리를 의미한다는 것은 익히 들어 봤을 것이다. 만약 $p=∞$라면, 수식은 어떻게 될까, 과연 어떤 의미를 갖고 있을까??</p>

<p>이전과 똑같이 전개해보면
\(||x||_∞ = (|x_1|^∞ + |x_2|^∞+ ... + |x_n|^∞)^{1/∞}\), 이렇게 식이 도출될 것이다. 이제 괄호 내부 원소들의 지수가 <code class="language-plaintext highlighter-rouge">무한대</code>라는 점에 주목해보자. 직관적으로 무한대 값들 사이의 덧셈, 곱셈의 결과는 <code class="language-plaintext highlighter-rouge">무한대</code> 라는 것을 알 수 있다. 그렇다면 우리는 위 수식에서 절대값이 가장 큰 $|x_i|$만 남겨도 역시 무한대 값을 얻을 수 있다. <strong><u>무한대는 미지수 개념에 가깝지 실제 실수 개념은 아니기 때문이다.</u></strong> 따라서 괄호 내부에는 $|x_i|^p$ 값만 남게 되고 괄호 밖의 $1/p$와 남은 연산을 해주면 결국 $|x_i|$만 남게 된다. 따라서 $||x||_∞ = max(|x_1|, \ |x_2|, \ … \ , |x_n|)$가 된다.</p>

<p>이와 같은 성질 때문에 <code class="language-plaintext highlighter-rouge">Lp-Norm</code> 은 <code class="language-plaintext highlighter-rouge">Lp-Pooling</code> 으로도 해석할 수 있으며, 수식의 우변에  $1/n$을 곱해주면 <code class="language-plaintext highlighter-rouge">Generalized Mean Pooling</code> 이 된다는 사실을 알 수 있다. 결국 <code class="language-plaintext highlighter-rouge">Norm</code>과 <code class="language-plaintext highlighter-rouge">Pooling</code> 은 같은 개념이었던 것이다. 
<strong>그래서 위에서 살펴본 $ L_∞ $ 역시 <code class="language-plaintext highlighter-rouge">Max Pooling</code> 이라 해석이 가능해진다.</strong></p>

<p>여담으로 맨앞의 대문자 L은 <code class="language-plaintext highlighter-rouge">Lebesgue</code> 의 이름에서 본따왔다고 알려져 있다. 그리고 예전부터 $L_2$값을 수식으로 표현할 때 왜 짝대기 두개를 사용할까 항상 궁금했는데  $L_p$와 <code class="language-plaintext highlighter-rouge">일반 절대값</code>을 구분하기 위해 짝대기를 두 개 사용하게 되었다고 한다.</p>
<p align="center">
<img src="/assets/images/220px-Vector-p-Norms_qtl1.svg.png" alt="Lp-Norm Image" class="align-center image-caption" width="50%&quot;, height=&quot;50%" />
<em>Lp-Norm</em>
</p>

<p>위 자료는 
$L_p$
norm을 p값 변화 추이에 따라 기하학적으로 표현한 그림이다. <code class="language-plaintext highlighter-rouge">p=1</code> 일 때는 $L_1: |x| + |y| =1$가 되기 때문에 마름모 형태의 영역을 갖는다. 한편 <code class="language-plaintext highlighter-rouge">p=2</code> 일 때는 $L_2: x^2 + y^2 =1^2$가 되기 때문에 원의 영역을 갖는다. $p=∞$ 일 때는 $L_∞: max(|x_1|…|x_n|) = 1$ 이 되기 때문에 정사각형 형태의 영역을 갖게 될 것이다.</p>]]></content><author><name>qcqced</name><email>qcqced123@gmail.com</email></author><category term="Linear Algebra" /><category term="Linear Algebra" /><category term="Norm" /><category term="Pooling" /><summary type="html"><![CDATA[Concept of Lp-Norm & GeM Pool]]></summary></entry></feed>