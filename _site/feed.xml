<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2023-08-04T17:38:50+09:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">AI/Business Study Log</title><subtitle>NLP, Marketing</subtitle><author><name>qcqced</name><email>qcqced123@gmail.com</email></author><entry><title type="html">ğŸ”¥Â Pytorch Tensor Indexing ìì£¼ ì‚¬ìš©í•˜ëŠ” ë©”ì„œë“œ ëª¨ìŒì§‘</title><link href="http://localhost:4000/framework-library/torch-indexing-function" rel="alternate" type="text/html" title="ğŸ”¥Â Pytorch Tensor Indexing ìì£¼ ì‚¬ìš©í•˜ëŠ” ë©”ì„œë“œ ëª¨ìŒì§‘" /><published>2023-08-04T00:00:00+09:00</published><updated>2023-08-05T02:00:00+09:00</updated><id>http://localhost:4000/framework-library/Pytorch-Tensor-Indexing-Function</id><content type="html" xml:base="http://localhost:4000/framework-library/torch-indexing-function"><![CDATA[<p>íŒŒì´í† ì¹˜ì—ì„œ í•„ìê°€ ìì£¼ ì‚¬ìš©í•˜ëŠ” í…ì„œ ì¸ë±ì‹± ê´€ë ¨ ë©”ì„œë“œì˜ ì‚¬ìš©ë²• ë° ì‚¬ìš© ì˜ˆì‹œë¥¼ í•œë°©ì— ì •ë¦¬í•œ í¬ìŠ¤íŠ¸ë‹¤. ë©”ì„œë“œ í•˜ë‚˜ë‹¹ í•˜ë‚˜ì˜ í¬ìŠ¤íŠ¸ë¡œ ë§Œë“¤ê¸°ì—ëŠ” ë„ˆë¬´ ê¸¸ì´ê°€ ì§§ë‹¤ ìƒê°í•´ í•œ í˜ì´ì§€ì— ëª¨ë‘ ë„£ê²Œ ë˜ì—ˆë‹¤. ì§€ì†ì ìœ¼ë¡œ ì—…ë°ì´íŠ¸ ë  ì˜ˆì •ì´ë‹¤. ë˜í•œ í…ì„œ ì¸ë±ì‹± ë§ê³ ë„ ë‹¤ë¥¸ ì£¼ì œë¡œë„ ê´€ë ¨ ë©”ì„œë“œë¥¼ ì •ë¦¬í•´ ì˜¬ë¦´ ì˜ˆì •ì´ë‹ˆ ë§ì€ ê´€ì‹¬ ë¶€íƒë“œë¦°ë‹¤.</p>

<h3 id="torchargmax"><code class="language-plaintext highlighter-rouge">ğŸ”Â torch.argmax</code></h3>

<p>ì…ë ¥ í…ì„œì—ì„œ ê°€ì¥ í° ê°’ì„ ê°–ê³  ìˆëŠ” ì›ì†Œì˜ ì¸ë±ìŠ¤ë¥¼ ë°˜í™˜í•œë‹¤. ìµœëŒ€ê°’ì„ ì°¾ì„ ì°¨ì›ì„ ì§€ì •í•´ì¤„ ìˆ˜ ìˆë‹¤. ì•„ë˜ ì˜ˆì‹œ ì½”ë“œë¥¼ í™•ì¸í•´ë³´ì.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># torch.argmax params
</span><span class="n">torch</span><span class="p">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

<span class="c1"># torch.argmax example 1
</span><span class="n">test</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">29</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">45</span><span class="p">,</span><span class="mi">22</span><span class="p">,</span><span class="mi">3</span><span class="p">])</span>
<span class="n">torch</span><span class="p">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">test</span><span class="p">)</span>
<span class="n">torch</span><span class="p">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">test2</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="o">&lt;</span><span class="n">Result</span><span class="o">&gt;</span>
<span class="n">tensor</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>

<span class="c1"># torch.argmax example 2
</span><span class="n">test</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
                     <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">]])</span>

<span class="n">torch</span><span class="p">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">test</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="o">&lt;</span><span class="n">Result</span><span class="o">&gt;</span>
<span class="n">tensor</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>

<span class="c1"># torch.argmax example 3
</span><span class="n">test</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
                     <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">]])</span>

<span class="n">torch</span><span class="p">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">test</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">tensor</span><span class="p">([[</span><span class="mi">0</span><span class="p">],</span>
        <span class="p">[</span><span class="mi">2</span><span class="p">]])</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">dim</code> ë§¤ê°œë³€ìˆ˜ì— ì›í•˜ëŠ” ì°¨ì›ì„ ì…ë ¥í•˜ë©´ í•´ë‹¹ ì°¨ì› ë·°ì—ì„œ ê°€ì¥ í° ì›ì†Œë¥¼ ì°¾ì•„ ì¸ë±ìŠ¤ ê°’ì„ ë°˜í™˜í•´ì¤„ ê²ƒì´ë‹¤. ì´ ë•Œ <code class="language-plaintext highlighter-rouge">keepdim=True</code> ë¡œ ì„¤ì •í•œë‹¤ë©´ ì…ë ¥ ì°¨ì›ì—ì„œ ê°€ì¥ í° ì›ì†Œì˜ ì¸ë±ìŠ¤ë¥¼ ë°˜í™˜í•˜ë˜ ì›ë³¸ í…ì„œì˜ ì°¨ì›ê³¼ ë™ì¼í•œ í˜•íƒœë¡œ ì¶œë ¥í•´ì¤€ë‹¤. <code class="language-plaintext highlighter-rouge">example 2</code> ì˜ ê²½ìš° <code class="language-plaintext highlighter-rouge">dim=0</code> ë¼ì„œ í–‰ì´ ëˆ„ì ëœ ë°©í–¥ìœ¼ë¡œ í…ì„œë¥¼ ë°”ë¼ë´ì•¼ í•œë‹¤. í–‰ì´ ëˆ„ì ëœ ë°©í–¥ìœ¼ë¡œ í…ì„œë¥¼ ë³´ê²Œ ë˜ë©´ <code class="language-plaintext highlighter-rouge">tensor([[0, 1, 1]])</code>ì´ ëœë‹¤.</p>

<h3 id="torchstack"><code class="language-plaintext highlighter-rouge">ğŸ“šÂ torch.stack</code></h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="s">"""
torch.stack
Args:
	tensors(sequence of Tensors): í…ì„œê°€ ë‹´ê¸´ íŒŒì´ì¬ ì‹œí€€ìŠ¤ ê°ì²´
	dim(int): ì¶”ê°€í•  ì°¨ì› ë°©í–¥ì„ ì„¸íŒ…, ê¸°ë³¸ê°’ì€ 0
"""</span>
<span class="n">torch</span><span class="p">.</span><span class="n">stack</span><span class="p">(</span><span class="n">tensors</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</code></pre></div></div>

<p>ë§¤ê°œë³€ìˆ˜ë¡œ ì£¼ì–´ì§„ íŒŒì´ì¬ ì‹œí€€ìŠ¤ ê°ì²´(ë¦¬ìŠ¤íŠ¸, íŠœí”Œ)ë¥¼ ì‚¬ìš©ìê°€ ì§€ì •í•œ ìƒˆë¡œìš´ ì°¨ì›ì— ìŒ“ëŠ” ê¸°ëŠ¥ì„ í•œë‹¤. ë§¤ê°œë³€ìˆ˜ <code class="language-plaintext highlighter-rouge">tensors</code> ëŠ” í…ì„œê°€ ë‹´ê¸´ íŒŒì´ì¬ì˜ ì‹œí€€ìŠ¤ ê°ì²´ë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ëŠ”ë‹¤. <code class="language-plaintext highlighter-rouge">dim</code> ì€ ì‚¬ìš©ìê°€ í…ì„œ ì ì¬ë¥¼ í•˜ê³  ì‹¶ì€ ìƒˆë¡œìš´ ì°¨ì›ì„ ì§€ì •í•´ì£¼ë©´ ëœë‹¤. ê¸°ë³¸ê°’ì€ 0ì°¨ì›ìœ¼ë¡œ ì§€ì • ë˜ì–´ìˆìœ¼ë©°, í…ì„œì˜ ë§¨ ì•ì°¨ì›ì´ ìƒˆë¡­ê²Œ ìƒê¸°ê²Œ ëœë‹¤. <code class="language-plaintext highlighter-rouge">torch.stack</code> ì€ ê¸°ê³„í•™ìŠµ, íŠ¹íˆ ë”¥ëŸ¬ë‹ì—ì„œ ì •ë§ ìì£¼ ì‚¬ìš©ë˜ê¸° ë•Œë¬¸ì— ì‚¬ìš©ë²• ë° ì‚¬ìš©ìƒí™©ì„ ìµí˜€ë‘ë©´ ë„ì›€ì´ ëœë‹¤. ì˜ˆì‹œë¥¼ í†µí•´ í•´ë‹¹ ë©”ì„œë“œë¥¼ ì–´ë–¤ ìƒí™©ì—ì„œ ì–´ë–»ê²Œ ì‚¬ìš©í•˜ëŠ”ì§€ ì•Œì•„ë³´ì.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="s">""" torch.stack example """</span>

<span class="k">class</span> <span class="nc">Projector</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="s">"""
    Making projection matrix(Q, K, V) for each attention head
    When you call this class, it returns projection matrix of each attention head
    For example, if you call this class with 8 heads, it returns 8 set of projection matrices (Q, K, V)
    Args:
        num_heads: number of heads in MHA, default 8
        dim_head: dimension of each attention head, default 64
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span> <span class="n">dim_head</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">64</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Projector</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span> <span class="o">=</span> <span class="n">num_heads</span> <span class="o">*</span> <span class="n">dim_head</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dim_head</span> <span class="o">=</span> <span class="n">dim_head</span>

    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">fc_q</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">dim_head</span><span class="p">)</span>
        <span class="n">fc_k</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">dim_head</span><span class="p">)</span>
        <span class="n">fc_v</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">dim_head</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">fc_q</span><span class="p">,</span> <span class="n">fc_k</span><span class="p">,</span> <span class="n">fc_v</span>

<span class="n">num_heads</span> <span class="o">=</span> <span class="mi">8</span>
<span class="n">dim_head</span> <span class="o">=</span> <span class="mi">64</span>
<span class="n">projector</span> <span class="o">=</span> <span class="n">Projector</span><span class="p">(</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">dim_head</span><span class="p">)</span>  <span class="c1"># init instance
</span><span class="n">projector_list</span> <span class="o">=</span> <span class="p">[</span><span class="nb">list</span><span class="p">(</span><span class="n">projector</span><span class="p">())</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_heads</span><span class="p">)]</span>  <span class="c1"># call instance
</span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">512</span><span class="p">)</span> <span class="c1"># x.shape: [Batch_Size, Sequence_Length, Dim_model]
</span><span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">V</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[],</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_heads</span><span class="p">):</span>
    <span class="n">Q</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">projector_list</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">0</span><span class="p">](</span><span class="n">x</span><span class="p">))</span> <span class="c1"># [10, 512, 64]
</span>    <span class="n">K</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">projector_list</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">1</span><span class="p">](</span><span class="n">x</span><span class="p">))</span> <span class="c1"># [10, 512, 64]
</span>	  <span class="n">V</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">projector_list</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">2</span><span class="p">](</span><span class="n">x</span><span class="p">))</span> <span class="c1"># [10, 512, 64]
</span> 
<span class="n">Q</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">stack</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># Q.shape: [10, 8, 512, 64]
</span><span class="n">K</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">stack</span><span class="p">(</span><span class="n">K</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># K.shape: [10, 8, 512, 64]
</span><span class="n">V</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">stack</span><span class="p">(</span><span class="n">V</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># V.shape: [10, 8, 512, 64]
</span></code></pre></div></div>

<p>ìœ„ ì½”ë“œëŠ” <code class="language-plaintext highlighter-rouge">Transformer</code> ì˜ <code class="language-plaintext highlighter-rouge">Multi-Head Attention</code> êµ¬í˜„ì²´ ì¼ë¶€ë¥¼ ë°œì·Œí•´ì˜¨ ê²ƒì´ë‹¤. <code class="language-plaintext highlighter-rouge">Multi-Head Attention</code> ì€ ê°œë³„ ì–´í…ì…˜ í•´ë“œë³„ë¡œ í–‰ë ¬ $Q, K, V$ë¥¼ ê°€ì ¸ì•¼ í•œë‹¤. ë”°ë¼ì„œ ì…ë ¥ ì„ë² ë”©ì„ ê°œë³„ ì–´í…ì…˜ í—¤ë“œì— <code class="language-plaintext highlighter-rouge">Linear Combination</code> í•´ì¤˜ì•¼ í•˜ëŠ”ë° í—¤ë“œ ê°œìˆ˜ê°€ 8ê°œë‚˜ ë˜ê¸° ë•Œë¬¸ì— ê°œë³„ì ìœ¼ë¡œ <code class="language-plaintext highlighter-rouge">Projection Matrix</code> ë¥¼ ì„ ì–¸í•´ì£¼ëŠ” ê²ƒì€ ë§¤ìš° ë¹„íš¨ìœ¨ì ì´ë‹¤. ë”°ë¼ì„œ ê°ì²´  <code class="language-plaintext highlighter-rouge">Projector</code> ì— í–‰ë ¬ $Q, K, V$ì— ëŒ€í•œ <code class="language-plaintext highlighter-rouge">Projection Matrix</code> ë¥¼ ì •ì˜í•´ì¤¬ë‹¤. ì´í›„ í—¤ë“œ ê°œìˆ˜ë§Œí¼ ê°ì²´  <code class="language-plaintext highlighter-rouge">Projector</code> ë¥¼ í˜¸ì¶œí•´ ë¦¬ìŠ¤íŠ¸ì— í•´ë“œë³„ <code class="language-plaintext highlighter-rouge">Projection Matrix</code> ë¥¼ ë‹´ì•„ì¤€ë‹¤. ê·¸ ë‹¤ìŒ <code class="language-plaintext highlighter-rouge">torch.stack</code>ì„ ì‚¬ìš©í•´ <code class="language-plaintext highlighter-rouge">Attention Head</code> ë°©í–¥ì˜ ì°¨ì›ìœ¼ë¡œ ë¦¬ìŠ¤íŠ¸ ë‚´ë¶€ í…ì„œë“¤ì„ ìŒ“ì•„ì£¼ë©´ ëœë‹¤.</p>

<h3 id="torcharange"><code class="language-plaintext highlighter-rouge">ğŸ”¢Â torch.arange</code></h3>

<p>ì‚¬ìš©ìê°€ ì§€ì •í•œ ì‹œì‘ì ë¶€í„° ëì ê¹Œì§€ ì¼ì •í•œ ê°„ê²©ìœ¼ë¡œ í…ì„œë¥¼ ë‚˜ì—´í•œë‹¤. Pythonì˜ ë‚´ì¥ ë©”ì„œë“œ <code class="language-plaintext highlighter-rouge">range</code>ì™€ ë™ì¼í•œ ì—­í• ì„ í•˜ëŠ”ë°, ëŒ€ì‹  í…ì„œ ê·¸ ê²°ê³¼ë¥¼ í…ì„œ êµ¬ì¡°ì²´ë¡œ ë°˜í™˜í•œë‹¤ê³  ìƒê°í•˜ë©´ ë˜ê² ë‹¤.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># torch.arange usage
</span><span class="n">torch</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="n">start</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">end</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
<span class="n">tensor</span><span class="p">([</span> <span class="mi">0</span><span class="p">,</span>  <span class="mi">1</span><span class="p">,</span>  <span class="mi">2</span><span class="p">,</span>  <span class="mi">3</span><span class="p">,</span>  <span class="mi">4</span><span class="p">])</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">tensor</span><span class="p">([</span> <span class="mi">1</span><span class="p">,</span>  <span class="mi">2</span><span class="p">,</span>  <span class="mi">3</span><span class="p">])</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">)</span>
<span class="n">tensor</span><span class="p">([</span> <span class="mf">1.0000</span><span class="p">,</span>  <span class="mf">1.5000</span><span class="p">,</span>  <span class="mf">2.0000</span><span class="p">])</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">step</code> ë§¤ê°œë³€ìˆ˜ë¡œ ì›ì†Œê°„ ê°„ê²© ì¡°ì •ì„ í•  ìˆ˜ ìˆëŠ”ë°, ê¸°ë³¸ì€ 1ë¡œ ì§€ì • ë˜ì–´ ìˆìœ¼ë‹ˆ ì°¸ê³ í•˜ì. í•„ìì˜ ê²½ìš°ì—ëŠ” <code class="language-plaintext highlighter-rouge">nn.Embedding</code>ì˜ ì…ë ¥ í…ì„œë¥¼ ë§Œë“¤ ë•Œ ê°€ì¥ ë§ì´ ì‚¬ìš©í–ˆë‹¤. <code class="language-plaintext highlighter-rouge">nn.Embedding</code> ì˜ ê²½ìš° Inputìœ¼ë¡œ <code class="language-plaintext highlighter-rouge">IntTensor</code>, <code class="language-plaintext highlighter-rouge">LongTensor</code>ë¥¼ ë°›ê²Œ ë˜ì–´ ìˆìœ¼ë‹ˆ ì•Œì•„ë‘ì.</p>

<h3 id="torchrepeat"><code class="language-plaintext highlighter-rouge">ğŸ”Â torch.repeat</code></h3>

<p>ì…ë ¥ê°’ìœ¼ë¡œ ì£¼ì–´ì§„ í…ì„œë¥¼ ì‚¬ìš©ìê°€ ì§€ì •í•œ ë°˜ë³µ íšŸìˆ˜ë§Œí¼ íŠ¹ì • ì°¨ì› ë°©í–¥ìœ¼ë¡œ ëŠ˜ë¦°ë‹¤. ì˜ˆë¥¼ ë“¤ë©´ <code class="language-plaintext highlighter-rouge">[1,2,3] * 3</code>ì˜ ê²°ê³¼ëŠ” <code class="language-plaintext highlighter-rouge">[1, 2, 3, 1, 2, 3, 1, 2, 3]</code> ì¸ë°, ì´ê²ƒì„ ì‚¬ìš©ìê°€ ì§€ì •í•œ ë°˜ë³µ íšŸìˆ˜ë§Œí¼ íŠ¹ì • ì°¨ì›ìœ¼ë¡œ ìˆ˜í–‰í•˜ê² ë‹¤ëŠ” ê²ƒì´ë‹¤. ì•„ë˜ ì‚¬ìš© ì˜ˆì œë¥¼ í™•ì¸í•´ë³´ì.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># torch.repeat example
</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">x</span><span class="p">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">tensor</span><span class="p">([[</span> <span class="mi">1</span><span class="p">,</span>  <span class="mi">2</span><span class="p">,</span>  <span class="mi">3</span><span class="p">,</span>  <span class="mi">1</span><span class="p">,</span>  <span class="mi">2</span><span class="p">,</span>  <span class="mi">3</span><span class="p">],</span>
        <span class="p">[</span> <span class="mi">1</span><span class="p">,</span>  <span class="mi">2</span><span class="p">,</span>  <span class="mi">3</span><span class="p">,</span>  <span class="mi">1</span><span class="p">,</span>  <span class="mi">2</span><span class="p">,</span>  <span class="mi">3</span><span class="p">],</span>
        <span class="p">[</span> <span class="mi">1</span><span class="p">,</span>  <span class="mi">2</span><span class="p">,</span>  <span class="mi">3</span><span class="p">,</span>  <span class="mi">1</span><span class="p">,</span>  <span class="mi">2</span><span class="p">,</span>  <span class="mi">3</span><span class="p">],</span>
        <span class="p">[</span> <span class="mi">1</span><span class="p">,</span>  <span class="mi">2</span><span class="p">,</span>  <span class="mi">3</span><span class="p">,</span>  <span class="mi">1</span><span class="p">,</span>  <span class="mi">2</span><span class="p">,</span>  <span class="mi">3</span><span class="p">]])</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">x</span><span class="p">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">tensor</span><span class="p">([[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
         <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]],</span>

        <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
         <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]],</span>

        <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
         <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]],</span>

        <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
         <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]]])</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">x</span><span class="p">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">).</span><span class="n">size</span>
<span class="n">torch</span><span class="p">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">x</span><span class="p">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">tensor</span><span class="p">([[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
         <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]],</span>

        <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
         <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]],</span>

        <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
         <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]],</span>

        <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
         <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]]])</span>
</code></pre></div></div>

<p>$t$ë¥¼ ì–´ë–¤ í…ì„œ êµ¬ì¡°ì²´ $x$ì˜ ìµœëŒ€ ì°¨ì›ì´ë¼ê³  í–ˆì„ , $x_t$ë¥¼ ê°€ì¥ ì™¼ìª½ì— ë„£ê³  ê°€ì¥ ë‚®ì€ ì°¨ì›ì¸ 0ì°¨ì›ì— ëŒ€í•œ ë°˜ë³µ íšŸìˆ˜ë¥¼ ì˜¤ë¥¸ìª½ ëì— ëŒ€ì…í•´ì„œ ì‚¬ìš©í•˜ë©´ ëœë‹¤. (<code class="language-plaintext highlighter-rouge">torch.repeat(</code>$x_t, x_{t-1}, â€¦ x_2, x_1, x_0$<code class="language-plaintext highlighter-rouge">))</code>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># torch.arange &amp; torch.repeate usage example
</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">pos_x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">num_patches</span> <span class="o">+</span> <span class="mi">1</span><span class="p">).</span><span class="n">repeat</span><span class="p">(</span><span class="n">inputs</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]).</span><span class="n">to</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">pos_x</span><span class="p">.</span><span class="n">shape</span>
<span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">16</span><span class="p">,</span> <span class="mi">1025</span><span class="p">])</span>
</code></pre></div></div>

<p>í•„ìì˜ ê²½ìš°, <code class="language-plaintext highlighter-rouge">position embedding</code>ì˜ ì…ë ¥ì„ ë§Œë“¤ê³  ì‹¶ì„ ë•Œ <code class="language-plaintext highlighter-rouge">torch.arange</code> ì™€ ì—°ê³„í•´ ìì£¼ ì‚¬ìš© í–ˆë˜ ê²ƒ ê°™ë‹¤. ìœ„ ì½”ë“œë¥¼ ì°¸ê³ í•˜ì.</p>

<h3 id="torchclamp"><code class="language-plaintext highlighter-rouge">ğŸ”¬Â torch.clamp</code></h3>

<p>ì…ë ¥ í…ì„œì˜ ì›ì†Œê°’ì„ ì‚¬ìš©ìê°€ ì§€ì •í•œ ìµœëŒ€â€¢ìµœì†Œê°’ ë²”ìœ„ ì´ë‚´ë¡œ ì œí•œí•˜ëŠ” ë©”ì„œë“œë‹¤.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># torch.clamp params
</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">clamp</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="nb">min</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span> <span class="err">â†’</span> <span class="n">Tensor</span>

<span class="c1"># torch.clamp usage example
</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">a</span>
<span class="n">tensor</span><span class="p">([</span><span class="o">-</span><span class="mf">1.7120</span><span class="p">,</span>  <span class="mf">0.1734</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0478</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0922</span><span class="p">])</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="nb">min</span><span class="o">=-</span><span class="mf">0.5</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">tensor</span><span class="p">([</span><span class="o">-</span><span class="mf">0.5000</span><span class="p">,</span>  <span class="mf">0.1734</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0478</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0922</span><span class="p">])</span>
</code></pre></div></div>

<p>ì…ë ¥ëœ í…ì„œì˜ ì›ì†Œë¥¼ ì§€ì • ìµœëŒ€â€¢ìµœì†Œ ì„¤ì •ê°’ê³¼ í•˜ë‚˜ í•˜ë‚˜ ëŒ€ì¡°í•´ì„œ í…ì„œ ë‚´ë¶€ì˜ ëª¨ë“  ì›ì†Œê°€ ì§€ì • ë²”ìœ„ ì•ˆì— ë“¤ë„ë¡ ë§Œë“¤ì–´ì¤€ë‹¤. <code class="language-plaintext highlighter-rouge">torch.clamp</code> ì—­ì‹œ ë‹¤ì–‘í•œ ìƒí™©ì—ì„œ ì‚¬ìš©ë˜ëŠ”ë°, í•„ìì˜ ê²½ìš° ëª¨ë¸ ë ˆì´ì–´ ì¤‘ê°„ì— ì œê³±ê·¼ì´ë‚˜ ì§€ìˆ˜, ë¶„ìˆ˜ í˜¹ì€ ê°ë„ ê´€ë ¨ ì—°ì‚°ì´ ë“¤ì–´ê°€ <code class="language-plaintext highlighter-rouge">Backward Pass</code>ì—ì„œ <code class="language-plaintext highlighter-rouge">NaN</code>ì´ ë°œìƒí•  ìˆ˜ ìˆëŠ” ê²½ìš°ì— ì•ˆì „ì¥ì¹˜ë¡œ ë§ì´ ì‚¬ìš©í•˜ê³  ìˆë‹¤. (<a href="https://qcqced123.github.io/framework-library/backward-nan/">ìì„¸íˆ ì•Œê³  ì‹¶ë‹¤ë©´ í´ë¦­</a>)</p>

<h3 id="torchgather"><code class="language-plaintext highlighter-rouge">ğŸ‘©â€ğŸ‘©â€ğŸ‘§â€ğŸ‘¦Â torch.gather</code></h3>

<p>í…ì„œ ê°ì²´ ë‚´ë¶€ì—ì„œ ì›í•˜ëŠ” ì¸ë±ìŠ¤ì— ìœ„ì¹˜í•œ ì›ì†Œë§Œ ì¶”ì¶œí•˜ê³  ì‹¶ì„ ë•Œ ì‚¬ìš©í•˜ë©´ ë§¤ìš° ìœ ìš©í•œ ë©”ì„œë“œë‹¤. í…ì„œ ì—­ì‹œ <code class="language-plaintext highlighter-rouge">iterable</code> ê°ì²´ë¼ì„œ <code class="language-plaintext highlighter-rouge">loop</code> ë¥¼ ì‚¬ìš©í•´ ì ‘ê·¼í•˜ëŠ” ê²ƒì´ ì§ê´€ì ìœ¼ë¡œ ë³´ì¼ ìˆ˜ ìˆìœ¼ë‚˜, í†µìƒì ìœ¼ë¡œ í…ì„œë¥¼ ì‚¬ìš©í•˜ëŠ” ìƒí™©ì´ë¼ë©´ ê°ì²´ì˜ ì°¨ì›ì´ ì–´ë§ˆë¬´ì‹œ í•˜ê¸° ë•Œë¬¸ì— ë£¨í”„ë¡œ ì ‘ê·¼í•´ ê´€ë¦¬í•˜ëŠ” ê²ƒì€ ë§¤ìš° ë¹„íš¨ìœ¨ì ì´ë‹¤. ë£¨í”„ë¥¼ í†µí•´ ì ‘ê·¼í•˜ë©´ íŒŒì´ì¬ì˜ ë‚´ì¥ ë¦¬ìŠ¤íŠ¸ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒê³¼ ë³„ë°˜ ë‹¤ë¥¼ê²Œ ì—†ì–´ì§€ê¸° ë•Œë¬¸ì—, í…ì„œë¥¼ ì‚¬ìš©í•˜ëŠ” ë©”ë¦¬íŠ¸ê°€ ì‚¬ë¼ì§„ë‹¤. ë¹„êµì  í¬ì§€ ì•Šì€ 2~3ì°¨ì›ì˜ í…ì„œ ì •ë„ë¼ë©´ ì‚¬ìš©í•´ë„ í¬ê²Œ ë¬¸ì œëŠ” ì—†ì„ê±°ë¼ ìƒê°í•˜ì§€ë§Œ ê·¸ë˜ë„ ì½”ë“œì˜ ì¼ê´€ì„±ì„ ìœ„í•´ <code class="language-plaintext highlighter-rouge">torch.gather</code> ì‚¬ìš©ì„ ê¶Œì¥í•œë‹¤. ì´ì œ <code class="language-plaintext highlighter-rouge">torch.gather</code>ì˜ ì‚¬ìš©ë²•ì— ëŒ€í•´ ì•Œì•„ë³´ì.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># torch.gather params
</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">gather</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">sparse_grad</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">dim</code>ê³¼ <code class="language-plaintext highlighter-rouge">index</code>ì— ì£¼ëª©í•´ë³´ì. ë¨¼ì € <code class="language-plaintext highlighter-rouge">dim</code>ì€ ì‚¬ìš©ìê°€ ì¸ë±ì‹±ì„ ì ìš©í•˜ê³  ì‹¶ì€ ì°¨ì›ì„ ì§€ì •í•´ì£¼ëŠ” ì—­í• ì„ í•œë‹¤. <code class="language-plaintext highlighter-rouge">index</code> ë§¤ê°œë³€ìˆ˜ë¡œ ì „ë‹¬í•˜ëŠ” í…ì„œ ì•ˆì—ëŠ” ì›ì†Œì˜ <code class="language-plaintext highlighter-rouge">â€˜ì¸ë±ìŠ¤â€™</code>ë¥¼ ì˜ë¯¸í•˜ëŠ” ìˆ«ìë“¤ì´ ë§ˆêµ¬ì¡ì´ë¡œ ë‹´ê²¨ìˆëŠ”ë°, í•´ë‹¹ ì¸ë±ìŠ¤ê°€ ëŒ€ìƒ í…ì„œì˜ ì–´ëŠ ì°¨ì›ì„ ê°€ë¦¬í‚¬ ê²ƒì¸ì§€ë¥¼ ì»´í“¨í„°ì—ê²Œ ì•Œë ¤ì¤€ë‹¤ê³  ìƒê°í•˜ë©´ ëœë‹¤. <code class="language-plaintext highlighter-rouge">index</code> ëŠ” ì•ì—ì„œ ì„¤ëª…í–ˆë“¯ì´ ì›ì†Œì˜ <code class="language-plaintext highlighter-rouge">â€˜ì¸ë±ìŠ¤â€™</code>ë¥¼ ì˜ë¯¸í•˜ëŠ” ìˆ«ìë“¤ì´ ë‹´ê¸´ í…ì„œë¥¼ ì…ë ¥ìœ¼ë¡œ í•˜ëŠ” ë§¤ê°œë³€ìˆ˜ë‹¤. ì´ ë•Œ ì£¼ì˜í•  ì ì€ ëŒ€ìƒ í…ì„œ(<code class="language-plaintext highlighter-rouge">input</code>)ì™€ ì¸ë±ìŠ¤ í…ì„œì˜ ì°¨ì› í˜•íƒœê°€ ë°˜ë“œì‹œ ë™ì¼í•´ì•¼ í•œë‹¤ëŠ” ê²ƒì´ë‹¤. ì—­ì‹œ ë§ë¡œë§Œ ë“¤ìœ¼ë©´ ì´í•´í•˜ê¸° í˜ë“œë‹ˆ ì‚¬ìš© ì˜ˆì‹œë¥¼ í•¨ê¼ ì‚´í´ë³´ì.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># torch.gather usage example
</span><span class="o">&gt;&gt;&gt;</span> <span class="n">q</span><span class="p">,</span> <span class="n">kr</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1024</span><span class="p">,</span> <span class="mi">64</span><span class="p">),</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1024</span><span class="p">,</span> <span class="mi">64</span><span class="p">)</span> <span class="c1"># [batch, sequence, dim_head], [batch, 2*sequence, dim_head]
</span><span class="o">&gt;&gt;&gt;</span> <span class="n">tmp_c2p</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">kr</span><span class="p">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">))</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">tmp_c2p</span><span class="p">,</span> <span class="n">tmp_c2p</span><span class="p">.</span><span class="n">shape</span>
<span class="p">(</span><span class="n">tensor</span><span class="p">([[</span><span class="o">-</span><span class="mf">2.6477</span><span class="p">,</span> <span class="o">-</span><span class="mf">4.7478</span><span class="p">,</span> <span class="o">-</span><span class="mf">5.3250</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mf">1.6062</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.9717</span><span class="p">,</span>  <span class="mf">3.8004</span><span class="p">],</span>
         <span class="p">[</span> <span class="mf">0.0662</span><span class="p">,</span>  <span class="mf">1.5240</span><span class="p">,</span>  <span class="mf">0.1182</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mf">0.1653</span><span class="p">,</span>  <span class="mf">2.8476</span><span class="p">,</span>  <span class="mf">1.6337</span><span class="p">],</span>
         <span class="p">[</span><span class="o">-</span><span class="mf">0.5010</span><span class="p">,</span> <span class="o">-</span><span class="mf">4.2267</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.1179</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mf">1.1447</span><span class="p">,</span>  <span class="mf">1.7845</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1493</span><span class="p">],</span>
         <span class="p">...,</span>
         <span class="p">[</span><span class="o">-</span><span class="mf">2.1073</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2149</span><span class="p">,</span> <span class="o">-</span><span class="mf">4.8630</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mf">0.8238</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5833</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2066</span><span class="p">],</span>
         <span class="p">[</span> <span class="mf">2.1747</span><span class="p">,</span>  <span class="mf">3.2924</span><span class="p">,</span>  <span class="mf">6.5808</span><span class="p">,</span>  <span class="p">...,</span> <span class="o">-</span><span class="mf">0.2926</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2511</span><span class="p">,</span>  <span class="mf">2.6996</span><span class="p">],</span>
         <span class="p">[</span><span class="o">-</span><span class="mf">2.8362</span><span class="p">,</span>  <span class="mf">2.8700</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.9729</span><span class="p">,</span>  <span class="p">...,</span> <span class="o">-</span><span class="mf">4.9913</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3616</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1708</span><span class="p">]],</span>
        <span class="n">grad_fn</span><span class="o">=&lt;</span><span class="n">MmBackward0</span><span class="o">&gt;</span><span class="p">)</span>
<span class="n">torch</span><span class="p">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">1024</span><span class="p">,</span> <span class="mi">1024</span><span class="p">]))</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">max_seq</span><span class="p">,</span> <span class="n">max_relative_position</span> <span class="o">=</span> <span class="mi">1024</span><span class="p">,</span> <span class="mi">512</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">q_index</span><span class="p">,</span> <span class="n">k_index</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="n">max_seq</span><span class="p">),</span> <span class="n">torch</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">max_relative_position</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">q_index</span><span class="p">,</span> <span class="n">k_index</span>
<span class="p">(</span><span class="n">tensor</span><span class="p">([</span>   <span class="mi">0</span><span class="p">,</span>    <span class="mi">1</span><span class="p">,</span>    <span class="mi">2</span><span class="p">,</span>  <span class="p">...,</span> <span class="mi">1021</span><span class="p">,</span> <span class="mi">1022</span><span class="p">,</span> <span class="mi">1023</span><span class="p">]),</span>
 <span class="n">tensor</span><span class="p">([</span>   <span class="mi">0</span><span class="p">,</span>    <span class="mi">1</span><span class="p">,</span>    <span class="mi">2</span><span class="p">,</span>  <span class="p">...,</span> <span class="mi">1021</span><span class="p">,</span> <span class="mi">1022</span><span class="p">,</span> <span class="mi">1023</span><span class="p">]))</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">tmp_pos</span> <span class="o">=</span> <span class="n">q_index</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="o">-</span> <span class="n">k_index</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">rel_pos_matrix</span> <span class="o">=</span> <span class="n">tmp_pos</span> <span class="o">+</span> <span class="n">max_relative_position</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">rel_pos_matrix</span>
<span class="n">tensor</span><span class="p">([[</span> <span class="mi">512</span><span class="p">,</span>  <span class="mi">511</span><span class="p">,</span>  <span class="mi">510</span><span class="p">,</span>  <span class="p">...,</span> <span class="o">-</span><span class="mi">509</span><span class="p">,</span> <span class="o">-</span><span class="mi">510</span><span class="p">,</span> <span class="o">-</span><span class="mi">511</span><span class="p">],</span>
        <span class="p">[</span> <span class="mi">513</span><span class="p">,</span>  <span class="mi">512</span><span class="p">,</span>  <span class="mi">511</span><span class="p">,</span>  <span class="p">...,</span> <span class="o">-</span><span class="mi">508</span><span class="p">,</span> <span class="o">-</span><span class="mi">509</span><span class="p">,</span> <span class="o">-</span><span class="mi">510</span><span class="p">],</span>
        <span class="p">[</span> <span class="mi">514</span><span class="p">,</span>  <span class="mi">513</span><span class="p">,</span>  <span class="mi">512</span><span class="p">,</span>  <span class="p">...,</span> <span class="o">-</span><span class="mi">507</span><span class="p">,</span> <span class="o">-</span><span class="mi">508</span><span class="p">,</span> <span class="o">-</span><span class="mi">509</span><span class="p">],</span>
        <span class="p">...,</span>
        <span class="p">[</span><span class="mi">1533</span><span class="p">,</span> <span class="mi">1532</span><span class="p">,</span> <span class="mi">1531</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mi">512</span><span class="p">,</span>  <span class="mi">511</span><span class="p">,</span>  <span class="mi">510</span><span class="p">],</span>
        <span class="p">[</span><span class="mi">1534</span><span class="p">,</span> <span class="mi">1533</span><span class="p">,</span> <span class="mi">1532</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mi">513</span><span class="p">,</span>  <span class="mi">512</span><span class="p">,</span>  <span class="mi">511</span><span class="p">],</span>
        <span class="p">[</span><span class="mi">1535</span><span class="p">,</span> <span class="mi">1534</span><span class="p">,</span> <span class="mi">1533</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mi">514</span><span class="p">,</span>  <span class="mi">513</span><span class="p">,</span>  <span class="mi">512</span><span class="p">]])</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">rel_pos_matrix</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">rel_pos_matrix</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="o">*</span><span class="n">max_relative_position</span> <span class="o">-</span> <span class="mi">1</span><span class="p">).</span><span class="n">repeat</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">tmp_c2p</span> <span class="o">=</span> <span class="n">tmp_c2p</span><span class="p">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">rel_pos_matrix</span><span class="p">,</span> <span class="n">rel_pos_matrix</span><span class="p">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">tmp_c2p</span><span class="p">.</span><span class="n">shape</span> 
<span class="p">(</span><span class="n">tensor</span><span class="p">([[[</span> <span class="mi">512</span><span class="p">,</span>  <span class="mi">511</span><span class="p">,</span>  <span class="mi">510</span><span class="p">,</span>  <span class="p">...,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">],</span>
          <span class="p">[</span> <span class="mi">513</span><span class="p">,</span>  <span class="mi">512</span><span class="p">,</span>  <span class="mi">511</span><span class="p">,</span>  <span class="p">...,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">],</span>
          <span class="p">[</span> <span class="mi">514</span><span class="p">,</span>  <span class="mi">513</span><span class="p">,</span>  <span class="mi">512</span><span class="p">,</span>  <span class="p">...,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">],</span>
          <span class="p">...,</span>
          <span class="p">[</span><span class="mi">1023</span><span class="p">,</span> <span class="mi">1023</span><span class="p">,</span> <span class="mi">1023</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mi">512</span><span class="p">,</span>  <span class="mi">511</span><span class="p">,</span>  <span class="mi">510</span><span class="p">],</span>
          <span class="p">[</span><span class="mi">1023</span><span class="p">,</span> <span class="mi">1023</span><span class="p">,</span> <span class="mi">1023</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mi">513</span><span class="p">,</span>  <span class="mi">512</span><span class="p">,</span>  <span class="mi">511</span><span class="p">],</span>
          <span class="p">[</span><span class="mi">1023</span><span class="p">,</span> <span class="mi">1023</span><span class="p">,</span> <span class="mi">1023</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mi">514</span><span class="p">,</span>  <span class="mi">513</span><span class="p">,</span>  <span class="mi">512</span><span class="p">]],</span>
 
         <span class="p">[[</span> <span class="mi">512</span><span class="p">,</span>  <span class="mi">511</span><span class="p">,</span>  <span class="mi">510</span><span class="p">,</span>  <span class="p">...,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">],</span>
          <span class="p">[</span> <span class="mi">513</span><span class="p">,</span>  <span class="mi">512</span><span class="p">,</span>  <span class="mi">511</span><span class="p">,</span>  <span class="p">...,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">],</span>
          <span class="p">[</span> <span class="mi">514</span><span class="p">,</span>  <span class="mi">513</span><span class="p">,</span>  <span class="mi">512</span><span class="p">,</span>  <span class="p">...,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">],</span>
          <span class="p">...,</span>
          <span class="p">[</span><span class="mi">1023</span><span class="p">,</span> <span class="mi">1023</span><span class="p">,</span> <span class="mi">1023</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mi">512</span><span class="p">,</span>  <span class="mi">511</span><span class="p">,</span>  <span class="mi">510</span><span class="p">],</span>
          <span class="p">[</span><span class="mi">1023</span><span class="p">,</span> <span class="mi">1023</span><span class="p">,</span> <span class="mi">1023</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mi">513</span><span class="p">,</span>  <span class="mi">512</span><span class="p">,</span>  <span class="mi">511</span><span class="p">],</span>
          <span class="p">[</span><span class="mi">1023</span><span class="p">,</span> <span class="mi">1023</span><span class="p">,</span> <span class="mi">1023</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mi">514</span><span class="p">,</span>  <span class="mi">513</span><span class="p">,</span>  <span class="mi">512</span><span class="p">]],</span>
<span class="n">torch</span><span class="p">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1024</span><span class="p">,</span> <span class="mi">1024</span><span class="p">]),</span>
<span class="n">torch</span><span class="p">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1024</span><span class="p">,</span> <span class="mi">1024</span><span class="p">]))</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">gather</span><span class="p">(</span><span class="n">tmp_c2p</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">rel_pos_matrix</span><span class="p">)</span>
<span class="n">tensor</span><span class="p">([[[</span><span class="o">-</span><span class="mf">0.8579</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2178</span><span class="p">,</span>  <span class="mf">1.6323</span><span class="p">,</span>  <span class="p">...,</span> <span class="o">-</span><span class="mf">2.6477</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.6477</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.6477</span><span class="p">],</span>
         <span class="p">[</span> <span class="mf">1.1601</span><span class="p">,</span>  <span class="mf">2.1752</span><span class="p">,</span>  <span class="mf">0.7187</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mf">0.0662</span><span class="p">,</span>  <span class="mf">0.0662</span><span class="p">,</span>  <span class="mf">0.0662</span><span class="p">],</span>
         <span class="p">[</span> <span class="mf">3.4379</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2573</span><span class="p">,</span>  <span class="mf">0.1375</span><span class="p">,</span>  <span class="p">...,</span> <span class="o">-</span><span class="mf">0.5010</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5010</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5010</span><span class="p">],</span>
         <span class="p">...,</span>
         <span class="p">[</span><span class="o">-</span><span class="mf">1.2066</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2066</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2066</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mf">0.5943</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5169</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.0820</span><span class="p">],</span>
         <span class="p">[</span> <span class="mf">2.6996</span><span class="p">,</span>  <span class="mf">2.6996</span><span class="p">,</span>  <span class="mf">2.6996</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mf">0.2014</span><span class="p">,</span>  <span class="mf">1.1458</span><span class="p">,</span>  <span class="mf">3.2626</span><span class="p">],</span>
         <span class="p">[</span><span class="o">-</span><span class="mf">0.1708</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1708</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1708</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mf">1.9955</span><span class="p">,</span>  <span class="mf">4.1549</span><span class="p">,</span>  <span class="mf">2.6356</span><span class="p">]],</span>
</code></pre></div></div>

<p>ìœ„ ì½”ë“œëŠ” <code class="language-plaintext highlighter-rouge">DeBERTa</code> ì˜ <code class="language-plaintext highlighter-rouge">Disentangled Self-Attention</code>ì„ êµ¬í˜„í•œ ì½”ë“œì˜ ì¼ë¶€ë¶„ì´ë‹¤. ìì„¸í•œ ì›ë¦¬ëŠ” <code class="language-plaintext highlighter-rouge">DeBERTa</code> ë…¼ë¬¸ ë¦¬ë·° í¬ìŠ¤íŒ…ì—ì„œ í™•ì¸í•˜ë©´ ë˜ê³ , ìš°ë¦¬ê°€ ì§€ê¸ˆ ì£¼ëª©í•  ë¶€ë¶„ì€ ë°”ë¡œ <code class="language-plaintext highlighter-rouge">tmp_c2p</code>, <code class="language-plaintext highlighter-rouge">rel_pos_matrix</code> ê·¸ë¦¬ê³  ë§ˆì§€ë§‰ ì¤„ì— ìœ„ì¹˜í•œ <code class="language-plaintext highlighter-rouge">torch.gather</code> ë‹¤. <code class="language-plaintext highlighter-rouge">[10, 1024, 1024]</code> ëª¨ì–‘ì„ ê°€ì§„ ëŒ€ìƒ í…ì„œ <code class="language-plaintext highlighter-rouge">tmp_c2p</code> ì—ì„œ ë‚´ê°€ ì›í•˜ëŠ” ì›ì†Œë§Œ ì¶”ì¶œí•˜ë ¤ëŠ” ìƒí™©ì¸ë°, ì¶”ì¶œí•´ì•¼í•  ì›ì†Œì˜ ì¸ë±ìŠ¤ ê°’ì´ ë‹´ê¸´ í…ì„œë¥¼ <code class="language-plaintext highlighter-rouge">rel_pos_matrix</code> ë¡œ ì •ì˜í–ˆë‹¤. <code class="language-plaintext highlighter-rouge">rel_pos_matrix</code> ì˜ ì°¨ì›ì€ <code class="language-plaintext highlighter-rouge">[10, 1024, 1024]</code>ë¡œ <code class="language-plaintext highlighter-rouge">tmp_c2p</code>ì™€ ë™ì¼í•˜ë‹¤. ì°¸ê³ ë¡œ ì¶”ì¶œí•´ì•¼ í•˜ëŠ” ì°¨ì› ë°©í–¥ì€ ê°€ë¡œ ë°©í–¥(ë‘ ë²ˆì§¸ 1024)ì´ë‹¤.</p>

<p>ì´ì œ <code class="language-plaintext highlighter-rouge">torch.gather</code>ì˜ ë™ì‘ì„ ì‚´í´ë³´ì. ìš°ë¦¬ê°€ í˜„ì¬ ì¶”ì¶œí•˜ê³  ì‹¶ì€ ëŒ€ìƒì€ 3ì°¨ì› í…ì„œì˜ ê°€ë¡œ ë°©í–¥(ë‘ ë²ˆì§¸ 1024, í…ì„œì˜ í–‰ ë²¡í„°), ì¦‰ <code class="language-plaintext highlighter-rouge">2 * max_sequence_length</code> ë¥¼ ì˜ë¯¸í•˜ëŠ” ì°¨ì› ë°©í–¥ì˜ ì›ì†Œë‹¤. ë”°ë¼ì„œ <code class="language-plaintext highlighter-rouge">dim=-1</code>ìœ¼ë¡œ ì„¤ì •í•´ì¤€ë‹¤. ì´ì œ ë©”ì„œë“œê°€ ì˜ë„ëŒ€ë¡œ ì ìš©ë˜ì—ˆëŠ”ì§€ í™•ì¸í•´ë³´ì. <code class="language-plaintext highlighter-rouge">rel_pos_matrix</code> ì˜ 0ë²ˆ ë°°ì¹˜, 0ë²ˆì§¸ ì‹œí€€ìŠ¤ì˜ ê°€ì¥ ë§ˆì§€ë§‰ ì°¨ì›ì˜ ê°’ì€ <code class="language-plaintext highlighter-rouge">0</code>ìœ¼ë¡œ ì´ˆê¸°í™” ë˜ì–´ ìˆë‹¤. ë‹¤ì‹œ ë§í•´, ëŒ€ìƒ í…ì„œì˜ ëŒ€ìƒ ì°¨ì›ì—ì„œ 0ë²ˆì§¸ ì¸ë±ìŠ¤ì— í•´ë‹¹í•˜ëŠ” ê°’ì„ ê°€ì ¸ì˜¤ë¼ëŠ” ì˜ë¯¸ë¥¼ ë‹´ê³  ìˆë‹¤. ê·¸ë ‡ë‹¤ë©´ <code class="language-plaintext highlighter-rouge">torch.gather</code> ì‹¤í–‰ ê²°ê³¼ê°€ <code class="language-plaintext highlighter-rouge">tmp_c2p</code>ì˜ 0ë²ˆ ë°°ì¹˜, 0ë²ˆì§¸ ì‹œí€€ìŠ¤ì˜ 0ë²ˆì§¸ ì°¨ì› ê°’ê³¼ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸í•´ë³´ì. ë‘˜ ë‹¤ <code class="language-plaintext highlighter-rouge">-2.6477</code>, <code class="language-plaintext highlighter-rouge">-2.6477</code> ìœ¼ë¡œ ê°™ì€ ê°’ì„ ë‚˜íƒ€ë‚´ê³  ìˆë‹¤. ë”°ë¼ì„œ ìš°ë¦¬ ì˜ë„ëŒ€ë¡œ ì˜ ì‹¤í–‰ë˜ì—ˆë‹¤ëŠ” ì‚¬ì‹¤ì„ ì•Œ ìˆ˜ ìˆë‹¤.</p>

<h3 id="torchtriu-torchtril"><code class="language-plaintext highlighter-rouge">ğŸ‘©â€ğŸ‘©â€ğŸ‘§â€ğŸ‘¦Â torch.triu, torch.tril</code></h3>

<p>ê°ê° ì…ë ¥ í…ì„œë¥¼ <code class="language-plaintext highlighter-rouge">ìƒì‚¼ê°í–‰ë ¬</code>, <code class="language-plaintext highlighter-rouge">í•˜ì‚¼ê°í–‰ë ¬</code>ë¡œ ë§Œë“ ë‹¤. <code class="language-plaintext highlighter-rouge">triu</code>ë‚˜ <code class="language-plaintext highlighter-rouge">tril</code>ì€ ì‚¬ì‹¤ ë’¤ì§‘ìœ¼ë©´ ê°™ì€ ê²°ê³¼ë¥¼ ë°˜í™˜í•˜ê¸° ë•Œë¬¸ì— <code class="language-plaintext highlighter-rouge">tril</code>ì„ ê¸°ì¤€ìœ¼ë¡œ ì„¤ëª…ì„ í•˜ê² ë‹¤. ë©”ì„œë“œì˜ ë§¤ê°œë³€ìˆ˜ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># torch.triu, tril params
</span><span class="n">upper_tri_matrix</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">triu</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span> <span class="n">diagonal</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
<span class="n">lower_tri_matrix</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tril</span><span class="p">(</span><span class="n">input_tensors</span><span class="p">,</span> <span class="n">diagonal</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">diagonal</code> ì— ì£¼ëª©í•´ë³´ì. ì–‘ìˆ˜ë¥¼ ì „ë‹¬í•˜ë©´ ì£¼ëŒ€ê°ì„±ë¶„ì—ì„œ í•´ë‹¹í•˜ëŠ” ê°’ë§Œí¼ ë–¨ì–´ì§„ ê³³ì˜ ëŒ€ê°ì„±ë¶„ê¹Œì§€ ê·¸ ê°’ì„ ì‚´ë ¤ë‘”ë‹¤. í•œí¸ ìŒìˆ˜ë¥¼ ì „ë‹¬í•˜ë©´ ì£¼ëŒ€ê°ì„±ë¶„ì„ í¬í•¨í•´ ì£¼ì–´ì§„ ê°’ë§Œí¼ ë–¨ì–´ì§„ ê³³ê¹Œì§€ì˜ ëŒ€ê°ì„±ë¶„ì„ ëª¨ë‘ 0ìœ¼ë¡œ ë§Œë“¤ì–´ë²„ë¦°ë‹¤. ê¸°ë³¸ì€ 0ìœ¼ë¡œ ì„¤ì •ë˜ì–´ ìˆìœ¼ë©°, ì´ëŠ” ì£¼ëŒ€ê°ì„±ë¶„ë¶€í„° ì™¼ìª½ í•˜ë‹¨ì˜ ì›ì†Œë¥¼ ëª¨ë‘ ì‚´ë ¤ë‘ê² ë‹¤ëŠ” ì˜ë¯¸ê°€ ëœë‹¤.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># torch.tril usage example
</span><span class="o">&gt;&gt;&gt;</span> <span class="n">lm_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tril</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">ones</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">lm_mask</span>
<span class="mi">1</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span>
<span class="mi">1</span> <span class="mi">1</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span>
<span class="mi">1</span> <span class="mi">1</span> <span class="mi">1</span> <span class="mi">0</span> <span class="mi">0</span>
<span class="mi">1</span> <span class="mi">1</span> <span class="mi">1</span> <span class="mi">1</span> <span class="mi">0</span>
</code></pre></div></div>

<p>ë‘ ë©”ì„œë“œëŠ” ì„ í˜•ëŒ€ìˆ˜í•™ì´ í•„ìš”í•œ ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ ì‚¬ìš©ë˜ëŠ”ë°, í•„ìì˜ ê²½ìš°, <code class="language-plaintext highlighter-rouge">GPT</code>ì²˜ëŸ¼ <code class="language-plaintext highlighter-rouge">Transformer</code>ì˜ <code class="language-plaintext highlighter-rouge">Decoder</code> ë¥¼ ì‚¬ìš©í•˜ëŠ” ëª¨ë¸ì„ ë¹Œë“œí•  ë•Œ ê°€ì¥ ë§ì´ ì‚¬ìš©í–ˆë˜ ê²ƒ ê°™ë‹¤. <code class="language-plaintext highlighter-rouge">Decoder</code>ë¥¼ ì‚¬ìš©í•˜ëŠ” ëª¨ë¸ì€ ëŒ€ë¶€ë¶„ êµ¬ì¡°ìƒ <code class="language-plaintext highlighter-rouge">Language Modeling</code>ì„ ìœ„í•´ì„œ <code class="language-plaintext highlighter-rouge">Masked Multi-Head Self-Attention Block</code>ì„ ì‚¬ìš©í•˜ëŠ”ë° ì´ ë•Œ ë¯¸ë˜ ì‹œì ì˜ í† í° ì„ë² ë”© ê°’ì— ë§ˆìŠ¤í‚¹ì„ í•´ì£¼ê¸° ìœ„í•´ <code class="language-plaintext highlighter-rouge">torch.tril</code> ì„ ì‚¬ìš©í•˜ê²Œ ë˜ë‹ˆ ì°¸ê³ í•˜ì.</p>

<h3 id="torchtensormasked_fill"><code class="language-plaintext highlighter-rouge">ğŸ‘©â€ğŸ‘©â€ğŸ‘§â€ğŸ‘¦Â torch.Tensor.masked_fill</code></h3>

<p>ì‚¬ìš©ìê°€ ì§€ì •í•œ ê°’ì— í•´ë‹¹ë˜ëŠ” ì›ì†Œë¥¼ ëª¨ë‘ ë§ˆìŠ¤í‚¹ ì²˜ë¦¬í•´ì£¼ëŠ” ë©”ì„œë“œë‹¤. ë¨¼ì € ë§¤ê°œë³€ìˆ˜ë¥¼ í™•ì¸í•´ë³´ì.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># torch.Tensor.masked_fill params
</span><span class="n">input_tensors</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">6</span><span class="p">]])</span>
<span class="n">input_tensors</span><span class="p">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">mask</span><span class="p">:</span> <span class="n">BoolTensor</span><span class="p">,</span> <span class="n">value</span><span class="p">:</span> <span class="nb">float</span><span class="p">)</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">masked_fill</code> ì€ í…ì„œ ê°ì²´ì˜ ë‚´ë¶€ <code class="language-plaintext highlighter-rouge">attribute</code> ë¡œ ì •ì˜ë˜ê¸° ë•Œë¬¸ì— í•´ë‹¹ ë©”ì„œë“œë¥¼ ì‚¬ìš©í•˜ê³  ì‹¶ë‹¤ë©´ ë¨¼ì € ë§ˆìŠ¤í‚¹ ëŒ€ìƒ í…ì„œë¥¼ ë§Œë“¤ì–´ì•¼ í•œë‹¤. í…ì„œë¥¼ ì •ì˜í–ˆë‹¤ë©´ í…ì„œ ê°ì²´ì˜ <code class="language-plaintext highlighter-rouge">attributes</code> ì ‘ê·¼ì„ í†µí•´ <code class="language-plaintext highlighter-rouge">masked_fill()</code> ì„ í˜¸ì¶œí•œ ë’¤, í•„ìš”í•œ ë§¤ê°œë³€ìˆ˜ë¥¼ ì „ë‹¬í•´ì£¼ëŠ” ë°©ì‹ìœ¼ë¡œ ì‚¬ìš©í•˜ë©´ ëœë‹¤.</p>

<p><code class="language-plaintext highlighter-rouge">mask</code> ë§¤ê°œë³€ìˆ˜ì—ëŠ” ë§ˆìŠ¤í‚¹ í…ì„œë¥¼ ì „ë‹¬í•´ì•¼ í•˜ëŠ”ë°, ì´ ë•Œ ë‚´ë¶€ ì›ì†ŒëŠ” ëª¨ë‘ <code class="language-plaintext highlighter-rouge">boolean</code>ì´ì–´ì•¼ í•˜ê³  í…ì„œì˜ í˜•íƒœëŠ” ëŒ€ìƒ í…ì„œì™€ ë™ì¼í•´ì•¼ í•œë‹¤(ì™„ì „íˆ ê°™ì„ í•„ìš”ëŠ” ì—†ê³ , ë¸Œë¡œë“œ ìºìŠ¤íŒ…ë§Œ ê°€ëŠ¥í•˜ë©´ ìƒê´€ ì—†ìŒ).</p>

<p><code class="language-plaintext highlighter-rouge">value</code> ë§¤ê°œë³€ìˆ˜ì—ëŠ” ë§ˆìŠ¤í‚¹ ëŒ€ìƒ ì›ì†Œë“¤ì— ì¼ê´„ì ìœ¼ë¡œ ì ìš©í•´ì£¼ê³  ì‹¶ì€ ê°’ì„ ì „ë‹¬í•œë‹¤. ì´ê²Œ ë§ë¡œë§Œ ë“¤ìœ¼ë©´ ì´í•´í•˜ê¸° ì‰½ì§€ ì•Šë‹¤. ì•„ë˜ ì‚¬ìš© ì˜ˆì‹œë¥¼ í•¨ê»˜ ì²¨ë¶€í–ˆìœ¼ë‹ˆ ì°¸ê³  ë°”ë€ë‹¤.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># torch.masked_fill usage
</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">lm_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tril</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">ones</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">lm_mask</span>
<span class="mi">1</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span>
<span class="mi">1</span> <span class="mi">1</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span>
<span class="mi">1</span> <span class="mi">1</span> <span class="mi">1</span> <span class="mi">0</span> <span class="mi">0</span>
<span class="mi">1</span> <span class="mi">1</span> <span class="mi">1</span> <span class="mi">1</span> <span class="mi">0</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">attention_matrix</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">))</span> <span class="o">/</span> <span class="n">dot_scale</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">attention_matrix</span>
<span class="mf">1.22</span> <span class="mf">2.1</span> <span class="mf">3.4</span> <span class="mf">1.2</span> <span class="mf">1.1</span>
<span class="mf">1.22</span> <span class="mf">2.1</span> <span class="mf">3.4</span> <span class="mf">9.9</span> <span class="mf">9.9</span>
<span class="mf">1.22</span> <span class="mf">2.1</span> <span class="mf">3.4</span> <span class="mf">9.9</span> <span class="mf">9.9</span>
<span class="mf">1.22</span> <span class="mf">2.1</span> <span class="mf">3.4</span> <span class="mf">9.9</span> <span class="mf">9.9</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">attention_matrix</span> <span class="o">=</span> <span class="n">attention_matrix</span><span class="p">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">lm_mask</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="s">'-inf'</span><span class="p">))</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">attention_matrix</span>
<span class="mf">1.22</span> <span class="o">-</span><span class="n">inf</span> <span class="o">-</span><span class="n">inf</span> <span class="o">-</span><span class="n">inf</span> <span class="o">-</span><span class="n">inf</span>
<span class="mf">1.22</span> <span class="mf">2.1</span> <span class="o">-</span><span class="n">inf</span> <span class="o">-</span><span class="n">inf</span> <span class="o">-</span><span class="n">inf</span>
<span class="mf">1.22</span> <span class="mf">2.1</span> <span class="mf">3.4</span> <span class="o">-</span><span class="n">inf</span> <span class="o">-</span><span class="n">inf</span>
<span class="mf">1.22</span> <span class="mf">2.1</span> <span class="mf">3.4</span> <span class="mf">9.9</span> <span class="o">-</span><span class="n">inf</span>
</code></pre></div></div>]]></content><author><name>qcqced</name><email>qcqced123@gmail.com</email></author><category term="Framework &amp; Library" /><category term="Pytorch" /><category term="Tensor" /><category term="Linear Algebra" /><summary type="html"><![CDATA[íŒŒì´í† ì¹˜ì—ì„œ ìì£¼ ì‚¬ìš©í•˜ëŠ” í…ì„œ ì¸ë±ì‹± ê´€ë ¨ ë©”ì„œë“œ ëª¨ìŒ]]></summary></entry><entry><title type="html">ğŸ¤–Â [Transformer] Attention Is All You Need</title><link href="http://localhost:4000/nlp/vit" rel="alternate" type="text/html" title="ğŸ¤–Â [Transformer] Attention Is All You Need" /><published>2023-08-04T00:00:00+09:00</published><updated>2023-07-27T02:00:00+09:00</updated><id>http://localhost:4000/nlp/Transformer</id><content type="html" xml:base="http://localhost:4000/nlp/vit"><![CDATA[]]></content><author><name>qcqced</name><email>qcqced123@gmail.com</email></author><category term="Computer Vision" /><category term="Computer Vision" /><category term="Vision Transformer" /><category term="ViT" /><category term="Transformer" /><category term="Self-Attention" /><category term="Image Classification" /><summary type="html"><![CDATA[Transformer Official Paper Review with Pytorch Implementation]]></summary></entry><entry><title type="html">ğŸ“ˆÂ Gradient: Directional Derivative</title><link href="http://localhost:4000/calculus/gradient" rel="alternate" type="text/html" title="ğŸ“ˆÂ Gradient: Directional Derivative" /><published>2023-07-31T00:00:00+09:00</published><updated>2023-07-31T23:00:00+09:00</updated><id>http://localhost:4000/calculus/gradient</id><content type="html" xml:base="http://localhost:4000/calculus/gradient"><![CDATA[<h3 id="concept-of-gradient"><code class="language-plaintext highlighter-rouge">ğŸ¤”Â Concept of Gradient</code></h3>

<p>ê·¸ë¼ë””ì–¸íŠ¸ëŠ” ë‹¤ë³€ìˆ˜ í•¨ìˆ˜ì˜ ê¸°ìš¸ê¸°ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ë²¡í„°ë¥¼ ë§í•œë‹¤. ê·¸ë¼ë””ì–¸íŠ¸ì˜ ì›ì†ŒëŠ” í•¨ìˆ˜ì— ì¡´ì¬í•˜ëŠ” ëª¨ë“  ë³€ìˆ˜ë¥¼ ëŒ€ìƒìœ¼ë¡œ í¸ë¯¸ë¶„í•œ ê²°ê³¼ë¡œ êµ¬ì„±ë˜ëŠ”ë°, ì˜ˆë¥¼ ë“¤ì–´ ë³€ìˆ˜ê°€ $x_1, x_2$ 2ê°œì¸ ë‹¤ë³€ìˆ˜ í•¨ìˆ˜ $f(x_1, x_2)$ê°€ ìˆë‹¤ê³  ê°€ì •í•´ë³´ì. ë‹¤ë³€ìˆ˜ í•¨ìˆ˜ $f$ì˜ ê·¸ë¼ë””ì–¸íŠ¸ëŠ” ì•„ë˜ ìˆ˜ì‹ì²˜ëŸ¼ í‘œí˜„í•  ìˆ˜ ìˆë‹¤.</p>

\[f'(x_1, x_2) = \begin{vmatrix}
  \frac{âˆ‚f}{âˆ‚x_1} \\
  \frac{âˆ‚f}{âˆ‚x_2}
\end{vmatrix}\]

<p>ì´ëŸ¬í•œ ê·¸ë¼ë””ì–¸íŠ¸ëŠ” ë¨¸ì‹  ëŸ¬ë‹, ìˆ˜ì¹˜ ìµœì í™” í•™ë¬¸ì—ì„œ ë§¤ìš° ì¤‘ìš”í•œ ê°œë…ìœ¼ë¡œ ê¼½íŒë‹¤. ê·¸ë¼ë””ì–¸íŠ¸ ë²¡í„°ê°€ ê°€ë¦¬í‚¤ëŠ” ë°©í–¥ì´ ë°”ë¡œ ë‹¤ë³€ìˆ˜ í•¨ìˆ˜ê°€ íŠ¹ì • ì§€ì ì—ì„œ ê°€ì¥ ê°€íŒŒë¥´ê²Œ ì¦ê°€í•˜ëŠ” ë°©í–¥ì„ ê°€ë¦¬í‚¤ê¸° ë•Œë¬¸ì´ë‹¤. ì´ì²˜ëŸ¼ ê·¸ë¼ë””ì–¸íŠ¸ëŠ” í•¨ìˆ˜ì˜ ì…ë ¥ ê³µê°„ì„ ë”°ë¼ í•¨ìˆ˜ê°€ ì–´ë–»ê²Œ ë³€í™”í•˜ëŠ”ì§€ë¥¼ ì•Œë ¤ì£¼ëŠ” ê¸¸ì¡ì´ ì—­í• ì„ í•˜ê¸° ë•Œë¬¸ì—, ê·¸ë¼ë””ì–¸íŠ¸ ë°©í–¥ì„ ë”°ë¼ ë³€ìˆ˜ê°’ì„ íŠœë‹í•˜ë‹¤ ë³´ë©´ í•¨ìˆ˜ì˜ ìµœëŒ€ê°’â€¢ìµœì†Œê°’ì— ë„ë‹¬í•˜ì—¬ ìµœì í™” ë¬¸ì œë¥¼ í•´ê²°í•  ìˆ˜ ìˆê²Œ ëœë‹¤. ê·¸ë ‡ë‹¤ë©´ ì™œ ê·¸ë¼ë””ì–¸íŠ¸ ë²¡í„°ì˜ ë°©í–¥ì´ íŠ¹ì • ì§€ì ì—ì„œ í•¨ìˆ˜ê°€ ê°€ì¥ ê°€íŒŒë¥´ê²Œ ì¦ê°€í•˜ëŠ” ë°©í–¥ì„ ë‚˜íƒ€ë‚´ëŠ” ê²ƒì¼ê¹Œ?? í¸ë¯¸ë¶„, ë„í•¨ìˆ˜ ì •ì˜ ê·¸ë¦¬ê³  ë‚´ì ì„ í™œìš©í•´ ì¦ëª…í•  ìˆ˜ ìˆë‹¤.</p>

<h3 id="-proof-of-gradient"><code class="language-plaintext highlighter-rouge">ğŸªª Proof of Gradient</code></h3>

<p align="center">
<img src="/assets/images/gradient/gradient.jpg" alt="Example of multivariate function" class="align-center image-caption" width="60%&quot;, height=&quot;25%" />
<strong><em><a href="https://m.blog.naver.com/PostView.naver?isHttpsRedirect=true&amp;blogId=galaxyenergy&amp;logNo=221431325545">Example of multivariate function</a></em></strong>
</p>

<p>ê·¸ë¼ë””ì–¸íŠ¸ ë²¡í„°ì˜ ë°©í–¥ì´ í•¨ìˆ˜ê°€ ê°€ì¥ ê°€íŒŒë¥´ê²Œ ì¦ê°€í•˜ëŠ” ë°©í–¥ê³¼ ì¼ì¹˜í•œë‹¤ëŠ” ëª…ì œë¥¼ ì¦ëª…í•˜ê¸° ìœ„í•´ ìµœë‹¨ ê²½ë¡œë¡œ ì‚° ì •ìƒì— ì˜¤ë¥´ëŠ” ê³¼ì •ì„ ë– ì˜¬ë ¤ë³´ë ¤ í•œë‹¤. ìš°ë¦¬ëŠ” í˜„ì¬ ì´ë³€ìˆ˜ í•¨ìˆ˜ë¡œ ì •ì˜ë˜ëŠ” ì‚° ì¤‘í„± ì–´ë”˜ê°€, ì  $(x_1^0, x_2^0)$ë¥¼ ì§€ë‚˜ê³  ìˆë‹¤. ì‚° ì •ìƒì„ ìµœë‹¨ ê²½ë¡œë¡œ ì˜¤ë¥´ë ¤ë©´ ì–´ë–»ê²Œ í•´ì•¼í• ê¹Œ?? ê°€ì¥ ê²½ì‚¬ê°€ ê°€íŒŒë¥¸ ê¸‰ê²½ì‚¬ ì§€ëŒ€ë¥¼ í–¥í•´ ë‚˜ì•„ê°€ë©´ ë  ê²ƒì´ë‹¤. í•˜ì§€ë§Œ ì‚° ì¤‘í„±ì— ìˆëŠ” ìš°ë¦¬ê°€ ì–´ëŠ ë°©í–¥ì´ ê°€ì¥ ê°€íŒŒë¥¸ ê¸‰ê²½ì‚¬ ì§€ëŒ€ì¸ì§€ ì§ê´€ì ìœ¼ë¡œ ì•Œ ê¸¸ì´ ì—†ë‹¤. ê·¸ë˜ì„œ ë°©í–¥ ë„í•¨ìˆ˜ë¥¼ ë„ì…í•´ ê¸‰ê²½ì‚¬ ì§€ëŒ€ë¡œ í–¥í•  ìˆ˜ ìˆëŠ” ë°©í–¥ì„ êµ¬í•´ ë³´ê¸°ë¡œ í–ˆë‹¤. ì•„ë˜ ìˆ˜ì‹ì„ ë³´ì.</p>

\[\lim_{\Delta{x}-&gt;0}\frac{f(x+\Delta{x}) - f(x)}{\Delta{x}} =    \frac{df}{dx}= f'(x) \\
df = f'(x)dx\]

<p>ë„ˆë¬´ë‚˜ë„ ìµìˆ™í•œ í˜•íƒœ ì•„ë‹Œê°€?? ìš°ë¦¬ê°€ ì¼ë°˜ì ìœ¼ë¡œ ì•Œê³  ìˆëŠ” ì¼ë³€ìˆ˜ í•¨ìˆ˜ì˜ ë¯¸ë¶„ ì •ì˜ ê·¸ë¦¬ê³  ì¢Œë³€ì˜ $dx$ë¥¼ ìš°ë³€ìœ¼ë¡œ ë„˜ê²¨ ì‚´ì§ ë³€í˜•í•œ ì‹ì´ë‹¤. ì´ê²ƒì„ ì´ì œ ë‹¤ë³€ìˆ˜ í•¨ìˆ˜ì— ì ìš©í•˜ë©´ ë°”ë¡œ ë°©í–¥ ë„í•¨ìˆ˜ê°€ ëœë‹¤. ë‹¤ì‹œ ìš°ë¦¬ê°€ ì˜¤ë¥´ë ¤ëŠ” ì‚°(ì´ë³€ìˆ˜ í•¨ìˆ˜)ìœ¼ë¡œ ëŒì•„ì™€ ë³´ì.</p>

\[f(x_1 + dx_1, x_2) = f(x_1, x_2) + f'(x_1, x_2)dx_1 \\
f(x_1, x_2 + dx_2) = f(x_1, x_2) + f'(x_1, x_2)dx_2 \\\]

<p>ìœ„ì—ì„œ ì„œìˆ í•œ ë„í•¨ìˆ˜ ì •ì˜ë¥¼ í™œìš©í•´ ìš°ë¦¬ê°€ ë‹¤ìŒì— ë°œê±¸ìŒì„ ì˜®ê¸¸ ìœ„ì¹˜ë¥¼ ì   $A$ë¥¼ $(x_1^0 + dx_1, x_2^0+dx_2)$ ì´ë¼ê³  í‘œí˜„í•  ìˆ˜ ìˆë‹¤. ì´ í‘œí˜„ì„ í™œìš©í•´ ë‹¤ë³€ìˆ˜ í•¨ìˆ˜ì˜ ë¯¸ë¶„ì„ ì •ì˜í•´ë³´ì. ìš°ë¦¬ëŠ” ì´ë¯¸ ë‹¤ë³€ìˆ˜ í•¨ìˆ˜ì˜ ê°œë³„ ë³€ìˆ˜ì— í¸ë¯¸ë¶„ì„ ì·¨í•˜ê³  í–‰ë²¡í„°ë¡œ ìŒ“ì€ ê²°ê³¼ê°€ ë°”ë¡œ ì „ë¯¸ë¶„ì´ë¼ëŠ” ê²ƒì„ ì•Œê³  ìˆë‹¤.</p>

\[f(x_1 + dx_1, x_2 + dx_2) - f(x_1, x_2) = f'(x_1)dx_1 + f'(x_2)dx_2\]

<p>ë‹¤ì‹œ í¸ë¯¸ë¶„ì˜ ì •ì˜ë¥¼ í™œìš©í•´ ìˆ˜ì‹ì„ ì •ë¦¬í•˜ë©´ ë°©í–¥ ë²¡í„°ì™€ í¸ë¯¸ë¶„ ê²°ê³¼ì˜ ë‚´ì ìœ¼ë¡œ í‘œí˜„í•  ìˆ˜ ìˆë‹¤.</p>

\[dL = \frac{âˆ‚L}{âˆ‚{x_1}}dx_1 + \frac{âˆ‚L}{âˆ‚{x_2}}dx_2 \\
dL = [dx_1, dx_2]\ â€¢\ \begin{vmatrix}
  \frac{âˆ‚L}{âˆ‚x_1} \\
  \frac{âˆ‚L}{âˆ‚x_2}
\end{vmatrix}\]

<p>ìŸì•„ì§€ëŠ” ìˆ˜ì‹ ì†ì— ìš°ë¦¬ì˜ ë³¸ë˜ ëª©ì ì„ ìŠì–´ì„œëŠ” ì•ˆëœë‹¤. ìš°ë¦¬ëŠ” ì§€ê¸ˆ ê°€ì¥ ë¹ ë¥´ê²Œ ì‚° ì •ìƒì— ë„ë‹¬í•  ìˆ˜ ìˆëŠ” ë°©ë²•ì„ ì°¾ê¸° ìœ„í•´ ì§€ê¸ˆê¹Œì§€ ë‹¬ë ¤ì™”ë‹¤. ì‚° ì •ìƒì— ê°€ì¥ ë¹ ë¥´ê²Œ ë„ë‹¬í•˜ê¸° ìœ„í•´ ê°€ì¥ ê°€íŒŒë¥¸ ê¸‰ê²½ì‚¬ ì§€ëŒ€ë§Œ ì°¾ì•„ì„œ ì˜¬ë¼ê°€ëŠ” ì „ëµì„ ì„¸ì› ì—ˆë‹¤. ë‹¤ì‹œ ë§í•´, ë‹¤ë³€ìˆ˜ í•¨ìˆ˜ $f(x)$ì˜ ê·¹ì†Œ ë³€í™”ëŸ‰ $dL$ì´ ìµœëŒ€ê°€ ë˜ëŠ” ë°©í–¥ìœ¼ë¡œ ë°œê±¸ìŒì„ ì˜®ê¸°ë©´ ëœë‹¤ëŠ” ê²ƒì´ë‹¤. ê·¸ë ‡ë‹¤ë©´ ê·¹ì†Œ ë³€í™”ëŸ‰ $dL$ì€ ì–¸ì œ ìµœëŒ€ê°€ ë ê¹Œ??</p>

<p>ì´ì œ ê¹Œë¨¹ê³  ìˆì—ˆë˜ ë‚´ì ì˜ ê°œë…ì„ ë‹¤ì‹œ í•œ ë²ˆ ìƒê¸°ì‹œì¼œë³´ì. ë‚´ì ì€ ë‹¤ì–‘í•˜ê²Œ í•´ì„ë˜ì§€ë§Œ, ë³¸ë”” ì„œë¡œ ë‹¤ë¥¸ ë‘ ë²¡í„°ì˜ <code class="language-plaintext highlighter-rouge">ë‹®ì€ ì •ë„</code>ë¥¼ ë‚˜íƒ€ë‚¸ë‹¤. ê·¹ì†Œ ë³€í™”ëŸ‰ $dL$ì´ ìµœëŒ€ê°€ ë˜ë ¤ë©´ ìš°ë³€ì˜ ë‚´ì  ê²°ê³¼ê°€ ìµœëŒ€ê°€ ë˜ì–´ì•¼ í•œë‹¤. ë‚´ì ì˜ ìµœëŒ€ê°’ì€ ì„œë¡œ ë‹¤ë¥¸ ë‘ ë²¡í„° ì‚¬ì´ì˜ ë¼ì¸ê°ë„ê°€ 0Ëšì¼ ë•Œ ì¦‰, ë‘ ë²¡í„°ê°€ ë™ì¼í•œ ë°©í–¥ì„ ë‚˜íƒ€ë‚¼ ë•Œ ì •ì˜ëœë‹¤. <strong><u>ë”°ë¼ì„œ ë°©í–¥ ë²¡í„°ê°€ ê·¸ë¼ë””ì–¸íŠ¸(í¸ë¯¸ë¶„ì˜ í–‰ë²¡í„°) ë°©í–¥ì¼ ë•Œ</u></strong> <code class="language-plaintext highlighter-rouge">ë‚´ì  ê²°ê³¼</code>(ê·¹ì†Œ ë³€í™”ëŸ‰ $dL$)<strong><u>ê°€ ìµœëŒ€ê°€ ëœë‹¤.</u></strong></p>

<p><strong><u>í•œí¸, ì‹¤ì œ ê¸°ê³„í•™ìŠµì—ì„œëŠ” ì†ì‹¤í•¨ìˆ˜ì˜ ìµœì í™”ë¥¼ ëª©ì  í•¨ìˆ˜ë¡œ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì— ê·¸ë¼ë””ì–¸íŠ¸(ì†ì‹¤í•¨ìˆ˜ì˜ ì „ë¯¸ë¶„) ë°©í–¥ì— ìŒìˆ˜ë¥¼ ì·¨í•´ì¤€ ê°’ì„ ì‚¬ìš©í•˜ê²Œ ëœë‹¤.</u></strong></p>]]></content><author><name>qcqced</name><email>qcqced123@gmail.com</email></author><category term="Calculus" /><category term="Calculus" /><category term="Partial Derivative" /><category term="Total Derivative" /><category term="loss function" /><category term="Gradient" /><category term="Gradient Descent" /><category term="Machine Learning" /><summary type="html"><![CDATA[Proof of gradient direction with Total Derivative]]></summary></entry><entry><title type="html">ğŸŒ†Â [ViT] An Image Is Worth 16x16 Words: Transformers For Image Recognition At Scale</title><link href="http://localhost:4000/cv/vit" rel="alternate" type="text/html" title="ğŸŒ†Â [ViT] An Image Is Worth 16x16 Words: Transformers For Image Recognition At Scale" /><published>2023-07-26T00:00:00+09:00</published><updated>2023-07-27T02:00:00+09:00</updated><id>http://localhost:4000/cv/ViT</id><content type="html" xml:base="http://localhost:4000/cv/vit"><![CDATA[<h3 id="overview"><code class="language-plaintext highlighter-rouge">ğŸ”­Â Overview</code></h3>

<p>ì‹œì‘í•˜ê¸° ì•ì„œ, ë³¸ ë…¼ë¬¸ ë¦¬ë·°ë¥¼ ìˆ˜ì›”í•˜ê²Œ ì½ìœ¼ë ¤ë©´ <code class="language-plaintext highlighter-rouge">Transformer</code> ì— ëŒ€í•œ ì„ ì´í•´ê°€ í•„ìˆ˜ì ì´ë‹¤. ì•„ì§ <code class="language-plaintext highlighter-rouge">Transformer</code> ì— ëŒ€í•´ì„œ ì˜ ëª¨ë¥¸ë‹¤ë©´ í•„ìê°€ ì‘ì„±í•œ í¬ìŠ¤íŠ¸ë¥¼ ì½ê³  ì˜¤ê¸¸ ê¶Œì¥í•œë‹¤. ë˜í•œ ë³¸ë¬¸ ë‚´ìš©ì„ ì‘ì„±í•˜ë©´ì„œ ì°¸ê³ í•œ ë…¼ë¬¸ê³¼ ì—¬ëŸ¬ í¬ìŠ¤íŠ¸ì˜ ë§í¬ë¥¼ ë§¨ ë°‘ í•˜ë‹¨ì— ì²¨ë¶€í–ˆìœ¼ë‹ˆ ì°¸ê³  ë°”ë€ë‹¤. ì‹œê°„ì´ ì—†ìœ¼ì‹  ë¶„ë“¤ì€ ì¤‘ê°„ì˜ ì½”ë“œ êµ¬í˜„ë¶€ë¥¼ ìƒëµí•˜ê³  <code class="language-plaintext highlighter-rouge">Insight</code> ë¶€í„° ì½ê¸°ë¥¼ ê¶Œì¥í•œë‹¤.</p>

<p><code class="language-plaintext highlighter-rouge">Vision Transformer</code>(ì´í•˜ <code class="language-plaintext highlighter-rouge">ViT</code>)ëŠ” 2020ë…„ 10ì›” Googleì—ì„œ ë°œí‘œí•œ ì»´í“¨í„° ë¹„ì „ìš© ëª¨ë¸ì´ë‹¤. ìì—°ì–´ ì²˜ë¦¬ì—ì„œ ëŒ€ì„±ê³µì„ ê±°ë‘” íŠ¸ë ŒìŠ¤í¬ë¨¸ êµ¬ì¡°ì™€ ê¸°ë²•ì„ ê±°ì˜ ê·¸ëŒ€ë¡œ ë¹„ì „ ë¶„ì•¼ì— ì´ì‹í–ˆë‹¤ëŠ” ì ì—ì„œ í° ì˜ì˜ê°€ ìˆìœ¼ë©°, ì´í›„ ì»´í“¨í„° ë¹„ì „ ë¶„ì•¼ì˜ íŠ¸ë ŒìŠ¤í¬ë¨¸ ì „ì„±ì‹œëŒ€ê°€ ì—´ë¦¬ê²Œ ëœ ê³„ê¸°ë¡œ ì‘ìš©í•œë‹¤.</p>

<p>í•œí¸, <code class="language-plaintext highlighter-rouge">ViT</code> ì˜ ì„¤ê³„ ì² í•™ì€ ë°”ë¡œ <code class="language-plaintext highlighter-rouge">scalability(ë²”ìš©ì„±)</code>ì´ë‹¤. ì‹ ê²½ë§ ì„¤ê³„ì—ì„œ ë²”ìš©ì„±ì´ë€, ëª¨ë¸ì˜ í™•ì¥ ê°€ëŠ¥ì„±ì„ ë§í•œë‹¤. ì˜ˆë¥¼ ë“¤ë©´ í•™ìŠµ ë°ì´í„°ë³´ë‹¤ ë” í¬ê³  ë³µì¡í•œ ë°ì´í„° ì„¸íŠ¸ë¥¼ ì‚¬ìš©í•˜ê±°ë‚˜ ëª¨ë¸ì˜ íŒŒë¼ë¯¸í„°ë¥¼ ëŠ˜ë ¤ ì‚¬ì´ì¦ˆë¥¼ í‚¤ì›Œë„ ì—¬ì „íˆ ìœ íš¨í•œ ì¶”ë¡  ê²°ê³¼ë¥¼ ë„ì¶œí•˜ê±°ë‚˜ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ê³  ë‚˜ì•„ê°€ ê°œì„ ì˜ ì—¬ì§€ê°€ ì—¬ì „íˆ ë‚¨ì•„ìˆì„ ë•Œ <code class="language-plaintext highlighter-rouge">â€œí™•ì¥ì„±ì´ ë†’ë‹¤â€</code> ë¼ê³  í‘œí˜„í•œë‹¤. ì €ìë“¤ì€ ë…¼ë¬¸ ì´ˆë°˜ì— ì½• ì°ì–´ì„œ ì»´í“¨í„° ë¹„ì „ ë¶„ì•¼ì˜ <code class="language-plaintext highlighter-rouge">scalability</code> ë†’ì´ëŠ” ê²ƒì´ ì´ë²ˆ ëª¨ë¸ ì„¤ê³„ì˜ ëª©í‘œì˜€ë‹¤ê³  ë°íˆê³  ìˆë‹¤. <code class="language-plaintext highlighter-rouge">ë²”ìš©ì„±</code>ì€ ì‹ ê²½ë§ ëª¨ë¸ ì„¤ê³„ì—ì„œ ê°€ì¥ í° í™”ë‘ê°€ ë˜ëŠ”ë° ë„ë©”ì¸ë§ˆë‹¤ ì •ì˜í•˜ëŠ” ì˜ë¯¸ì— ì°¨ì´ê°€ ë¯¸ì„¸í•˜ê²Œ ì¡´ì¬í•œë‹¤. ë”°ë¼ì„œ  <code class="language-plaintext highlighter-rouge">ViT</code>ì˜ ì €ìë“¤ì´ ë§í•˜ëŠ” <code class="language-plaintext highlighter-rouge">ë²”ìš©ì„±</code>ì´ ë¬´ì—‡ì„ ì˜ë¯¸í•˜ëŠ”ì§€ ì•Œì•„ë³´ëŠ” ê²ƒì€ êµ¬ì²´ì ì¸ ëª¨ë¸ êµ¬ì¡°ë¥¼ ì´í•´í•˜ëŠ”ë° í° ë„ì›€ì´ ë  ê²ƒì´ë‹¤.</p>

<h3 id="scalability-in-vit"><code class="language-plaintext highlighter-rouge">ğŸ§ Â Scalability in ViT</code></h3>

<p>ë…¼ë¬¸ ì´ˆë°˜ë¶€ì—ì„œ ë‹¤ìŒê³¼ ê°™ì€ ë¬¸ì¥ì´ ì„œìˆ  ë˜ì–´ìˆë‹¤.</p>

<p><code class="language-plaintext highlighter-rouge">â€œOur Vision Transformer (ViT) attains excellent results when pre-trained at sufficient scale and transferred to tasks with fewer datapoints"</code></p>

<p>ì´ êµ¬ë¬¸ì´ <code class="language-plaintext highlighter-rouge">ViT</code> ì˜ <code class="language-plaintext highlighter-rouge">Scalability</code>ë¥¼ ê°€ì¥ ì˜ ì„¤ëª…í•˜ê³  ìˆë‹¤ê³  ìƒê°í•œë‹¤. ì €ìë“¤ì´ ë§í•˜ëŠ” ë²”ìš©ì„±ì€ ê²°êµ­ <code class="language-plaintext highlighter-rouge">backbone</code> êµ¬ì¡°ì˜ í™œìš©ì„ ì˜ë¯¸í•œë‹¤. ìì—°ì–´ ì²˜ë¦¬ì— ìµìˆ™í•œ ë…ìë¼ë©´ ì‰½ê²Œ ì´í•´ê°€ ê°€ëŠ¥í•  ê²ƒì´ë‹¤. <code class="language-plaintext highlighter-rouge">Transformer</code>, <code class="language-plaintext highlighter-rouge">GPT</code>, <code class="language-plaintext highlighter-rouge">BERT</code>ì˜ ë“±ì¥ ì´í›„, ìì—°ì–´ ì²˜ë¦¬ëŠ” ë²”ìš©ì„±ì„ ê°–ëŠ” ë°ì´í„° ì„¸íŠ¸ë¡œ ì‚¬ì „ í›ˆë ¨í•œ ëª¨ë¸ì„ í™œìš©í•´ <code class="language-plaintext highlighter-rouge">Task-Agnostic</code>í•˜ê²Œ í•˜ë‚˜ì˜ <code class="language-plaintext highlighter-rouge">backbone</code>ìœ¼ë¡œ ê±°ì˜ ëª¨ë“  Taskë¥¼ ìˆ˜í–‰í•  ìˆ˜ ìˆìœ¼ë©°, ì‘ì€ ì‚¬ì´ì¦ˆì˜ ë°ì´í„°ë¼ë„ ìƒë‹¹íˆ ë†’ì€ ìˆ˜ì¤€ì˜ ì¶”ë¡  ì„±ëŠ¥ì„ ë‚¼ ìˆ˜ ìˆì—ˆë‹¤. ê·¸ëŸ¬ë‚˜ ë‹¹ì‹œ ì»´í“¨í„° ë¹„ì „ì˜ ë©”ì¸ì´ì—ˆë˜ <code class="language-plaintext highlighter-rouge">Conv</code> ê¸°ë°˜ ëª¨ë¸ë“¤ì€ íŒŒì¸íŠœë‹í•´ë„ ë°ì´í„° í¬ê¸°ê°€ ì‘ìœ¼ë©´ ì¼ë°˜í™” ì„±ëŠ¥ì´ ë§¤ìš° ë–¨ì–´ì§€ê³ , Taskì— ë”°ë¼ì„œ ë‹¤ë¥¸ ì•„í‚¤í…ì²˜ë¥¼ ê°–ëŠ” ëª¨ë¸ì„ ìƒˆë¡­ê²Œ ì •ì˜í•˜ê±°ë‚˜ ë¶ˆëŸ¬ì™€ ì‚¬ìš©í•´ì•¼ í•˜ëŠ” ë²ˆê±°ë¡œì›€ì´ ìˆì—ˆë‹¤. ì˜ˆë¥¼ ë“¤ë©´ <code class="language-plaintext highlighter-rouge">Image Classfication</code> ì—ëŠ” <code class="language-plaintext highlighter-rouge">ResNet</code>, <code class="language-plaintext highlighter-rouge">Segmentation</code> ì—ëŠ” <code class="language-plaintext highlighter-rouge">U-Net</code>, <code class="language-plaintext highlighter-rouge">Object Detection</code> ì€ <code class="language-plaintext highlighter-rouge">YOLO</code> ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì²˜ëŸ¼ ë§ì´ë‹¤. ë°˜ë©´ ìì—°ì–´ ì²˜ë¦¬ëŠ” ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ í•˜ë‚˜ë¡œ ëª¨ë“  NLU, ì‹¬ì§€ì–´ëŠ” NLG Taskë„ ìˆ˜í–‰í•  ìˆ˜ ìˆë‹¤. ì €ìë“¤ì€ ì´ëŸ¬í•œ ë²”ìš©ì„±ì„ ì»´í“¨í„° ë¹„ì „ì—ë„ ì´ì‹ ì‹œí‚¤ê³  ì‹¶ì—ˆë˜ ê²ƒ ê°™ë‹¤. ê·¸ë ‡ë‹¤ë©´ ë¨¼ì € ìì—°ì–´ ì²˜ë¦¬ì—ì„œ íŠ¸ëœìŠ¤í¬ë¨¸ ê³„ì—´ì´ ë²”ìš©ì„±ì„ ê°€ì§ˆ ìˆ˜ ìˆì—ˆë˜ ì´ìœ ëŠ” ë¬´ì—‡ì¸ì§€ ê°„ë‹¨íˆ ì‚´í´ë³´ì.</p>

<p>ì €ìë“¤ì€ <code class="language-plaintext highlighter-rouge">self-attention</code>(ë‚´ì )ì˜ íš¨ìœ¨ì„±, ëª¨ë¸ì˜ êµ¬ì¡°ì  íƒì›”ì„± ê·¸ë¦¬ê³  <code class="language-plaintext highlighter-rouge">self-supervised task</code>ì˜ ì¡´ì¬ë¥¼ ê¼½ëŠ”ë‹¤. ê·¸ëŸ¼ ì´ê²ƒë“¤ì´ ì™œ ë²”ìš©ì„±ì„ ë†’ì´ëŠ”ë° ë„ì›€ì´ ë ê¹Œ??</p>

<p><code class="language-plaintext highlighter-rouge">self-attention(ë‚´ì )</code>ì€ í–‰ë ¬ ê°„ ê³±ì…‰ìœ¼ë¡œ ì •ì˜ ë˜ì–´ ì„¤ê³„ê°€ ë§¤ìš° ê°„í¸í•˜ê³  ë³‘ë ¬ë¡œ í•œë²ˆì— ì²˜ë¦¬í•˜ëŠ” ê²ƒì´ ê°€ëŠ¥í•˜ê¸° ë•Œë¬¸ì— íš¨ìœ¨ì ìœ¼ë¡œ ì „ì²´ ë°ì´í„°ë¥¼ ëª¨ë‘ ê³ ë ¤í•œ ì—°ì‚° ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆë‹¤.</p>

<p><code class="language-plaintext highlighter-rouge">Multi-Head Attention</code> êµ¬ì¡°ëŠ” ì—¬ëŸ¬ ì°¨ì›ì˜ ì˜ë¯¸ ê´€ê³„ë¥¼ ë™ì‹œì— í¬ì°©í•˜ê³  ê·¸ê²ƒì„ ì•™ìƒë¸”í•œ ê²ƒê³¼ ê°™ì€(ì‹¤ì œë¡œëŠ” MLP) ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆë‹¤ëŠ” ì ì—ì„œ êµ¬ì¡°ì ìœ¼ë¡œ íƒì›”í•˜ë‹¤.</p>

<p>ë§ˆì§€ë§‰ìœ¼ë¡œ <code class="language-plaintext highlighter-rouge">MLM</code>, <code class="language-plaintext highlighter-rouge">Auto-Regression(LM) Task</code>ëŠ” ë°ì´í„° ì„¸íŠ¸ì— ë³„ë„ì˜ ì¸ê°„ì˜ ê°œì…(ë¼ë²¨ë§)ì´ í•„ìš”í•˜ì§€ ì•Šê¸° ë•Œë¬¸ì— ê°€ì„±ë¹„ ìˆê²Œ ë°ì´í„°ì™€ ëª¨ë¸ì˜ ì‚¬ì´ì¦ˆë¥¼ ëŠ˜ë¦´ ìˆ˜ ìˆê²Œ ëœë‹¤.<br />
ì´ì œ ë…¼ë¬¸ì—ì„œ íŠ¸ëœìŠ¤í¬ë¨¸ ê³„ì—´ì´ ê°€ì§„ ë²”ìš©ì„±ì„ ì–´ë–»ê²Œ ë¹„ì „ ë¶„ì•¼ì— ì ìš©í–ˆëŠ”ì§€ ì£¼ëª©í•˜ë©´ì„œ ëª¨ë¸ êµ¬ì¡°ë¥¼ í•˜ë‚˜ í•˜ë‚˜ ì‚´í´ë³´ì.</p>

<h3 id="modeling"><code class="language-plaintext highlighter-rouge">ğŸŒŸÂ Modeling</code></h3>

<p align="center">
<img src="/assets/images/vision_transformer/modeling_overview.png" alt="ViT Model Structure" class="align-center image-caption" width="100%&quot;, height=&quot;50%" />
<strong><em><a href="https://arxiv.org/abs/2010.11929">ViT Model Structure</a></em></strong>
</p>

<ul>
  <li><strong>1) Transfer <code class="language-plaintext highlighter-rouge">Scalability</code> from pure <code class="language-plaintext highlighter-rouge">Transformer</code> to Computer Vision</strong>
    <ul>
      <li><strong>Overcome <code class="language-plaintext highlighter-rouge">reliance</code> on Convolution(<code class="language-plaintext highlighter-rouge">Inductive Bias</code>) in Computer Vision</strong></li>
      <li><strong>Apply Self-Attention &amp; Architecture from vanilla NLP Transformers as <code class="language-plaintext highlighter-rouge">closely</code> as possible</strong></li>
      <li><strong>Treat Image as sequence of text token</strong></li>
      <li><strong>Make $P$ sub-patches from whole image, playing same role as token in NLP Transformer</strong></li>
    </ul>
  </li>
</ul>

<p>ì €ìë“¤ì€ ë¨¼ì € <code class="language-plaintext highlighter-rouge">Conv</code> ì— ëŒ€í•œ ì˜ì¡´ì„ ë²„ë¦´ ê²ƒì„ ì£¼ì¥í•œë‹¤. <code class="language-plaintext highlighter-rouge">Conv</code>ê°€ ê°€ì§„ <code class="language-plaintext highlighter-rouge">Inductive Bias</code> ë•Œë¬¸ì— íŒŒì¸íŠœë‹ ë ˆë²¨ì—ì„œ ë°ì´í„° í¬ê¸°ê°€ ì‘ìœ¼ë©´ ì¼ë°˜í™” ì„±ëŠ¥ì´ ë–¨ì–´ì§€ëŠ” ê²ƒì´ë¼ê³  ì„¤ëª…í•˜ê³  ìˆë‹¤. ì´ ë§ì„ ì´í•´í•˜ë ¤ë©´ <code class="language-plaintext highlighter-rouge">Inductive Bias</code>ì— ëŒ€í•´ì„œ ë¨¼ì € ì•Œì•„ì•¼ í•œë‹¤. <code class="language-plaintext highlighter-rouge">Inductive Bias</code>ë€, ì£¼ì–´ì§„ ë°ì´í„°ë¡œë¶€í„° ì¼ë°˜í™” ì„±ëŠ¥ì„ ë†’ì´ê¸° ìœ„í•´ <code class="language-plaintext highlighter-rouge">â€˜ì…ë ¥ë˜ëŠ” ë°ì´í„°ëŠ” ~ í•  ê²ƒì´ë‹¤â€™</code>, <code class="language-plaintext highlighter-rouge">â€˜ì´ëŸ° íŠ¹ì§•ì„ ê°–ê³  ìˆì„ ê²ƒì´ë‹¤â€™</code>ì™€ ê°™ì€ ê°€ì •, ê°€ì¤‘ì¹˜, ê°€ì„¤ ë“±ì„ ê¸°ê³„í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ì— ì ìš©í•˜ëŠ” ê²ƒì„ ë§í•œë‹¤.</p>

<p><code class="language-plaintext highlighter-rouge">Conv</code> ì—°ì‚° ìì²´ (ê°€ì¤‘ì¹˜ ê³µìœ , í’€ë§ ìˆëŠ” <code class="language-plaintext highlighter-rouge">Conv Block</code>ì´ <code class="language-plaintext highlighter-rouge">Invariance</code>)ì˜ ê¸°ë³¸ ê°€ì •ì€ <code class="language-plaintext highlighter-rouge">translation equivariance</code>, <code class="language-plaintext highlighter-rouge">locality</code>ì´ë‹¤. ì‚¬ì‹¤ ì €ìì˜ ì£¼ì¥ì„ ì´í•´í•˜ëŠ”ë° <code class="language-plaintext highlighter-rouge">equivariance</code>ì™€ <code class="language-plaintext highlighter-rouge">locality</code>ì˜ ëœ»ì´ ë¬´ì—‡ì¸ì§€ íŒŒì•…í•˜ëŠ” ê²ƒì€ í¬ê²Œ ì˜ë¯¸ê°€ ì—†ë‹¤ (<code class="language-plaintext highlighter-rouge">equivariance</code>ì™€ <code class="language-plaintext highlighter-rouge">invariance</code>ì— ëŒ€í•´ì„œëŠ” ë‹¤ë¥¸ í¬ìŠ¤íŒ…ì—ì„œ ìì„¸íˆ ì‚´í´ë³´ë„ë¡ í•˜ê² ë‹¤). <strong><u>ì¤‘ìš”í•œ ê²ƒì€ ì…ë ¥ ë°ì´í„°ì— ê°€ì •ì„ ë”í•œë‹¤ëŠ” ì ì´ë‹¤.</u></strong> ë§Œì•½ ì£¼ì–´ì§„ ì…ë ¥ì´ ë¯¸ë¦¬ ê°€ì •í•œ <code class="language-plaintext highlighter-rouge">Inductive Bias</code> ì— ë²—ì–´ë‚œë‹¤ë©´ ì–´ë–»ê²Œ ë ê¹Œ??</p>

<p>ì•„ë§ˆ ì˜¤ë²„í”¼íŒ… ë˜ê±°ë‚˜ ëª¨ë¸ í•™ìŠµì´ ìˆ˜ë ´ì„±ì„ ê°–ì§€ ëª»í•˜ê²Œ ë  ê²ƒì´ë‹¤. ì´ë¯¸ì§€ ë°ì´í„°ë„ Taskì— ë”°ë¼ í•„ìš”í•œ <code class="language-plaintext highlighter-rouge">Inductive Bias</code>ê°€ ë‹¬ë¼ì§„ë‹¤. ì˜ˆë¥¼ ë“¤ì–´ <code class="language-plaintext highlighter-rouge">Segmentation</code>, <code class="language-plaintext highlighter-rouge">Detection</code> ì˜ ê²½ìš°ëŠ” ì´ë¯¸ì§€ ì† ê°ì²´ì˜ ìœ„ì¹˜, í”½ì…€ ì‚¬ì´ì˜ <code class="language-plaintext highlighter-rouge">spatial variance</code> ì •ë³´ê°€ ë§¤ìš° ì¤‘ìš”í•˜ë‹¤. í•œí¸, <code class="language-plaintext highlighter-rouge">Classification</code>ì€ <code class="language-plaintext highlighter-rouge">spatial invariance</code>ê°€ ì¤‘ìš”í•˜ë‹¤. ëª©í‘œ ê°ì²´ì˜ ìœ„ì¹˜ì™€ ì£¼ë³€ íŠ¹ì§•ë³´ë‹¤ íƒ€ê²Ÿ ìì²´ë¥¼ ì‹ ê²½ë§ì´ ì¸ì‹í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•˜ê¸° ë•Œë¬¸ì´ë‹¤. ë”°ë¼ì„œ  <code class="language-plaintext highlighter-rouge">ViT</code> ì €ìë“¤ì€ ì–´ë–¤ Biasë˜ ìƒê´€ì—†ì´ í¸í–¥ì„ ê°–ê³  ë°ì´í„°ë¥¼ ë³¸ë‹¤ëŠ” ê²ƒ ìì²´ì— ì˜ë¬¸ì„ í‘œí•˜ë©°, ì´ë¯¸ì§€ ì—­ì‹œ <code class="language-plaintext highlighter-rouge">Inductive Bias</code>ì—ì„œ ë²—ì–´ë‚˜, ì£¼ì–´ì§„ ë°ì´í„° ì „ì²´ íŠ¹ì§•(íŒ¨ì¹˜) ì‚¬ì´ì˜ ê´€ê³„ë¥¼ íŒŒì•…í•˜ëŠ” ê³¼ì •ì—ì„œ <code class="language-plaintext highlighter-rouge">scalability</code>ë¥¼ íšë“í•  ìˆ˜ ìˆë‹¤ê³  ì£¼ì¥í•œë‹¤.</p>

<p>ê·¸ë˜ì„œ <code class="language-plaintext highlighter-rouge">Conv</code>ì˜ ëŒ€ì•ˆìœ¼ë¡œ ìƒëŒ€ì ìœ¼ë¡œ <code class="language-plaintext highlighter-rouge">Inductive Bias</code> ê°€ ë¶€ì¡±í•œ <code class="language-plaintext highlighter-rouge">Self-Attention</code>, <code class="language-plaintext highlighter-rouge">Transformer Architecture</code>ë¥¼ ì‚¬ìš©í•œë‹¤. ë‘ê°€ì§€ì˜ íš¨ìš©ì„±ì— ëŒ€í•´ì„œëŠ” ì´ë¯¸ ìœ„ì—ì„œ ì–¸ê¸‰í–ˆê¸° ë•Œë¬¸ì— ìƒëµí•˜ê³ , ì—¬ê¸°ì„œ ì§šê³  ë„˜ì–´ê°€ì•¼í•  ì ì€ <code class="language-plaintext highlighter-rouge">Self-Attention</code>ì´ <code class="language-plaintext highlighter-rouge">Conv</code> ëŒ€ë¹„ <code class="language-plaintext highlighter-rouge">Inductive Bias</code>ê°€ ì ë‹¤ëŠ” ì ì´ë‹¤. Self-Attention ê³¼ì •ì—ëŠ” ì—¬ëŸ¬ ì—°ì‚°, ìŠ¤ì¼€ì¼ ì¡°ì •ê°’ë“¤ì´ í¬í•¨ë˜ì§€ë§Œ ë³¸ì§ˆì ìœ¼ë¡œ <code class="language-plaintext highlighter-rouge">â€œë‚´ì â€</code> ì´ ì¤‘ì‹¬ì´ë‹¤. ë‚´ì ì€ ê·¸ ì–´ë–¤ í¸í–¥ (<code class="language-plaintext highlighter-rouge">Conv</code>ì™€ ëŒ€ì¡°í•˜ë ¤ê³  ì´ë ‡ê²Œ ì„œìˆ í–ˆì§€ë§Œ ì‚¬ì‹¤ <code class="language-plaintext highlighter-rouge">Position Embedding</code> ë”í•˜ëŠ” ê²ƒë„ ì¼ì¢…ì˜ ì•½í•œ <code class="language-plaintext highlighter-rouge">Inductive Bias</code>)ì´ ì¡´ì¬í•˜ì§€ ì•ŠëŠ”ë‹¤. ì¼ë‹¨ ì£¼ì–´ì§„ ëª¨ë“  ë°ì´í„°ì— ëŒ€í•´ì„œ ë‚´ì ê°’ì„ ì‚°ì¶œí•˜ê³  ê·¸ ë‹¤ìŒì— ê´€ê³„ê°€ ìˆë‹¤ê³  ìƒê°ë˜ëŠ” ì •ë³´ë¥¼ ì¶”ë¦¬ê¸° ë•Œë¬¸ì´ë‹¤. <code class="language-plaintext highlighter-rouge">Conv</code> ë•Œì™€ ë‹¬ë¦¬ <code class="language-plaintext highlighter-rouge">â€˜ì…ë ¥ë˜ëŠ” ë°ì´í„°ëŠ” ~ í•  ê²ƒì´ë‹¤â€™</code>, <code class="language-plaintext highlighter-rouge">â€˜ì´ëŸ° íŠ¹ì§•ì„ ê°–ê³  ìˆì„ ê²ƒì´ë‹¤â€™</code> ë¼ëŠ” ê°€ì •ì´ ì—†ë‹¤. ì´ë²ˆ í¬ìŠ¤íŒ…ì˜ ë§ˆì§€ë§‰ ì¯¤ì—ì„œ ë‹¤ì‹œ ë‹¤ë£¨ê² ì§€ë§Œ ê·¸ë˜ì„œ <code class="language-plaintext highlighter-rouge">ViT</code>ëŠ” ì¸ìŠ¤í„´ìŠ¤ ì‚¬ì´ì˜ ëª¨ë“  ê´€ê³„ë¥¼ ë½‘ì•„ë³´ëŠ” <code class="language-plaintext highlighter-rouge">Self-Attention(ë‚´ì )</code> ì„ ê¸°ë°˜ìœ¼ë¡œ ë§Œë“¤ì–´ì¡Œê¸° ë•Œë¬¸ì— ì´ë¯¸ì§€ì˜ <code class="language-plaintext highlighter-rouge">Global Information</code>ì„ í¬ì°©í•˜ëŠ”ë° íƒì›”í•œ ì„±ëŠ¥ì„ ë³´ì´ê³ , <code class="language-plaintext highlighter-rouge">Conv</code> ëŠ” <strong><u>â€œì¤‘ìš”í•œ ì •ë³´ëŠ” ê·¼ì²˜ í”½ì…€ì— ëª°ë ¤ìˆë‹¤ë¼ëŠ”â€</u></strong> <code class="language-plaintext highlighter-rouge">Inductive Bias</code>  ë•ë¶„ì— <code class="language-plaintext highlighter-rouge">Local Information</code>ì„ í¬ì°©í•˜ëŠ”ë° íƒì›”í•œ ì„±ëŠ¥ì„ ë‚¸ë‹¤.</p>

<p>ê·¸ë ‡ë‹¤ë©´ í”½ì…€ í•˜ë‚˜ í•˜ë‚˜ë¼ë¦¬ ë‚´ì í•´ì¤€ë‹¤ëŠ” ê²ƒì¼ê¹Œ?? ì•„ë‹ˆë‹¤ ì—¬ê¸°ì„œ ë…¼ë¬¸ì˜ ì œëª©ì´ <code class="language-plaintext highlighter-rouge">An Image Is Worth 16x16 Words</code> ì¸ ì´ìœ ê°€ ë“œëŸ¬ë‚œë‹¤. ì¼ë‹¨ í”½ì…€ í•˜ë‚˜ í•˜ë‚˜ë¼ë¦¬ ìœ ì‚¬ë„ë¥¼ ì¸¡ì •í•˜ëŠ” ê²ƒì´ ìœ ì˜ë¯¸í• ê¹Œ ìƒê°í•´ë³´ì. ìì—°ì–´ì˜ í† í°ê³¼ ë‹¬ë¦¬ ì´ë¯¸ì§€ì˜ ë‹¨ì¼ í”½ì…€ í•œ ê°œëŠ” í° ì¸ì‚¬ì´íŠ¸ë¥¼ ì–»ê¸° í˜ë“¤ë‹¤. í”½ì…€ì€ ë§ ê·¸ëŒ€ë¡œ ì  í•˜ë‚˜ì¼ ë¿ì´ë‹¤. í”½ì…€ì„ ì—¬ëŸ¬ ê°œ ë¬¶ì–´ íŒ¨ì¹˜ ë‹¨ìœ„ë¡œ ë¬¶ëŠ”ë‹¤ë©´ ì´ì•¼ê¸°ëŠ” ë‹¬ë¼ì§„ë‹¤. ì¼ì • í¬ê¸° ì´ìƒì˜ íŒ¨ì¹˜ë¼ë©´ ìì—°ì–´ì˜ í† í°ì²˜ëŸ¼ ê·¸ ìì²´ë¡œ ì–´ë–¤ ì˜ë¯¸ë¥¼ ë‹´ì„ ìˆ˜ ìˆë‹¤. ë”°ë¼ì„œ ì €ìëŠ” ì „ì²´ ì´ë¯¸ì§€ë¥¼ ì—¬ëŸ¬ ê°œì˜ 16x16 í˜¹ì€ 14x14 ì‚¬ì´ì¦ˆ íŒ¨ì¹˜ë¡œ ë‚˜ëˆ„ì–´ í•˜ë‚˜ í•˜ë‚˜ë¥¼ í† í°ìœ¼ë¡œ ê°„ì£¼í•´ ì´ë¯¸ì§€ ì‹œí€€ìŠ¤ë¥¼ ë§Œë“¤ê³  ê·¸ê²ƒì„ ëª¨ë¸ì˜ Inputìœ¼ë¡œ ì‚¬ìš©í•œë‹¤.</p>

<p align="center">
<img src="/assets/images/vision_transformer/class_diagram.png" alt="Class Diagram" class="align-center image-caption" width="50%&quot;, height=&quot;50%" />
<strong><em>Class Diagram</em></strong>
</p>

<p>ëª¨ë¸ êµ¬ì¡°ì˜ ë¼ˆëŒ€ê°€ ë˜ëŠ” ë‚´ìš©ë“¤ì„ ëª¨ë‘ ì‚´í´ë³´ì•˜ê³ , ìœ„ì—ì„œ ì„œìˆ í•œ ë‚´ìš©ì„ êµ¬í˜„í•˜ê¸° ìœ„í•´ ì–´ë–¤ ë¸”ë¡ë“¤ì„ ì‚¬ìš©í–ˆëŠ”ì§€ í•„ìê°€ ì§ì ‘ ë…¼ë¬¸ì„ ë³´ê³  ë”°ë¼ êµ¬í˜„í•œ ì½”ë“œì™€ í•¨ê»˜ ì•Œì•„ë³´ë„ë¡ í•˜ì. ìœ„ì— ì²¨ë¶€í•œ ëª¨ë¸ ëª¨ì‹ë„ì— ë‚˜ì™€ ìˆëŠ” ë¸”ë¡ë“¤ í•˜ë‚˜ í•˜ë‚˜ ì‚´í´ë³¼ ì˜ˆì •ì´ë‹¤. ì—¬ë‹´ìœ¼ë¡œ Google Researchì˜ Official Repo ì—­ì‹œ í•¨ê»˜ ì°¸ê³ í–ˆëŠ”ë°, ì½”ë“œê°€ ëª¨ë‘ êµ¬ê¸€ì´ ìš”ìƒˆ ìƒˆë¡­ê²Œ ë¯¸ëŠ” <code class="language-plaintext highlighter-rouge">Jax</code>, <code class="language-plaintext highlighter-rouge">Flax</code> ë¡œ êµ¬í˜„ ë˜ì–´ ìˆì—ˆë‹¤. íŒŒì´í† ì¹˜ë‚˜ ì¢€ ì¨ë³¸ í•„ì ì…ì¥ì—ì„œëŠ” ì •ë§ â€¦ ì§€ì˜¥ë¶ˆì„ ê²½í—˜í–ˆë‹¤. ì˜¤ëŠ˜ë„ ë‹¤ì‹œ í•œ ë²ˆ í˜ì´ìŠ¤ë¶ íŒŒì´í† ì¹˜ ê°œë°œíŒ€ì— í°ì ˆ ë“œë¦¬ê³  ì‹¶ë‹¤.</p>

<h4 id="linear-projection-of-flattened-patches"><code class="language-plaintext highlighter-rouge">ğŸ”¬Â Linear Projection of Flattened Patches</code></h4>

\[x_p \in R^{N * (P^2â€¢C)}\]

\[z_{0} = [x_{class}; x_p^1E;x_p^2E;x_p^3E....x_p^NE]\]

\[N = \frac{H*W}{P*P}\]

<p><code class="language-plaintext highlighter-rouge">ViT</code>ì˜ ì…ë ¥ ì„ë² ë”©ì„ ìƒì„±í•˜ëŠ” ì—­í• ì„ í•œë‹¤. <code class="language-plaintext highlighter-rouge">ViT</code>ëŠ” $x \in R^{H * W * C}$(H: height, W: width, C: channel)ì˜ í˜•ìƒì„ ê°–ëŠ” ì´ë¯¸ì§€ë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ì•„ ê°€ë¡œ ì„¸ë¡œ ê¸¸ì´ê°€ $P$, ì±„ë„ ê°œìˆ˜ $C$ì¸ $N$ê°œì˜ íŒ¨ì¹˜ë¡œ <code class="language-plaintext highlighter-rouge">reshape</code> í•œë‹¤. í•„ìê°€ ì½”ë“œ êµ¬í˜„ ì¤‘ ê°€ì¥ í˜¼ë™í•œ ë¶€ë¶„ì´ ë°”ë¡œ íŒ¨ì¹˜ ê°œìˆ˜ $N$ì´ì—ˆë‹¤. ì§ê´€ì ìœ¼ë¡œ íŒ¨ì¹˜ ê°œìˆ˜ë¼ê³  í•˜ë©´, ì „ì²´ ì´ë¯¸ì§€ ì‚¬ì´ì¦ˆì—ì„œ íŒ¨ì¹˜ í¬ê¸°ë¥¼ ë‚˜ëˆˆ ê°’ì´ë¼ê³  ìƒê°í•˜ê¸° ì‰½ê¸° ë•Œë¬¸ì´ë‹¤. ì˜ˆë¥¼ ë“¤ë©´ <code class="language-plaintext highlighter-rouge">512x512</code>ì§œë¦¬ ì´ë¯¸ì§€ë¥¼ <code class="language-plaintext highlighter-rouge">16x16</code> ì‚¬ì´ì¦ˆì˜ íŒ¨ì¹˜ë¡œ ë‚˜ëˆˆë‹¤ê³  í•´ë³´ì. í•„ìëŠ” ë‹¨ìˆœíˆ <code class="language-plaintext highlighter-rouge">512/16=32</code> ë¼ëŠ” ê²°ê³¼ë¥¼ ì´ìš©í•´ $N=32$ë¡œ ì„¤ì •í•˜ê³  ì‹¤í—˜ì„ ì§„í–‰í•˜ë‹¤ê°€ í…ì„œ ì°¨ì›ì´ ë§ì§€ ì•Šì•„ ë°œìƒí•˜ëŠ” ì—ëŸ¬ ë¡œê·¸ë¥¼ ë§ˆì£¼í–ˆì—ˆë‹¤. ê·¸ëŸ¬ë‚˜ ë…¼ë¬¸ ì† ìˆ˜ì‹ì„ í™•ì¸í•´ë³´ë©´,  $H * W / P^2$ì´ ë°”ë¡œ íŒ¨ì¹˜ ê°œìˆ˜$N$ìœ¼ë¡œ ì •ì˜ëœë‹¤. ê·¸ë˜ì„œ ë§Œì•½ <code class="language-plaintext highlighter-rouge">512x512</code> ì‚¬ì´ì¦ˆì˜ <code class="language-plaintext highlighter-rouge">RGB</code> ì´ë¯¸ì§€ <code class="language-plaintext highlighter-rouge">10ì¥</code>ì„ ViT ì…ë ¥ ì„ë² ë”©ì— ë§ê²Œ ì°¨ì› ë³€í™˜í•œë‹¤ë©´ ê²°ê³¼ëŠ” <code class="language-plaintext highlighter-rouge">[10, 3, 1024, 768]</code> ì´ ë  ê²ƒì´ë‹¤. (ì´ ì˜ˆì‹œë¥¼ ì•ìœ¼ë¡œ ê³„ì† ì´ìš©í•˜ê² ë‹¤)</p>

<p>ì´ë ‡ê²Œ ì°¨ì›ì„ ë°”ê¿”ì¤€ ì´ë¯¸ì§€ë¥¼ <code class="language-plaintext highlighter-rouge">nn.Linear((channels * patch_size**2), dim_model)</code> ë¥¼ í†µí•´ <code class="language-plaintext highlighter-rouge">ViT</code>ì˜ ì„ë² ë”© ë ˆì´ì–´ì— ì„ í˜• íˆ¬ì˜í•´ì¤€ë‹¤. ì—¬ê¸°ì„œ ìì—°ì–´ ì²˜ë¦¬ì™€ íŒŒì´í† ì¹˜ë¥¼ ìì£¼ ì‚¬ìš©í•˜ì‹œëŠ” ë…ìë¼ë©´ ì™œ <code class="language-plaintext highlighter-rouge">nn.Embedding</code>ì„ ì‚¬ìš©í•˜ì§€ ì•Šì•˜ëŠ”ê°€ ì˜ë¬¸ì„ ê°€ì§ˆ ìˆ˜ ìˆë‹¤.</p>

<p>ìì—°ì–´ ì²˜ë¦¬ì—ì„œ ì…ë ¥ ì„ë² ë”©ì„ ë§Œë“¤ë•ŒëŠ” ëª¨ë¸ì˜ í† í¬ë‚˜ì´ì €ì— ì˜í•´ ì‚¬ì „ ì •ì˜ëœ vocabì˜ ì‚¬ì´ì¦ˆê°€ ì…ë ¥ ë¬¸ì¥ì— ì†í•œ í† í° ê°œìˆ˜ë³´ë‹¤ í›¨ì”¬ í¬ê¸° ë•Œë¬¸ì— ë°ì´í„° ë£©ì—… í…Œì´ë¸” ë°©ì‹ì˜ <code class="language-plaintext highlighter-rouge">nn.Embedding</code> ì„ ì‚¬ìš©í•˜ê²Œ ëœë‹¤. ì´ê²Œ ë¬´ìŠ¨ ë§ì´ëƒë©´, í† í¬ë‚˜ì´ì €ì— ì˜í•´ ì‚¬ì „ì— ì •ì˜ëœ <code class="language-plaintext highlighter-rouge">vocab</code> ì „ì²´ê°€ <code class="language-plaintext highlighter-rouge">nn.Embedding(vocab_size, dim_model)</code>ë¡œ íˆ¬ì˜ ë˜ì–´ ê°€ë¡œëŠ” vocab ì‚¬ì´ì¦ˆ, ì„¸ë¡œëŠ” ëª¨ë¸ì˜ ì°¨ì› í¬ê¸°ì— í•´ë‹¹í•˜ëŠ” ë£©ì—… í…Œì´ë¸”ì´ ìƒì„±ë˜ê³ , ë‚´ê°€ ì…ë ¥í•œ í† í°ë“¤ì€ ì „ì²´ <code class="language-plaintext highlighter-rouge">vocab</code>ì˜ ì¼ë¶€ë¶„ì¼í…Œë‹ˆ ì „ì²´ ì„ë² ë”© ë£©ì—… í…Œì´ë¸”ì—ì„œ ë‚´ê°€ ì„ë² ë”©í•˜ê³  ì‹¶ì€ í† í°ë“¤ì˜ ì¸ë±ìŠ¤ë§Œ ì•Œì•„ë‚¸ë‹¤ëŠ” ê²ƒì´ë‹¤.</p>

<p>ê·¸ë˜ì„œ <code class="language-plaintext highlighter-rouge">nn.Embedding</code> ì— ì •ì˜ëœ ì°¨ì›ê³¼ ì‹¤ì œ ì…ë ¥ ë°ì´í„°ì˜ ì°¨ì›ì´ ë§ì§€ ì•Šì•„ë„ í•¨ìˆ˜ê°€ ë™ì‘í•˜ê²Œ ë˜ëŠ” ê²ƒì´ë‹¤. ê·¸ëŸ¬ë‚˜ ë¹„ì „ì˜ ê²½ìš°, ì‚¬ì „ì— ì •ì˜ëœ <code class="language-plaintext highlighter-rouge">vocab</code>ì´ë¼ëŠ” ê°œë…ì´ ì „í˜€ ì—†ê³  ì…ë ¥ ì´ë¯¸ì§€ ì—­ì‹œ í•­ìƒ ê³ ì •ëœ í¬ê¸°ì˜ ì°¨ì›ìœ¼ë¡œ ë“¤ì–´ì˜¤ê¸° ë•Œë¬¸ì— <code class="language-plaintext highlighter-rouge">nn.Embedding</code>ì´ ì•„ë‹Œ  <code class="language-plaintext highlighter-rouge">nn.Linear</code> ì„ ì‚¬ìš©í•´ ê³§ë°”ë¡œ ì„ í˜• íˆ¬ì˜ì„ êµ¬í˜„í•œ ê²ƒì´ë‹¤. ë‘ ë©”ì„œë“œì— ëŒ€í•œ ìì„¸í•œ ë¹„êµëŠ” íŒŒì´í† ì¹˜ ê´€ë ¨ í¬ìŠ¤íŠ¸ì—ì„œ ë‹¤ì‹œ í•œ ë²ˆ ìì„¸íˆ ë‹¤ë£¨ë„ë¡ í•˜ê² ë‹¤.</p>

<p>í•œí¸, <code class="language-plaintext highlighter-rouge">Position Embedding</code>ì„ ë”í•˜ê¸° ì „, <code class="language-plaintext highlighter-rouge">Input Embedding</code>ì˜ ì°¨ì›ì€ <code class="language-plaintext highlighter-rouge">[10, 1024, 1024]</code> ì´ ëœë‹¤. ì§€ê¸ˆê¹Œì§€ ì„¤ëª…í•œ ë¶€ë¶„(<code class="language-plaintext highlighter-rouge">Linear Projection of Flattened Patches</code> )ì„ íŒŒì´í† ì¹˜ ì½”ë“œë¡œ êµ¬í˜„í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">VisionTransformer</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="p">...</span>
    <span class="n">ì¤‘ëµ</span>
    <span class="p">...</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">num_patches</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">image_size</span> <span class="o">/</span> <span class="n">patch_size</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">input_embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">((</span><span class="n">channels</span> <span class="o">*</span> <span class="n">patch_size</span><span class="o">**</span><span class="mi">2</span><span class="p">),</span> <span class="n">dim_model</span><span class="p">)</span> <span class="c1"># Projection Layer for Input Embedding
</span>    <span class="p">...</span>
    <span class="n">ì¤‘ëµ</span>
    <span class="p">...</span>  
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">any</span><span class="p">:</span>
        <span class="s">""" For cls pooling """</span>
        <span class="k">assert</span> <span class="n">inputs</span><span class="p">.</span><span class="n">ndim</span> <span class="o">!=</span> <span class="mi">4</span><span class="p">,</span> <span class="sa">f</span><span class="s">"Input shape should be [BS, CHANNEL, IMAGE_SIZE, IMAGE_SIZE], but got </span><span class="si">{</span><span class="n">inputs</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="s">"</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">inputs</span> 
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">input_embedding</span><span class="p">(</span>
            <span class="n">x</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="p">.</span><span class="n">num_patches</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">patch_size</span><span class="o">**</span><span class="mi">2</span> <span class="o">*</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span> <span class="c1"># Projection Layer for Input Embedding
</span>        <span class="p">)</span>
        <span class="n">cls_token</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">,</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>  <span class="c1"># can change init method
</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cat</span><span class="p">([</span><span class="n">cls_token</span><span class="p">,</span> <span class="n">x</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="p">...</span>
</code></pre></div></div>

<p>ì„ë² ë”© ë ˆì´ì–´ë¥¼ ê°ì²´ë¡œ ë”°ë¡œ êµ¬í˜„í•´ë„ ë˜ì§€ë§Œ, í•„ìëŠ” êµ³ì´ ì¶”ìƒí™”ê°€ í•„ìš”í•˜ì§€ ì•Šë‹¤ê³  ìƒê°í•´ ViTì˜ ìµœìƒìœ„ í´ë˜ìŠ¤ì¸ <code class="language-plaintext highlighter-rouge">VisionTransformer</code>ì˜ <code class="language-plaintext highlighter-rouge">forward</code> ë©”ì„œë“œ ë§¨ ì´ˆë°˜ë¶€ì— êµ¬í˜„í•˜ê²Œ ë˜ì—ˆë‹¤. ì…ë ¥ ë°›ì€ ì´ë¯¸ì§€ í…ì„œë¥¼ <code class="language-plaintext highlighter-rouge">torch.reshape</code> ì„ í†µí•´ <code class="language-plaintext highlighter-rouge">[íŒ¨ì¹˜ ê°œìˆ˜, í”½ì…€ê°œìˆ˜*ì±„ë„ê°œìˆ˜]</code> ë¡œ ë°”ê¾¼ ë’¤, ë¯¸ë¦¬ ì •ì˜í•´ë‘” <code class="language-plaintext highlighter-rouge">self.input_embedding</code> ì— ë§¤ê°œë³€ìˆ˜ë¡œ ì „ë‹¬í•´ <code class="language-plaintext highlighter-rouge">â€œìœ„ì¹˜ ì„ë² ë”©â€</code> ê°’ì´ ë”í•´ì§€ê¸° ì „ <code class="language-plaintext highlighter-rouge">Input Embedding</code>ì„ ë§Œë“ ë‹¤.</p>

<p>í•œí¸, <code class="language-plaintext highlighter-rouge">CLS Pooling</code>ì„ ìœ„í•´ ë§ˆì§€ë§‰ì— <code class="language-plaintext highlighter-rouge">[batch, 1, image_size]</code> ì˜ ì°¨ì›ì„ ê°–ëŠ” <code class="language-plaintext highlighter-rouge">cls_token</code> ì„ ì •ì˜í•´ íŒ¨ì¹˜ ì‹œí€€ìŠ¤ì™€ <code class="language-plaintext highlighter-rouge">concat</code> (ë§¨ ì•ì—)í•´ì¤€ë‹¤. ì´ ë•Œ ë…¼ë¬¸ì— ì œì‹œëœ ìˆ˜ì‹ ìƒ, <code class="language-plaintext highlighter-rouge">CLS Token</code>ì€ ì„ í˜• íˆ¬ì˜í•˜ì§€ ì•Šìœ¼ë©°, íŒ¨ì¹˜ ì‹œí€€ìŠ¤ì— ì„ í˜• íˆ¬ì˜ì´ ì´ë¤„ì§€ê³  ë‚œ ë’¤ì— ë§¨ ì•ì— <code class="language-plaintext highlighter-rouge">Concat</code> í•˜ê²Œ ëœë‹¤.</p>

<p><code class="language-plaintext highlighter-rouge">CLS Token</code>ê¹Œì§€ ë”í•œ ìµœì¢… <code class="language-plaintext highlighter-rouge">Input Embedding</code> ì˜ í…ì„œ ì°¨ì›ì€ <code class="language-plaintext highlighter-rouge">[10, 1025, 1024]</code> ê°€ ëœë‹¤.</p>

<h4 id="positional-embedding"><code class="language-plaintext highlighter-rouge">ğŸ”¢Â Positional Embedding</code></h4>

\[E_{pos} \in R^{(N+1)*D}\]

<p>ì´ë¯¸ì§€ë¥¼ íŒ¨ì¹˜ ë‹¨ìœ„ì˜ ì„ë² ë”©ìœ¼ë¡œ ë§Œë“¤ì—ˆë‹¤ë©´ ì´ì œ ìœ„ì¹˜ ì„ë² ë”©ì„ ì •ì˜í•´ì„œ ë”í•´ì£¼ë©´ ëª¨ì‹ë„ ì† <code class="language-plaintext highlighter-rouge">Embedded Patches</code> , ì¦‰ ì¸ì½”ë”ì— ë“¤ì–´ê°ˆ ìµœì¢… <code class="language-plaintext highlighter-rouge">Patch Embedding</code> ì´ ì™„ì„± ëœë‹¤. ìœ„ì¹˜ ì„ë² ë”©ì„ ë§Œë“œëŠ” ë°©ì‹ì€ ê¸°ì¡´ <code class="language-plaintext highlighter-rouge">Transformer</code>, <code class="language-plaintext highlighter-rouge">BERT</code> ì™€ ë™ì¼í•˜ë‹¤. ì•„ë˜ <code class="language-plaintext highlighter-rouge">VisionEncoder</code> í´ë˜ìŠ¤ë¥¼ êµ¬í˜„í•œ ì½”ë“œë¥¼ ì‚´í´ë³´ì.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="k">class</span> <span class="nc">VisionEncoder</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="p">...</span>
    <span class="n">ì¤‘ëµ</span>
    <span class="p">...</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">positional_embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Embedding</span><span class="p">((</span><span class="bp">self</span><span class="p">.</span><span class="n">num_patches</span> <span class="o">+</span> <span class="mi">1</span><span class="p">),</span> <span class="n">dim_model</span><span class="p">)</span>  <span class="c1"># add 1 for cls token
</span>    <span class="p">...</span>
    <span class="n">ì¤‘ëµ</span>
    <span class="p">...</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
        <span class="n">layer_output</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">pos_x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">num_patches</span> <span class="o">+</span> <span class="mi">1</span><span class="p">).</span><span class="n">repeat</span><span class="p">(</span><span class="n">inputs</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]).</span><span class="n">to</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>  <span class="c1"># inputs.shape[0] = Batch Size of Input
</span>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">dropout</span><span class="p">(</span>
            <span class="n">inputs</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">positional_embedding</span><span class="p">(</span><span class="n">pos_x</span><span class="p">)</span>
        <span class="p">)</span>
    <span class="p">...</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">Input Embedding</code>ê³¼ ë‹¤ë¥´ê²Œ ìœ„ì¹˜ ì„ë² ë”©ì€ <code class="language-plaintext highlighter-rouge">nn.Embedding</code>ìœ¼ë¡œ êµ¬í˜„í–ˆëŠ”ë°, ì—¬ê¸°ì„œë„ ì‚¬ì‹¤ <code class="language-plaintext highlighter-rouge">nn.Linear</code>ë¥¼ ì‚¬ìš©í•´ë„ ë¬´ë°©í•˜ë‹¤. ê·¸ê²ƒë³´ë‹¤ <code class="language-plaintext highlighter-rouge">nn.Embedding</code>ì˜ ì…ë ¥ ì°¨ì›ì¸ <code class="language-plaintext highlighter-rouge">self.num_patches + 1</code> ì— ì£¼ëª©í•´ë³´ì. ì™œ 1ì„ ë”í•´ì¤€ ê°’ì„ ì‚¬ìš©í–ˆì„ê¹Œ??</p>

<p><code class="language-plaintext highlighter-rouge">ViT</code>ëŠ” BERTì˜ <code class="language-plaintext highlighter-rouge">CLS Token Pooling</code> ì„ ì°¨ìš©í•˜ê¸° ìœ„í•´ íŒ¨ì¹˜ ì‹œí€€ìŠ¤ ë§¨ ì•ì— CLS í† í°ì„ ì¶”ê°€í•˜ê¸° ë•Œë¬¸ì´ë‹¤. ì´ë ‡ê²Œ ì¶”ê°€ëœ <code class="language-plaintext highlighter-rouge">CLS Token</code>ì€ ì¸ì½”ë”ë¥¼ ê±°ì³ ìµœì¢… <code class="language-plaintext highlighter-rouge">MLP Head</code>ì— í˜ëŸ¬ë“¤ì–´ê°€ ë¡œì§“ìœ¼ë¡œ ë³€í™˜ëœë‹¤. ë§Œì•½ ë…ìê»˜ì„œ <code class="language-plaintext highlighter-rouge">CLS Token Pooling</code> ëŒ€ì‹  ë‹¤ë¥¸ í’€ë§ ë°©ì‹ì„ ì‚¬ìš©í• ê±°ë¼ë©´ 1ì„ ì¶”ê°€í•´ì¤„ í•„ìš”ëŠ” ì—†ë‹¤.</p>

<p>ì• ì´ˆì— ê°ì²´ ì¸ìŠ¤í„´ìŠ¤ ì´ˆê¸°í™” ë‹¹ì‹œì— <code class="language-plaintext highlighter-rouge">CLS Token</code> ì„ ì¶”ê°€ë¥¼ ë°˜ì˜í•œ ê°’ì„ ì „ë‹¬í•˜ë©´ ë˜ì§€ ì•ŠëŠ”ê°€í•˜ëŠ” ì˜ë¬¸ì´ ë“¤ ìˆ˜ë„ ìˆë‹¤. í•˜ì§€ë§Œ <code class="language-plaintext highlighter-rouge">VisionEncoder</code> ê°ì²´ ì¸ìŠ¤í„´ìŠ¤ ì´ˆê¸°í™” ë‹¹ì‹œì—ëŠ” <code class="language-plaintext highlighter-rouge">num_patches</code> ê°’ìœ¼ë¡œ <code class="language-plaintext highlighter-rouge">CLS Token</code>ì´ ì¶”ê°€ë˜ê¸° ì´ì „ ê°’(+1 ë°˜ì˜ì´ ì•ˆë˜ì–´ ìˆìŒ)ì„ ì „ë‹¬í•˜ë„ë¡ ì„¤ê³„ ë˜ì–´ ìˆì–´ì„œ  <code class="language-plaintext highlighter-rouge">CLS Pooling</code>ì„ ì‚¬ìš©í• ê±°ë¼ë©´ 1 ì¶”ê°€ë¥¼ ê¼­ í•´ì¤˜ì•¼ í•œë‹¤.</p>

<p align="center">
<img src="/assets/images/vision_transformer/insight5.png" alt="Performance Table by making Position Embedding method" class="align-center image-caption" width="75%&quot;, height=&quot;50%" />
<strong><em><a href="https://arxiv.org/abs/2010.11929">Performance Table by making Position Embedding method</a></em></strong>
</p>

<p>í•œí¸ ì €ìëŠ” <code class="language-plaintext highlighter-rouge">2D Postion Embedding</code>, <code class="language-plaintext highlighter-rouge">Relative Position Embedding</code> ë°©ì‹ë„ ì ìš©í•´ë´¤ì§€ë§Œ, êµ¬í˜„ ë³µì¡ë„ &amp; ì—°ì‚°ëŸ‰ ëŒ€ë¹„ ì„±ëŠ¥ í–¥ìƒ í­ì´ ë§¤ìš° ë¯¸ë¯¸í•´ ì¼ë°˜ì ì¸ <code class="language-plaintext highlighter-rouge">1D Position Embedding</code>ì„ ì‚¬ìš©í•  ê²ƒì„ ì¶”ì²œí•˜ê³  ìˆë‹¤.</p>

<h4 id="-multi-head-attention"><code class="language-plaintext highlighter-rouge">ğŸ‘©â€ğŸ‘©â€ğŸ‘§â€ğŸ‘¦ Multi-Head Attention</code></h4>

\[z_t^{'} = MSA(LN(z_{t-1}) + z_{t-1})\]

\[MSA(z) = [SA_1();SA_2();SA_3()...SA_k()]*U_{msa}, \ \ U_{msa} \in R^{(k*D_h)*D} \\\]

<p>íŠ¸ëœìŠ¤í¬ë¨¸ ê³„ì—´ ëª¨ë¸ì˜ í•µì‹¬ <code class="language-plaintext highlighter-rouge">Multi-Head Self-Attention</code> ëª¨ë“ˆì— ëŒ€í•´ì„œ ì•Œì•„ë³´ì. ì‚¬ì‹¤ ê¸°ì¡´ ìì—°ì–´ ì²˜ë¦¬ <code class="language-plaintext highlighter-rouge">Transformer</code>, <code class="language-plaintext highlighter-rouge">BERT</code> ë“±ì˜ ë™ì‘ ë°©ì‹ê³¼ ì™„ì „íˆ ë™ì¼í•˜ë©°, ì½”ë“œë¡œ êµ¬í˜„í•  ë•Œ ì—­ì‹œ ë™ì¼í•˜ê²Œ ë§Œë“¤ì–´ì£¼ë©´ ëœë‹¤. ìì„¸í•œ ì›ë¦¬ì™€ ë™ì‘ ë°©ì‹ì€ <strong><u>Attention Is All You Need</u></strong> ë¦¬ë·° í¬ìŠ¤íŠ¸ì—ì„œ ì„¤ëª…í–ˆê¸° ë•Œë¬¸ì— ìƒëµí•˜ê³  ë„˜ì–´ê°€ê² ë‹¤. í•œí¸ íŒŒì´í† ì¹˜ë¡œ êµ¬í˜„í•œ <code class="language-plaintext highlighter-rouge">Multi-Head Self-Attention</code> ë¸”ëŸ­ì— ëŒ€í•œ ì½”ë“œëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">scaled_dot_product_attention</span><span class="p">(</span><span class="n">q</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">k</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">v</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">dot_scale</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
    <span class="s">"""
    Scaled Dot-Product Attention
    Args:
        q: query matrix, shape (batch_size, seq_len, dim_head)
        k: key matrix, shape (batch_size, seq_len, dim_head)
        v: value matrix, shape (batch_size, seq_len, dim_head)
        dot_scale: scale factor for Qâ€¢K^T result, same as pure transformer
    Math:
        A = softmax(qâ€¢k^t/sqrt(D_h)), SA(z) = Av
    """</span>
    <span class="n">attention_dist</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="n">softmax</span><span class="p">(</span>
        <span class="n">torch</span><span class="p">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">))</span> <span class="o">/</span> <span class="n">dot_scale</span><span class="p">,</span>
        <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span>
    <span class="p">)</span>
    <span class="n">attention_matrix</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">attention_dist</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">attention_matrix</span>

<span class="k">class</span> <span class="nc">AttentionHead</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="s">"""
    In this class, we implement workflow of single attention head
    Args:
        dim_model: dimension of model's latent vector space, default 1024 from official paper
        dim_head: dimension of each attention head, default 64 from official paper (1024 / 16)
        dropout: dropout rate, default 0.1
    Math:
        [q,k,v]=zâ€¢U_qkv, A = softmax(qâ€¢k^t/sqrt(D_h)), SA(z) = Av
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim_model</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span>  <span class="mi">1024</span><span class="p">,</span> <span class="n">dim_head</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">64</span><span class="p">,</span> <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">AttentionHead</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span> <span class="o">=</span> <span class="n">dim_model</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dim_head</span> <span class="o">=</span> <span class="n">dim_head</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">dropout</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dot_scale</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">dim_head</span><span class="p">))</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">fc_q</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">dim_head</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">fc_k</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">dim_head</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">fc_v</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">dim_head</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="n">attention_matrix</span> <span class="o">=</span> <span class="n">scaled_dot_product_attention</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">fc_q</span><span class="p">(</span><span class="n">x</span><span class="p">),</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">fc_k</span><span class="p">(</span><span class="n">x</span><span class="p">),</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">fc_v</span><span class="p">(</span><span class="n">x</span><span class="p">),</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">dot_scale</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">attention_matrix</span>

<span class="k">class</span> <span class="nc">MultiHeadAttention</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="s">"""
    In this class, we implement workflow of Multi-Head Self-Attention
    Args:
        dim_model: dimension of model's latent vector space, default 1024 from official paper
        num_heads: number of heads in MHSA, default 16 from official paper for ViT-Large
        dim_head: dimension of each attention head, default 64 from official paper (1024 / 16)
        dropout: dropout rate, default 0.1
    Math:
        MSA(z) = [SA1(z); SA2(z); Â· Â· Â· ; SAk(z)]â€¢Umsa
    Reference:
        https://arxiv.org/abs/2010.11929
        https://arxiv.org/abs/1706.03762
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim_model</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1024</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span> <span class="n">dim_head</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">64</span><span class="p">,</span> <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MultiHeadAttention</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span> <span class="o">=</span> <span class="n">dim_model</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dim_head</span> <span class="o">=</span> <span class="n">dim_head</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">dropout</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">attention_heads</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">ModuleList</span><span class="p">(</span>
            <span class="p">[</span><span class="n">AttentionHead</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">dim_head</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">dropout</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">num_heads</span><span class="p">)]</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">fc_concat</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="s">""" x is already passed nn.Layernorm """</span>
        <span class="k">assert</span> <span class="n">x</span><span class="p">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">3</span><span class="p">,</span> <span class="sa">f</span><span class="s">'Expected (batch, seq, hidden) got </span><span class="si">{</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="s">'</span>
        <span class="n">attention_output</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">fc_concat</span><span class="p">(</span>
            <span class="n">torch</span><span class="p">.</span><span class="n">cat</span><span class="p">([</span><span class="n">head</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">head</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">attention_heads</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># concat all dim_head = num_heads * dim_head
</span>        <span class="p">)</span>
        <span class="k">return</span> <span class="n">attention_output</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">MultiHeadAttention</code>ì„ ê°€ì¥ ìµœìƒìœ„ ê°ì²´ë¡œ ë‘ê³ , í•˜ìœ„ì— <code class="language-plaintext highlighter-rouge">AttentionHead</code>ê°ì²´ë¥¼ ë”°ë¡œ êµ¬í˜„í–ˆë‹¤. ì´ë ‡ê²Œ êµ¬í˜„í•˜ë©´, ì–´í…ì…˜ í•´ë“œë³„ë¡œ ì¿¼ë¦¬, í‚¤, ë²¨ë¥˜ ì„ ì˜ íˆ¬ì˜ í–‰ë ¬(<code class="language-plaintext highlighter-rouge">nn.Linear</code>)ì„ ë”°ë¡œ êµ¬í˜„í•´ì¤„ í•„ìš”ê°€ ì—†ì–´ì§€ë©°, <code class="language-plaintext highlighter-rouge">nn.ModuleList</code> ë¥¼ í†µí•´ ê°œë³„ í•´ë“œë¥¼ í•œ ë²ˆì— ê·¸ë£¹í•‘í•˜ê³  <code class="language-plaintext highlighter-rouge">loop</code> ë¥¼ í†µí•´ ì¶œë ¥ ê²°ê³¼ë¥¼ <code class="language-plaintext highlighter-rouge">concat</code> í•´ì¤„ ìˆ˜ ìˆì–´ ë³µì¡í•˜ê³  ë§ì€ ì—ëŸ¬ë¥¼ ìœ ë°œí•˜ëŠ” <strong><u>í…ì„œ ì°¨ì› ì¡°ì‘ì„ í”¼í•  ìˆ˜ ìˆìœ¼ë©°</u></strong>, ì½”ë“œì˜ ê°€ë…ì„±ì´ ì˜¬ë¼ê°€ëŠ” íš¨ê³¼ê°€ ìˆë‹¤.</p>

<h4 id="ï¸-mlp"><code class="language-plaintext highlighter-rouge">ğŸ—³ï¸ MLP</code></h4>

\[z_{t} = MLP(LN(z_{t}^{'}) + z_{t}^{'})\]

<p>ì´ë¦„ë§Œ <code class="language-plaintext highlighter-rouge">MLP</code>ë¡œ ë°”ë€Œì—ˆì„ ë¿, ê¸°ì¡´ íŠ¸ëœìŠ¤í¬ë¨¸ì˜ í”¼ë“œ í¬ì›Œë“œ ë¸”ëŸ­ê³¼ ë™ì¼í•œ ì—­í• ì„ í•œë‹¤. ì—­ì‹œ ìì„¸í•œ ë™ì‘ ë°©ì‹ì€ ì—¬ê¸° í¬ìŠ¤íŠ¸ì—ì„œ í™•ì¸í•˜ì. íŒŒì´í† ì¹˜ë¡œ êµ¬í˜„í•œ ì½”ë“œëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">MLP</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="s">"""
    Class for MLP module in ViT-Large
    Args:
        dim_model: dimension of model's latent vector space, default 512
        dim_mlp: dimension of FFN's hidden layer, default 2048 from official paper
        dropout: dropout rate, default 0.1
    Math:
        MLP(x) = MLP(LN(x))+x
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim_model</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1024</span><span class="p">,</span> <span class="n">dim_mlp</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">4096</span><span class="p">,</span> <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MLP</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">mlp</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">dim_model</span><span class="p">,</span> <span class="n">dim_mlp</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">GELU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">dropout</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">dim_mlp</span><span class="p">,</span> <span class="n">dim_model</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">dropout</span><span class="p">),</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">mlp</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</code></pre></div></div>

<p>íŠ¹ì´í•œ ì ì€ <code class="language-plaintext highlighter-rouge">Activation Function</code>ìœ¼ë¡œ <code class="language-plaintext highlighter-rouge">GELU</code>ë¥¼ ì‚¬ìš©(ê¸°ì¡´ íŠ¸ëœìŠ¤í¬ë¨¸ëŠ” <code class="language-plaintext highlighter-rouge">RELU</code>)í–ˆë‹¤ëŠ” ì ì´ë‹¤.</p>

<h4 id="-vision-encoder-layer"><code class="language-plaintext highlighter-rouge">ğŸ“˜ Vision Encoder Layer</code></h4>

<p><code class="language-plaintext highlighter-rouge">ViT</code> ì¸ì½”ë” ë¸”ëŸ­ 1ê°œì— í•´ë‹¹í•˜ëŠ” í•˜ìœ„ ëª¨ë“ˆê³¼ ë™ì‘ì„ êµ¬í˜„í•œ ê°ì²´ì´ë‹¤. êµ¬í˜„í•œ ì½”ë“œëŠ” ì•„ë˜ì™€ ê°™ë‹¤.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">VisionEncoderLayer</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="s">"""
    Class for encoder_model module in ViT-Large
    In this class, we stack each encoder_model module (Multi-Head Attention, Residual-Connection, Layer Normalization, MLP)
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim_model</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1024</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">16</span><span class="p">,</span> <span class="n">dim_mlp</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">4096</span><span class="p">,</span> <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">VisionEncoderLayer</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">self_attention</span> <span class="o">=</span> <span class="n">MultiHeadAttention</span><span class="p">(</span>
            <span class="n">dim_model</span><span class="p">,</span>
            <span class="n">num_heads</span><span class="p">,</span>
            <span class="nb">int</span><span class="p">(</span><span class="n">dim_model</span> <span class="o">/</span> <span class="n">num_heads</span><span class="p">),</span>
            <span class="n">dropout</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">layer_norm1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">dim_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">layer_norm2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">dim_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">dropout</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">mlp</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">(</span>
            <span class="n">dim_model</span><span class="p">,</span>
            <span class="n">dim_mlp</span><span class="p">,</span>
            <span class="n">dropout</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="n">ln_x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">layer_norm1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">residual_x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">dropout</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">self_attention</span><span class="p">(</span><span class="n">ln_x</span><span class="p">))</span> <span class="o">+</span> <span class="n">x</span>

        <span class="n">ln_x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">layer_norm2</span><span class="p">(</span><span class="n">residual_x</span><span class="p">)</span>
        <span class="n">fx</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">mlp</span><span class="p">(</span><span class="n">ln_x</span><span class="p">)</span> <span class="o">+</span> <span class="n">residual_x</span>  <span class="c1"># from official paper &amp; code by Google Research
</span>        <span class="k">return</span> <span class="n">fx</span>
</code></pre></div></div>

<p><del>íŠ¹ì´ì ì€ ë§ˆì§€ë§‰ <code class="language-plaintext highlighter-rouge">MLP Layer</code>ì™€ <code class="language-plaintext highlighter-rouge">Residual</code> ê²°ê³¼ë¥¼ ë”í•œ ë’¤, ë‹¤ìŒ ì¸ì½”ë” ë¸”ë¡ì— ì „ë‹¬í•˜ê¸° ì „ì— ì¸µ ì •ê·œí™”ë¥¼ í•œ ë²ˆ ë” ì ìš©í•œë‹¤ëŠ” ê²ƒì´ë‹¤. ëª¨ë¸ ëª¨ì‹ë„ì—ëŠ” ë‚˜ì™€ ìˆì§€ ì•Šì§€ë§Œ, ë³¸ë¬¸ì— í•´ë‹¹ ë‚´ìš©ì´ ì‹¤ë ¤ ìˆë‹¤.</del>
ë§ˆì§€ë§‰ ì¸ì½”ë”ì˜ ì¶œë ¥ê°’ì—ë§Œ í•œë²ˆ ë” <code class="language-plaintext highlighter-rouge">layernorm</code>ì„ ì ìš©í•œë‹¤.</p>

<h4 id="-visionencoder"><code class="language-plaintext highlighter-rouge">ğŸ“š VisionEncoder</code></h4>

<p>ì…ë ¥ ì´ë¯¸ì§€ë¥¼ <code class="language-plaintext highlighter-rouge">Patch Embedding</code>ìœ¼ë¡œ ì¸ì½”ë”© í•˜ê³  Nê°œì˜ <code class="language-plaintext highlighter-rouge">VisionEncoderLayer</code>ë¥¼ ìŒ“ê¸° ìœ„í•´ êµ¬í˜„ëœ ê°ì²´ì´ë‹¤. <code class="language-plaintext highlighter-rouge">Patch Embedding</code>ì„ ë§Œë“œëŠ” ë¶€ë¶„ì€ ì´ë¯¸ ìœ„ì—ì„œ ì„¤ëª…í–ˆê¸° ë•Œë¬¸ì— ë„˜ì–´ê°€ê³ , ì¸ì½”ë” ë¸”ëŸ­ì„ Nê°œ ìŒ“ëŠ” ë°©ë²•ì€ ì—­ì‹œë‚˜ <code class="language-plaintext highlighter-rouge">nn.ModuleList</code> ë¥¼ ì‚¬ìš©í•˜ë©´ ê°„í¸í•˜ê²Œ êµ¬í˜„í•  ìˆ˜ ìˆë‹¤. ì•„ë˜ ì½”ë“œë¥¼ ì‚´í´ë³´ì.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">VisionEncoder</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="s">"""
    In this class, encode input sequence(Image) and then we stack N VisionEncoderLayer
    This model is implemented by cls pooling method for classification
    First, we define "positional embedding" and then add to input embedding for making patch embedding
    Second, forward patch embedding to N EncoderLayer and then get output embedding
    Args:
        num_patches: number of patches in input image =&gt; (image_size / patch_size)**2
        N: number of EncoderLayer, default 24 for large model
    """</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_patches</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">N</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">24</span><span class="p">,</span> <span class="n">dim_model</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1024</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">16</span><span class="p">,</span> <span class="n">dim_mlp</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">4096</span><span class="p">,</span> <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">VisionEncoder</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">num_patches</span> <span class="o">=</span> <span class="n">num_patches</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">positional_embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Embedding</span><span class="p">((</span><span class="bp">self</span><span class="p">.</span><span class="n">num_patches</span> <span class="o">+</span> <span class="mi">1</span><span class="p">),</span> <span class="n">dim_model</span><span class="p">)</span>  <span class="c1"># add 1 for cls token
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">num_layers</span> <span class="o">=</span> <span class="n">N</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span> <span class="o">=</span> <span class="n">dim_model</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dim_mlp</span> <span class="o">=</span> <span class="n">dim_mlp</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">dropout</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">encoder_layers</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">ModuleList</span><span class="p">(</span>
            <span class="p">[</span><span class="n">VisionEncoderLayer</span><span class="p">(</span><span class="n">dim_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">dim_mlp</span><span class="p">,</span> <span class="n">dropout</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">num_layers</span><span class="p">)]</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">layer_norm</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">dim_model</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
        <span class="n">layer_output</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">pos_x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">num_patches</span> <span class="o">+</span> <span class="mi">1</span><span class="p">).</span><span class="n">repeat</span><span class="p">(</span><span class="n">inputs</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]).</span><span class="n">to</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">dropout</span><span class="p">(</span>
            <span class="n">inputs</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">positional_embedding</span><span class="p">(</span><span class="n">pos_x</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">encoder_layers</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            <span class="n">layer_output</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">encoded_x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">layer_norm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># from official paper &amp; code by Google Research
</span>        <span class="n">layer_output</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">stack</span><span class="p">(</span><span class="n">layer_output</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">).</span><span class="n">to</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>  <span class="c1"># For Weighted Layer Pool: [N, BS, SEQ_LEN, DIM]
</span>        <span class="k">return</span> <span class="n">encoded_x</span><span class="p">,</span> <span class="n">layer_output</span>
</code></pre></div></div>
<p>ë§ˆì§€ë§‰ ì¸µì˜ ì¸ì½”ë” ì¶œë ¥ê°’ì—ëŠ” <code class="language-plaintext highlighter-rouge">layernorm</code>ì„ ì ìš©í•´ì¤˜ì•¼ í•¨ì„ ìŠì§€ ë§ì. í•œí¸, <code class="language-plaintext highlighter-rouge">layer_output</code>ëŠ” ë ˆì´ì–´ ë³„ ì–´í…ì…˜ ê²°ê³¼ë¥¼ ì‹œê°í™” í•˜ê±°ë‚˜ ë‚˜ì¤‘ì— <code class="language-plaintext highlighter-rouge">WeightedLayerPool</code>ì— ì‚¬ìš©í•˜ë ¤ê³  ë§Œë“¤ì—ˆë‹¤.</p>
<h4 id="-visiontransformer"><code class="language-plaintext highlighter-rouge">ğŸ¤– VisionTransformer</code></h4>

<p align="center">
<img src="/assets/images/vision_transformer/model_variant.png" alt="ViT Model Variant" class="align-center image-caption" width="75%&quot;, height=&quot;50%" />
<strong><em><a href="https://arxiv.org/abs/2010.11929">ViT Model Variant</a></em></strong>
</p>

<p><code class="language-plaintext highlighter-rouge">ViT</code> ëª¨ë¸ì˜ ê°€ì¥ ìµœìƒìœ„ ê°ì²´ë¡œ, ì•ì—ì„œ ì„¤ëª…í•œ ëª¨ë“  ëª¨ë“ˆë“¤ì˜ ë™ì‘ì´ ì´ë¤„ì§€ëŠ” ê³³ì´ë‹¤. ì‚¬ìš©ìë¡œë¶€í„° í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì…ë ¥ ë°›ì•„ ëª¨ë¸ì˜ í¬ê¸°, ê¹Šì´, íŒ¨ì¹˜ í¬ê¸°, ì´ë¯¸ì§€ ì„ë² ë”© ì¶”ì¶œ ë°©ì‹ì„ ì§€ì •í•œë‹¤. ê·¸ë¦¬ê³  ì…ë ¥ ì´ë¯¸ì§€ë¥¼ ì „ë‹¬ë°›ì•„ ì„ë² ë”©ì„ ë§Œë“¤ê³  ì¸ì½”ë”ì— ì „ë‹¬í•œ ë’¤, <code class="language-plaintext highlighter-rouge">MLP Head</code> ë¥¼ í†µí•´ ìµœì¢… ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ë°˜í™˜í•˜ëŠ” ì—­í• ì„ í•œë‹¤.</p>

<p>ì´ë¯¸ì§€ ì„ë² ë”© ì¶”ì¶œ ë°©ì‹ì€ <code class="language-plaintext highlighter-rouge">Linear Projection</code>ê³¼ <code class="language-plaintext highlighter-rouge">Convolution</code>ì´ ìˆë‹¤. ì „ìê°€ ë…¼ë¬¸ì—ì„œ ë§í•˜ëŠ” ì¼ë°˜ì ì¸ <code class="language-plaintext highlighter-rouge">ViT</code>ë¥¼ ë§í•˜ë©° í›„ìëŠ” ì €ìê°€ <code class="language-plaintext highlighter-rouge">Hybrid ViT</code>ë¼ê³  ë”°ë¡œ ëª…ëª…í•˜ëŠ” ëª¨ë¸ì´ë‹¤. ì„ë² ë”© ì¶”ì¶œ ë°©ì‹ ì´ì™¸ì— ë‹¤ë¥¸ ì°¨ì´ëŠ” ì „í˜€ ì—†ë‹¤. <code class="language-plaintext highlighter-rouge">extractor</code> ë§¤ê°œë³€ìˆ˜ë¥¼ í†µí•´ ì„ë² ë”© ì¶”ì¶œ ë°©ì‹ì„ ì§€ì •í•  ìˆ˜ ìˆìœ¼ë‹ˆ ì•„ë˜ ì½”ë“œë¥¼ í™•ì¸í•´ë³´ì.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">VisionTransformer</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="s">"""
    Main class for ViT of cls pooling, Pytorch implementation
    We implement pure ViT, Not hybrid version which is using CNN for extracting patch embedding
    input must be [BS, CHANNEL, IMAGE_SIZE, IMAGE_SIZE]
    In NLP, input_sequence is always smaller than vocab size
    But in vision, input_sequence is always same as image size, not concept of vocab in vision
    So, ViT use nn.Linear instead of nn.Embedding for input_embedding
    Args:
        num_classes: number of classes for classification task
        image_size: size of input image, default 512
        patch_size: size of patch, default 16 from official paper for ViT-Large
        extractor: option for feature extractor, default 'base' which is crop &amp; just flatten
                   if you want to use Convolution for feature extractor, set extractor='cnn' named hybrid ver in paper
        classifier: option for pooling method, default token meaning that do cls pooling
                    if you want to use mean pooling, set classifier='mean'
        mode: option for train type, default fine-tune, if you want pretrain, set mode='pretrain'
              In official paper &amp; code by Google Research, they use different classifier head for pretrain, fine-tune
    Math:
        image2sequence: [batch, channel, image_size, image_size] -&gt; [batch, patch, patch_size^2*channel]
        input_embedding: R^(P^2 Â·C)Ã—D
    Reference:
        https://arxiv.org/abs/2010.11929
        https://arxiv.org/abs/1706.03762
        https://github.com/google-research/vision_transformer/blob/main/vit_jax/models_vit.py#L184
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span>
            <span class="n">num_classes</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
            <span class="n">channels</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span>
            <span class="n">image_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">512</span><span class="p">,</span>
            <span class="n">patch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">16</span><span class="p">,</span>
            <span class="n">num_layers</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">24</span><span class="p">,</span>
            <span class="n">dim_model</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1024</span><span class="p">,</span>
            <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">16</span><span class="p">,</span>
            <span class="n">dim_mlp</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">4096</span><span class="p">,</span>
            <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
            <span class="n">extractor</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s">'base'</span><span class="p">,</span>
            <span class="n">classifier</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s">'token'</span><span class="p">,</span>
            <span class="n">mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s">'fine_tune'</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">VisionTransformer</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">num_patches</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">image_size</span> <span class="o">/</span> <span class="n">patch_size</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">num_layers</span> <span class="o">=</span> <span class="n">num_layers</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">patch_size</span> <span class="o">=</span> <span class="n">patch_size</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span> <span class="o">=</span> <span class="n">dim_model</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dim_mlp</span> <span class="o">=</span> <span class="n">dim_mlp</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">dropout</span><span class="p">)</span>

        <span class="c1"># Input Embedding
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">extractor</span> <span class="o">=</span> <span class="n">extractor</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">input_embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">((</span><span class="n">channels</span> <span class="o">*</span> <span class="n">patch_size</span><span class="o">**</span><span class="mi">2</span><span class="p">),</span> <span class="n">dim_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">conv</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Conv2d</span><span class="p">(</span>
            <span class="n">in_channels</span><span class="o">=</span><span class="n">channels</span><span class="p">,</span>
            <span class="n">out_channels</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span><span class="p">,</span>
            <span class="n">kernel_size</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">patch_size</span><span class="p">,</span>
            <span class="n">stride</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">patch_size</span>
        <span class="p">)</span>

        <span class="c1"># Encoder Multi-Head Self-Attention
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">VisionEncoder</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">num_patches</span><span class="p">,</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">num_layers</span><span class="p">,</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span><span class="p">,</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">num_heads</span><span class="p">,</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">dim_mlp</span><span class="p">,</span>
            <span class="n">dropout</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">classifier</span> <span class="o">=</span> <span class="n">classifier</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">pretrain_classifier</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">Tanh</span><span class="p">(),</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">fine_tune_classifier</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">mode</span> <span class="o">=</span> <span class="n">mode</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">any</span><span class="p">:</span>
        <span class="s">""" For cls pooling """</span>
        <span class="k">assert</span> <span class="n">inputs</span><span class="p">.</span><span class="n">ndim</span> <span class="o">!=</span> <span class="mi">4</span><span class="p">,</span> <span class="sa">f</span><span class="s">"Input shape should be [BS, CHANNEL, IMAGE_SIZE, IMAGE_SIZE], but got </span><span class="si">{</span><span class="n">inputs</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="s">"</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">inputs</span>

        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">extractor</span> <span class="o">==</span> <span class="s">'cnn'</span><span class="p">:</span>
            <span class="c1"># self.conv(x).shape == [batch, dim, image_size/patch_size, image_size/patch_size]
</span>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">conv</span><span class="p">(</span><span class="n">x</span><span class="p">).</span><span class="n">reshape</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="p">.</span><span class="n">dim_model</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">num_patches</span><span class="o">**</span><span class="mi">2</span><span class="p">).</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># self.extractor == 'base':
</span>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">input_embedding</span><span class="p">(</span>
                <span class="n">x</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="p">.</span><span class="n">num_patches</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">patch_size</span><span class="o">**</span><span class="mi">2</span> <span class="o">*</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
            <span class="p">)</span>
        <span class="n">cls_token</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">,</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>  <span class="c1"># can change init method
</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cat</span><span class="p">([</span><span class="n">cls_token</span><span class="p">,</span> <span class="n">x</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="n">x</span><span class="p">,</span> <span class="n">layer_output</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># output
</span>
        <span class="c1"># classification
</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">,</span> <span class="p">:]</span>  <span class="c1"># select cls token, which is position 0 in sequence
</span>        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">mode</span> <span class="o">==</span> <span class="s">'fine_tune'</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">fine_tune_classifier</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">mode</span> <span class="o">==</span> <span class="s">'pretrain'</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">fine_tune_classifier</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">pretrain_classifier</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">x</span>
</code></pre></div></div>

<p>í•œí¸, ì½”ë“œì—ì„œ ëˆˆì—¬ê²¨ë´ì•¼ í•  ì ì€ <code class="language-plaintext highlighter-rouge">MLP Head</code>ë¡œ, ì €ìëŠ” <code class="language-plaintext highlighter-rouge">pre-train</code> ì‹œì ê³¼ <code class="language-plaintext highlighter-rouge">fine-tune</code> ì‹œì ì— ì„œë¡œ ë‹¤ë¥¸ <code class="language-plaintext highlighter-rouge">Classifier Head</code>ë¥¼ ì‚¬ìš©í•œë‹¤. ì „ìì—ëŠ” <code class="language-plaintext highlighter-rouge">Activation Function</code> 1ê°œì™€ ë‘ ê°œì˜ <code class="language-plaintext highlighter-rouge">MLP Layer</code>ë¥¼ ì‚¬ìš©í•˜ê³ , í›„ìì—ëŠ” 1ê°œì˜ <code class="language-plaintext highlighter-rouge">MLP Layer</code>ë¥¼ ì‚¬ìš©í•œë‹¤.</p>

<p>ë‹¤ë§Œ, <code class="language-plaintext highlighter-rouge">pretrain_classifier</code>ì˜ ì…ì¶œë ¥ ì°¨ì›ì— ëŒ€í•œ ì •í™•í•œ ìˆ˜ì¹˜ë¥¼ ë…¼ë¬¸ì´ë‚˜ official repo codeë¥¼ í™•ì¸í•´ë„ ì°¾ì„ ìˆ˜ ì—†ì—ˆë‹¤, ê·¸ë˜ì„œ ì„ì‹œë¡œ ëª¨ë¸ì˜ ì°¨ì›ê³¼ ë˜‘ê°™ì´ ì„¸íŒ…í•˜ê²Œ ë˜ì—ˆë‹¤.</p>

<p>ë˜í•œ ì €ìëŠ” <code class="language-plaintext highlighter-rouge">CLS Pooling</code>ê³¼ ë”ë¶ˆì–´ <code class="language-plaintext highlighter-rouge">GAP</code> ë°©ì‹ë„ ì œì‹œí•˜ëŠ”ë°, <code class="language-plaintext highlighter-rouge">GAP</code> ë°©ì‹ì€ ì¶”í›„ì— ë”°ë¡œ ì¶”ê°€ê°€ í•„ìš”í•˜ë‹¤. ê·¸ë¦¬ê³  ì‚¬ì „ í›ˆë ¨ê³¼ íŒŒì¸ íŠœë‹ ëª¨ë‘ ë¶„ë¥˜ í…ŒìŠ¤í¬ë¥¼ ìˆ˜í–‰í–ˆëŠ”ë° (ì‹¬ì§€ì–´ ê°™ì€ ë°ì´í„° ì„¸íŠ¸ë¥¼ ì‚¬ìš©í•¨) ì™œ êµ³ì´ ì„œë¡œ ë‹¤ë¥¸ <code class="language-plaintext highlighter-rouge">Classifier Head</code>ë¥¼ ì •ì˜í–ˆëŠ”ì§€ ì˜ë„ë¥¼ ì•Œ ìˆ˜ ì—†ì–´ ë…¼ë¬¸ì„ ë‹¤ì‹œ ì½ì–´ë´¤ì§€ë§Œ, ì´ìœ ì— ëŒ€í•´ì„œ ìƒì„¸íˆ ì–¸ê¸‰í•˜ëŠ” ë¶€ë¶„ì´ ì—†ì—ˆë‹¤.</p>

<p><code class="language-plaintext highlighter-rouge">ViT</code>ëŠ” ì…ë ¥ ì„ë² ë”©ì„ ì •ì˜í•˜ëŠ” ë¶€ë¶„ì„ ì œì™¸í•˜ë©´ ì €ìì˜ ì˜ë„ëŒ€ë¡œ ê¸°ì¡´ íŠ¸ëœìŠ¤í¬ë¨¸ì™€ ë™ì¼í•œ ëª¨ë¸ êµ¬ì¡°ë¥¼ ê°€ì¡Œë‹¤. ì™„ì „íˆ ë‹¤ë¥¸ ë°ì´í„°ì¸ ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ì— ê°™ì€ êµ¬ì¡°ì˜ ëª¨ë¸ì„ ì ìš©í•œë‹¤ëŠ” ê²ƒì´ ì •ë§ ì‰½ì§€ ì•Šì•„ ë³´ì˜€ëŠ”ë°, íŒ¨ì¹˜ ê°œë…ì„ ë§Œë“¤ì–´ ìì—°ì–´ì˜ í† í°ì²˜ëŸ¼ ê°„ì£¼í•˜ê³  ì‚¬ìš©í•œ ê²ƒì´ ì˜ë„ëŒ€ë¡œ êµ¬í˜„í•˜ëŠ”ë° ì§ê´€ì ì´ë©´ì„œë„ ì •ë§ íš¨ê³¼ì ì´ì—ˆë‹¤ê³  ìƒê°í•œë‹¤. ì´ì œ ì´ë ‡ê²Œ ë§Œë“¤ì–´ì§„ ëª¨ë¸ì„ í†µí•´ ì§„í–‰í•œ ì—¬ëŸ¬ ì‹¤í—˜ ê²°ê³¼ì— ì–´ë–¤ ì¸ì‚¬ì´íŠ¸ê°€ ë‹´ê²¨ ìˆëŠ”ì§€ ì•Œì•„ë³´ì.</p>

<h3 id="insight-from-experiment"><code class="language-plaintext highlighter-rouge">ğŸ”¬Â Insight from Experiment</code></h3>

<h4 id="insight-1-vitì˜-scalability-ì¦ëª…"><code class="language-plaintext highlighter-rouge">ğŸ’¡Â Insight 1. ViTì˜ Scalability ì¦ëª…</code></h4>
<ul>
  <li><strong><code class="language-plaintext highlighter-rouge">Pre-Train</code>ì— ì‚¬ìš©ë˜ëŠ” ì´ë¯¸ì§€ ë°ì´í„° ì„¸íŠ¸ì˜ í¬ê¸°ê°€ ì»¤ì§ˆìˆ˜ë¡ <code class="language-plaintext highlighter-rouge">Fine-Tune Stage</code>ì—ì„œ <code class="language-plaintext highlighter-rouge">ViT</code>ê°€ <code class="language-plaintext highlighter-rouge">CNN</code>ë³´ë‹¤ ë†’ì€ ì„±ëŠ¥</strong></li>
  <li><strong>ê°™ì€ ì„±ëŠ¥ì´ë¼ë©´ <code class="language-plaintext highlighter-rouge">ViT</code>ê°€ ìƒëŒ€ì ìœ¼ë¡œ ì ì€ ì—°ì‚°ëŸ‰ì„ ê¸°ë¡</strong></li>
</ul>

<p align="center">
<img src="/assets/images/vision_transformer/insight1.png" alt="Performance per Dataset Scale" class="align-center image-caption" width="75%&quot;, height=&quot;50%" />
<strong><em><a href="https://arxiv.org/abs/2010.11929">Performance per Dataset Scale</a></em></strong>
</p>

<p>ìœ„ ë„í‘œëŠ” <code class="language-plaintext highlighter-rouge">Pre-Train Stage</code>ì— ì‚¬ìš©ëœ ì´ë¯¸ì§€ ë°ì´í„° ì„¸íŠ¸ì— ë”°ë¥¸ ëª¨ë¸ì˜ <code class="language-plaintext highlighter-rouge">Fine-Tune</code> ì„±ëŠ¥ ì¶”ì´ë¥¼ ë‚˜íƒ€ë‚¸ ìë£Œë‹¤. ì‚¬ì „ í›ˆë ¨ ë°ì´í„° ìŠ¤ì¼€ì¼ì´ í¬ì§€ ì•Šì„ ë•ŒëŠ” <code class="language-plaintext highlighter-rouge">Conv</code> ê¸°ë°˜ì˜ <code class="language-plaintext highlighter-rouge">ResNet</code> ì‹œë¦¬ì¦ˆê°€ <code class="language-plaintext highlighter-rouge">ViT</code> ì‹œë¦¬ì¦ˆë¥¼ ì••ë„í•˜ëŠ” ëª¨ìŠµì„ ë³´ì—¬ì¤€ë‹¤. í•˜ì§€ë§Œ ë°ì´í„° ì„¸íŠ¸ì˜ í¬ê¸°ê°€ ì»¤ì§ˆìˆ˜ë¡ ì ì  <code class="language-plaintext highlighter-rouge">ViT</code> ì‹œë¦¬ì¦ˆì˜ ì„±ëŠ¥ì´ <code class="language-plaintext highlighter-rouge">ResNet</code>ì„ ëŠ¥ê°€í•˜ëŠ” ê²°ê³¼ë¥¼ ë³¼ ìˆ˜ ìˆë‹¤.</p>

<p>í•œí¸, ViT &amp; ResNet ì„±ëŠ¥ ê²°ê³¼ ëª¨ë‘ ImageNetê³¼ JFT-Imageë¡œ ì‚¬ì „ í›ˆë ¨ ë° íŒŒì¸ íŠœë‹ì„ ê±°ì³ ë‚˜ì™”ë‹¤ê³  í•˜ë‹ˆ ì°¸ê³ í•˜ì. <strong><u>ì¶”ê°€ë¡œ íŒŒì¸ íŠœë‹ ê³¼ì •ì—ì„œ ì‚¬ì „ í›ˆë ¨ ë•Œë³´ë‹¤ ì´ë¯¸ì§€ ì‚¬ì´ì¦ˆë¥¼ í‚¤ì›Œì„œ í›ˆë ¨ì„ ì‹œì¼°ë‹¤ê³  ë…¼ë¬¸ì—ì„œ ë°íˆê³  ìˆëŠ”ë°, ì´ëŠ” ì €ìì˜ ì‹¤í—˜ ê²°ê³¼ì— ê¸°ì¸í•œ ê²ƒì´ë‹¤</u></strong>. ë…¼ë¬¸ì— ë”°ë¥´ë©´ íŒŒì¸ íŠœë‹ ë•Œ ì‚¬ì „ í›ˆë ¨ ë‹¹ì‹œë³´ë‹¤ ë” ë†’ì€ í•´ìƒë„ì˜ ì´ë¯¸ì§€ë¥¼ ì‚¬ìš©í•˜ë©´ ì„±ëŠ¥ì´ í–¥ìƒ ëœë‹¤ê³  í•˜ë‹ˆ ê¸°ì–µí–ˆë‹¤ê°€  ì¨ë¨¹ì–´ë³´ì.</p>

<p align="center">
<img src="/assets/images/vision_transformer/insight1_2.png" alt="Performance per FLOPs Scale" class="align-center image-caption" width="75%&quot;, height=&quot;50%" />
<strong><em><a href="https://arxiv.org/abs/2010.11929">Performance per FLOPs</a></em></strong>
</p>

<p>ìœ„ ë„í‘œëŠ” ì—°ì‚°ëŸ‰ ë³€í™”ì— ë”°ë¥¸ ëª¨ë¸ì˜ ì„±ëŠ¥ ì¶”ì´ë¥¼ ë‚˜íƒ€ë‚¸ ê·¸ë¦¼ì´ë‹¤. ë‘ ì§€í‘œ ëª¨ë‘ ê°™ì€ ì ìˆ˜ë¼ë©´ <code class="language-plaintext highlighter-rouge">ViT</code> ì‹œë¦¬ì¦ˆì˜ ì—°ì‚°ëŸ‰ì´ í˜„ì €íˆ ì ìŒì„ ì•Œ ìˆ˜ ìˆë‹¤. ë˜í•œ ì •í™•ë„ 95% ì´í•˜ êµ¬ê°„ì—ì„œ ê°™ì€ ì„±ëŠ¥ì´ë¼ë©´  <code class="language-plaintext highlighter-rouge">ViT</code>ì˜ <code class="language-plaintext highlighter-rouge">Hybrid</code> ë²„ì „ ëª¨ë¸ì˜ ì—°ì‚°ëŸ‰ì´ ì¼ë°˜ <code class="language-plaintext highlighter-rouge">ViT</code> ë²„ì „ë³´ë‹¤ í˜„ì €íˆ ì ìŒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤. ì´ëŸ¬í•œ ì‚¬ì‹¤ì€ ì¶”í›„ì— <code class="language-plaintext highlighter-rouge">Swin-Transformer</code> ì„¤ê³„ì— ì˜ê°ì„ ì¤€ë‹¤.</p>

<p>ë‘ ê°œì˜ ì‹¤í—˜ ê²°ê³¼ë¥¼ ì¢…í•©í–ˆì„ ë•Œ, <code class="language-plaintext highlighter-rouge">ViT</code>ê°€ <code class="language-plaintext highlighter-rouge">ResNet</code>ë³´ë‹¤ ì¼ë°˜í™” ì„±ëŠ¥ì´ ë” ë†’ìœ¼ë©°(ë„í‘œ 1) ëª¨ë¸ì˜ <code class="language-plaintext highlighter-rouge">Saturation</code> í˜„ìƒì´ ë‘ë“œëŸ¬ì§€ì§€ ì•Šì•„ ì„±ëŠ¥ì˜ í•œê³„ì¹˜(ë„í‘œ 2) ì—­ì‹œ ë” ë†’ë‹¤ê³  ë³¼ ìˆ˜ ìˆë‹¤. ë”°ë¼ì„œ ê¸°ì¡´ íŠ¸ëœìŠ¤í¬ë¨¸ì˜ ì—°ì‚°â€¢êµ¬ì¡°ì  ì¸¡ë©´ì—ì„œ <code class="language-plaintext highlighter-rouge">Scalability</code>ë¥¼ ì„±ê³µì ìœ¼ë¡œ ì´ì‹í–ˆë‹¤ê³  í‰ê°€í•  ìˆ˜ ìˆê² ë‹¤.</p>

<h4 id="insight-2-pure-self-attentionì€-ì¢‹ì€-ì´ë¯¸ì§€-í”¼ì²˜ë¥¼-ì¶”ì¶œí•˜ê¸°ì—-ì¶©ë¶„í•˜ë‹¤"><code class="language-plaintext highlighter-rouge">ğŸ’¡Â Insight 2. Pure Self-Attentionì€ ì¢‹ì€ ì´ë¯¸ì§€ í”¼ì²˜ë¥¼ ì¶”ì¶œí•˜ê¸°ì— ì¶©ë¶„í•˜ë‹¤</code></h4>
<ul>
  <li><strong>Patch Embedding Layerì˜ PCA ê²°ê³¼, íŒ¨ì¹˜ì˜ ê¸°ì €ê°€ ë˜ëŠ” ì°¨ì›ê³¼ ìœ ì‚¬í•œ ëª¨ì–‘ì„ ì¶”ì¶œ</strong>
    <ul>
      <li><strong><code class="language-plaintext highlighter-rouge">Convolution</code> ì—†ì´ <code class="language-plaintext highlighter-rouge">Self-Attention</code>ë§Œìœ¼ë¡œë„ ì¶©ë¶„íˆ ì´ë¯¸ì§€ì˜ ì¢‹ì€ í”¼ì²˜ë¥¼ ì¶”ì¶œí•˜ëŠ” ê²ƒì´ ê°€ëŠ¥</strong></li>
      <li><strong><code class="language-plaintext highlighter-rouge">Vision</code>ì—ì„œ <code class="language-plaintext highlighter-rouge">Convolution</code>ì— ëŒ€í•œ <code class="language-plaintext highlighter-rouge">reliance</code> íƒˆí”¼ ê°€ëŠ¥</strong></li>
    </ul>
  </li>
</ul>

<p align="center">
<img src="/assets/images/vision_transformer/insight2.png" alt="Patch Embedding Layerâ€™s Filter" class="align-center image-caption" width="75%&quot;, height=&quot;50%" />
<strong><em><a href="https://arxiv.org/abs/2010.11929">Patch Embedding Layerâ€™s Filter</a></em></strong>
</p>

<p>ìœ„ ìë£ŒëŠ” ì¶©ë¶„í•œ í•™ìŠµì„ ê±°ì¹˜ê³  ë‚œ <code class="language-plaintext highlighter-rouge">ViT</code>ì˜ <code class="language-plaintext highlighter-rouge">Patch Embedding Layer</code>ì˜ í•„í„°ë¥¼ <code class="language-plaintext highlighter-rouge">PCA</code>í•œ ê²°ê³¼ ì¤‘ì—ì„œ íŠ¹ì‡ê°’ì´ ë†’ì€ ìƒìœ„ 28ê°œì˜ í”¼ì²˜ë¥¼ ë‚˜ì—´í•œ ê·¸ë¦¼ì´ë‹¤. ì´ë¯¸ì§€ì˜ ê¸°ë³¸ ë¼ˆëŒ€ê°€ ë˜ê¸°ì— ì í•©í•´ ë³´ì´ëŠ” í”¼ì²˜ë“¤ì´ ì¶”ì¶œëœ ëª¨ìŠµì„ ë³¼ ìˆ˜ ìˆë‹¤.</p>

<p>ë”°ë¼ì„œ <code class="language-plaintext highlighter-rouge">Inductive Bias</code> ì—†ì´, ë‹¨ì¼  <code class="language-plaintext highlighter-rouge">Self-Attention</code>ë§Œìœ¼ë¡œ ì´ë¯¸ì§€ì˜ í”¼ì²˜ë¥¼ ì¶”ì¶œí•˜ëŠ” ê²ƒì´ ì¶©ë¶„íˆ ê°€ëŠ¥í•˜ë‹¤. ë¹„ì „ ë¶„ì•¼ì— ë§Œì—°í•œ <code class="language-plaintext highlighter-rouge">Convolution</code> ì˜ì¡´ì—ì„œ ë²—ì–´ë‚˜ ìƒˆë¡œìš´ ì•„í‚¤í…ì²˜ì˜ ë„ì…ì´ ê°€ëŠ¥í•¨ì„ ì‹œì‚¬í•œ ë¶€ë¶„ì´ë¼ê³  í•  ìˆ˜ ìˆê² ë‹¤.</p>

<h4 id="insight-3-bottom2general-information-top2specific-information"><code class="language-plaintext highlighter-rouge">ğŸ’¡Â Insight 3. Bottom2General Information, Top2Specific Information</code></h4>

<ul>
  <li><strong><code class="language-plaintext highlighter-rouge">ì…ë ¥</code>ê³¼ ê°€ê¹Œìš´ ì¸ì½”ë”ì¼ìˆ˜ë¡ <code class="language-plaintext highlighter-rouge">Global &amp; General</code>í•œ Informationì„ í¬ì°©</strong></li>
  <li><strong><code class="language-plaintext highlighter-rouge">ì¶œë ¥</code>ê³¼ ê°€ê¹Œìš´ ì¸ì½”ë”ì¼ìˆ˜ë¡ <code class="language-plaintext highlighter-rouge">Local &amp; Specific</code>í•œ Informationì„ í¬ì°©</strong></li>
</ul>
<p align="center">
<img src="/assets/images/vision_transformer/insight3.png" alt="Multi-Head Attention Distance per Network Depth" class="align-center image-caption" width="75%&quot;, height=&quot;50%" />
<strong><em><a href="https://arxiv.org/abs/2010.11929">Multi-Head Attention Distance per Network Depth</a></em></strong>
</p>

<p>ë‹¤ìŒ ìë£ŒëŠ” ì¸ì½”ë”ì˜ ê°œìˆ˜ ë³€í™”ì— ë”°ë¥¸ ê°œë³„ ì–´í…ì…˜ í•´ë“œì˜ ì–´í…ì…˜ ê±°ë¦¬ ë³€í™” ì¶”ì´ë¥¼ ë‚˜íƒ€ë‚¸ ê·¸ë¦¼ì´ë‹¤. ì—¬ê¸°ì„œ ì–´í…ì…˜ ê±°ë¦¬ë€, í•´ë“œê°€ ì–¼ë§ˆë‚˜ ë©€ë¦¬ ë–¨ì–´ì§„ íŒ¨ì¹˜ë¥¼ ì–´í…ì…˜í–ˆëŠ”ì§€ í”½ì…€ ë‹¨ìœ„ë¡œ í‘œí˜„í•œ ì§€í‘œë‹¤. í•´ë‹¹ ê°’ì´ ë†’ì„ìˆ˜ë¡ ê±°ë¦¬ìƒ ë©€ë¦¬ ë–¨ì–´ì§„ íŒ¨ì¹˜ì™€ ì–´í…ì…˜ì„, ì‘ì„ìˆ˜ë¡ ê°€ê¹Œìš´ íŒ¨ì¹˜ì™€ ì–´í…ì…˜ í–ˆë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•œë‹¤. ë‹¤ì‹œ ë„í‘œë¥¼ ì‚´í´ë³´ì. ì…ë ¥ê³¼ ê°€ê¹Œìš´ ì¸ì½”ë”ì¼ìˆ˜ë¡(Depth 0) í•´ë“œë³„ ì–´í…ì…˜ ê±°ë¦¬ì˜ ë¶„ì‚°ì´ ì»¤ì§€ê³ , ì¶œë ¥ê³¼ ê°€ê¹Œìš´ ì¸ì½”ë”ì¼ìˆ˜ë¡(Depth 23) ë¶„ì‚°ì´ ì ì ì¤„ì–´ë“¤ë‹¤ê°€ ê±°ì˜ í•œ ì ì— ìˆ˜ë ´í•˜ëŠ”ë“¯í•œ ì–‘ìƒì„ ë³´ì—¬ì¤€ë‹¤. ë‹¤ì‹œ ë§í•´, ì…ë ¥ê³¼ ê°€ê¹Œìš´ <code class="language-plaintext highlighter-rouge">Bottom Encoder</code>ëŠ” ë©€ë¦¬ ë–¨ì–´ì§„ íŒ¨ì¹˜ë¶€í„° ê°€ê¹Œìš´ íŒ¨ì¹˜ê¹Œì§€ ëª¨ë‘ ì „ì—­ì (<code class="language-plaintext highlighter-rouge">Global</code>)ìœ¼ë¡œ ì–´í…ì…˜ì„ ìˆ˜í–‰í•´ <code class="language-plaintext highlighter-rouge">General</code> í•œ ì •ë³´ë¥¼ í¬ì°©í•˜ê²Œ ë˜ê³  ì¶œë ¥ê³¼ ê°€ê¹Œìš´ <code class="language-plaintext highlighter-rouge">Top Encoder</code>ëŠ” ê°œë³„ í•´ë“œë“¤ì´ ëª¨ë‘ ë¹„ìŠ·í•œ ê±°ë¦¬ì— ìœ„ì¹˜í•œ íŒ¨ì¹˜(<code class="language-plaintext highlighter-rouge">Local</code>)ì— ì–´í…ì…˜ì„ ìˆ˜í–‰í•´ <code class="language-plaintext highlighter-rouge">Specific</code> í•œ ì •ë³´ë¥¼ í¬ì°©í•˜ê²Œ ëœë‹¤.</p>

<p>ì´ ë•Œ <code class="language-plaintext highlighter-rouge">Global</code>ê³¼ <code class="language-plaintext highlighter-rouge">Local</code>ì´ë¼ëŠ” ìš©ì–´ ë•Œë¬¸ì— <code class="language-plaintext highlighter-rouge">Bottom Encoder</code> ëŠ” ë©€ë¦¬ ë–¨ì–´ì§„ íŒ¨ì¹˜ì™€ ì–´í…ì…˜í•˜ê³ , <code class="language-plaintext highlighter-rouge">Top Encoder</code>ëŠ” ê°€ê¹Œìš´ íŒ¨ì¹˜ì™€ ì–´í…ì…˜í•œë‹¤ê³  ì°©ê°í•˜ê¸° ì‰½ë‹¤. <strong><u>ê·¸ëŸ¬ë‚˜ ê°œë³„ í•´ë“œë“¤ì˜ ì–´í…ì…˜ ê±°ë¦¬ê°€ ì–¼ë§ˆë‚˜ ë¶„ì‚°ë˜ì–´ ìˆëŠ”ê°€ê°€ ë°”ë¡œ </u></strong><code class="language-plaintext highlighter-rouge">Global</code>, <code class="language-plaintext highlighter-rouge">Local</code><strong><u>ì„ êµ¬ë¶„í•˜ëŠ” ê¸°ì¤€ì´ ëœë‹¤.</u></strong> ì…ë ¥ë¶€ì— ê°€ê¹Œìš´ ë ˆì´ì–´ë“¤ì€ í—¤ë“œë“¤ì˜ ì–´í…ì…˜ ê±°ë¦¬ ë¶„ì‚°ì´ ë§¤ìš° í° í¸ì¸ë°, ì´ê²ƒì„ ì´íŒ¨ì¹˜ ì €íŒ¨ì¹˜ ëª¨ë‘ ì–´í…ì…˜ í•´ë³´ê³  ë¹„êµí•´ë³¸ë‹¤ê³  í•´ì„í•´ì„œ <code class="language-plaintext highlighter-rouge">Global</code>ì´ë¼ê³  ë¶€ë¥´ê³ , ì¶œë ¥ë¶€ì— ê°€ê¹Œìš´ ë ˆì´ì–´ëŠ” í—¤ë“œë“¤ì˜ ì–´í…ì…˜ ê±°ë¦¬ ë¶„ì‚°ì´ ë§¤ìš° ì‘ì€ í¸ì¸ë°, ì´ê²Œ ë°”ë¡œ ê°ê°ì˜ í—¤ë“œë“¤ì´ ì–´ë–¤ ì •ë³´ì— ì£¼ëª©í•´ì•¼í• ì§€(ë¶„ë¥˜ ì†ì‹¤ì´ ê°€ì¥ ì‘ì•„ì§€ëŠ” íŒ¨ì¹˜) ë²”ìœ„ë¥¼ ì¶©ë¶„íˆ ì¢íŒ ìƒíƒœì—ì„œ íŠ¹ì • ë¶€ë¶„ì—ë§Œ ì§‘ì¤‘í•œë‹¤ëŠ” ì˜ë¯¸ë¡œ í•´ì„í•´ <code class="language-plaintext highlighter-rouge">Local</code> ì´ë¼ê³  ë¶€ë¥´ê²Œ ë˜ì—ˆë‹¤.</p>

<p>&lt;<strong><a href="https://arxiv.org/abs/2006.05987">Revisiting Few-sample BERT Fine-tuning</a></strong>&gt;ë„ ìœ„ì™€ ë¹„ìŠ·í•œ ë§¥ë½ì˜ ì‚¬ì‹¤ì— ëŒ€í•´ ì–¸ê¸‰í•˜ê³  ìˆìœ¼ë‹ˆ ì°¸ê³ í•´ë³´ì. ì´ëŸ¬í•œ ì‚¬ì‹¤ì€ íŠ¸ëœìŠ¤í¬ë¨¸ ì¸ì½”ë” ê³„ì—´ ëª¨ë¸ì„ íŠœë‹í•  ë•Œ <code class="language-plaintext highlighter-rouge">Depth</code> ë³„ë¡œ ë‹¤ë¥¸ <code class="language-plaintext highlighter-rouge">Learning Rate</code>ì„ ì ìš©í•˜ëŠ” <code class="language-plaintext highlighter-rouge">Layerwise Learning Rate Decay</code> ì˜ ì´ˆì„ì´ ë˜ê¸°ë„ í•œë‹¤. <code class="language-plaintext highlighter-rouge">Layerwise Learning Rate Decay</code> ì— ëŒ€í•´ì„œëŠ” <strong><a href="https://towardsdatascience.com/advanced-techniques-for-fine-tuning-transformers-82e4e61e16e">ì—¬ê¸° í¬ìŠ¤íŠ¸</a></strong>ë¥¼ ì°¸ê³ í•˜ë„ë¡ í•˜ì.</p>

<p align="center">
<img src="/assets/images/vision_transformer/insight3_2.png" alt="Output from Last Encoder" class="align-center image-caption" width="40%&quot;, height=&quot;10%" />
<strong><em><a href="https://arxiv.org/abs/2010.11929">Output from Last Encoder</a></em></strong>
</p>

<p>í•œí¸ ë…¼ë¬¸ì—ëŠ” ì–¸ê¸‰ë˜ì§€ ì•Šì€, í•„ìì˜ ë‡Œí”¼ì…œì— ê°€ê¹ì§€ë§Œ, <strong><u>ì¶œë ¥ì— ê°€ê¹Œìš´ ì¸ì½”ë”ë“¤ì˜ í•´ë“œê°€ ê°€ì§„</u></strong> <code class="language-plaintext highlighter-rouge">Attention Distance</code><strong><u>ì´ ëª¨ë‘ ë¹„ìŠ·í•˜ë‹¤ëŠ” ì‚¬ì‹¤ë¡œ ì´ë¯¸ì§€ ë¶„ë¥˜ì— ê²°ì •ì ì¸ ì—­í• ì„ í•˜ëŠ” í”¼ì²˜ê°€ ì´ë¯¸ì§€ì˜ íŠ¹ì • êµ¬ì—­ì— ëª¨ì—¬ ìˆìœ¼ë©°, ê·¸ ìŠ¤íŒŸì€ ì´ë¯¸ì§€ì˜ ì¤‘ì•™ ë¶€ê·¼ì¼ ê°€ëŠ¥ì„±ì´ ë†’ë‹¤ê³  ì¶”ì¸¡ í•´ë³¼ ìˆ˜ ìˆë‹¤.</u></strong> ëª¨ë“  í•´ë“œì˜ í”½ì…€ ê±°ë¦¬ê°€ ì„œë¡œ ë¹„ìŠ·í•˜ë ¤ë©´ ì¼ë‹¨ ë¹„ìŠ·í•œ ìœ„ì¹˜ì˜ íŒ¨ì¹˜ì— ì–´í…ì…˜ì„ í•´ì•¼í•˜ê¸° ë•Œë¬¸ì— ë¶„ë¥˜ ì†ì‹¤ê°’ì„ ìµœì†Œë¡œ ì¤„ì—¬ì£¼ëŠ” í”¼ì²˜ëŠ” ë³´í†µ í•œ êµ¬ì—­(íŒ¨ì¹˜)ì— ëª°ë ¤ ìˆì„ ê²ƒì´ë¼ê³  ìœ ì¶”ê°€ ê°€ëŠ¥í•˜ë‹¤. ë˜í•œ íŠ¹ì • ìŠ¤íŒŸì´ ì¤‘ì•™ì— ìœ„ì¹˜í• ìˆ˜ë¡ ì–´í…ì…˜ ê±°ë¦¬ì˜ ë¶„ì‚°ì´ ì¤„ì–´ë“¤ê²ƒì´ë¼ê³  ìƒê° í•´ë³¼ ìˆ˜ë„ ìˆì—ˆë‹¤. ì €ìëŠ” <code class="language-plaintext highlighter-rouge">Attention Rollout</code>ì´ë¼ëŠ” ê°œë…ì„ í†µí•´ <code class="language-plaintext highlighter-rouge">Attention Distance</code>ì„ ì‚°ì¶œí–ˆë‹¤ê³  ì–¸ê¸‰í•˜ëŠ”ë°, ìì„¸í•œ ë‚´ìš©ì€ ì˜†ì— ë‘ ë§í¬ë¥¼ ì°¸ê³ í•´ë³´ì(<a href="https://hongl.tistory.com/234">í•œêµ­ì–´ ì„¤ëª… ë¸”ë¡œê·¸</a>,  <a href="https://arxiv.org/abs/2005.00928">ì›ë…¼ë¬¸</a>). ì´ëŸ¬í•œ í•„ìì˜ ê°€ì„¤ì´ ë§ë‹¤ë©´, <code class="language-plaintext highlighter-rouge">Convolution</code> ì˜ <code class="language-plaintext highlighter-rouge">Inductive Bias</code>  ì¤‘ <code class="language-plaintext highlighter-rouge">Locality</code> ì˜ íš¨ê³¼ì„±ì„ <code class="language-plaintext highlighter-rouge">Self-Attention</code>ì„ í†µí•´ ì…ì¦ì´ ê°€ëŠ¥í•˜ë©°, ë°˜ëŒ€ë¡œ <code class="language-plaintext highlighter-rouge">Convolution</code>ì— ëŒ€í•œ ì˜ì¡´ì—ì„œ ë²—ì–´ë‚˜ ë‹¨ì¼ <code class="language-plaintext highlighter-rouge">Self-Attention</code> ìœ¼ë¡œë„ ê°™ì€ íš¨ê³¼ë¥¼ ë‚¼ ìˆ˜ ìˆë‹¤ëŠ” ì¦ê±° ì¤‘ í•˜ë‚˜ê°€ ë  ê²ƒì´ë‹¤.</p>

<h4 id="insight-4-vitëŠ”-cls-pooling-ì‚¬ìš©í•˜ëŠ”ê²Œ-íš¨ìœ¨ì "><code class="language-plaintext highlighter-rouge">ğŸ’¡Â Insight 4. ViTëŠ” CLS Pooling ì‚¬ìš©í•˜ëŠ”ê²Œ íš¨ìœ¨ì </code></h4>

<ul>
  <li><strong><code class="language-plaintext highlighter-rouge">CLS Pooling</code>ì€ <code class="language-plaintext highlighter-rouge">GAP</code> ë³´ë‹¤ 2ë°° ì´ìƒ í° í•™ìŠµë¥ ì„ ì‚¬ìš©í•´ë„ ë¹„ìŠ·í•œ ì„±ëŠ¥ì„ ê¸°ë¡</strong>
    <ul>
      <li><strong><u>í•™ìŠµ ì†ë„ëŠ” ë” ë¹ ë¥´ë˜ ì„±ëŠ¥ì´ ë¹„ìŠ·í•˜ê¸° ë•Œë¬¸ì—</u></strong> <code class="language-plaintext highlighter-rouge">CLS Pooling</code> <strong><u>ì´ ë” íš¨ìœ¨ì </u></strong></li>
    </ul>
  </li>
</ul>

<p align="center">
<img src="/assets/images/vision_transformer/insight4.png" alt="Performance Trend by Pooling Method with LR" class="align-center image-caption" width="75%&quot;, height=&quot;50%" />
<strong><em><a href="https://arxiv.org/abs/2010.11929">Performance Trend by Pooling Method with LR</a></em></strong>
</p>

<p>ë‹¤ìŒ ë„í‘œëŠ” í’€ë§ ë°©ì‹ê³¼ í•™ìŠµë¥ ì˜ ë³€ë™ì— ë”°ë¥¸ ì •í™•ë„ ë³€í™” ì¶”ì´ë¥¼ ë‚˜íƒ€ë‚¸ ê·¸ë¦¼ì´ë‹¤. ë¹„ìŠ·í•œ ì„±ëŠ¥ì´ë¼ë©´ <code class="language-plaintext highlighter-rouge">CLS Pooling</code>ì´ <code class="language-plaintext highlighter-rouge">GAP</code>ë³´ë‹¤ 2ë°° ì´ìƒ í° í•™ìŠµë¥ ì„ ì‚¬ìš©í–ˆë‹¤. í•™ìŠµë¥ ì´ í¬ë©´ ëª¨ë¸ì˜ ìˆ˜ë ´ ì†ë„ê°€ ë¹¨ë¼ì ¸ í•™ìŠµ ì†ë„ê°€ ë¹¨ë¼ì§€ëŠ” ì¥ì ì´ ìˆë‹¤. ê·¸ëŸ°ë° ì„±ëŠ¥ê¹Œì§€ ë¹„ìŠ·í•˜ë‹¤ë©´ <code class="language-plaintext highlighter-rouge">ViT</code>ëŠ” <code class="language-plaintext highlighter-rouge">CLS Pooling</code>ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ë” íš¨ìœ¨ì ì´ë¼ê³  í•  ìˆ˜ ìˆê² ë‹¤.</p>

<p>ë‚˜ì¤‘ì— ì‹œê°„ì´ ëœë‹¤ë©´ ë‹¤ë¥¸ í’€ë§ ë°©ì‹, ì˜ˆë¥¼ ë“¤ë©´ <code class="language-plaintext highlighter-rouge">Weighted Layer Pooling</code>, <code class="language-plaintext highlighter-rouge">GeM Pooling</code>, <code class="language-plaintext highlighter-rouge">Attention Pooling</code> ê°™ì€ ê²ƒì„ ì ìš©í•´ ì‹¤í—˜í•´ë³´ê² ë‹¤.</p>

<h4 id="insight-5-vitëŠ”-absolute-1d-position-embedding-ì‚¬ìš©í•˜ëŠ”ê²Œ-ê°€ì¥-íš¨ìœ¨ì "><code class="language-plaintext highlighter-rouge">ğŸ’¡Â Insight 5. ViTëŠ” Absolute 1D-Position Embedding ì‚¬ìš©í•˜ëŠ”ê²Œ ê°€ì¥ íš¨ìœ¨ì </code></h4>

<ul>
  <li><strong>ì–´ë–¤ í˜•íƒœë¡œë“  ìœ„ì¹˜ ì„ë² ë”© ê°’ì„ ì •ì˜í•´ì¤€ë‹¤ë©´, í˜•íƒœì™€ ì¢…ë¥˜ì— ìƒê´€ì—†ì´ ê±°ì˜ ë¹„ìŠ·í•œ ì„±ëŠ¥ì„ ë³´ì„</strong></li>
  <li><strong>ì„±ëŠ¥ì´ ë¹„ìŠ·í•˜ë©´, ì§ê´€ì ì´ê³  êµ¬í˜„ì´ ê°„í¸í•œ <code class="language-plaintext highlighter-rouge">Absolute 1D-Position Embedding</code> ë°©ë²•ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ê°€ì¥ íš¨ìœ¨ì </strong></li>
  <li><strong><code class="language-plaintext highlighter-rouge">ViT</code>ëŠ” <code class="language-plaintext highlighter-rouge">Patch-Level</code> ì‚¬ìš©í•´, <code class="language-plaintext highlighter-rouge">Pixel-Level</code>ë³´ë‹¤ ìƒëŒ€ì ìœ¼ë¡œ ì‹œí€€ìŠ¤ ê¸¸ì´ê°€ ì§§ì•„ ìœ„ì¹˜â€¢ê³µê°„ ì •ë³´ë¥¼ ì¸ì½”ë”©í•˜ëŠ” ë°©ì‹ì— ì˜í–¥ì„ ëœ ë°›ìŒ</strong></li>
</ul>

<p align="center">
<img src="/assets/images/vision_transformer/insight5.png" alt="Performance Table by making Position Embedding method" class="align-center image-caption" width="75%&quot;, height=&quot;50%" />
<strong><em><a href="https://arxiv.org/abs/2010.11929">Performance Table by making Position Embedding method</a></em></strong>
</p>

<p>ìœ„ ì‹¤í—˜ ê²°ê³¼ëŠ” <code class="language-plaintext highlighter-rouge">Position Embedding</code> ì¸ì½”ë”© ë°©ì‹ì— ë”°ë¥¸ <code class="language-plaintext highlighter-rouge">ViT</code> ëª¨ë¸ì˜ ì„±ëŠ¥ ë³€í™” ì¶”ì´ë¥¼ ë‚˜íƒ€ë‚¸ ìë£Œë‹¤. ì¸ì½”ë”© í˜•íƒœì™€ ìƒê´€ì—†ì´ ìœ„ì¹˜ ì„ë² ë”©ì˜ ìœ ë¬´ê°€ ì„±ëŠ¥ì— í° ì˜í–¥ì„ ë¯¸ì¹œë‹¤ëŠ” ì‚¬ì‹¤ì„ ì•Œë ¤ì£¼ê³  ìˆë‹¤. í•œí¸, ì¸ì½”ë”© í˜•íƒœ ë³€í™”ì— ë”°ë¥¸ ìœ ì˜ë¯¸í•œ ì„±ëŠ¥ ë³€í™”ëŠ” ì—†ì—ˆë‹¤. í•˜ì§€ë§Œ <code class="language-plaintext highlighter-rouge">Absolute 1D-Position Embedding</code>ì˜ ì»¨ì…‰ì´ ê°€ì¥ ì§ê´€ì ì´ë©° êµ¬í˜„í•˜ê¸° í¸í•˜ê³  ì—°ì‚°ëŸ‰ì´ ë‹¤ë¥¸ ì¸ì½”ë”©ë³´ë‹¤ ì ë‹¤ëŠ” ê²ƒì„ ê°ì•ˆí•˜ë©´ ViTì— ê°€ì¥ íš¨ìœ¨ì ì¸ ìœ„ì¹˜ ì„ë² ë”© ë°©ì‹ì´ë¼ê³  íŒë‹¨í•  ìˆ˜ ìˆë‹¤.</p>

<p>ë…¼ë¬¸ì€ ê²°ê³¼ì— ëŒ€í•´ <code class="language-plaintext highlighter-rouge">ViT</code>ê°€ ì‚¬ìš©í•˜ëŠ” <code class="language-plaintext highlighter-rouge">Patch-Level Embedding</code>ì´ <code class="language-plaintext highlighter-rouge">Pixel-Level</code>ë³´ë‹¤ ìƒëŒ€ì ìœ¼ë¡œ ì§§ì€ ì‹œí€€ìŠ¤ ê¸¸ì´ë¥¼ ê°–ê¸° ë•Œë¬¸ì´ë¼ê³  ì„¤ëª…í•œë‹¤. ì˜ˆë¥¼ ë“¤ì–´ <code class="language-plaintext highlighter-rouge">224x224</code> ì‚¬ì´ì¦ˆì˜ ì´ë¯¸ì§€ë¥¼ <code class="language-plaintext highlighter-rouge">16x16</code> ì‚¬ì´ì¦ˆì˜ íŒ¨ì¹˜ ì—¬ëŸ¬ì¥ìœ¼ë¡œ ë§Œë“ ë‹¤ê³  ìƒê°í•´ë³´ì. ì„ë² ë”© ì°¨ì›ì— ë“¤ì–´ê°€ëŠ” $N$ ì€ $(224/16)^2$ , ì¦‰ <code class="language-plaintext highlighter-rouge">196</code>ì´ ëœë‹¤. í•œí¸ ì´ê²ƒì„ <code class="language-plaintext highlighter-rouge">Pixel-Level</code>ë¡œ ì„ë² ë”© í•˜ê²Œ ë˜ë©´ $224^2$, ì¦‰ <code class="language-plaintext highlighter-rouge">50176</code> ê°œì˜ ì‹œí€€ìŠ¤ê°€ ìƒê¸´ë‹¤. ë”°ë¼ì„œ <code class="language-plaintext highlighter-rouge">Pixel-Level</code> ì— ë¹„í•˜ë©´ í›¨ì”¬ ì§§ì€ ì‹œí€€ìŠ¤ ê¸¸ì´ë¥¼ ê°–ê¸° ë•Œë¬¸ì— <code class="language-plaintext highlighter-rouge">Absolute 1D-Position Embedding</code> ë§Œìœ¼ë¡œë„ ì¶©ë¶„íˆ <code class="language-plaintext highlighter-rouge">Spatial Relation</code>ì„ í•™ìŠµí•  ìˆ˜ ìˆëŠ” ê²ƒì´ë‹¤.</p>

<p align="center">
<img src="/assets/images/vision_transformer/insight5_2.png" alt="Absolute 1D-Position Embedding" class="align-center image-caption" width="50%&quot;, height=&quot;50%" />
<strong><em><a href="https://arxiv.org/abs/2010.11929">Absolute 1D-Position Embedding</a></em></strong>
</p>

<p>í•˜ì§€ë§Œ, í•„ìëŠ” ìì—°ì–´ ì²˜ë¦¬ì˜ <code class="language-plaintext highlighter-rouge">Transformer-XL</code>, <code class="language-plaintext highlighter-rouge">XLNet</code>, <code class="language-plaintext highlighter-rouge">DeBERTa</code> ê°™ì€ ëª¨ë¸ë“¤ì´ <code class="language-plaintext highlighter-rouge">Relative Position Embedding</code> ë°©ì‹ì„ ì ìš©í•´ í° ì„±ê³µì„ ê±°ë‘” ë°”ê°€ ìˆë‹¤ëŠ” ì ì„ ìƒê°í•˜ë©´ ì´ëŸ° ê²°ê³¼ê°€ ë‚©ë“ì´ ê°€ë©´ì„œë„ ì˜ì•„í–ˆë‹¤.</p>

<p>ì €ìëŠ” ì‹¤í—˜ì— ì‚¬ìš©í•œ ëª¨ë“  ë°ì´í„° ì„¸íŠ¸ë¥¼ <code class="language-plaintext highlighter-rouge">224x224</code>ë¡œ <code class="language-plaintext highlighter-rouge">resize</code> í–ˆë‹¤ê³  ë°íˆê³  ìˆëŠ”ë°, ë§Œì•½ ì´ë¯¸ì§€ ì‚¬ì´ì¦ˆê°€ <code class="language-plaintext highlighter-rouge">512x512</code>ì •ë„ë§Œ ë˜ë”ë¼ë„ $N$ ê°’ì´ <code class="language-plaintext highlighter-rouge">1024</code> ì´ë¼ì„œ ìœ„ ê²°ê³¼ì™€ ìƒë‹¹íˆ ë‹¤ë¥¸ ì–‘ìƒì´ ë‚˜íƒ€ë‚˜ì§€ ì•Šì„ê¹Œ í•˜ëŠ” ìƒê°ì´ ë“ ë‹¤. ì¶”í›„ì— ì‹œê°„ì´ ëœë‹¤ë©´ ì´ ë¶€ë¶„ë„ ê¼­ ì‹¤í—˜í•´ë´ì•¼ê² ë‹¤. ì˜ˆì¸¡ì»¨ë° ì´ë¯¸ì ì‚¬ì´ì¦ˆê°€ ì»¤ì§ˆìˆ˜ë¡ <code class="language-plaintext highlighter-rouge">2D Position Embedding</code> í˜¹ì€ <code class="language-plaintext highlighter-rouge">Relative Position Embedding</code>ì´ ë” íš¨ìœ¨ì ì¼ ê²ƒì´ë¼ ì˜ˆìƒí•œë‹¤.</p>

<h3 id="ï¸conclusion"><code class="language-plaintext highlighter-rouge">ğŸ§‘â€âš–ï¸Â Conclusion</code></h3>

<p>ì´ë ‡ê²Œ <code class="language-plaintext highlighter-rouge">ViT</code> ëª¨ë¸ì„ ì œì•ˆí•œ <a href="https://arxiv.org/abs/2010.11929">&lt;An Image Is Worth 16x16 Words: Transformers For Image Recognition At Scale&gt;</a>ì— ì‹¤ë¦° ë‚´ìš©ì„ ëª¨ë‘ ì‚´í´ë³´ì•˜ë‹¤. <code class="language-plaintext highlighter-rouge">Conv</code> ì— ëŒ€í•œ ì˜ì¡´ì„ íƒˆí”¼ í–ˆë‹¤ëŠ” ì ì—ì„œ ë§¤ìš° ì˜ë¯¸ê°€ ìˆëŠ” ì‹œë„ì˜€ìœ¼ë©°, Self-Attention &amp; Transformer êµ¬ì¡° ì±„íƒë§Œìœ¼ë¡œë„ ì»´í“¨í„° ë¹„ì „ ì˜ì—­ì— ì–´ëŠ ì •ë„  <code class="language-plaintext highlighter-rouge">scalability</code> ë¥¼  ì´ì‹í•˜ëŠ”ë° ì„±ê³µí–ˆë‹¤ëŠ” ì ì—ì„œ í›„ëŒ€ ì—°êµ¬ì— ì¤‘ìš”í•œ ì‹œì‚¬ì ì„ ë‚¨ê²¼ë‹¤. ìƒëŒ€ì ìœ¼ë¡œ ì •ì²´(??)ë˜ì–´ ìˆë˜ ë¹„ì „ ì˜ì—­ì´ ì„±ëŠ¥ì˜ í•œê³„ë¥¼ í•œë‹¨ê³„ ë›°ì–´ë„˜ì„ ìˆ˜ ìˆëŠ” ì´ˆì„ì„ ë§ˆë ¨í•´ì¤€ ì…ˆì´ë‹¤.</p>

<p>í•˜ì§€ë§Œ, <code class="language-plaintext highlighter-rouge">ViT</code>ì˜ <code class="language-plaintext highlighter-rouge">Pretrain Stage</code>ì— ì í•©í•œ <code class="language-plaintext highlighter-rouge">Self-Supervised Learning</code> ë°©ë²•ì„ ì°¾ì§€ ëª»í•´ ì—¬ì „íˆ <code class="language-plaintext highlighter-rouge">Supervised Learning</code> ë°©ì‹ì„ ì±„íƒí•œ ì ì€ ë§¤ìš° ì•„ì‰¬ì› ë‹¤. <strong><u>ì´ëŠ” ê²°êµ­ ë°ì´í„°</u></strong> <code class="language-plaintext highlighter-rouge">Scale</code> <strong><u>í™•ì¥ì— í•œê³„ë¥¼ ì˜ë¯¸í•˜ê¸° ë•Œë¬¸ì´ë‹¤.</u></strong> ì˜¤ëŠ˜ë‚  BERTì™€ GPTì˜ ì„±ê³µ ì‹ í™”ëŠ” ë¹„ë‹¨ <code class="language-plaintext highlighter-rouge">Self-Attention</code>ì™€ <code class="language-plaintext highlighter-rouge">Transformer</code>ì˜ êµ¬ì¡°ì  íƒì›”ì„±ì— ì˜í•´ì„œë§Œ íƒ„ìƒí•œê²Œ ì•„ë‹ˆë‹¤. ì´ì— ëª»ì§€ ì•Šê²Œ(ê°œì¸ì ìœ¼ë¡œ ì œì¼ ì¤‘ìš”í•˜ë‹¤ ìƒê°) ì£¼ìš”í–ˆë˜ ê²ƒì´ ë°”ë¡œ ë°ì´í„° <code class="language-plaintext highlighter-rouge">Scale</code> í™•ì¥ì´ë‹¤.  <code class="language-plaintext highlighter-rouge">MLM</code>, <code class="language-plaintext highlighter-rouge">AR</code> ë“±ì˜ <code class="language-plaintext highlighter-rouge">Self-Supervised Learning</code> ë•ë¶„ì— ë°ì´í„° <code class="language-plaintext highlighter-rouge">Scale</code>ì„ íš¨ìœ¨ì ìœ¼ë¡œ ìŠ¤ì¼€ì¼ ì—… ì‹œí‚¬ ìˆ˜ ìˆì—ˆê³ , ì‚¬ì „ í›ˆë ¨ ë°ì´í„°ì˜ ì¦ê°€ëŠ” ëª¨ë¸ ê¹Šì´, ë„ˆë¹„, ì°¨ì›ê¹Œì§€ ë”ìš± í¬ì¼€ í‚¤ìš°ëŠ”ë° ê¸°ì—¬í–ˆë‹¤.</p>

<p>ë˜í•œ <code class="language-plaintext highlighter-rouge">ViT</code>ëŠ” ì„ ì²œì ìœ¼ë¡œ <code class="language-plaintext highlighter-rouge">Patch-Level Embedding</code>ì„ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì— ë‹¤ì–‘í•œ ì´ë¯¸ì§€ í…ŒìŠ¤í¬ì— ì ìš©í•˜ëŠ” ê²ƒì´ í˜ë“¤ë‹¤. <code class="language-plaintext highlighter-rouge">Segmentation</code>, <code class="language-plaintext highlighter-rouge">Object Detection</code> ê°™ì€ TaskëŠ” í”½ì…€ ë‹¨ìœ„ë¡œ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•´ ê°ì²´ë¥¼ íƒì§€í•˜ê±°ë‚˜ ë¶„í• í•´ì•¼ í•œë‹¤. í•˜ì§€ë§Œ <code class="language-plaintext highlighter-rouge">Patch</code> ë‹¨ìœ„ë¡œ í›ˆë ¨ì„ ìˆ˜í–‰í–ˆë˜ <code class="language-plaintext highlighter-rouge">ViT</code>ëŠ” <code class="language-plaintext highlighter-rouge">Pixel</code> ë‹¨ìœ„ì˜ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•˜ëŠ”ë° ì–´ë ¤ì›€ì„ ê²ªëŠ”ë‹¤.</p>

<p>ë§ˆì§€ë§‰ìœ¼ë¡œ <code class="language-plaintext highlighter-rouge">Self-Attention</code> ìì²´ì˜ <code class="language-plaintext highlighter-rouge">Computational Overhead</code>ê°€ ë„ˆë¬´ ì‹¬í•´ ê³ í•´ìƒë„ì˜ ì´ë¯¸ì§€ë¥¼ ì ì ˆíˆ ë‹¤ë£¨ê¸° í˜ë“¤ë‹¤. ìœ„ì—ì„œë„ ì–¸ê¸‰í–ˆì§€ë§Œ ì´ë¯¸ì§€ì˜ ì‚¬ì´ì¦ˆê°€ <code class="language-plaintext highlighter-rouge">512x512</code>ë§Œ ë˜ì–´ë„ ì´ë¯¸ íŒ¨ì¹˜ì˜ ê°œìˆ˜ê°€ <code class="language-plaintext highlighter-rouge">1024</code>ê°€ ëœë‹¤. ì‚¬ì´ì¦ˆê°€ ì»¤ì§ˆìˆ˜ë¡ ì‹œí€€ìŠ¤ ê¸¸ì´ ì—­ì‹œ ê¸°í•˜ê¸‰ìˆ˜ì ìœ¼ë¡œ ì»¤ì§€ëŠ”ë°ë‹¤ê°€ <code class="language-plaintext highlighter-rouge">Self-Attention</code> ëŠ” ì¿¼ë¦¬ì™€ í‚¤ í–‰ë ¬ì„ ë‚´ì  (ìê¸° ìì‹ ê³¼ ê³±ì´ë¼ ë³¼ ìˆ˜ ìˆìŒ) í•˜ê¸° ë•Œë¬¸ì— <code class="language-plaintext highlighter-rouge">Computational Overhead</code>ê°€ $N^2$ì´ ëœë‹¤.</p>

<p>í•„ìëŠ” <code class="language-plaintext highlighter-rouge">ViT</code>ë¥¼ ì ˆë°˜ì˜ ì„±ê³µì´ë¼ê³  í‰í•˜ê³  ì‹¶ë‹¤. ë³¸ë˜ <code class="language-plaintext highlighter-rouge">ViT</code>ì˜ ì„¤ê³„ ëª©ì ì€ ë¹„ì „ ë¶„ì•¼ì˜ <code class="language-plaintext highlighter-rouge">Conv</code>ì— ëŒ€í•œ ì˜ì¡´ì„ íƒˆí”¼í•˜ë©´ì„œ, í“¨ì–´í•œ <code class="language-plaintext highlighter-rouge">Self-Attention</code>ì„ ë„ì…í•´ <code class="language-plaintext highlighter-rouge">Scalabilty</code> ë¥¼ ì´ì‹í•˜ëŠ” ê²ƒì´ì—ˆë‹¤. <code class="language-plaintext highlighter-rouge">Self-Attention</code>ì„ ë„ì…í•˜ëŠ”ë°ëŠ” ì„±ê³µí–ˆì§€ë§Œ, ì—¬ì „íˆ ë‹¤ë£° ìˆ˜ ìˆëŠ” ì´ë¯¸ì§€ ì‚¬ì´ì¦ˆë‚˜ Taskì—ëŠ” í•œê³„ê°€ ë¶„ëª…í•˜ë©° ê²°ì •ì ìœ¼ë¡œ <code class="language-plaintext highlighter-rouge">Self-Supervised Learning</code> ë°©ì‹ì„ ë„ì…í•˜ì§€ ëª»í–ˆë‹¤. <code class="language-plaintext highlighter-rouge">Scalabilty</code> ë¼ëŠ” ë‹¨ì–´ì˜ ì˜ë¯¸ë¥¼ ìƒê°í•˜ë©´, ë°©ê¸ˆ ë§í•œ ë¶€ë¶„ì—ì„œê¹Œì§€ í™•ì¥ì„±ì´ ìˆì–´ì•¼ ì„¤ê³„ ì˜ë„ì— ë¶€í•©í•˜ëŠ” ê²°ê³¼ë¼ê³  ìƒê°í•œë‹¤.</p>]]></content><author><name>qcqced</name><email>qcqced123@gmail.com</email></author><category term="Computer Vision" /><category term="Computer Vision" /><category term="Vision Transformer" /><category term="ViT" /><category term="Transformer" /><category term="Self-Attention" /><category term="Image Classification" /><summary type="html"><![CDATA[ViT Official Paper Review with Pytorch Implementation]]></summary></entry><entry><title type="html">ğŸ”¢Â Vector Space: Column Space, Basis, Rank, Null Space</title><link href="http://localhost:4000/linear-algebra/vector-subspace" rel="alternate" type="text/html" title="ğŸ”¢Â Vector Space: Column Space, Basis, Rank, Null Space" /><published>2023-07-19T00:00:00+09:00</published><updated>2023-07-10T13:00:00+09:00</updated><id>http://localhost:4000/linear-algebra/vector_space</id><content type="html" xml:base="http://localhost:4000/linear-algebra/vector-subspace"><![CDATA[<h3 id="-column-space"><code class="language-plaintext highlighter-rouge">ğŸ”¢ Column Space</code></h3>

\[C(A) = Range(A)\]

<p>ì—´ë²¡í„°ê°€ <code class="language-plaintext highlighter-rouge">span</code>í•˜ëŠ” ê³µê°„ì„ ì˜ë¯¸í•œë‹¤. <code class="language-plaintext highlighter-rouge">span</code> ì´ë€, ë²¡í„°ì˜ ì§‘í•©ì— ì˜í•´ ìƒì„±ëœ ëª¨ë“  <code class="language-plaintext highlighter-rouge">linear combination</code>ì˜ ê²°ê³¼ë¡œ ìƒì„±í•  ìˆ˜ ìˆëŠ” ë¶€ë¶„ ê³µê°„ì„ ë§í•œë‹¤. ë”°ë¼ì„œ <code class="language-plaintext highlighter-rouge">column space</code> ëŠ” ì—´ë²¡í„°ì˜ <code class="language-plaintext highlighter-rouge">linear combination</code> ê²°ê³¼ë¡œ ìƒì„±í•  ìˆ˜ ìˆëŠ” <code class="language-plaintext highlighter-rouge">vector space</code>ì˜ ë¶€ë¶„ ê³µê°„ì„ ë§í•œë‹¤.</p>
<h3 id="-basis"><code class="language-plaintext highlighter-rouge">ğŸ– Basis</code></h3>
<figure class="half">
  <a href="https://twlab.tistory.com/24"><img src="/assets/images/linear_independent.png" title="Linear Independent" /></a>
  <a href="https://twlab.tistory.com/24"><img src="/assets/images/linear_dependent.png" title="Linear Independent" /></a>
</figure>
<p>ê¸°ì €ì— ëŒ€í•´ ì•Œê¸° ìœ„í•´ì„œëŠ” ë¨¼ì € <code class="language-plaintext highlighter-rouge">linear independent(ì„ í˜• ë…ë¦½)</code>ì˜ ì˜ë¯¸ë¥¼ ì•Œì•„ì•¼ í•œë‹¤. ì„ í˜•ë…ë¦½ì´ë€, ì™¼ìª½ ê·¸ë¦¼ì²˜ëŸ¼ ì„œë¡œ ë‹¤ë¥¸ ë²¡í„°ë“¤ì´ ê´€ë ¨ì„± ì—†ì´ ë…ë¦½ì ìœ¼ë¡œ ì¡´ì¬í•˜ëŠ” ìƒíƒœë¥¼ ë§í•œë‹¤. ë”°ë¼ì„œ ì„œë¡œ ë‹¤ë¥¸ ë‘ ë²¡í„°ê°€ ì„ í˜• ë…ë¦½ì´ë¼ë©´ í•œ ë²¡í„°ì˜ ì„ í˜•ì¡°í•©ìœ¼ë¡œ ë‹¤ë¥¸ ë²¡í„°ë¥¼ í‘œí˜„í•  ìˆ˜ ì—†ë‹¤. ë°˜ëŒ€ë¡œ ì„ í˜• ì¢…ì† ìƒíƒœë©´ ì˜¤ë¥¸ìª½ ê·¸ë¦¼ì²˜ëŸ¼ ë²¡í„°ë¥¼ ë‹¤ë¥¸ ë²¡í„°ì˜ ì„ í˜•ì¡°í•©ìœ¼ë¡œ í‘œí˜„ ê°€ëŠ¥í•˜ë‹¤.</p>

<p>ì´ì œ ê¸°ì €ì— ëŒ€í•´ ì•Œì•„ë³´ì. ê¸°ì €ë€ ì„ í˜• ë…ë¦½ì´ë©´ì„œ ë²¡í„° ê³µê°„ì„ <code class="language-plaintext highlighter-rouge">span</code> í•˜ëŠ” ë²¡í„° ì§‘í•©ì„ ë§í•œë‹¤. ë‹¤ì‹œ ë§í•´, ê³µê°„ ë˜ëŠ” ì°¨ì›ì„ í‘œí˜„í•˜ëŠ”ë° í•„ìš”í•œ ìš”ì†Œë“¤ì˜ ì§‘í•©ì´ë¼ê³  ë³¼ ìˆ˜ ìˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ 2ì°¨ì› ê³µê°„ì„ í‘œí˜„í•˜ê³  ì‹¶ë‹¤ë©´ ì„œë¡œ ì„ í˜• ë…ë¦½ì¸ ë²¡í„° 2ê°œê°€ í•„ìš”í•˜ë‹¤. ì˜¤ë¥¸ìª½ ê·¸ë¦¼ì²˜ëŸ¼ ë²¡í„° 2ê°œê°€ ì¡´ì¬í•´ë„ ì„œë¡œ ì¢…ì† ê´€ê³„ë¼ë©´ í‘œí˜„(span)í•  ìˆ˜ ìˆëŠ” ê³µê°„ì€ 1ì°¨ì›ì˜ ì§ì„ ì´ ë˜ê¸° ë•Œë¬¸ì´ë‹¤. ì •ë¦¬í•˜ë©´, $N$ì°¨ì› ê³µê°„ì˜ ê¸°ì €ë€ ì„ í˜• ë…ë¦½ì´ë©´ì„œ ë²¡í„° ê³µê°„ì„ <code class="language-plaintext highlighter-rouge">span</code>í•˜ëŠ” ë²¡í„°ê°€ $N$ê°œ ìˆëŠ” ìƒíƒœë‹¤. ì¶”ê°€ë¡œ, $N$ì°¨ì› ê³µê°„ì˜ ê¸°ì €ëŠ”<code class="language-plaintext highlighter-rouge">NxN</code> í¬ê¸°ì˜ <code class="language-plaintext highlighter-rouge">Invertable</code>í•œ í–‰ë ¬ê³¼ ë™ì¹˜ë¥¼ ì´ë£¬ë‹¤. ë’¤ì—ì„œ ë” ìì„¸íˆ ë‹¤ë£¨ê² ì§€ë§Œ ì—­í–‰ë ¬ì€ ì¢Œí‘œí‰ë©´ ìƒì—ì„œ <code class="language-plaintext highlighter-rouge">reverse linear combination</code> ì˜ ì—­í• ì„ í•˜ê¸° ë•Œë¬¸ì´ë‹¤.<br />
í•œí¸ ê¸°ì €ëŠ” ìœ ì¼í•˜ì§€ ì•Šë‹¤. ìœ„ì—ì„œ ì–¸ê¸‰í•œ $N$ì°¨ì› ê¸°ì €ì˜ í•„ìš”ì¶©ë¶„ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” ëª¨ë“  ë²¡í„° ì§‘í•©ì€ ëª¨ë‘ ê¸°ì €ê°€ ë  ìˆ˜ ìˆë‹¤.</p>

<h3 id="-standard-basis"><code class="language-plaintext highlighter-rouge">ğŸ¦´ Standard Basis</code></h3>

\[I= 
   \begin{pmatrix} 
   1 &amp; 0 &amp; 0  \\
   0 &amp; 1 &amp; 0  \\
   0 &amp; 0 &amp; 1  \\
   \end{pmatrix}\]

<p>í‘œì¤€ ê¸°ì €ë€, ê¸°ì €ê°€ í‘œí˜„í•˜ëŠ” ì°¨ì›ì˜ ì¶•ì´ ìš°ë¦¬ê°€ í”íˆ ì•„ëŠ” <code class="language-plaintext highlighter-rouge">xì¶•, yì¶•, zì¶•</code> ì´ ë˜ëŠ” ê¸°ì € ë²¡í„°ë¥¼ ë§í•œë‹¤. ìˆ˜í•™ì ìœ¼ë¡œëŠ” ì£¼ëŒ€ê°ì„±ë¶„ì˜ ê°’ì´ ëª¨ë‘ 1ì¸ ëŒ€ê°í–‰ë ¬ $D$, ì¦‰ ë‹¨ìœ„ í–‰ë ¬ $I$ê°€ ê¸°ì €ì¼ ë•Œ ìš°ë¦¬ëŠ” í‘œì¤€ ê¸°ì €ë¼ê³  ì •ì˜í•œë‹¤.</p>

<h3 id="-rank"><code class="language-plaintext highlighter-rouge">ğŸ§® Rank</code></h3>

<p align="center">
<img src="/assets/images/column_space.png" alt="Column Space Image" class="align-center image-caption" width="100%&quot;, height=&quot;50%" />
<strong><em><a href="https://www.researchgate.net/figure/Example-of-a-projection-of-a-matrix-3-2-on-the-column-space_fig2_220103928">Column Space Image</a></em></strong>
</p>

<p>í–‰ë ¬ì—ì„œ <code class="language-plaintext highlighter-rouge">independent</code>í•œ <code class="language-plaintext highlighter-rouge">column</code>ì˜ ê°œìˆ˜ë¥¼ ì˜ë¯¸í•˜ë©°, ê¸°í•˜í•™ì ìœ¼ë¡œëŠ” <code class="language-plaintext highlighter-rouge">column space</code>ê°€ ì‹¤ì œ <code class="language-plaintext highlighter-rouge">span</code>í•˜ëŠ” ê³µê°„ì˜ ì°¨ì›ì„ ë§í•œë‹¤. <code class="language-plaintext highlighter-rouge">Rank Theorem</code> ì— ì˜í•´, í–‰ë ¬ $A$ column vectorëŠ” í–‰ë ¬ $A^T$ì˜ row vectorì™€ ê°™ë‹¤. ë”°ë¼ì„œ column rankì™€ row rank ê°’ ì—­ì‹œ í•­ìƒ ë™ì¼í•˜ë‹¤. í–‰ë ¬ $A$ì˜ ë­í¬ëŠ” $rank(A)$ë¡œ í‘œê¸°í•œë‹¤.</p>

<p>í–‰ë ¬ì˜ ë­í¬ëŠ” í–‰ë ¬ì˜ ìƒê¹€ìƒˆì— ë”°ë¼ ë¶€ë¥´ëŠ” ëª…ì¹­ì´ ì¡°ê¸ˆì”© ë°”ë€ë‹¤. ì˜ˆë¥¼ ë“¤ì–´ ì—´ë²¡í„°ê°€ ëª¨ë‘ ì„ í˜• ë…ë¦½ì´ë©´ì„œ í¬ê¸°ê°€ <code class="language-plaintext highlighter-rouge">10x3</code> ì¸ í–‰ë ¬ $C$ê°€ ìˆë‹¤ê³  ê°€ì •í•´ë³´ì. ëª¨ë“  ì—´ë²¡í„°ê°€ ì„ í˜• ë…ë¦½ì´ê¸° ë•Œë¬¸ì— ìš°ë¦¬ëŠ” í–‰ë ¬ $C$ì˜ ë­í¬ê°€ 3ì´ë¼ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤. ì´ ë•Œ í–‰ë ¬ $C$ë¥¼  <code class="language-plaintext highlighter-rouge">full-column rank</code> ë¼ê³  ë¶€ë¥¸ë‹¤. ê·¸ë¦¬ê³  í–‰ë²¡í„°ì˜ ë­í¬ ì—­ì‹œ ë­í¬ ì •ë¦¬ ì´ë¡ ì— ì˜í•´ 3ì´ ë  ê²ƒì´ë‹¤. ì´ë²ˆì—ëŠ” í–‰ë ¬ $C$ì˜ ì—´ë²¡í„° ë­í¬ê°€ 2ë¼ê³  ê°€ì •í•´ë³´ì. ìš°ë¦¬ëŠ” ì´ ë•Œ í–‰ë ¬ $C$ë¥¼ <code class="language-plaintext highlighter-rouge">rank-deficient</code>ë¡œ ì •ì˜í•œë‹¤. ë§Œì•½ í–‰ë ¬ $C$ì˜ ì—´ë²¡í„°ê°€ ëª¨ë‘ ì„ í˜•ë…ë¦½ì´ê³  ê·¸ í¬ê¸°ê°€ <code class="language-plaintext highlighter-rouge">10x10</code>ì´ë¼ë©´ ë­ë¼ê³  ë¶€ë¥¼ê¹Œ?? ì´ ë•ŒëŠ” ì—´ë²¡í„°, í–‰ë²¡í„° ëª¨ë‘ ë­í¬ê°€ 10ì´ ë˜ê¸° ë•Œë¬¸ì— <code class="language-plaintext highlighter-rouge">full-rank</code> ë¼ê³  ë¶€ë¥¸ë‹¤.</p>

<p>ì •ë¦¬í•˜ë©´ í–‰ë ¬ì˜ ë­í¬ë€, í–‰ë ¬ì˜ í–‰ì˜ í¬ê¸° M ê·¸ë¦¬ê³  ì—´ì˜ í¬ê¸° N ì¤‘ì—ì„œ ë” ì‘ì€ê°’ë³´ë‹¤ ê°™ê±°ë‚˜ ì‘ìœ¼ë©´ì„œ <code class="language-plaintext highlighter-rouge">independent</code>í•œ <code class="language-plaintext highlighter-rouge">column</code>ì˜ ê°œìˆ˜ë¼ëŠ” ì˜ë¯¸ë¥¼ ë‚´í¬í•œ ê°œë…ì´ë¼ê³  ë³¼ ìˆ˜ ìˆê² ë‹¤.</p>

<p>ì¶”ê°€ë¡œ, column vectorì™€ row vectorë¥¼ ìˆœì„œëŒ€ë¡œ ê³±í•˜ë©´ í•­ìƒ $Rank = 1$ì¸ í–‰ë ¬ $A$ê°€ ë§Œë“¤ì–´ì§„ë‹¤ëŠ” ê²ƒì´ë‹¤. ê·¸ë ‡ê²Œ ë§Œë“¤ì–´ì§„ í–‰ë ¬ì˜ ì›ì†Œê°€ ë‘ ë²¡í„°ì˜ <code class="language-plaintext highlighter-rouge">linear combination</code>  ìœ¼ë¡œ êµ¬ì„±ëœ ê²ƒì´ë¼ì„œ ë‹¹ì—°í•œ ì†Œë¦¬ë¼ê³  ìƒê°í•  ìˆ˜ ìˆì§€ë§Œ, ì´ê²ƒì€ ì„ í˜•ëŒ€ìˆ˜í•™ì—ì„œ ë§¤ìš° ì¤‘ìš”í•œ ì„±ì§ˆì´ ëœë‹¤. ë’¤ì§‘ì–´ì„œ ë³´ë©´ ì–´ë–¤ í–‰ë ¬ì˜ $Rank=1$ì´ë¼ëŠ” ê²ƒì€ ê·¸ í–‰ë ¬ì´ ì–´ë–¤ ë‹¤ë¥¸ í–‰ë ¬ì˜ ê¸°ë³¸ ë‹¨ìœ„ ìš”ì†Œê°€ ëœë‹¤ëŠ” ì˜ë¯¸ì´ê¸° ë•Œë¬¸ì´ë‹¤. ì–´ë–¤ í–‰ë ¬ì˜ ë­í¬ê°€ 4ë¼ëŠ” ê²ƒì€ ë­í¬ 1ì§œë¦¬ í–‰ë ¬ 4ê°œì˜ ì¡°í•©ì´ë¼ê³  ìƒê°í•´ë³¼ ìˆ˜ ìˆë‹¤.</p>

<h3 id="-null-space"><code class="language-plaintext highlighter-rouge">ğŸ‘Œ Null Space</code></h3>

\[Ax=0\]

<p>ìœ„ ìˆ˜ì‹ì„ ë§Œì¡±í•˜ëŠ” ë²¡í„° $x$ì˜ ì§‘í•©ì„ ë§í•œë‹¤. ë‹¤ì‹œ ë§í•´, ì„ í˜• ë³€í™˜ $A$(í¬ê¸°ê°€ MxNì¸ í–‰ë ¬)ë¥¼ í†µí•´ 0ì´ ë˜ëŠ” ë²¡í„° ì§‘í•© $x$ê°€ ë°”ë¡œ <code class="language-plaintext highlighter-rouge">null space(ì˜ê³µê°„)</code>ì´ë‹¤. ì˜ê³µê°„ì€ ì„ í˜•ë³€í™˜ $A$ì˜ ë­í¬ì™€ ë¬´ê´€í•˜ë©° ì„ í˜•ë³€í™˜ Aì˜ ì—´ì°¨ì›ì¸ $R^N$ìƒì— ì¡´ì¬í•˜ëŠ” ê³µê°„ì´ë‹¤. ê·¸ë˜ì„œ $Ax=0$ì„ í–‰ë ¬ê³¼ ë²¡í„°ì˜ ë‚´ì ìœ¼ë¡œ í•´ì„í•˜ë©´ ì˜ê³µê°„ì€ ì„ í˜•ë³€í™˜ $A$ì˜ row spaceì™€ ìˆ˜ì§ì´ë‹¤ë¼ëŠ” ì‚¬ì‹¤ì„ ì•Œ ìˆ˜ ìˆë‹¤.</p>

\[N_A = dim(Null(A)) - rank(A)\]

<p>í•œí¸, ì˜ê³µê°„ì´ <code class="language-plaintext highlighter-rouge">span</code> í•˜ëŠ” ê³µê°„ì˜ ì°¨ì›ê³¼ ì…˜í˜•ë³€í™˜ $A$ì˜ ë­í¬ë¥¼ ë”í•˜ë©´ ì…˜í˜•ë³€í™˜ $A$ì˜ ì—´ì°¨ì›ì„ ì•Œ ìˆ˜ ìˆë‹¤. ìˆ˜ì‹ìœ¼ë¡œ í‘œí˜„í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.</p>

<h3 id="-left-null-space"><code class="language-plaintext highlighter-rouge">ğŸ«² Left Null Space</code></h3>

\[A^Tx=0\]

<p>ì„ í˜•ë³€í™˜ $A$ì˜ í¬ê¸°ê°€ MxNì¼ ë•Œ, $A$ì˜ ì¢Œ ì˜ê³µê°„ì€ $A$ì˜ ëª¨ë“  ì—´ë“¤ì— ëŒ€í•´ ì„ í˜• ì¡°í•©ìœ¼ë¡œ 0 ë²¡í„°(ì˜ë²¡í„°)ê°€ ë˜ëŠ” ëª¨ë“  ë²¡í„° ì§‘í•© $x$ì˜ ê³µê°„ì„ <code class="language-plaintext highlighter-rouge">ì¢Œì˜ê³µê°„</code>ì´ë¼ê³  í•œë‹¤. $A$ì˜ ì—´ë²¡í„°ì— ëŒ€í•œ ì˜ê³µê°„ì´ë¼ëŠ” ê²ƒì´ í¬ì¸íŠ¸ê°€ ëœë‹¤. ë”°ë¼ì„œ ì¢Œì˜ê³µê°„ì€ ì„ í˜•ë³€í™˜ $A$ì˜ ì „ì¹˜í–‰ë ¬ì¸ $A^T$ì˜ ì˜ê³µê°„ì„ êµ¬í•˜ëŠ” ê²ƒê³¼ ê°™ìœ¼ë©°, ì„ í˜•ë³€í™˜ $A$ì˜ column spaceì™€ ìˆ˜ì§í•˜ê²Œ ëœë‹¤.</p>]]></content><author><name>qcqced</name><email>qcqced123@gmail.com</email></author><category term="Linear Algebra" /><category term="Linear Algebra" /><category term="linear independent" /><category term="vector space" /><category term="rank" /><category term="column space" /><category term="null space" /><category term="basis" /><summary type="html"><![CDATA[ğŸ’¡ Concept of main sub-space]]></summary></entry><entry><title type="html">ğŸ² RuntimeError: CUDA error: CUBLAS_STATUS_ALLOC_FAILED when callingÂ cublasCreate(handâ‰¤)</title><link href="http://localhost:4000/framework-library/mismatch-embedding" rel="alternate" type="text/html" title="ğŸ² RuntimeError: CUDA error: CUBLAS_STATUS_ALLOC_FAILED when callingÂ cublasCreate(handâ‰¤)" /><published>2023-07-17T00:00:00+09:00</published><updated>2023-07-18T02:00:00+09:00</updated><id>http://localhost:4000/framework-library/embedding_mismatch</id><content type="html" xml:base="http://localhost:4000/framework-library/mismatch-embedding"><![CDATA[<h3 id="-nnembedding-ì°¨ì›--ì‹¤ì œ-ë°ì´í„°-ì…ë ¥-ì°¨ì›"><code class="language-plaintext highlighter-rouge">ğŸ˜µ nn.Embedding ì°¨ì› â‰  ì‹¤ì œ ë°ì´í„° ì…ë ¥ ì°¨ì›</code></h3>
<p><code class="language-plaintext highlighter-rouge">torch.nn.Embedding</code>ì—ì„œ ì •ì˜í•œ ì…ì¶œë ¥ ì°¨ì›ê³¼ ì‹¤ì œ ë°ì´í„°ì˜ ì°¨ì›ì´ ë‹¤ë¥¸ ê²½ìš°ì— ë°œìƒí•˜ëŠ” ì—ëŸ¬ë‹¤. ë‹¤ì–‘í•œ ìƒí™©ì—ì„œ ë§ˆì£¼í•  ìˆ˜ ìˆëŠ” ì—ëŸ¬ì§€ë§Œ, í•„ìì˜ ê²½ìš° <code class="language-plaintext highlighter-rouge">Huggingface</code>ì—ì„œ ë¶ˆëŸ¬ì˜¨<code class="language-plaintext highlighter-rouge">pretrained tokenizer</code>ì— <code class="language-plaintext highlighter-rouge">special token</code> ì„ ì¶”ê°€í•´ ì‚¬ìš©í•  ë•Œ, í† í°ì„ ì¶”ê°€í–ˆë‹¤ëŠ” ì‚¬ì‹¤ì„ ìŠê³  <code class="language-plaintext highlighter-rouge">nn.Embedding</code> ì— ì •ì˜í•œ ì…ì¶œë ¥ ì°¨ì›ì„ ë³€ê²½í•˜ì§€ ì•Šì•„ì„œ ë°œìƒí•˜ëŠ” ê²½ìš°ê°€ ë§ì•˜ë‹¤.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoConfig</span><span class="p">,</span> <span class="n">AutoModel</span>

<span class="k">class</span> <span class="nc">CFG</span><span class="p">:</span>
    <span class="n">model_name</span> <span class="o">=</span> <span class="s">'microsoft/deberta-v3-large'</span>
    <span class="n">config</span> <span class="o">=</span> <span class="n">AutoConfig</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>
    <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">AutoModel</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">add_markdown_token</span><span class="p">(</span><span class="n">cfg</span><span class="p">:</span> <span class="n">sCFG</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
    <span class="s">"""
    Add MarkDown token to pretrained tokenizer ('[MD]')
    Args:
        cfg: CFG, needed to load tokenizer from Huggingface AutoTokenizer
    """</span>
    <span class="n">markdown_token</span> <span class="o">=</span> <span class="s">'[MD]'</span>
    <span class="n">special_tokens_dict</span> <span class="o">=</span> <span class="p">{</span><span class="s">'additional_special_tokens'</span><span class="p">:</span> <span class="p">[</span><span class="sa">f</span><span class="s">'</span><span class="si">{</span><span class="n">markdown_token</span><span class="si">}</span><span class="s">'</span><span class="p">]}</span>
    <span class="n">cfg</span><span class="p">.</span><span class="n">tokenizer</span><span class="p">.</span><span class="n">add_special_tokens</span><span class="p">(</span><span class="n">special_tokens_dict</span><span class="p">)</span>
    <span class="n">markdown_token_id</span> <span class="o">=</span> <span class="n">cfg</span><span class="p">.</span><span class="n">tokenizer</span><span class="p">(</span><span class="sa">f</span><span class="s">'</span><span class="si">{</span><span class="n">markdown_token</span><span class="si">}</span><span class="s">'</span><span class="p">,</span> <span class="n">add_special_tokens</span><span class="o">=</span><span class="bp">False</span><span class="p">)[</span><span class="s">'input_ids'</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>

    <span class="nb">setattr</span><span class="p">(</span><span class="n">cfg</span><span class="p">.</span><span class="n">tokenizer</span><span class="p">,</span> <span class="s">'markdown_token'</span><span class="p">,</span> <span class="sa">f</span><span class="s">'</span><span class="si">{</span><span class="n">markdown_token</span><span class="si">}</span><span class="s">'</span><span class="p">)</span>
    <span class="nb">setattr</span><span class="p">(</span><span class="n">cfg</span><span class="p">.</span><span class="n">tokenizer</span><span class="p">,</span> <span class="s">'markdown_token_id'</span><span class="p">,</span> <span class="n">markdown_token_id</span><span class="p">)</span>
    <span class="n">cfg</span><span class="p">.</span><span class="n">tokenizer</span><span class="p">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="sa">f</span><span class="s">'</span><span class="si">{</span><span class="n">cfg</span><span class="p">.</span><span class="n">checkpoint_dir</span><span class="si">}</span><span class="s">/tokenizer/'</span><span class="p">)</span>


<span class="n">add_markdown_token</span><span class="p">(</span><span class="n">CFG</span><span class="p">)</span>
<span class="n">CFG</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">resize_token_embeddings</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">))</span>
</code></pre></div></div>
<p>êµ¬ê¸€ë§í•´ë³´ë‹ˆ í•´ê²°í•˜ëŠ” ë°©ë²•ì€ ë‹¤ì–‘í•œ ê²ƒ ê°™ì€ë°, <code class="language-plaintext highlighter-rouge">torch.nn.Embedding</code>ì— ì •ì˜ëœ ì…ì¶œë ¥ ì°¨ì›ì„ ì‹¤ì œ ë°ì´í„° ì°¨ì›ê³¼ ë§ì¶°ì£¼ë©´ ê°„ë‹¨í•˜ê²Œ í•´ê²°ëœë‹¤. í•„ìì²˜ëŸ¼ <code class="language-plaintext highlighter-rouge">special token</code> ì„ ì¶”ê°€í•´ ì‚¬ìš©í•˜ë‹¤ í•´ë‹¹ ì—ëŸ¬ê°€ ë°œìƒí•˜ëŠ” ìƒí™©ì´ë¼ë©´ ìƒˆë¡œìš´ í† í°ì´ ì¶”ê°€ëœ í† í¬ë‚˜ì´ì €ì˜ ê¸¸ì´ë¥¼ ë‹¤ì‹œ ì¸¡ì •í•œ ë’¤ ê°’ì„ <code class="language-plaintext highlighter-rouge">resize_token_embeddings</code> ë©”ì„œë“œì— ì „ë‹¬í•´ <code class="language-plaintext highlighter-rouge">nn.Embedding</code>ì„ ì—…ë°ì´íŠ¸ í•´ì£¼ë©´ ëœë‹¤.</p>]]></content><author><name>qcqced</name><email>qcqced123@gmail.com</email></author><category term="Framework &amp; Library" /><category term="Pytorch" /><category term="Dimension Mismatch" /><category term="nn.Embedding" /><category term="CUDA" /><category term="Error Handling" /><summary type="html"><![CDATA[Pytorch Error: Mis-match between pre-defined dimension and input dimension]]></summary></entry><entry><title type="html">ğŸ² RuntimeError: CUDA error: device-side assert triggered</title><link href="http://localhost:4000/framework-library/mismatch-dimension" rel="alternate" type="text/html" title="ğŸ² RuntimeError: CUDA error: device-side assert triggered" /><published>2023-07-17T00:00:00+09:00</published><updated>2023-07-18T07:00:00+09:00</updated><id>http://localhost:4000/framework-library/dim_mismatch</id><content type="html" xml:base="http://localhost:4000/framework-library/mismatch-dimension"><![CDATA[<h3 id="-ì‚¬ì „ì—-ì •ì˜-ì…ì¶œë ¥-ì°¨ì›--ì‹¤ì œ-ì…ì¶œë ¥-ì°¨ì›"><code class="language-plaintext highlighter-rouge">ğŸ˜µ ì‚¬ì „ì— ì •ì˜ ì…ì¶œë ¥ ì°¨ì› â‰  ì‹¤ì œ ì…ì¶œë ¥ ì°¨ì›</code></h3>

<p>ë‹¤ì–‘í•œ ì›ì¸ì´ ìˆë‹¤ê³  ì•Œë ¤ì ¸ ìˆëŠ” ì—ëŸ¬ì§€ë§Œ, í•„ìì˜ ê²½ìš° ìœ„ ì—ëŸ¬ëŠ” ì‚¬ì „ì— ì •ì˜í•œ ë°ì´í„°ì˜ ì…ì¶œë ¥ ì°¨ì›ê³¼ ì‹¤ì œ ì…ì¶œë ¥ ë°ì´í„° ì°¨ì›ì´ ì„œë¡œ ìƒì´í•  ë•Œ ë°œìƒí–ˆë‹¤. í•˜ì§€ë§Œ ì›ì¸ì„ í™•ì‹¤íˆ íŠ¹ì •í•˜ê³  ì‹¶ë‹¤ë©´ ì•„ë˜ ì˜ˆì‹œ ì½”ë“œë¥¼ ë¨¼ì € ì¶”ê°€í•œ ë’¤, ë‹¤ì‹œ í•œ ë²ˆ ì—ëŸ¬ ë¡œê·¸ë¥¼ í™•ì¸í•´ë³´ê¸¸ ê¶Œì¥í•œë‹¤.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">os</span>
<span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="s">'CUDA_LAUNCH_BLOCKING'</span><span class="p">]</span> <span class="o">=</span> <span class="s">"1"</span>
<span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="s">"CUDA_VISIBLE_DEVICES"</span><span class="p">]</span> <span class="o">=</span> <span class="s">"0"</span>
</code></pre></div></div>
<p>ì˜ˆì‹œ ì½”ë“œì²˜ëŸ¼ í™˜ê²½ë³€ìˆ˜ë¥¼ ì¶”ê°€í•˜ë©´ ì—ëŸ¬ê°€ ì–´ëŠ ë¶€ë¶„ì—ì„œ ë°œìƒí–ˆëŠ”ì§€ ë¡œê·¸ê°€ ì¢€ ë” êµ¬ì²´ì ìœ¼ë¡œ ë‚˜ì˜¨ë‹¤. ê±°ì˜ ëŒ€ë¶€ë¶„ì´ ì…ì¶œë ¥ ì°¨ì› ë¬¸ì œì¼í…Œë‹ˆ ê·€ì°®ìœ¼ë©´ ë°”ë¡œ ì°¨ì›ì„ ìˆ˜ì •í•˜ë„ë¡ í•˜ì.</p>]]></content><author><name>qcqced</name><email>qcqced123@gmail.com</email></author><category term="Framework &amp; Library" /><category term="Pytorch" /><category term="Dimension Mismatch" /><category term="CUDA" /><category term="Error Handling" /><summary type="html"><![CDATA[Pytorch Error: Mis-match between pre-defined dimension and input dimension]]></summary></entry><entry><title type="html">ğŸ–¥ï¸ RuntimeError: Attempting to deserialize object on CUDA device 2 but torch.cuda.device_count() is 1. Please use torch.load with map_location to map your storages to an existing device</title><link href="http://localhost:4000/framework-library/cuda-num/" rel="alternate" type="text/html" title="ğŸ–¥ï¸ RuntimeError: Attempting to deserialize object on CUDA device 2 but torch.cuda.device_count() is 1. Please use torch.load with map_location to map your storages to an existing device" /><published>2023-07-11T00:00:00+09:00</published><updated>2023-07-11T13:00:00+09:00</updated><id>http://localhost:4000/framework-library/cuda-device-num</id><content type="html" xml:base="http://localhost:4000/framework-library/cuda-num/"><![CDATA[<h3 id="-pytorch-ì˜ëª»ëœ-cuda-ì¥ì¹˜-ë²ˆí˜¸-ì‚¬ìš©-ë¬¸ì œ"><code class="language-plaintext highlighter-rouge">ğŸ”¢ Pytorch ì˜ëª»ëœ CUDA ì¥ì¹˜ ë²ˆí˜¸ ì‚¬ìš© ë¬¸ì œ</code></h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span><span class="p">.</span><span class="n">load_state_dict</span><span class="p">(</span>
    <span class="n">torch</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">map_location</span><span class="o">=</span><span class="s">'cuda:0'</span><span class="p">)</span> 
<span class="p">)</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">pretrained model</code>, <code class="language-plaintext highlighter-rouge">weight</code>ë¥¼ <code class="language-plaintext highlighter-rouge">load</code>í•˜ê±°ë‚˜ í˜¹ì€ í›ˆë ¨ ë£¨í”„ë¥¼ <code class="language-plaintext highlighter-rouge">resume</code> ì„ ìœ„í•´ <code class="language-plaintext highlighter-rouge">torch.load()</code> ë¥¼ ì‚¬ìš©í•  ë•Œ ë§ˆì£¼í•  ìˆ˜ ìˆëŠ” ì—ëŸ¬ ë¡œê·¸ë‹¤. ë°œìƒí•˜ëŠ” ì´ìœ ëŠ” í˜„ì¬ <code class="language-plaintext highlighter-rouge">GPU</code> ì— í• ë‹¹í•˜ë ¤ëŠ” ëª¨ë¸ì´ ì‚¬ì „ í›ˆë ¨ë•Œ í• ë‹¹ ë˜ì—ˆë˜ <code class="language-plaintext highlighter-rouge">GPU</code> ë²ˆí˜¸ì™€ í˜„ì¬ í• ë‹¹í•˜ë ¤ëŠ” <code class="language-plaintext highlighter-rouge">GPU</code> ë²ˆí˜¸ê°€ ì„œë¡œ ìƒì´í•˜ê¸° ë•Œë¬¸ì´ë‹¤. ë”°ë¼ì„œ <code class="language-plaintext highlighter-rouge">torch.load</code>ì˜ <code class="language-plaintext highlighter-rouge">map_location</code>ì¸ìì— í˜„ì¬ ìì‹ ì´ ì‚¬ìš©í•˜ë ¤ëŠ” <code class="language-plaintext highlighter-rouge">GPU</code> ë²ˆí˜¸ë¥¼ ì…ë ¥í•´ì£¼ì.</p>]]></content><author><name>qcqced</name><email>qcqced123@gmail.com</email></author><category term="Framework &amp; Library" /><category term="Pytorch" /><category term="CUDA" /><summary type="html"><![CDATA[Pytorch Error: Wrong CUDA Device Number]]></summary></entry><entry><title type="html">ğŸ¤” RuntimeError: Function â€˜LogSoftmaxBackward0â€™ returned nan values in its 0th output</title><link href="http://localhost:4000/framework-library/backward-nan/" rel="alternate" type="text/html" title="ğŸ¤” RuntimeError: Function â€˜LogSoftmaxBackward0â€™ returned nan values in its 0th output" /><published>2023-07-11T00:00:00+09:00</published><updated>2023-07-11T13:00:00+09:00</updated><id>http://localhost:4000/framework-library/backward-nan</id><content type="html" xml:base="http://localhost:4000/framework-library/backward-nan/"><![CDATA[<h3 id="-pytorch-backward-ê³¼ì •ì—ì„œ-nan-ë°œìƒí•˜ëŠ”-ë¬¸ì œ"><code class="language-plaintext highlighter-rouge">ğŸ”¥ Pytorch Backward ê³¼ì •ì—ì„œ NaN ë°œìƒí•˜ëŠ” ë¬¸ì œ</code></h3>

<p>ì»¤ìŠ¤í…€ìœ¼ë¡œ ëª¨ë¸, ì—¬ëŸ¬ í’€ë§, ë§¤íŠ¸ë¦­, ì†ì‹¤ í•¨ìˆ˜ë“¤ì„ ì •ì˜í•˜ë©´ì„œë¶€í„° ì œì¼ ë§ì´ ë§ˆì£¼í•˜ê²Œ ë˜ëŠ” ì—ëŸ¬ë‹¤. ì§„ì‹¬ìœ¼ë¡œ ìš”ì¦˜ <code class="language-plaintext highlighter-rouge">CUDA OOM</code> ë³´ë‹¤ í›¨ì”¬ ìì£¼ ë³´ëŠ” ê²ƒ ê°™ë‹¤. í•´ë‹¹ ì—ëŸ¬ëŠ” <code class="language-plaintext highlighter-rouge">LogSoftmax</code> ë ˆì´ì–´ì— ì „ë‹¬ëœ ì…ë ¥ê°’ ì¤‘ì—ì„œ <code class="language-plaintext highlighter-rouge">nan</code>, <code class="language-plaintext highlighter-rouge">inf</code> ê°€ í¬í•¨ë˜ì–´ ì—°ì‚°ì„ ì§„í–‰í•  ìˆ˜ ì—†ë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•œë‹¤. ë”¥ëŸ¬ë‹ ì‹¤í—˜ì„ ì§„í–‰í•˜ë©´ì„œ ê°€ì¥ í•´ê²°í•˜ê¸° ê¹Œë‹¤ë¡œìš´ ë…€ì„ìœ¼ë¡œ ì›ì¸ì„ íŠ¹ì •í•˜ê¸° í˜ë“¤ê¸° ë•Œë¬¸ì´ë‹¤. ì›ì¸ì„ ì¡ê¸° ì–´ë ¤ìš´ ì´ìœ ëŠ” ë°”ë¡œ ìš°ë¦¬ê°€ ì§€ê¸ˆ í•˜ê³  ìˆëŠ”ê²Œ <code class="language-plaintext highlighter-rouge">â€˜ë”¥ëŸ¬ë‹â€™</code> ì´ë¼ì„œ ê·¸ë ‡ë‹¤. ìœ„ ì—ëŸ¬ëŠ” ëŒ€ë¶€ë¶„ ì—°ì‚°ìê°€ ìš°ë¦¬ê°€ ì˜ë„í•˜ì§€ ì•Šì€ ë™ì‘ì„ í•˜ëŠ” ì¼€ì´ìŠ¤ ë•Œë¬¸ì¸ë°, í•˜ë‚˜ í•˜ë‚˜ ë””ë²„ê¹…í•˜ê¸°ì—ëŠ” ë„ˆë¬´ë‚˜ë„ ì—°ì‚°ìê°€ ë§ë‹¤. ë˜í•œ ë”¥ëŸ¬ë‹ì€ ì…ì¶œë ¥ìœ¼ë¡œ ì—„ì²­ë‚˜ê²Œ í° ì‚¬ì´ì¦ˆì˜ í–‰ë ¬ì„ ì‚¬ìš©í•œë‹¤. ìš°ë¦¬ê°€ <code class="language-plaintext highlighter-rouge">nan</code>, <code class="language-plaintext highlighter-rouge">inf</code> ê°’ ì¡´ì¬ì— ëŒ€í•´ì„œ ì¸ì§€í•˜ê¸° ì‰½ì§€ ì•Šë‹¤.</p>

<p><strong><u>ìœ„ ì—ëŸ¬ëŠ” í•„ìì˜ ê²½í—˜ìƒ ëŒ€ë¶€ë¶„ ì»¤ìŠ¤í…€ìœ¼ë¡œ ì •ì˜í•œ ë ˆì´ì–´ì—ì„œ ë°œìƒí•˜ëŠ” ê²½ìš°ê°€ ë§ì•˜ìœ¼ë©° íŠ¹íˆ</u></strong> <code class="language-plaintext highlighter-rouge">ë¶„ìˆ˜</code>, <code class="language-plaintext highlighter-rouge">ê°ë„</code>, <code class="language-plaintext highlighter-rouge">ì œê³±ê·¼</code>, <code class="language-plaintext highlighter-rouge">ì§€ìˆ˜</code> <strong><u>ê°œë…ì„ ì‚¬ìš©í•˜ëŠ” ì—°ì‚°ìê°€ ëŒ€ë¶€ë¶„ ì›ì¸ì´ì—ˆë‹¤.</u></strong> ì˜ˆë¥¼ ë“¤ì–´ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ êµ¬í•˜ëŠ” ê³¼ì •ì—ì„œ ì—°ì‚° ëŒ€ìƒ ë²¡í„°ê°’ì—  <code class="language-plaintext highlighter-rouge">zero-value</code> ê°€ í¬í•¨ëœ ê²½ìš° ë¶„ëª¨ê°€ 0ì´ ë˜ê¸° ë•Œë¬¸ì— ì—°ì‚° ì •ì˜ê°€ ë˜ì§€ ì•Šì•„ <code class="language-plaintext highlighter-rouge">nan</code> ì„ ë°˜í™˜í•´ ìœ„ì™€ ê°™ì€ ì—ëŸ¬ê°€ ë°œìƒí•˜ëŠ” ê²½ìš°ê°€ ìˆë‹¤.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">check_nan</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
    <span class="s">""" Check if there is NaN in tensor """</span>
    <span class="n">checker</span> <span class="o">=</span> <span class="bp">False</span>
    <span class="k">if</span> <span class="bp">True</span> <span class="ow">in</span> <span class="n">torch</span><span class="p">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
        <span class="n">checker</span> <span class="o">=</span> <span class="bp">True</span>
    <span class="k">return</span> <span class="n">checker</span>

<span class="k">def</span> <span class="nf">zero_filtering</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="s">"""
    Add eps value for zero embedding, because competition metric is cosine similarity
    Cosine Similarity will be returned NaN, when input value has zero, like as torch.clamp()
    """</span>
    <span class="n">eps</span> <span class="o">=</span> <span class="mf">1e-4</span>
    <span class="n">x</span><span class="p">[</span><span class="n">x</span> <span class="o">&lt;=</span> <span class="n">eps</span><span class="p">]</span> <span class="o">=</span> <span class="n">eps</span>
    <span class="k">return</span> <span class="n">x</span>

<span class="k">def</span> <span class="nf">nan_filtering</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">eps</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-4</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="s">"""
    Change eps value for NaN Embedding, because competition metric is cosine similarity
    Cosine Similarity will be returned NaN
    """</span>
    <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">nan</span><span class="o">=</span><span class="n">eps</span><span class="p">)</span>

<span class="k">class</span> <span class="nc">CLIPGEMPooling</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="s">"""
    Generalized Mean Pooling for Natural Language Processing
    This class version of GEMPooling for CLIP, Transfer from NLP Task Code
    ViT don't use attention mask, because input image shape will be same

    Mean Pooling &lt;= GEMPooling &lt;= Max Pooling
    Because of doing exponent to each token embeddings, GEMPooling is like as weight to more activation token

    In original paper, they use p=3, but in this class, we use p=4 because torch doesn't support pow calculation
    for negative value tensor, only for non-negative value in odd number exponent
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">auto_cfg</span><span class="p">:</span> <span class="n">AutoConfig</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">CLIPGEMPooling</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>

    <span class="o">@</span><span class="nb">staticmethod</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">last_hidden_state</span><span class="p">,</span> <span class="n">p</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="s">"""
        last_hidden_state.size: [batch_size, patches_sequence, hidden_size]
        1) Pow last_hidden_state with p and then take a averaging
        2) pow sum_embeddings with 1/p
        """</span>
        <span class="n">p_embeddings</span> <span class="o">=</span> <span class="n">zero_filtering</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nb">pow</span><span class="p">(</span><span class="n">last_hidden_state</span><span class="p">,</span> <span class="n">p</span><span class="p">))</span>
        <span class="c1"># Check NaN value in Embedding after applying torch.pow
</span>        <span class="k">if</span> <span class="n">check_nan</span><span class="p">(</span><span class="n">p_embeddings</span><span class="p">):</span>
            <span class="n">p_embeddings</span> <span class="o">=</span> <span class="n">nan_filtering</span><span class="p">(</span><span class="n">p_embeddings</span><span class="p">)</span>
        <span class="n">sum_embeddings</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">p_embeddings</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

        <span class="n">gem_embeddings</span> <span class="o">=</span> <span class="n">zero_filtering</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nb">pow</span><span class="p">(</span><span class="n">sum_embeddings</span><span class="p">,</span> <span class="mf">1.</span> <span class="o">/</span> <span class="n">p</span><span class="p">))</span>
        <span class="c1"># Check NaN value in Embedding after applying torch.pow
</span>        <span class="k">if</span> <span class="n">check_nan</span><span class="p">(</span><span class="n">gem_embeddings</span><span class="p">):</span>
            <span class="n">gem_embeddings</span> <span class="o">=</span> <span class="n">nan_filtering</span><span class="p">(</span><span class="n">gem_embeddings</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">gem_embeddings</span>

<span class="k">class</span> <span class="nc">CLIPMultipleNegativeRankingLoss</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="s">"""
    Multiple Negative Ranking Loss for CLIP Model
    main concept is same as original one, but append suitable for other type of model (Not Sentence-Transformers)
    if you set more batch size, you can get more negative pairs for each anchor &amp; positive pair
    Args:
        scale: output of similarity function is multiplied by this value =&gt; I don't know why this is needed
        similarity_fct: standard of distance metrics, default cosine similarity
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">reduction</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">scale</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">20.0</span><span class="p">,</span> <span class="n">similarity_fct</span><span class="o">=</span><span class="n">cos_sim</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">reduction</span> <span class="o">=</span> <span class="n">reduction</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">scale</span> <span class="o">=</span> <span class="n">scale</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">similarity_fct</span> <span class="o">=</span> <span class="n">similarity_fct</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">reduction</span> <span class="o">=</span> <span class="n">reduction</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">cross_entropy_loss</span> <span class="o">=</span> <span class="n">CrossEntropyLoss</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">reduction</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">embeddings_a</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">embeddings_b</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="n">similarity_scores</span> <span class="o">=</span> <span class="n">zero_filtering</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">similarity_fct</span><span class="p">(</span><span class="n">embeddings_a</span><span class="p">,</span> <span class="n">embeddings_b</span><span class="p">))</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">scale</span>
        <span class="k">if</span> <span class="n">check_nan</span><span class="p">(</span><span class="n">similarity_scores</span><span class="p">):</span>
            <span class="s">""" Check NaN Value in similarity_scores """</span>
            <span class="n">similarity_scores</span> <span class="o">=</span> <span class="n">nan_filtering</span><span class="p">(</span><span class="n">similarity_scores</span><span class="p">)</span>

        <span class="n">labels</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">(</span>
            <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">similarity_scores</span><span class="p">)),</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">long</span><span class="p">,</span>
            <span class="n">device</span><span class="o">=</span><span class="n">similarity_scores</span><span class="p">.</span><span class="n">device</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">cross_entropy_loss</span><span class="p">(</span><span class="n">similarity_scores</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
</code></pre></div></div>

<p>í•„ìì˜ ê²½ìš°, ë‘ ê°œì˜ ì…ë ¥ í–‰ë ¬ì— ê°ê°  <code class="language-plaintext highlighter-rouge">sqrt()</code> ë¥¼ ì ìš©í•˜ê³  ë‘ í–‰ë ¬ì˜ ê°œë³„ ì›ì†Œ ì‚¬ì´ì˜ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ êµ¬í•´ì•¼ í–ˆë˜ ì ì´ ìˆë‹¤. <code class="language-plaintext highlighter-rouge">sqrt</code> <strong><u>ê³¼ì •ì—ì„œ ë„ˆë¬´ ì‘ì€ ê°’ë“¤ì´ ì…ë ¥ìœ¼ë¡œ ë“¤ì–´ê°€</u></strong> <code class="language-plaintext highlighter-rouge">underflow</code> <strong><u>ê°€ ë°œìƒí•´ í–‰ë ¬ì—</u></strong> <code class="language-plaintext highlighter-rouge">zero-value</code> <strong><u>ê°€ ìƒê²¼ê³ , ì´ë¥¼ ëª¨ë¥¸ì±„ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ êµ¬í•˜ë‹¤ê°€ í•œì°¸ì„ ìœ„ ì—ëŸ¬ì™€ ì‹¸ì› ë˜ ì ì´ ìˆë‹¤.</u></strong> ì‹¬ì§€ì–´ ì—°ì‚°ì†ë„ í–¥ìƒì„ ìœ„í•´ì„œ <strong><code class="language-plaintext highlighter-rouge">torch.autocast</code></strong> í´ë˜ìŠ¤ì˜ <code class="language-plaintext highlighter-rouge">grad_scaler(float32 to float16)</code> ê¹Œì§€ ì ìš©í•˜ê³  ìˆì—ˆë‹¤.</p>

<h3 id="ï¸-ë‚´ê°€-í•´ê²°í•œ-ë°©ë²•"><code class="language-plaintext highlighter-rouge">ğŸ–ï¸ ë‚´ê°€ í•´ê²°í•œ ë°©ë²•</code></h3>
<p>ì´ ê¸€ì„ ì½ëŠ” ë‹¹ì‹ ì´ ë§Œì•½ <code class="language-plaintext highlighter-rouge">sqrt</code> í˜¹ì€ <code class="language-plaintext highlighter-rouge">pow</code>ë¥¼ í™œìš©í•˜ëŠ” ê²½ìš°, <code class="language-plaintext highlighter-rouge">underflow</code> ë°©ì§€ë¥¼ ìœ„í•´ì„œ <del>ìœ„ ì˜ˆì‹œ ì½”ë“œì²˜ëŸ¼ ê¼­ ì ë‹¹í•œ ì…ì‹¤ë¡  ê°’ì„ ì—°ì‚° ì „í›„ì— í•„ìš”ì— ë”°ë¼ ë”í•´ì¤„ ê²ƒì„ ê¶Œì¥í•œë‹¤.</del> ì…ì‹¤ë¡  ê°’ì˜ ì„¤ì •ì€ í˜„ì¬ ìì‹ ì´ ì‚¬ìš©í•˜ê³  ìˆëŠ” ë¶€ë™ ì†Œìˆ˜ì  ì •í™•ë„ì— ë§ê²Œ ì„¤ì •í•´ì£¼ë©´ ë  ê²ƒ ê°™ë‹¤. <code class="language-plaintext highlighter-rouge">float32</code> ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²½ìš°ì—ëŠ” ëŒ€ë¶€ë¶„ <code class="language-plaintext highlighter-rouge">1e-6</code> ì„ ë§ì´ ì‚¬ìš©í•˜ëŠ” ê²ƒ ê°™ë‹¤. í•„ìë„ ì •í™•íˆ ì–´ë–¤ ê°’ì´ ì ë‹¹í•œì§€ ì•„ì§ ì˜ ëª¨ë¥´ê² ë‹¤â€¦ ê·¸ë¦¬ê³  ë”¥ëŸ¬ë‹ ì‹¤í—˜í•˜ë©´ì„œ <code class="language-plaintext highlighter-rouge">overflow</code> ë•Œë¬¸ì— <code class="language-plaintext highlighter-rouge">inf</code> ì´ ë°œìƒí–ˆë˜ ì ì€ ì—†ì—ˆë‹¤.</p>

<p>ì…ì‹¤ë¡  ê°’ì„ ë¬¸ì œê°€ ë˜ëŠ” ì—°ì‚° ì „ì— ì¼ê´„ì ìœ¼ë¡œ ë”í•  ê²½ìš°, ì•„ë¬´ë¦¬ ì‘ì€ ê°’ì´ë¼ë„ ì—°ì‚° ì¢…ë¥˜ì— ë”°ë¼ì„œ ê²°ê³¼ê°€ í¬ê²Œ ì™œê³¡ë˜ëŠ” ê²½ìš°ê°€ ë°œìƒí•œë‹¤. ë”°ë¼ì„œ ì—°ì‚°ì„ ë¨¼ì € ì ìš©í•œ ë’¤ ê²°ê³¼ì— <code class="language-plaintext highlighter-rouge">NaN</code>, <code class="language-plaintext highlighter-rouge">Inf</code>, <code class="language-plaintext highlighter-rouge">Zero</code>ê°€ ë°œìƒí•˜ëŠ”ì§€ ì²´í¬í•˜ê³ , ë°œìƒí•œ ë¶€ë¶„ì— í•œí•´ì„œ ì…ì‹¤ë¡  ê°’ì„ ë”í•´ì£¼ëŠ” ì»¤ìŠ¤í…€ <code class="language-plaintext highlighter-rouge">function</code>ìš¸ ì •ì˜í•´ ë¬¸ì œë¥¼ í•´ê²°í–ˆë‹¤.<br />
(ìœ„ì˜ ì½”ë“œ ì˜ˆì œ <code class="language-plaintext highlighter-rouge">check_nan</code>, <code class="language-plaintext highlighter-rouge">zero_filtering</code>, <code class="language-plaintext highlighter-rouge">nan_filtering</code>)</p>

<p>í•œí¸ <code class="language-plaintext highlighter-rouge">torch.autograd.set_detect_anomaly(True)</code> ë¥¼ í›ˆë ¨ ë£¨í”„ ì´ˆë°˜ì— ì •ì˜í•´ì£¼ë©´, <code class="language-plaintext highlighter-rouge">NaN</code>ì´ ë°œìƒí•˜ëŠ” ì¦‰ì‹œ ì‹¤í–‰ì´ ë©ˆì¶”ê³  <code class="language-plaintext highlighter-rouge">NaN</code>ì„ ìœ ë°œí•œ ë¼ì¸ì„ ì¶œë ¥í•´ì¤€ë‹¤. ê¼­ í™œìš©í•´ë³´ì.</p>]]></content><author><name>qcqced</name><email>qcqced123@gmail.com</email></author><category term="Framework &amp; Library" /><category term="Pytorch" /><category term="Logsoftmax" /><category term="NaN" /><category term="Error Handling" /><summary type="html"><![CDATA[Pytorch Error: Backward NaN values]]></summary></entry><entry><title type="html">ğŸª¢ assert len(optimizer_state[â€œfound_inf_per_deviceâ€]) &amp;gt; 0, â€œNo inf checks were recorded for this optimizer.â€ AssertionError: No inf checks were recorded for this optimizer.</title><link href="http://localhost:4000/framework-library/inf-per-device" rel="alternate" type="text/html" title="ğŸª¢ assert len(optimizer_state[â€œfound_inf_per_deviceâ€]) &amp;gt; 0, â€œNo inf checks were recorded for this optimizer.â€ AssertionError: No inf checks were recorded for this optimizer." /><published>2023-07-11T00:00:00+09:00</published><updated>2023-07-11T13:00:00+09:00</updated><id>http://localhost:4000/framework-library/found_inf_per_device</id><content type="html" xml:base="http://localhost:4000/framework-library/inf-per-device"><![CDATA[<h3 id="-optimizerê°€-ì†ì‹¤ê°’ì„-ì œëŒ€ë¡œ-backward-í• -ìˆ˜-ì—†ëŠ”-ë¬¸ì œ"><code class="language-plaintext highlighter-rouge">ğŸ¤” Optimizerê°€ ì†ì‹¤ê°’ì„ ì œëŒ€ë¡œ Backward í•  ìˆ˜ ì—†ëŠ” ë¬¸ì œ</code></h3>

<p>í…ì„œì˜ ê³„ì‚° ê·¸ë˜í”„ê°€ ì¤‘ê°„ì— ëŠì–´ì ¸ ì˜µí‹°ë§ˆì´ì €ê°€ ê·¸ë¼ë””ì–¸íŠ¸ë¥¼ ì œëŒ€ë¡œ <code class="language-plaintext highlighter-rouge">Backward</code> í•˜ì§€ ëª»í•´ ë°œìƒí•˜ëŠ” ì—ëŸ¬ë‹¤. ê³µë¶€ë¥¼ ì‹œì‘í•˜ê³  ì •ë§ ì²˜ìŒ ë§ˆì£¼í•˜ëŠ” ì—ëŸ¬ë¼ì„œ ì •ë§ ë§ì´ ë‹¹í™©í–ˆë‹¤. ë˜í¼ëŸ°ìŠ¤ ìë£Œ ì—­ì‹œ ê±°ì˜ ì—†ì–´ì„œ í•´ê²°í•˜ëŠ”ë° ì• ë¥¼ ë¨¹ì—ˆë˜  ì“°ë¼ë¦° ì‚¬ì—°ì´ ìˆëŠ” ì—ëŸ¬ë‹¤. ì´ ê¸€ì„ ì½ëŠ” ë…ìë¼ë©´ ëŒ€ë¶€ë¶„ í…ì„œì˜ ê³„ì‚° ê·¸ë˜í”„ê°€ ì¤‘ê°„ì— ëŠì–´ì§„ë‹¤ëŠ” ê²ƒì´ ë¬´ìŠ¨ ì˜ë¯¸ì¼ì§€ ì´í•´í•˜ì‹œì§€ ëª»í• ê±°ë¼ ìƒê°í•œë‹¤. ê·¸ê²Œ ì •ìƒì´ë‹¤. í•„ì ì—­ì‹œ ì•Œê³  ì‹¶ì§€ ì•Šì•˜ìœ¼ë‚˜ ìš•ì‹¬ë§Œ ë§ê³  ë©ì²­í•œ íƒ“ì—â€¦ ì•Œê²Œ ë˜ì—ˆë‹¤. ì•„ë˜ ì˜ˆì‹œ ì½”ë“œë¥¼ ë¨¼ì € ì‚´í´ë³´ì.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Before Append
</span><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">:</span> <span class="nb">dict</span><span class="p">,</span> <span class="n">position_list</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">feature</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
        <span class="n">feature</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">.</span><span class="n">last_hidden_state</span>
        <span class="n">pred</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">batch_size</span><span class="p">):</span>
            <span class="s">""" Apply Pooling &amp; Fully Connected Layer for each unique cell in batch (one notebook_id) """</span>
            <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">position_list</span><span class="p">[</span><span class="n">i</span><span class="p">])):</span>
                <span class="n">src</span><span class="p">,</span> <span class="n">end</span> <span class="o">=</span> <span class="n">position_list</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">idx</span><span class="p">]</span>
                <span class="n">embedding</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">pooling</span><span class="p">(</span><span class="n">feature</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">src</span><span class="p">:</span><span class="n">end</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="p">:].</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>  <span class="c1"># maybe don't need mask
</span>                <span class="n">logit</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">fc</span><span class="p">(</span><span class="n">embedding</span><span class="p">)</span>
                <span class="n">pred</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">logit</span><span class="p">)</span>  
            <span class="n">pred</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">pred</span>

<span class="c1"># After Append
</span><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">:</span> <span class="nb">dict</span><span class="p">,</span> <span class="n">position_list</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">feature</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
        <span class="n">feature</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">.</span><span class="n">last_hidden_state</span>
        <span class="n">pred</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([],</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">batch_size</span><span class="p">):</span>
            <span class="s">""" Apply Pooling &amp; Fully Connected Layer for each unique cell in batch (one notebook_id) """</span>
            <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">position_list</span><span class="p">[</span><span class="n">i</span><span class="p">])):</span>
                <span class="n">src</span><span class="p">,</span> <span class="n">end</span> <span class="o">=</span> <span class="n">position_list</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">idx</span><span class="p">]</span>
                <span class="n">embedding</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">pooling</span><span class="p">(</span><span class="n">feature</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">src</span><span class="p">:</span><span class="n">end</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="p">:].</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>  <span class="c1"># maybe don't need mask
</span>                <span class="n">logit</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">fc</span><span class="p">(</span><span class="n">embedding</span><span class="p">)</span>
                <span class="n">pred</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cat</span><span class="p">([</span><span class="n">pred</span><span class="p">,</span> <span class="n">logit</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">pred</span>
</code></pre></div></div>

<p>ë‹¤ìŒ ì½”ë“œëŠ” í•„ìê°€ ê³µë¶€ë¥¼ ìœ„í•´ ë§Œë“  ëª¨ë¸ í´ë˜ìŠ¤ì˜ <code class="language-plaintext highlighter-rouge">forward</code> ë©”ì„œë“œì´ë‹¤. ì „ìëŠ” ì´ë²ˆ í¬ìŠ¤íŒ…ì˜ ì£¼ì œì¸ ì—ëŸ¬ë¥¼ ì¼ìœ¼í‚¨ ì£¼ì¸ê³µì´ê³ , í›„ìëŠ” ì—ëŸ¬ë¥¼ ìˆ˜ì •í•œ ì´í›„ ì •ìƒì ìœ¼ë¡œ ì‘ë™í•˜ëŠ” ì½”ë“œë‹¤. ë…ì ì—¬ëŸ¬ë¶„ë“¤ë„ ë‘ ì½”ë“œì— ì–´ë–¤ ì°¨ì´ê°€ ìˆëŠ”ì§€ ìŠ¤ìŠ¤ë¡œ ì§ˆë¬¸ì„ ë˜ì§€ë©´ì„œ ì½ì–´ì£¼ì‹œê¸¸ ë°”ë€ë‹¤.</p>
<p align="center">
<img src="/assets/images/marginrankingloss.png" alt="Model Overview" class="align-center image-caption" width="100%&quot;, height=&quot;50%" />
<em>Modeling Overview</em>
</p>

<p>ìœ„ì˜ ì½”ë“œë“¤ì€ <code class="language-plaintext highlighter-rouge">DeBERTa-V3-Large</code> ì˜ ë§ˆì§€ë§‰ ì¸ì½”ë” ë ˆì´ì–´ê°€ ë°˜í™˜í•˜ëŠ” <code class="language-plaintext highlighter-rouge">last_hidden_state</code> ë¥¼ ë¯¸ë¦¬ ì„¤ì •í•œ ì„œë¸Œ êµ¬ê°„ë³„ë¡œ ë‚˜ëˆ„ê³  ê°œë³„ì ìœ¼ë¡œ <code class="language-plaintext highlighter-rouge">pooling &amp; fully connected layer</code> ì— í†µê³¼ì‹œì¼œ ë¡œì§“ê°’ìœ¼ë¡œ ë³€í™˜í•˜ê¸° ìœ„í•´ ë§Œë“¤ì—ˆë‹¤. ì‰½ê²Œ ë§í•´ ì…ë ¥ìœ¼ë¡œ í† í°(ë‹¨ì–´) 384ê°œ ì§œë¦¬ ë¬¸ì¥ì„ í•˜ë‚˜ ë„£ì—ˆê³ , ëª¨ë¸ì€ 384ê°œì˜ ê°œë³„ í† í°ì— ëŒ€í•œ ì„ë² ë”© ê°’ì„ ë°˜í™˜í–ˆëŠ”ë° ê·¸ê²ƒì„ ì „ë¶€ ì´ìš©í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ ì˜ˆë¥¼ ë“¤ì–´ <code class="language-plaintext highlighter-rouge">2ë²ˆ~4ë²ˆ</code> í† í°ì„ 1ë²ˆ êµ¬ê°„, <code class="language-plaintext highlighter-rouge">6ë²ˆ~20ë²ˆ</code> í† í°ì„ 2ë²ˆ êµ¬ê°„, <code class="language-plaintext highlighter-rouge">30ë²ˆ~50ë²ˆ</code> í† í°ì„ 3ë²ˆ êµ¬ê°„ â€¦ <code class="language-plaintext highlighter-rouge">370ë²ˆ~380ë²ˆ</code> í† í°ì„ 30ë²ˆ êµ¬ê°„ìœ¼ë¡œ ì„¤ì •í•˜ê³  êµ¬ê°„ ë³„ë¡œ ë”°ë¡œ <code class="language-plaintext highlighter-rouge">pooling &amp; fully connected layer</code> ì— í†µê³¼ì‹œì¼œ ë¡œì§“ì„ êµ¬í•˜ëŠ” ê²ƒì´ë‹¤. ì¼ë°˜ì ì´ë¼ë©´ 1ê°œì˜ ë¬¸ì¥ì—ì„œ 1ê°œì˜ ìµœì¢… ë¡œì§“ê°’ì´ ë„ì¶œë˜ëŠ” ê²ƒì´ë¼ë©´, ìœ„ ì½”ë“œëŠ” 30ê°œì˜ ë¡œì§“ê°’ì´ ë„ì¶œëœë‹¤.</p>

<h3 id="ï¸-ë‚´ê°€-í•´ê²°í•œ-ë°©ë²•"><code class="language-plaintext highlighter-rouge">ğŸ–ï¸ ë‚´ê°€ í•´ê²°í•œ ë°©ë²•</code></h3>

<p>ì½”ë“œ ì´í•´ë¥¼ ìœ„í•œ ì„¤ëª…ì€ ë§ˆì³¤ìœ¼ë‹ˆ ë³¸ê²©ì ìœ¼ë¡œ ë³¸ ì—ëŸ¬ì™€ ì–´ë–¤ ì—°ê´€ì´ ìˆëŠ”ì§€ ì‚´í´ë³´ì. <code class="language-plaintext highlighter-rouge">Before</code> ì½”ë“œëŠ” <code class="language-plaintext highlighter-rouge">pred</code> ë¼ëŠ” ë¦¬ìŠ¤íŠ¸ì— ê°œë³„ êµ¬ê°„ì— ëŒ€í•œ ë¡œì§“ê°’ì„ <code class="language-plaintext highlighter-rouge">append</code> í•˜ê³  ë§ˆì§€ë§‰ì— <code class="language-plaintext highlighter-rouge">torch.as_tensor</code>ë¥¼ í™œìš©í•´ í…ì„œë¡œ ë³€í™˜í•˜ê³  ìˆë‹¤. í•œí¸ í›„ìëŠ” <code class="language-plaintext highlighter-rouge">pred</code> ë¥¼ ê¹¡í†µ í…ì„œë¡œ ì„ ì–¸í•œ ë’¤, <code class="language-plaintext highlighter-rouge">torch.cat</code>ìœ¼ë¡œ ëª¨ë“  êµ¬ê°„ì— ëŒ€í•œ ë¡œì§“ê°’ì„ í•˜ë‚˜ì˜ í…ì„œ êµ¬ì¡°ì²´ì— ë‹´ê³  ìˆë‹¤.</p>

<p>ì–¼í•ë³´ë©´ í¬ê²Œ ë‹¤ë¥¸ì ì´ ì—†ì–´ ë³´ì¸ë‹¤. í•˜ì§€ë§Œ ì „ìëŠ” í…ì„œ êµ¬ì¡°ì²´ë¥¼ ìƒˆë¡œ ì •ì˜ í•˜ë©´ì„œ <code class="language-plaintext highlighter-rouge">torch.Tensor[[logit1], [logit2], â€¦.]</code> í˜•íƒœë¥¼ ê°–ê³  í›„ìëŠ” <code class="language-plaintext highlighter-rouge">torch.Tensor[logit1, logit2, â€¦]</code> í˜•íƒœë¥¼ ê°–ëŠ”ë‹¤. ì„œë¡œ ë‹¤ë¥¸ í…ì„œ êµ¬ì¡°ì²´ë¥¼ ê·¸ëŒ€ë¡œ ëª¨ë¸ ê°ì²´ì˜ <code class="language-plaintext highlighter-rouge">forward</code> ë©”ì„œë“œ ë° <code class="language-plaintext highlighter-rouge">loss function</code>ì— í†µê³¼ì‹œí‚¤ê³  ì˜¤ì°¨ ì—­ì „ì„ í•˜ë©´ ì–´ë–¤ ì¼ì´ ìƒê¸°ëŠ”ì§€ ì§€ê¸ˆë¶€í„° ì•Œì•„ë³´ì.</p>

<p>ì „ìì˜ ê²½ìš°ëŠ” ë„ì¶œëœ ì†ì‹¤í•¨ìˆ˜ì˜ ë¯¸ë¶„ê°’ì´ ì •ì˜ëœ ê³„ì‚° ê·¸ë˜í”„ë¥¼ íƒ€ê³  ì—­ì „ë  ìˆ˜ ì—†ë‹¤. ì´ìœ ëŠ” ì „ìì˜ <code class="language-plaintext highlighter-rouge">pred</code> ê°€ forward ë©”ì„œë“œ ë‚´ë¶€ì—ì„œ ìƒˆë¡œì´ ì •ì˜ ë˜ì—ˆê¸° ë•Œë¬¸ì´ë‹¤. í›„ì ì—­ì‹œ ë§ˆì°¬ê°€ì§€ ì•„ë‹Œê°€ ì‹¶ì„ ê²ƒì´ë‹¤. í›„ìì˜ <code class="language-plaintext highlighter-rouge">pred</code> ì—­ì‹œ <code class="language-plaintext highlighter-rouge">forward</code> ë©”ì„œë“œ ë‚´ë¶€ì—ì„œ ì •ì˜ëœ ê²ƒì€ ë§ì§€ë§Œ <code class="language-plaintext highlighter-rouge">torch.cat</code>ì„ ì‚¬ìš©í•˜ë©´ì„œ êµ¬ê°„ì˜ ë¡œì§“ê°’ë“¤ ìœ„ì— ìƒˆë¡œì´ ì°¨ì›ì„ ë®ì–´ì“°ëŠ”ê²ƒì´ ì•„ë‹ˆê²Œ ëœë‹¤. ì´ê²ƒì´ ë§¤ìš° ì¤‘ìš”í•œ ì°¨ì´ê°€ ë˜ëŠ”ë°, í›„ìì™€ ê°™ì€ í˜•íƒœê°€ ë˜ëŠ” ê²½ìš°, ì†ì‹¤ê°’ìœ¼ë¡œ ë¶€í„° <code class="language-plaintext highlighter-rouge">Backward</code> ë˜ëŠ” ë¯¸ë¶„ê°’ë“¤ì´ ê³§ë°”ë¡œ <code class="language-plaintext highlighter-rouge">forward</code> ê³¼ì •ì—ì„œ ê¸°ë¡ëœ ìì‹ ì˜ ê³„ì‚° ê·¸ë˜í”„ë¡œ ì°¾ì•„ ê°ˆ ìˆ˜ ìˆë‹¤. í•œí¸ ì „ìì˜ ê²½ìš° ìƒˆë¡­ê²Œ ë®ì–´ ì“°ì—¬ì§„ ì°¨ì› ë•Œë¬¸ì— ë¯¸ë¶„ê°’ë“¤ì´ ìì‹ ì˜ ê³„ì‚° ê·¸ë˜í”„ë¡œ ì°¾ì•„ê°ˆ ìˆ˜ ì—†ê²Œ ëœë‹¤. ë”°ë¼ì„œ ì˜µí‹°ë§ˆì´ì €ê°€ ë” ì´ìƒ <code class="language-plaintext highlighter-rouge">Backward</code> ë¥¼ ìˆ˜í–‰í•  ìˆ˜ ì—†ì–´ ì œëª©ê³¼ ê°™ì€ ì—ëŸ¬ë¥¼ ë°˜í™˜í•˜ê²Œ ë˜ëŠ” ê²ƒì´ë‹¤.</p>

<p>ì²˜ìŒ ì´ ì—ëŸ¬ë¥¼ ë§ˆì£¼í–ˆì„ ë•ŒëŠ”  <code class="language-plaintext highlighter-rouge">found_inf_per_device</code>, <code class="language-plaintext highlighter-rouge">No inf checks</code> ë¼ëŠ” í‚¤ì›Œë“œì— ê½‚í˜€ (íŠ¹íˆ <code class="language-plaintext highlighter-rouge">inf</code>)  <code class="language-plaintext highlighter-rouge">&lt;RuntimeError: Function 'LogSoftmaxBackward0' returned nan values in its 0th output&gt;</code> ì´ê²ƒê³¼ ìœ ì‚¬í•œ ì¢…ë¥˜ì˜ ì—ëŸ¬ë¼ ìƒê°í•˜ê³  ì—´ì‹¬íˆ ì—°ì‚° ê³¼ì •ì— ë¬¸ì œê°€ ì—†ëŠ”ì§€, ì–´ë””ì„œ NaNì´ ë°œìƒí•˜ëŠ”ì§€, í•™ìŠµë¥ ì„ ë„ˆë¬´ í¬ê²Œ ì„¤ì •í–ˆëŠ”ì§€ ë“±ì„ ê²€í† í•˜ë©° í•˜ë£¨ë¥¼ ë‚ ë ¸ì—ˆë˜ ê¸°ì–µì´ ìˆë‹¤.</p>]]></content><author><name>qcqced</name><email>qcqced123@gmail.com</email></author><category term="Framework &amp; Library" /><category term="Pytorch" /><category term="CUDA" /><category term="Error Handling" /><summary type="html"><![CDATA[Pytorch Error: Optimizer can't backward loss]]></summary></entry></feed>